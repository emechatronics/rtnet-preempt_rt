diff -Nur linux-5.4.5/arch/alpha/include/asm/spinlock_types.h linux-5.4.5-new/arch/alpha/include/asm/spinlock_types.h
--- linux-5.4.5/arch/alpha/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/alpha/include/asm/spinlock_types.h	2020-06-15 16:12:05.075788326 +0300
@@ -2,10 +2,6 @@
 #ifndef _ALPHA_SPINLOCK_TYPES_H
 #define _ALPHA_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Nur linux-5.4.5/arch/arc/kernel/entry.S linux-5.4.5-new/arch/arc/kernel/entry.S
--- linux-5.4.5/arch/arc/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arc/kernel/entry.S	2020-06-15 16:12:05.155788045 +0300
@@ -337,11 +337,11 @@
 resume_kernel_mode:
 
 	; Disable Interrupts from this point on
-	; CONFIG_PREEMPT: This is a must for preempt_schedule_irq()
-	; !CONFIG_PREEMPT: To ensure restore_regs is intr safe
+	; CONFIG_PREEMPTION: This is a must for preempt_schedule_irq()
+	; !CONFIG_PREEMPTION: To ensure restore_regs is intr safe
 	IRQ_DISABLE	r9
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 
 	; Can't preempt if preemption disabled
 	GET_CURR_THR_INFO_FROM_SP   r10
diff -Nur linux-5.4.5/arch/arm/include/asm/irq.h linux-5.4.5-new/arch/arm/include/asm/irq.h
--- linux-5.4.5/arch/arm/include/asm/irq.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/include/asm/irq.h	2020-06-15 16:12:05.675786224 +0300
@@ -23,6 +23,8 @@
 #endif
 
 #ifndef __ASSEMBLY__
+#include <linux/cpumask.h>
+
 struct irqaction;
 struct pt_regs;
 
diff -Nur linux-5.4.5/arch/arm/include/asm/spinlock_types.h linux-5.4.5-new/arch/arm/include/asm/spinlock_types.h
--- linux-5.4.5/arch/arm/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/include/asm/spinlock_types.h	2020-06-15 16:12:05.679786210 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #define TICKET_SHIFT	16
 
 typedef struct {
diff -Nur linux-5.4.5/arch/arm/include/asm/switch_to.h linux-5.4.5-new/arch/arm/include/asm/switch_to.h
--- linux-5.4.5/arch/arm/include/asm/switch_to.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/include/asm/switch_to.h	2020-06-15 16:12:05.679786210 +0300
@@ -4,13 +4,20 @@
 
 #include <linux/thread_info.h>
 
+#if defined CONFIG_PREEMPT_RT && defined CONFIG_HIGHMEM
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p);
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 /*
  * For v7 SMP cores running a preemptible kernel we may be pre-empted
  * during a TLB maintenance operation, so execute an inner-shareable dsb
  * to ensure that the maintenance completes in case we migrate to another
  * CPU.
  */
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_SMP) && defined(CONFIG_CPU_V7)
+#if defined(CONFIG_PREEMPTION) && defined(CONFIG_SMP) && defined(CONFIG_CPU_V7)
 #define __complete_pending_tlbi()	dsb(ish)
 #else
 #define __complete_pending_tlbi()
@@ -26,6 +33,7 @@
 #define switch_to(prev,next,last)					\
 do {									\
 	__complete_pending_tlbi();					\
+	switch_kmaps(prev, next);					\
 	last = __switch_to(prev,task_thread_info(prev), task_thread_info(next));	\
 } while (0)
 
diff -Nur linux-5.4.5/arch/arm/include/asm/thread_info.h linux-5.4.5-new/arch/arm/include/asm/thread_info.h
--- linux-5.4.5/arch/arm/include/asm/thread_info.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/include/asm/thread_info.h	2020-06-15 16:12:05.679786210 +0300
@@ -46,6 +46,7 @@
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			preempt_lazy_count; /* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
 	__u32			cpu;		/* cpu */
@@ -139,7 +140,8 @@
 #define TIF_SYSCALL_TRACE	4	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	5	/* syscall auditing active */
 #define TIF_SYSCALL_TRACEPOINT	6	/* syscall tracepoint instrumentation */
-#define TIF_SECCOMP		7	/* seccomp syscall filtering active */
+#define TIF_SECCOMP		8	/* seccomp syscall filtering active */
+#define TIF_NEED_RESCHED_LAZY	7
 
 #define TIF_NOHZ		12	/* in adaptive nohz mode */
 #define TIF_USING_IWMMXT	17
@@ -149,6 +151,7 @@
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
@@ -164,7 +167,8 @@
  * Change these and you break ASM code in entry-common.S
  */
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
-				 _TIF_NOTIFY_RESUME | _TIF_UPROBE)
+				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
+				 _TIF_NEED_RESCHED_LAZY)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_ARM_THREAD_INFO_H */
diff -Nur linux-5.4.5/arch/arm/Kconfig linux-5.4.5-new/arch/arm/Kconfig
--- linux-5.4.5/arch/arm/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/Kconfig	2020-06-15 16:12:06.075784822 +0300
@@ -32,6 +32,7 @@
 	select ARCH_OPTIONAL_KERNEL_RWX if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_OPTIONAL_KERNEL_RWX_DEFAULT if CPU_V7
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT if MMU
@@ -64,7 +65,7 @@
 	select HARDIRQS_SW_RESEND
 	select HAVE_ARCH_AUDITSYSCALL if AEABI && !OABI_COMPAT
 	select HAVE_ARCH_BITREVERSE if (CPU_32v7M || CPU_32v7) && !CPU_32v6
-	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU
+	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU && !PREEMPT_RT
 	select HAVE_ARCH_KGDB if !CPU_ENDIAN_BE32 && MMU
 	select HAVE_ARCH_MMAP_RND_BITS if MMU
 	select HAVE_ARCH_SECCOMP_FILTER if AEABI && !OABI_COMPAT
@@ -102,6 +103,7 @@
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_RCU_TABLE_FREE if SMP && ARM_LPAE
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RSEQ
diff -Nur linux-5.4.5/arch/arm/kernel/asm-offsets.c linux-5.4.5-new/arch/arm/kernel/asm-offsets.c
--- linux-5.4.5/arch/arm/kernel/asm-offsets.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/asm-offsets.c	2020-06-15 16:12:05.703786126 +0300
@@ -53,6 +53,7 @@
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
+  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
   DEFINE(TI_CPU,		offsetof(struct thread_info, cpu));
diff -Nur linux-5.4.5/arch/arm/kernel/entry-armv.S linux-5.4.5-new/arch/arm/kernel/entry-armv.S
--- linux-5.4.5/arch/arm/kernel/entry-armv.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/entry-armv.S	2020-06-15 16:12:05.707786111 +0300
@@ -211,13 +211,20 @@
 	svc_entry
 	irq_handler
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
-	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
 	teq	r8, #0				@ if preempt count != 0
+	bne	1f				@ return from exeption
+	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+	tst	r0, #_TIF_NEED_RESCHED		@ if NEED_RESCHED is set
+	blne	svc_preempt			@ preempt!
+
+	ldr	r8, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r8, #0				@ if preempt lazy count != 0
 	movne	r0, #0				@ force flags to 0
-	tst	r0, #_TIF_NEED_RESCHED
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	blne	svc_preempt
+1:
 #endif
 
 	svc_exit r5, irq = 1			@ return from exception
@@ -226,14 +233,20 @@
 
 	.ltorg
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 svc_preempt:
 	mov	r8, lr
 1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
 	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
 	tst	r0, #_TIF_NEED_RESCHED
+	bne	1b
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	reteq	r8				@ go again
-	b	1b
+	ldr	r0, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r0, #0				@ if preempt lazy count != 0
+	beq	1b
+	ret	r8				@ go again
+
 #endif
 
 __und_fault:
diff -Nur linux-5.4.5/arch/arm/kernel/entry-common.S linux-5.4.5-new/arch/arm/kernel/entry-common.S
--- linux-5.4.5/arch/arm/kernel/entry-common.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/entry-common.S	2020-06-15 16:12:05.707786111 +0300
@@ -53,7 +53,9 @@
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	fast_work_pending
+	tst	r1, #_TIF_SECCOMP
 	bne	fast_work_pending
 
 
@@ -90,8 +92,11 @@
 	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
-	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
+	tst	r1, #((_TIF_SYSCALL_WORK | _TIF_WORK_MASK) & ~_TIF_SECCOMP)
+	bne	do_slower_path
+	tst	r1, #_TIF_SECCOMP
 	beq	no_work_pending
+do_slower_path:
  UNWIND(.fnend		)
 ENDPROC(ret_fast_syscall)
 
diff -Nur linux-5.4.5/arch/arm/kernel/signal.c linux-5.4.5-new/arch/arm/kernel/signal.c
--- linux-5.4.5/arch/arm/kernel/signal.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/signal.c	2020-06-15 16:12:05.707786111 +0300
@@ -649,7 +649,8 @@
 	 */
 	trace_hardirqs_off();
 	do {
-		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
+		if (likely(thread_flags & (_TIF_NEED_RESCHED |
+					   _TIF_NEED_RESCHED_LAZY))) {
 			schedule();
 		} else {
 			if (unlikely(!user_mode(regs)))
diff -Nur linux-5.4.5/arch/arm/kernel/smp.c linux-5.4.5-new/arch/arm/kernel/smp.c
--- linux-5.4.5/arch/arm/kernel/smp.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/smp.c	2020-06-15 16:12:05.707786111 +0300
@@ -678,11 +678,9 @@
 		break;
 
 	case IPI_CPU_BACKTRACE:
-		printk_nmi_enter();
 		irq_enter();
 		nmi_cpu_backtrace(regs);
 		irq_exit();
-		printk_nmi_exit();
 		break;
 
 	default:
diff -Nur linux-5.4.5/arch/arm/kernel/traps.c linux-5.4.5-new/arch/arm/kernel/traps.c
--- linux-5.4.5/arch/arm/kernel/traps.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/kernel/traps.c	2020-06-15 16:12:05.707786111 +0300
@@ -248,6 +248,8 @@
 
 #ifdef CONFIG_PREEMPT
 #define S_PREEMPT " PREEMPT"
+#elif defined(CONFIG_PREEMPT_RT)
+#define S_PREEMPT " PREEMPT_RT"
 #else
 #define S_PREEMPT ""
 #endif
diff -Nur linux-5.4.5/arch/arm/mm/cache-v7m.S linux-5.4.5-new/arch/arm/mm/cache-v7m.S
--- linux-5.4.5/arch/arm/mm/cache-v7m.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/mm/cache-v7m.S	2020-06-15 16:12:06.103784724 +0300
@@ -183,13 +183,13 @@
 	and	r1, r1, #7			@ mask of the bits for current cache only
 	cmp	r1, #2				@ see what cache we have at this level
 	blt	skip				@ skip if no cache, or just i-cache
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	save_and_disable_irqs_notrace r9	@ make cssr&csidr read atomic
 #endif
 	write_csselr r10, r1			@ set current cache level
 	isb					@ isb to sych the new cssr&csidr
 	read_ccsidr r1				@ read the new csidr
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	restore_irqs_notrace r9
 #endif
 	and	r2, r1, #7			@ extract the length of the cache lines
diff -Nur linux-5.4.5/arch/arm/mm/cache-v7.S linux-5.4.5-new/arch/arm/mm/cache-v7.S
--- linux-5.4.5/arch/arm/mm/cache-v7.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/mm/cache-v7.S	2020-06-15 16:12:06.103784724 +0300
@@ -135,13 +135,13 @@
 	and	r1, r1, #7			@ mask of the bits for current cache only
 	cmp	r1, #2				@ see what cache we have at this level
 	blt	skip				@ skip if no cache, or just i-cache
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	save_and_disable_irqs_notrace r9	@ make cssr&csidr read atomic
 #endif
 	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
 	isb					@ isb to sych the new cssr&csidr
 	mrc	p15, 1, r1, c0, c0, 0		@ read the new csidr
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	restore_irqs_notrace r9
 #endif
 	and	r2, r1, #7			@ extract the length of the cache lines
diff -Nur linux-5.4.5/arch/arm/mm/fault.c linux-5.4.5-new/arch/arm/mm/fault.c
--- linux-5.4.5/arch/arm/mm/fault.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/mm/fault.c	2020-06-15 16:12:06.103784724 +0300
@@ -414,6 +414,9 @@
 	if (addr < TASK_SIZE)
 		return do_page_fault(addr, fsr, regs);
 
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	if (user_mode(regs))
 		goto bad_area;
 
@@ -481,6 +484,9 @@
 static int
 do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	do_bad_area(addr, fsr, regs);
 	return 0;
 }
diff -Nur linux-5.4.5/arch/arm/mm/highmem.c linux-5.4.5-new/arch/arm/mm/highmem.c
--- linux-5.4.5/arch/arm/mm/highmem.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm/mm/highmem.c	2020-06-15 16:12:06.103784724 +0300
@@ -31,6 +31,11 @@
 	return *ptep;
 }
 
+static unsigned int fixmap_idx(int type)
+{
+	return FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+}
+
 void *kmap(struct page *page)
 {
 	might_sleep();
@@ -51,12 +56,13 @@
 
 void *kmap_atomic(struct page *page)
 {
+	pte_t pte = mk_pte(page, kmap_prot);
 	unsigned int idx;
 	unsigned long vaddr;
 	void *kmap;
 	int type;
 
-	preempt_disable();
+	preempt_disable_nort();
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
@@ -76,7 +82,7 @@
 
 	type = kmap_atomic_idx_push();
 
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = fixmap_idx(type);
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	/*
@@ -90,7 +96,10 @@
 	 * in place, so the contained TLB flush ensures the TLB is updated
 	 * with the new mapping.
 	 */
-	set_fixmap_pte(idx, mk_pte(page, kmap_prot));
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
@@ -103,44 +112,75 @@
 
 	if (kvaddr >= (void *)FIXADDR_START) {
 		type = kmap_atomic_idx();
-		idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+		idx = fixmap_idx(type);
 
 		if (cache_is_vivt())
 			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 #ifdef CONFIG_DEBUG_HIGHMEM
 		BUG_ON(vaddr != __fix_to_virt(idx));
-		set_fixmap_pte(idx, __pte(0));
 #else
 		(void) idx;  /* to kill a warning */
 #endif
+		set_fixmap_pte(idx, __pte(0));
 		kmap_atomic_idx_pop();
 	} else if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
 		/* this address was obtained through kmap_high_get() */
 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
 	}
 	pagefault_enable();
-	preempt_enable();
+	preempt_enable_nort();
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
 void *kmap_atomic_pfn(unsigned long pfn)
 {
+	pte_t pte = pfn_pte(pfn, kmap_prot);
 	unsigned long vaddr;
 	int idx, type;
 	struct page *page = pfn_to_page(pfn);
 
-	preempt_disable();
+	preempt_disable_nort();
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
 
 	type = kmap_atomic_idx_push();
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = fixmap_idx(type);
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
 #endif
-	set_fixmap_pte(idx, pfn_pte(pfn, kmap_prot));
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
+#if defined CONFIG_PREEMPT_RT
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	int i;
+
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = fixmap_idx(i);
+
+		set_fixmap_pte(idx, __pte(0));
+	}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = fixmap_idx(i);
+
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_fixmap_pte(idx, next_p->kmap_pte[i]);
+	}
+}
+#endif
diff -Nur linux-5.4.5/arch/arm64/crypto/sha256-glue.c linux-5.4.5-new/arch/arm64/crypto/sha256-glue.c
--- linux-5.4.5/arch/arm64/crypto/sha256-glue.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/crypto/sha256-glue.c	2020-06-15 16:12:06.315783981 +0300
@@ -97,7 +97,7 @@
 		 * input when running on a preemptible kernel, but process the
 		 * data block by block instead.
 		 */
-		if (IS_ENABLED(CONFIG_PREEMPT) &&
+		if (IS_ENABLED(CONFIG_PREEMPTION) &&
 		    chunk + sctx->count % SHA256_BLOCK_SIZE > SHA256_BLOCK_SIZE)
 			chunk = SHA256_BLOCK_SIZE -
 				sctx->count % SHA256_BLOCK_SIZE;
diff -Nur linux-5.4.5/arch/arm64/include/asm/assembler.h linux-5.4.5-new/arch/arm64/include/asm/assembler.h
--- linux-5.4.5/arch/arm64/include/asm/assembler.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/include/asm/assembler.h	2020-06-15 16:12:06.355783841 +0300
@@ -699,8 +699,8 @@
  * where <label> is optional, and marks the point where execution will resume
  * after a yield has been performed. If omitted, execution resumes right after
  * the endif_yield_neon invocation. Note that the entire sequence, including
- * the provided patchup code, will be omitted from the image if CONFIG_PREEMPT
- * is not defined.
+ * the provided patchup code, will be omitted from the image if
+ * CONFIG_PREEMPTION is not defined.
  *
  * As a convenience, in the case where no patchup code is required, the above
  * sequence may be abbreviated to
@@ -728,7 +728,7 @@
 	.endm
 
 	.macro		if_will_cond_yield_neon
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	get_current_task	x0
 	ldr		x0, [x0, #TSK_TI_PREEMPT]
 	sub		x0, x0, #PREEMPT_DISABLE_OFFSET
diff -Nur linux-5.4.5/arch/arm64/include/asm/kvm_mmu.h linux-5.4.5-new/arch/arm64/include/asm/kvm_mmu.h
--- linux-5.4.5/arch/arm64/include/asm/kvm_mmu.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/include/asm/kvm_mmu.h	2020-06-15 16:12:06.355783841 +0300
@@ -91,6 +91,7 @@
 
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
+void kvm_compute_layout(void);
 
 static inline unsigned long __kern_hyp_va(unsigned long v)
 {
diff -Nur linux-5.4.5/arch/arm64/include/asm/preempt.h linux-5.4.5-new/arch/arm64/include/asm/preempt.h
--- linux-5.4.5/arch/arm64/include/asm/preempt.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/include/asm/preempt.h	2020-06-15 16:12:06.355783841 +0300
@@ -70,20 +70,43 @@
 	 * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
 	 * pair.
 	 */
-	return !pc || !READ_ONCE(ti->preempt_count);
+	if (!pc || !READ_ONCE(ti->preempt_count))
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if ((pc & ~PREEMPT_NEED_RESCHED))
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
 }
 
 static inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
+	if (pc == preempt_offset)
+		return true;
+
+	if ((pc & ~PREEMPT_NEED_RESCHED) != preempt_offset)
+		return false;
+
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	u64 pc = READ_ONCE(current_thread_info()->preempt_count);
 	return pc == preempt_offset;
+#endif
 }
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 void preempt_schedule(void);
 #define __preempt_schedule() preempt_schedule()
 void preempt_schedule_notrace(void);
 #define __preempt_schedule_notrace() preempt_schedule_notrace()
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 #endif /* __ASM_PREEMPT_H */
diff -Nur linux-5.4.5/arch/arm64/include/asm/spinlock_types.h linux-5.4.5-new/arch/arm64/include/asm/spinlock_types.h
--- linux-5.4.5/arch/arm64/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/include/asm/spinlock_types.h	2020-06-15 16:12:06.355783841 +0300
@@ -5,10 +5,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff -Nur linux-5.4.5/arch/arm64/include/asm/thread_info.h linux-5.4.5-new/arch/arm64/include/asm/thread_info.h
--- linux-5.4.5/arch/arm64/include/asm/thread_info.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/include/asm/thread_info.h	2020-06-15 16:12:06.355783841 +0300
@@ -29,6 +29,7 @@
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
+	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
 	union {
 		u64		preempt_count;	/* 0 => preemptible, <0 => bug */
 		struct {
@@ -63,6 +64,7 @@
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
+#define TIF_NEED_RESCHED_LAZY	6
 #define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	9	/* syscall auditing */
@@ -83,6 +85,7 @@
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
@@ -96,8 +99,9 @@
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
-				 _TIF_UPROBE | _TIF_FSCHECK)
+				 _TIF_UPROBE | _TIF_FSCHECK | _TIF_NEED_RESCHED_LAZY)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
 				 _TIF_NOHZ | _TIF_SYSCALL_EMU)
diff -Nur linux-5.4.5/arch/arm64/Kconfig linux-5.4.5-new/arch/arm64/Kconfig
--- linux-5.4.5/arch/arm64/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/Kconfig	2020-06-15 16:12:06.411783644 +0300
@@ -35,32 +35,32 @@
 	select ARCH_HAS_TEARDOWN_DMA_OPS if IOMMU_SUPPORT
 	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
-	select ARCH_INLINE_READ_LOCK if !PREEMPT
-	select ARCH_INLINE_READ_LOCK_BH if !PREEMPT
-	select ARCH_INLINE_READ_LOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_READ_LOCK_IRQSAVE if !PREEMPT
-	select ARCH_INLINE_READ_UNLOCK if !PREEMPT
-	select ARCH_INLINE_READ_UNLOCK_BH if !PREEMPT
-	select ARCH_INLINE_READ_UNLOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_READ_UNLOCK_IRQRESTORE if !PREEMPT
-	select ARCH_INLINE_WRITE_LOCK if !PREEMPT
-	select ARCH_INLINE_WRITE_LOCK_BH if !PREEMPT
-	select ARCH_INLINE_WRITE_LOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_WRITE_LOCK_IRQSAVE if !PREEMPT
-	select ARCH_INLINE_WRITE_UNLOCK if !PREEMPT
-	select ARCH_INLINE_WRITE_UNLOCK_BH if !PREEMPT
-	select ARCH_INLINE_WRITE_UNLOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_WRITE_UNLOCK_IRQRESTORE if !PREEMPT
-	select ARCH_INLINE_SPIN_TRYLOCK if !PREEMPT
-	select ARCH_INLINE_SPIN_TRYLOCK_BH if !PREEMPT
-	select ARCH_INLINE_SPIN_LOCK if !PREEMPT
-	select ARCH_INLINE_SPIN_LOCK_BH if !PREEMPT
-	select ARCH_INLINE_SPIN_LOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_SPIN_LOCK_IRQSAVE if !PREEMPT
-	select ARCH_INLINE_SPIN_UNLOCK if !PREEMPT
-	select ARCH_INLINE_SPIN_UNLOCK_BH if !PREEMPT
-	select ARCH_INLINE_SPIN_UNLOCK_IRQ if !PREEMPT
-	select ARCH_INLINE_SPIN_UNLOCK_IRQRESTORE if !PREEMPT
+	select ARCH_INLINE_READ_LOCK if !PREEMPTION
+	select ARCH_INLINE_READ_LOCK_BH if !PREEMPTION
+	select ARCH_INLINE_READ_LOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_READ_LOCK_IRQSAVE if !PREEMPTION
+	select ARCH_INLINE_READ_UNLOCK if !PREEMPTION
+	select ARCH_INLINE_READ_UNLOCK_BH if !PREEMPTION
+	select ARCH_INLINE_READ_UNLOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_READ_UNLOCK_IRQRESTORE if !PREEMPTION
+	select ARCH_INLINE_WRITE_LOCK if !PREEMPTION
+	select ARCH_INLINE_WRITE_LOCK_BH if !PREEMPTION
+	select ARCH_INLINE_WRITE_LOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_WRITE_LOCK_IRQSAVE if !PREEMPTION
+	select ARCH_INLINE_WRITE_UNLOCK if !PREEMPTION
+	select ARCH_INLINE_WRITE_UNLOCK_BH if !PREEMPTION
+	select ARCH_INLINE_WRITE_UNLOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_WRITE_UNLOCK_IRQRESTORE if !PREEMPTION
+	select ARCH_INLINE_SPIN_TRYLOCK if !PREEMPTION
+	select ARCH_INLINE_SPIN_TRYLOCK_BH if !PREEMPTION
+	select ARCH_INLINE_SPIN_LOCK if !PREEMPTION
+	select ARCH_INLINE_SPIN_LOCK_BH if !PREEMPTION
+	select ARCH_INLINE_SPIN_LOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_SPIN_LOCK_IRQSAVE if !PREEMPTION
+	select ARCH_INLINE_SPIN_UNLOCK if !PREEMPTION
+	select ARCH_INLINE_SPIN_UNLOCK_BH if !PREEMPTION
+	select ARCH_INLINE_SPIN_UNLOCK_IRQ if !PREEMPTION
+	select ARCH_INLINE_SPIN_UNLOCK_IRQRESTORE if !PREEMPTION
 	select ARCH_KEEP_MEMBLOCK
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select ARCH_USE_QUEUED_RWLOCKS
@@ -69,6 +69,7 @@
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_INT128 if GCC_VERSION >= 50000 || CC_IS_CLANG
 	select ARCH_SUPPORTS_NUMA_BALANCING
+	select ARCH_SUPPORTS_RT
 	select ARCH_WANT_COMPAT_IPC_PARSE_VERSION if COMPAT
 	select ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT
 	select ARCH_WANT_FRAME_POINTERS
@@ -158,6 +159,7 @@
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_FUNCTION_ARG_ACCESS_API
 	select HAVE_RCU_TABLE_FREE
diff -Nur linux-5.4.5/arch/arm64/kernel/asm-offsets.c linux-5.4.5-new/arch/arm64/kernel/asm-offsets.c
--- linux-5.4.5/arch/arm64/kernel/asm-offsets.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/asm-offsets.c	2020-06-15 16:12:06.387783728 +0300
@@ -30,6 +30,7 @@
   BLANK();
   DEFINE(TSK_TI_FLAGS,		offsetof(struct task_struct, thread_info.flags));
   DEFINE(TSK_TI_PREEMPT,	offsetof(struct task_struct, thread_info.preempt_count));
+  DEFINE(TSK_TI_PREEMPT_LAZY,	offsetof(struct task_struct, thread_info.preempt_lazy_count));
   DEFINE(TSK_TI_ADDR_LIMIT,	offsetof(struct task_struct, thread_info.addr_limit));
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
   DEFINE(TSK_TI_TTBR0,		offsetof(struct task_struct, thread_info.ttbr0));
diff -Nur linux-5.4.5/arch/arm64/kernel/entry.S linux-5.4.5-new/arch/arm64/kernel/entry.S
--- linux-5.4.5/arch/arm64/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/entry.S	2020-06-15 16:12:06.387783728 +0300
@@ -669,7 +669,7 @@
 
 	irq_handler
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	ldr	x24, [tsk, #TSK_TI_PREEMPT]	// get preempt count
 alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 	/*
@@ -679,9 +679,18 @@
 	mrs	x0, daif
 	orr	x24, x24, x0
 alternative_else_nop_endif
-	cbnz	x24, 1f				// preempt count != 0 || NMI return path
-	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
+
+	cbz	x24, 1f					// (need_resched + count) == 0
+	cbnz	w24, 2f					// count != 0
+
+	ldr	w24, [tsk, #TSK_TI_PREEMPT_LAZY]	// get preempt lazy count
+	cbnz	w24, 2f					// preempt lazy count != 0
+
+	ldr	x0, [tsk, #TSK_TI_FLAGS]		// get flags
+	tbz	x0, #TIF_NEED_RESCHED_LAZY, 2f		// needs rescheduling?
 1:
+	bl	arm64_preempt_schedule_irq		// irq en/disable is done inside
+2:
 #endif
 
 #ifdef CONFIG_ARM64_PSEUDO_NMI
diff -Nur linux-5.4.5/arch/arm64/kernel/fpsimd.c linux-5.4.5-new/arch/arm64/kernel/fpsimd.c
--- linux-5.4.5/arch/arm64/kernel/fpsimd.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/fpsimd.c	2020-06-15 16:12:06.387783728 +0300
@@ -213,6 +213,16 @@
 	__sve_free(task);
 }
 
+static void *sve_free_atomic(struct task_struct *task)
+{
+	void *sve_state = task->thread.sve_state;
+
+	WARN_ON(test_tsk_thread_flag(task, TIF_SVE));
+
+	task->thread.sve_state = NULL;
+	return sve_state;
+}
+
 /*
  * TIF_SVE controls whether a task can use SVE without trapping while
  * in userspace, and also the way a task's FPSIMD/SVE state is stored
@@ -1008,6 +1018,7 @@
 void fpsimd_flush_thread(void)
 {
 	int vl, supported_vl;
+	void *mem = NULL;
 
 	if (!system_supports_fpsimd())
 		return;
@@ -1020,7 +1031,7 @@
 
 	if (system_supports_sve()) {
 		clear_thread_flag(TIF_SVE);
-		sve_free(current);
+		mem = sve_free_atomic(current);
 
 		/*
 		 * Reset the task vector length as required.
@@ -1054,6 +1065,7 @@
 	}
 
 	put_cpu_fpsimd_context();
+	kfree(mem);
 }
 
 /*
diff -Nur linux-5.4.5/arch/arm64/kernel/signal.c linux-5.4.5-new/arch/arm64/kernel/signal.c
--- linux-5.4.5/arch/arm64/kernel/signal.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/signal.c	2020-06-15 16:12:06.387783728 +0300
@@ -910,7 +910,7 @@
 		/* Check valid user FS if needed */
 		addr_limit_user_check();
 
-		if (thread_flags & _TIF_NEED_RESCHED) {
+		if (thread_flags & _TIF_NEED_RESCHED_MASK) {
 			/* Unmask Debug and SError for the next task */
 			local_daif_restore(DAIF_PROCCTX_NOIRQ);
 
diff -Nur linux-5.4.5/arch/arm64/kernel/smp.c linux-5.4.5-new/arch/arm64/kernel/smp.c
--- linux-5.4.5/arch/arm64/kernel/smp.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/smp.c	2020-06-15 16:12:06.387783728 +0300
@@ -31,6 +31,7 @@
 #include <linux/of.h>
 #include <linux/irq_work.h>
 #include <linux/kexec.h>
+#include <linux/kvm_host.h>
 
 #include <asm/alternative.h>
 #include <asm/atomic.h>
@@ -39,6 +40,7 @@
 #include <asm/cputype.h>
 #include <asm/cpu_ops.h>
 #include <asm/daifflags.h>
+#include <asm/kvm_mmu.h>
 #include <asm/mmu_context.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
@@ -408,6 +410,8 @@
 			   "CPU: CPUs started in inconsistent modes");
 	else
 		pr_info("CPU: All CPU(s) started at EL1\n");
+	if (IS_ENABLED(CONFIG_KVM_ARM_HOST))
+		kvm_compute_layout();
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)
diff -Nur linux-5.4.5/arch/arm64/kernel/traps.c linux-5.4.5-new/arch/arm64/kernel/traps.c
--- linux-5.4.5/arch/arm64/kernel/traps.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kernel/traps.c	2020-06-15 16:12:06.387783728 +0300
@@ -143,9 +143,12 @@
 
 #ifdef CONFIG_PREEMPT
 #define S_PREEMPT " PREEMPT"
+#elif defined(CONFIG_PREEMPT_RT)
+#define S_PREEMPT " PREEMPT_RT"
 #else
 #define S_PREEMPT ""
 #endif
+
 #define S_SMP " SMP"
 
 static int __die(const char *str, int err, struct pt_regs *regs)
diff -Nur linux-5.4.5/arch/arm64/kvm/va_layout.c linux-5.4.5-new/arch/arm64/kvm/va_layout.c
--- linux-5.4.5/arch/arm64/kvm/va_layout.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/arm64/kvm/va_layout.c	2020-06-15 16:12:06.395783701 +0300
@@ -22,7 +22,7 @@
 static u64 tag_val;
 static u64 va_mask;
 
-static void compute_layout(void)
+__init void kvm_compute_layout(void)
 {
 	phys_addr_t idmap_addr = __pa_symbol(__hyp_idmap_text_start);
 	u64 hyp_va_msb;
@@ -110,9 +110,6 @@
 
 	BUG_ON(nr_inst != 5);
 
-	if (!has_vhe() && !va_mask)
-		compute_layout();
-
 	for (i = 0; i < nr_inst; i++) {
 		u32 rd, rn, insn, oinsn;
 
@@ -156,9 +153,6 @@
 		return;
 	}
 
-	if (!va_mask)
-		compute_layout();
-
 	/*
 	 * Compute HYP VA by using the same computation as kern_hyp_va()
 	 */
diff -Nur linux-5.4.5/arch/c6x/kernel/entry.S linux-5.4.5-new/arch/c6x/kernel/entry.S
--- linux-5.4.5/arch/c6x/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/c6x/kernel/entry.S	2020-06-15 16:12:06.431783574 +0300
@@ -18,7 +18,7 @@
 #define DP	B14
 #define SP	B15
 
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 #define resume_kernel restore_all
 #endif
 
@@ -287,7 +287,7 @@
 	;; is a little bit different
 	;;
 ENTRY(ret_from_exception)
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	MASK_INT B2
 #endif
 
@@ -557,7 +557,7 @@
 	;;
 	;; Jump to schedule() then return to ret_from_isr
 	;;
-#ifdef	CONFIG_PREEMPT
+#ifdef	CONFIG_PREEMPTION
 resume_kernel:
 	GET_THREAD_INFO A12
 	LDW	.D1T1	*+A12(THREAD_INFO_PREEMPT_COUNT),A1
@@ -582,7 +582,7 @@
 	B	.S2	preempt_schedule_irq
 #endif
 	ADDKPC	.S2	preempt_schedule,B3,4
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 ENTRY(enable_exception)
 	DINT
diff -Nur linux-5.4.5/arch/csky/kernel/entry.S linux-5.4.5-new/arch/csky/kernel/entry.S
--- linux-5.4.5/arch/csky/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/csky/kernel/entry.S	2020-06-15 16:12:06.467783448 +0300
@@ -277,7 +277,7 @@
 	zero_fp
 	psrset	ee
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	mov	r9, sp			/* Get current stack  pointer */
 	bmaski	r10, THREAD_SHIFT
 	andn	r9, r10			/* Get thread_info */
@@ -294,7 +294,7 @@
 	mov	a0, sp
 	jbsr	csky_do_IRQ
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	subi	r12, 1
 	stw	r12, (r9, TINFO_PREEMPT)
 	cmpnei	r12, 0
diff -Nur linux-5.4.5/arch/h8300/kernel/entry.S linux-5.4.5-new/arch/h8300/kernel/entry.S
--- linux-5.4.5/arch/h8300/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/h8300/kernel/entry.S	2020-06-15 16:12:06.487783378 +0300
@@ -284,12 +284,12 @@
 	mov.l	er0,@(LER0:16,sp)
 	bra	resume_userspace
 
-#if !defined(CONFIG_PREEMPT)
+#if !defined(CONFIG_PREEMPTION)
 #define resume_kernel restore_all
 #endif
 
 ret_from_exception:
-#if defined(CONFIG_PREEMPT)
+#if defined(CONFIG_PREEMPTION)
 	orc	#0xc0,ccr
 #endif
 ret_from_interrupt:
@@ -319,7 +319,7 @@
 restore_all:
 	RESTORE_ALL			/* Does RTE */
 
-#if defined(CONFIG_PREEMPT)
+#if defined(CONFIG_PREEMPTION)
 resume_kernel:
 	mov.l	@(TI_PRE_COUNT:16,er4),er0
 	bne	restore_all:8
diff -Nur linux-5.4.5/arch/hexagon/include/asm/spinlock_types.h linux-5.4.5-new/arch/hexagon/include/asm/spinlock_types.h
--- linux-5.4.5/arch/hexagon/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/hexagon/include/asm/spinlock_types.h	2020-06-15 16:12:06.503783322 +0300
@@ -8,10 +8,6 @@
 #ifndef _ASM_SPINLOCK_TYPES_H
 #define _ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Nur linux-5.4.5/arch/hexagon/kernel/vm_entry.S linux-5.4.5-new/arch/hexagon/kernel/vm_entry.S
--- linux-5.4.5/arch/hexagon/kernel/vm_entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/hexagon/kernel/vm_entry.S	2020-06-15 16:12:06.511783294 +0300
@@ -265,12 +265,12 @@
 	 * should be in the designated register (usually R19)
 	 *
 	 * If we were in kernel mode, we don't need to check scheduler
-	 * or signals if CONFIG_PREEMPT is not set.  If set, then it has
+	 * or signals if CONFIG_PREEMPTION is not set.  If set, then it has
 	 * to jump to a need_resched kind of block.
-	 * BTW, CONFIG_PREEMPT is not supported yet.
+	 * BTW, CONFIG_PREEMPTION is not supported yet.
 	 */
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	R0 = #VM_INT_DISABLE
 	trap1(#HVM_TRAP1_VMSETIE)
 #endif
diff -Nur linux-5.4.5/arch/ia64/include/asm/spinlock_types.h linux-5.4.5-new/arch/ia64/include/asm/spinlock_types.h
--- linux-5.4.5/arch/ia64/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/ia64/include/asm/spinlock_types.h	2020-06-15 16:12:06.539783196 +0300
@@ -2,10 +2,6 @@
 #ifndef _ASM_IA64_SPINLOCK_TYPES_H
 #define _ASM_IA64_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Nur linux-5.4.5/arch/ia64/kernel/entry.S linux-5.4.5-new/arch/ia64/kernel/entry.S
--- linux-5.4.5/arch/ia64/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/ia64/kernel/entry.S	2020-06-15 16:12:06.567783098 +0300
@@ -670,12 +670,12 @@
 	 *
 	 * p6 controls whether current_thread_info()->flags needs to be check for
 	 * extra work.  We always check for extra work when returning to user-level.
-	 * With CONFIG_PREEMPT, we also check for extra work when the preempt_count
+	 * With CONFIG_PREEMPTION, we also check for extra work when the preempt_count
 	 * is 0.  After extra work processing has been completed, execution
 	 * resumes at ia64_work_processed_syscall with p6 set to 1 if the extra-work-check
 	 * needs to be redone.
 	 */
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	RSM_PSR_I(p0, r2, r18)			// disable interrupts
 	cmp.eq pLvSys,p0=r0,r0			// pLvSys=1: leave from syscall
 (pKStk) adds r20=TI_PRE_COUNT+IA64_TASK_SIZE,r13
@@ -685,7 +685,7 @@
 (pUStk)	mov r21=0			// r21 <- 0
 	;;
 	cmp.eq p6,p0=r21,r0		// p6 <- pUStk || (preempt_count == 0)
-#else /* !CONFIG_PREEMPT */
+#else /* !CONFIG_PREEMPTION */
 	RSM_PSR_I(pUStk, r2, r18)
 	cmp.eq pLvSys,p0=r0,r0		// pLvSys=1: leave from syscall
 (pUStk)	cmp.eq.unc p6,p0=r0,r0		// p6 <- pUStk
@@ -814,12 +814,12 @@
 	 *
 	 * p6 controls whether current_thread_info()->flags needs to be check for
 	 * extra work.  We always check for extra work when returning to user-level.
-	 * With CONFIG_PREEMPT, we also check for extra work when the preempt_count
+	 * With CONFIG_PREEMPTION, we also check for extra work when the preempt_count
 	 * is 0.  After extra work processing has been completed, execution
 	 * resumes at .work_processed_syscall with p6 set to 1 if the extra-work-check
 	 * needs to be redone.
 	 */
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	RSM_PSR_I(p0, r17, r31)			// disable interrupts
 	cmp.eq p0,pLvSys=r0,r0			// pLvSys=0: leave from kernel
 (pKStk)	adds r20=TI_PRE_COUNT+IA64_TASK_SIZE,r13
@@ -1120,7 +1120,7 @@
 
 	/*
 	 * On entry:
-	 *	r20 = &current->thread_info->pre_count (if CONFIG_PREEMPT)
+	 *	r20 = &current->thread_info->pre_count (if CONFIG_PREEMPTION)
 	 *	r31 = current->thread_info->flags
 	 * On exit:
 	 *	p6 = TRUE if work-pending-check needs to be redone
diff -Nur linux-5.4.5/arch/ia64/kernel/kprobes.c linux-5.4.5-new/arch/ia64/kernel/kprobes.c
--- linux-5.4.5/arch/ia64/kernel/kprobes.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/ia64/kernel/kprobes.c	2020-06-15 16:12:06.567783098 +0300
@@ -841,7 +841,7 @@
 		return 1;
 	}
 
-#if !defined(CONFIG_PREEMPT)
+#if !defined(CONFIG_PREEMPTION)
 	if (p->ainsn.inst_flag == INST_FLAG_BOOSTABLE && !p->post_handler) {
 		/* Boost up -- we can execute copied instructions directly */
 		ia64_psr(regs)->ri = p->ainsn.slot;
diff -Nur linux-5.4.5/arch/Kconfig linux-5.4.5-new/arch/Kconfig
--- linux-5.4.5/arch/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/Kconfig	2020-06-15 16:12:05.119788172 +0300
@@ -31,6 +31,7 @@
 	tristate "OProfile system profiling"
 	depends on PROFILING
 	depends on HAVE_OPROFILE
+	depends on !PREEMPT_RT
 	select RING_BUFFER
 	select RING_BUFFER_ALLOW_SWAP
 	help
diff -Nur linux-5.4.5/arch/m68k/coldfire/entry.S linux-5.4.5-new/arch/m68k/coldfire/entry.S
--- linux-5.4.5/arch/m68k/coldfire/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/m68k/coldfire/entry.S	2020-06-15 16:12:06.595783000 +0300
@@ -108,7 +108,7 @@
 	btst	#5,%sp@(PT_OFF_SR)	/* check if returning to kernel */
 	jeq	Luser_return		/* if so, skip resched, signals */
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	movel	%sp,%d1			/* get thread_info pointer */
 	andl	#-THREAD_SIZE,%d1	/* at base of kernel stack */
 	movel	%d1,%a0
diff -Nur linux-5.4.5/arch/microblaze/kernel/entry.S linux-5.4.5-new/arch/microblaze/kernel/entry.S
--- linux-5.4.5/arch/microblaze/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/microblaze/kernel/entry.S	2020-06-15 16:12:06.719782565 +0300
@@ -728,7 +728,7 @@
 	bri	6f;
 /* MS: Return to kernel state. */
 2:
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	lwi	r11, CURRENT_TASK, TS_THREAD_INFO;
 	/* MS: get preempt_count from thread info */
 	lwi	r5, r11, TI_PREEMPT_COUNT;
diff -Nur linux-5.4.5/arch/mips/include/asm/asmmacro.h linux-5.4.5-new/arch/mips/include/asm/asmmacro.h
--- linux-5.4.5/arch/mips/include/asm/asmmacro.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/mips/include/asm/asmmacro.h	2020-06-15 16:12:06.931781823 +0300
@@ -63,7 +63,7 @@
 	.endm
 
 	.macro	local_irq_disable reg=t0
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	lw      \reg, TI_PRE_COUNT($28)
 	addi    \reg, \reg, 1
 	sw      \reg, TI_PRE_COUNT($28)
@@ -73,7 +73,7 @@
 	xori	\reg, \reg, 1
 	mtc0	\reg, CP0_STATUS
 	irq_disable_hazard
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	lw      \reg, TI_PRE_COUNT($28)
 	addi    \reg, \reg, -1
 	sw      \reg, TI_PRE_COUNT($28)
diff -Nur linux-5.4.5/arch/mips/Kconfig linux-5.4.5-new/arch/mips/Kconfig
--- linux-5.4.5/arch/mips/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/mips/Kconfig	2020-06-15 16:12:06.999781584 +0300
@@ -2586,7 +2586,7 @@
 #
 config HIGHMEM
 	bool "High Memory Support"
-	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA
+	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA && !PREEMPT_RT
 
 config CPU_SUPPORTS_HIGHMEM
 	bool
diff -Nur linux-5.4.5/arch/mips/kernel/entry.S linux-5.4.5-new/arch/mips/kernel/entry.S
--- linux-5.4.5/arch/mips/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/mips/kernel/entry.S	2020-06-15 16:12:07.039781444 +0300
@@ -19,7 +19,7 @@
 #include <asm/thread_info.h>
 #include <asm/war.h>
 
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 #define resume_kernel	restore_all
 #else
 #define __ret_from_irq	ret_from_exception
@@ -27,7 +27,7 @@
 
 	.text
 	.align	5
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 FEXPORT(ret_from_exception)
 	local_irq_disable			# preempt stop
 	b	__ret_from_irq
@@ -53,7 +53,7 @@
 	bnez	t0, work_pending
 	j	restore_all
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 resume_kernel:
 	local_irq_disable
 	lw	t0, TI_PRE_COUNT($28)
diff -Nur linux-5.4.5/arch/nds32/Kconfig linux-5.4.5-new/arch/nds32/Kconfig
--- linux-5.4.5/arch/nds32/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/nds32/Kconfig	2020-06-15 16:12:07.191780911 +0300
@@ -61,7 +61,7 @@
 
 config GENERIC_LOCKBREAK
 	def_bool y
-	depends on PREEMPT
+	depends on PREEMPTION
 
 config TRACE_IRQFLAGS_SUPPORT
 	def_bool y
diff -Nur linux-5.4.5/arch/nds32/kernel/ex-exit.S linux-5.4.5-new/arch/nds32/kernel/ex-exit.S
--- linux-5.4.5/arch/nds32/kernel/ex-exit.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/nds32/kernel/ex-exit.S	2020-06-15 16:12:07.215780827 +0300
@@ -72,7 +72,7 @@
 	restore_user_regs_last
 	.endm
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	.macro	preempt_stop
 	.endm
 #else
@@ -158,7 +158,7 @@
 /*
  * preemptive kernel
  */
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 resume_kernel:
 	gie_disable
 	lwi	$t0, [tsk+#TSK_TI_PREEMPT]
diff -Nur linux-5.4.5/arch/nios2/kernel/entry.S linux-5.4.5-new/arch/nios2/kernel/entry.S
--- linux-5.4.5/arch/nios2/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/nios2/kernel/entry.S	2020-06-15 16:12:07.247780715 +0300
@@ -365,7 +365,7 @@
 	ldw	r1, PT_ESTATUS(sp)	/* check if returning to kernel */
 	TSTBNZ	r1, r1, ESTATUS_EU, Luser_return
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	GET_THREAD_INFO	r1
 	ldw	r4, TI_PREEMPT_COUNT(r1)
 	bne	r4, r0, restore_all
diff -Nur linux-5.4.5/arch/parisc/Kconfig linux-5.4.5-new/arch/parisc/Kconfig
--- linux-5.4.5/arch/parisc/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/parisc/Kconfig	2020-06-15 16:12:07.343780378 +0300
@@ -81,7 +81,7 @@
 config GENERIC_LOCKBREAK
 	bool
 	default y
-	depends on SMP && PREEMPT
+	depends on SMP && PREEMPTION
 
 config ARCH_HAS_ILOG2_U32
 	bool
diff -Nur linux-5.4.5/arch/parisc/kernel/entry.S linux-5.4.5-new/arch/parisc/kernel/entry.S
--- linux-5.4.5/arch/parisc/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/parisc/kernel/entry.S	2020-06-15 16:12:07.327780434 +0300
@@ -940,14 +940,14 @@
 	rfi
 	nop
 
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 # define intr_do_preempt	intr_restore
-#endif /* !CONFIG_PREEMPT */
+#endif /* !CONFIG_PREEMPTION */
 
 	.import schedule,code
 intr_do_resched:
 	/* Only call schedule on return to userspace. If we're returning
-	 * to kernel space, we may schedule if CONFIG_PREEMPT, otherwise
+	 * to kernel space, we may schedule if CONFIG_PREEMPTION, otherwise
 	 * we jump back to intr_restore.
 	 */
 	LDREG	PT_IASQ0(%r16), %r20
@@ -979,7 +979,7 @@
 	 * and preempt_count is 0. otherwise, we continue on
 	 * our merry way back to the current running task.
 	 */
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	.import preempt_schedule_irq,code
 intr_do_preempt:
 	rsm	PSW_SM_I, %r0		/* disable interrupts */
@@ -999,7 +999,7 @@
 	nop
 
 	b,n	intr_restore		/* ssm PSW_SM_I done by intr_restore */
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 	/*
 	 * External interrupts.
diff -Nur linux-5.4.5/arch/powerpc/include/asm/spinlock_types.h linux-5.4.5-new/arch/powerpc/include/asm/spinlock_types.h
--- linux-5.4.5/arch/powerpc/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/include/asm/spinlock_types.h	2020-06-15 16:12:07.567779593 +0300
@@ -2,10 +2,6 @@
 #ifndef _ASM_POWERPC_SPINLOCK_TYPES_H
 #define _ASM_POWERPC_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int slock;
 } arch_spinlock_t;
diff -Nur linux-5.4.5/arch/powerpc/include/asm/stackprotector.h linux-5.4.5-new/arch/powerpc/include/asm/stackprotector.h
--- linux-5.4.5/arch/powerpc/include/asm/stackprotector.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/include/asm/stackprotector.h	2020-06-15 16:12:07.567779593 +0300
@@ -24,7 +24,11 @@
 	unsigned long canary;
 
 	/* Try to get a semi random initial value. */
+#ifdef CONFIG_PREEMPT_RT
+	canary = (unsigned long)&canary;
+#else
 	canary = get_random_canary();
+#endif
 	canary ^= mftb();
 	canary ^= LINUX_VERSION_CODE;
 	canary &= CANARY_MASK;
diff -Nur linux-5.4.5/arch/powerpc/include/asm/thread_info.h linux-5.4.5-new/arch/powerpc/include/asm/thread_info.h
--- linux-5.4.5/arch/powerpc/include/asm/thread_info.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/include/asm/thread_info.h	2020-06-15 16:12:07.571779579 +0300
@@ -30,6 +30,8 @@
 struct thread_info {
 	int		preempt_count;		/* 0 => preemptable,
 						   <0 => BUG */
+	int             preempt_lazy_count;	/* 0 => preemptable,
+						   <0 => BUG */
 	unsigned long	local_flags;		/* private flags for thread */
 #ifdef CONFIG_LIVEPATCH
 	unsigned long *livepatch_sp;
@@ -80,11 +82,12 @@
 #define TIF_SINGLESTEP		8	/* singlestepping active */
 #define TIF_NOHZ		9	/* in adaptive nohz mode */
 #define TIF_SECCOMP		10	/* secure computing */
-#define TIF_RESTOREALL		11	/* Restore all regs (implies NOERROR) */
-#define TIF_NOERROR		12	/* Force successful syscall return */
+
+#define TIF_NEED_RESCHED_LAZY	11	/* lazy rescheduling necessary */
+#define TIF_SYSCALL_TRACEPOINT	12	/* syscall tracepoint instrumentation */
+
 #define TIF_NOTIFY_RESUME	13	/* callback before returning to user */
 #define TIF_UPROBE		14	/* breakpointed or single-stepping */
-#define TIF_SYSCALL_TRACEPOINT	15	/* syscall tracepoint instrumentation */
 #define TIF_EMULATE_STACK_STORE	16	/* Is an instruction emulation
 						for stack store? */
 #define TIF_MEMDIE		17	/* is terminating due to OOM killer */
@@ -93,6 +96,9 @@
 #endif
 #define TIF_POLLING_NRFLAG	19	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_32BIT		20	/* 32 bit binary */
+#define TIF_RESTOREALL		21	/* Restore all regs (implies NOERROR) */
+#define TIF_NOERROR		22	/* Force successful syscall return */
+
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -112,6 +118,7 @@
 #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 #define _TIF_EMULATE_STACK_STORE	(1<<TIF_EMULATE_STACK_STORE)
 #define _TIF_NOHZ		(1<<TIF_NOHZ)
+#define _TIF_NEED_RESCHED_LAZY	(1<<TIF_NEED_RESCHED_LAZY)
 #define _TIF_FSCHECK		(1<<TIF_FSCHECK)
 #define _TIF_SYSCALL_EMU	(1<<TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_DOTRACE	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
@@ -121,8 +128,9 @@
 #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
 				 _TIF_RESTORE_TM | _TIF_PATCH_PENDING | \
-				 _TIF_FSCHECK)
+				 _TIF_FSCHECK | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 
 /* Bits in local_flags */
 /* Don't move TLF_NAPPING without adjusting the code in entry_32.S */
diff -Nur linux-5.4.5/arch/powerpc/Kconfig linux-5.4.5-new/arch/powerpc/Kconfig
--- linux-5.4.5/arch/powerpc/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/Kconfig	2020-06-15 16:12:07.647779313 +0300
@@ -106,7 +106,7 @@
 config GENERIC_LOCKBREAK
 	bool
 	default y
-	depends on SMP && PREEMPT
+	depends on SMP && PREEMPTION
 
 config GENERIC_HWEIGHT
 	bool
@@ -144,6 +144,7 @@
 	select ARCH_MIGHT_HAVE_PC_SERIO
 	select ARCH_OPTIONAL_KERNEL_RWX		if ARCH_HAS_STRICT_KERNEL_RWX
 	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_SUPPORTS_RT
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_CMPXCHG_LOCKREF		if PPC64
 	select ARCH_WANT_IPC_PARSE_VERSION
@@ -221,6 +222,7 @@
 	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_RCU_TABLE_FREE		if SMP
 	select HAVE_RCU_TABLE_NO_INVALIDATE	if HAVE_RCU_TABLE_FREE
 	select HAVE_MMU_GATHER_PAGE_SIZE
@@ -398,7 +400,7 @@
 
 config HIGHMEM
 	bool "High memory support"
-	depends on PPC32
+	depends on PPC32 && !PREEMPT_RT
 
 source "kernel/Kconfig.hz"
 
diff -Nur linux-5.4.5/arch/powerpc/kernel/asm-offsets.c linux-5.4.5-new/arch/powerpc/kernel/asm-offsets.c
--- linux-5.4.5/arch/powerpc/kernel/asm-offsets.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/asm-offsets.c	2020-06-15 16:12:07.623779396 +0300
@@ -167,6 +167,7 @@
 	OFFSET(TI_FLAGS, thread_info, flags);
 	OFFSET(TI_LOCAL_FLAGS, thread_info, local_flags);
 	OFFSET(TI_PREEMPT, thread_info, preempt_count);
+	OFFSET(TI_PREEMPT_LAZY, thread_info, preempt_lazy_count);
 
 #ifdef CONFIG_PPC64
 	OFFSET(DCACHEL1BLOCKSIZE, ppc64_caches, l1d.block_size);
diff -Nur linux-5.4.5/arch/powerpc/kernel/entry_32.S linux-5.4.5-new/arch/powerpc/kernel/entry_32.S
--- linux-5.4.5/arch/powerpc/kernel/entry_32.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/entry_32.S	2020-06-15 16:12:07.627779383 +0300
@@ -400,7 +400,9 @@
 	MTMSRD(r10)
 	lwz	r9,TI_FLAGS(r2)
 	li	r8,-MAX_ERRNO
-	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	lis	r0,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@h
+	ori	r0,r0, (_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@l
+	and.	r0,r9,r0
 	bne-	syscall_exit_work
 	cmplw	0,r3,r8
 	blt+	syscall_exit_cont
@@ -515,13 +517,13 @@
 	b	syscall_dotrace_cont
 
 syscall_exit_work:
-	andi.	r0,r9,_TIF_RESTOREALL
+	andis.	r0,r9,_TIF_RESTOREALL@h
 	beq+	0f
 	REST_NVGPRS(r1)
 	b	2f
 0:	cmplw	0,r3,r8
 	blt+	1f
-	andi.	r0,r9,_TIF_NOERROR
+	andis.	r0,r9,_TIF_NOERROR@h
 	bne-	1f
 	lwz	r11,_CCR(r1)			/* Load CR */
 	neg	r3,r3
@@ -530,12 +532,12 @@
 
 1:	stw	r6,RESULT(r1)	/* Save result */
 	stw	r3,GPR3(r1)	/* Update return value */
-2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)@h
 	beq	4f
 
 	/* Clear per-syscall TIF flags if any are set.  */
 
-	li	r11,_TIF_PERSYSCALL_MASK
+	li	r11,_TIF_PERSYSCALL_MASK@h
 	addi	r12,r2,TI_FLAGS
 3:	lwarx	r8,0,r12
 	andc	r8,r8,r11
@@ -897,13 +899,20 @@
 	bne-	0b
 1:
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	/* check current_thread_info->preempt_count */
 	lwz	r0,TI_PREEMPT(r2)
 	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
 	bne	restore_kuap
 	andi.	r8,r8,_TIF_NEED_RESCHED
+	bne+	1f
+	lwz	r0,TI_PREEMPT_LAZY(r2)
+	cmpwi	0,r0,0          /* if non-zero, just restore regs and return */
+	bne	restore_kuap
+	lwz	r0,TI_FLAGS(r2)
+	andi.	r0,r0,_TIF_NEED_RESCHED_LAZY
 	beq+	restore_kuap
+1:
 	lwz	r3,_MSR(r1)
 	andi.	r0,r3,MSR_EE	/* interrupts off? */
 	beq	restore_kuap	/* don't schedule if so */
@@ -921,7 +930,7 @@
 	 */
 	bl	trace_hardirqs_on
 #endif
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 restore_kuap:
 	kuap_restore r1, r2, r9, r10, r0
 
@@ -1224,7 +1233,7 @@
 #endif /* !(CONFIG_4xx || CONFIG_BOOKE) */
 
 do_work:			/* r10 contains MSR_KERNEL here */
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	beq	do_user_signal
 
 do_resched:			/* r10 contains MSR_KERNEL here */
@@ -1245,7 +1254,7 @@
 	SYNC
 	MTMSRD(r10)		/* disable interrupts */
 	lwz	r9,TI_FLAGS(r2)
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	bne-	do_resched
 	andi.	r0,r9,_TIF_USER_WORK_MASK
 	beq	restore_user
diff -Nur linux-5.4.5/arch/powerpc/kernel/entry_64.S linux-5.4.5-new/arch/powerpc/kernel/entry_64.S
--- linux-5.4.5/arch/powerpc/kernel/entry_64.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/entry_64.S	2020-06-15 16:12:07.627779383 +0300
@@ -240,7 +240,9 @@
 
 	ld	r9,TI_FLAGS(r12)
 	li	r11,-MAX_ERRNO
-	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	lis	r0,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@h
+	ori	r0,r0,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)@l
+	and.	r0,r9,r0
 	bne-	.Lsyscall_exit_work
 
 	andi.	r0,r8,MSR_FP
@@ -363,25 +365,25 @@
 	/* If TIF_RESTOREALL is set, don't scribble on either r3 or ccr.
 	 If TIF_NOERROR is set, just save r3 as it is. */
 
-	andi.	r0,r9,_TIF_RESTOREALL
+	andis.	r0,r9,_TIF_RESTOREALL@h
 	beq+	0f
 	REST_NVGPRS(r1)
 	b	2f
 0:	cmpld	r3,r11		/* r11 is -MAX_ERRNO */
 	blt+	1f
-	andi.	r0,r9,_TIF_NOERROR
+	andis.	r0,r9,_TIF_NOERROR@h
 	bne-	1f
 	ld	r5,_CCR(r1)
 	neg	r3,r3
 	oris	r5,r5,0x1000	/* Set SO bit in CR */
 	std	r5,_CCR(r1)
 1:	std	r3,GPR3(r1)
-2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+2:	andis.	r0,r9,(_TIF_PERSYSCALL_MASK)@h
 	beq	4f
 
 	/* Clear per-syscall TIF flags if any are set.  */
 
-	li	r11,_TIF_PERSYSCALL_MASK
+	lis	r11,(_TIF_PERSYSCALL_MASK)@h
 	addi	r12,r12,TI_FLAGS
 3:	ldarx	r10,0,r12
 	andc	r10,r10,r11
@@ -786,7 +788,7 @@
 	bl	restore_math
 	b	restore
 #endif
-1:	andi.	r0,r4,_TIF_NEED_RESCHED
+1:	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
 	beq	2f
 	bl	restore_interrupts
 	SCHEDULE_USER
@@ -846,12 +848,20 @@
 	bne-	0b
 1:
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	/* Check if we need to preempt */
+	lwz	r8,TI_PREEMPT(r9)
+	cmpwi	0,r8,0		/* if non-zero, just restore regs and return */
+	bne	restore
 	andi.	r0,r4,_TIF_NEED_RESCHED
+	bne+	check_count
+
+	andi.	r0,r4,_TIF_NEED_RESCHED_LAZY
 	beq+	restore
+	lwz	r8,TI_PREEMPT_LAZY(r9)
+
 	/* Check that preempt_count() == 0 and interrupts are enabled */
-	lwz	r8,TI_PREEMPT(r9)
+check_count:
 	cmpwi	cr0,r8,0
 	bne	restore
 	ld	r0,SOFTE(r1)
@@ -877,7 +887,7 @@
 	li	r10,MSR_RI
 	mtmsrd	r10,1		  /* Update machine state */
 #endif /* CONFIG_PPC_BOOK3E */
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 	.globl	fast_exc_return_irq
 fast_exc_return_irq:
diff -Nur linux-5.4.5/arch/powerpc/kernel/irq.c linux-5.4.5-new/arch/powerpc/kernel/irq.c
--- linux-5.4.5/arch/powerpc/kernel/irq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/irq.c	2020-06-15 16:12:07.627779383 +0300
@@ -679,10 +679,12 @@
 void *softirq_ctx[NR_CPUS] __read_mostly;
 void *hardirq_ctx[NR_CPUS] __read_mostly;
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	call_do_softirq(softirq_ctx[smp_processor_id()]);
 }
+#endif
 
 irq_hw_number_t virq_to_hw(unsigned int virq)
 {
diff -Nur linux-5.4.5/arch/powerpc/kernel/misc_32.S linux-5.4.5-new/arch/powerpc/kernel/misc_32.S
--- linux-5.4.5/arch/powerpc/kernel/misc_32.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/misc_32.S	2020-06-15 16:12:07.627779383 +0300
@@ -37,6 +37,7 @@
  * We store the saved ksp_limit in the unused part
  * of the STACK_FRAME_OVERHEAD
  */
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	stw	r0,4(r1)
@@ -52,6 +53,7 @@
 	stw	r10,THREAD+KSP_LIMIT(r2)
 	mtlr	r0
 	blr
+#endif
 
 /*
  * void call_do_irq(struct pt_regs *regs, void *sp);
diff -Nur linux-5.4.5/arch/powerpc/kernel/misc_64.S linux-5.4.5-new/arch/powerpc/kernel/misc_64.S
--- linux-5.4.5/arch/powerpc/kernel/misc_64.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/misc_64.S	2020-06-15 16:12:07.627779383 +0300
@@ -27,6 +27,7 @@
 
 	.text
 
+#ifndef CONFIG_PREEMPT_RT
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	std	r0,16(r1)
@@ -37,6 +38,7 @@
 	ld	r0,16(r1)
 	mtlr	r0
 	blr
+#endif
 
 _GLOBAL(call_do_irq)
 	mflr	r0
diff -Nur linux-5.4.5/arch/powerpc/kernel/traps.c linux-5.4.5-new/arch/powerpc/kernel/traps.c
--- linux-5.4.5/arch/powerpc/kernel/traps.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/traps.c	2020-06-15 16:12:07.627779383 +0300
@@ -171,7 +171,6 @@
 
 extern void panic_flush_kmsg_end(void)
 {
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 	bust_spinlocks(0);
 	debug_locks_off();
@@ -252,14 +251,19 @@
 
 static int __die(const char *str, struct pt_regs *regs, long err)
 {
+	const char *pr = "";
+
 	printk("Oops: %s, sig: %ld [#%d]\n", str, err, ++die_counter);
 
+	if (IS_ENABLED(CONFIG_PREEMPTION))
+		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
+
 	printk("%s PAGE_SIZE=%luK%s%s%s%s%s%s%s %s\n",
 	       IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN) ? "LE" : "BE",
 	       PAGE_SIZE / 1024,
 	       early_radix_enabled() ? " MMU=Radix" : "",
 	       early_mmu_has_feature(MMU_FTR_HPTE_TABLE) ? " MMU=Hash" : "",
-	       IS_ENABLED(CONFIG_PREEMPT) ? " PREEMPT" : "",
+	       pr,
 	       IS_ENABLED(CONFIG_SMP) ? " SMP" : "",
 	       IS_ENABLED(CONFIG_SMP) ? (" NR_CPUS=" __stringify(NR_CPUS)) : "",
 	       debug_pagealloc_enabled() ? " DEBUG_PAGEALLOC" : "",
diff -Nur linux-5.4.5/arch/powerpc/kernel/watchdog.c linux-5.4.5-new/arch/powerpc/kernel/watchdog.c
--- linux-5.4.5/arch/powerpc/kernel/watchdog.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kernel/watchdog.c	2020-06-15 16:12:07.627779383 +0300
@@ -181,11 +181,6 @@
 
 	wd_smp_unlock(&flags);
 
-	printk_safe_flush();
-	/*
-	 * printk_safe_flush() seems to require another print
-	 * before anything actually goes out to console.
-	 */
 	if (sysctl_hardlockup_all_cpu_backtrace)
 		trigger_allbutself_cpu_backtrace();
 
diff -Nur linux-5.4.5/arch/powerpc/kvm/Kconfig linux-5.4.5-new/arch/powerpc/kvm/Kconfig
--- linux-5.4.5/arch/powerpc/kvm/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/kvm/Kconfig	2020-06-15 16:12:07.647779313 +0300
@@ -178,6 +178,7 @@
 config KVM_MPIC
 	bool "KVM in-kernel MPIC emulation"
 	depends on KVM && E500
+	depends on !PREEMPT_RT
 	select HAVE_KVM_IRQCHIP
 	select HAVE_KVM_IRQFD
 	select HAVE_KVM_IRQ_ROUTING
diff -Nur linux-5.4.5/arch/powerpc/platforms/ps3/device-init.c linux-5.4.5-new/arch/powerpc/platforms/ps3/device-init.c
--- linux-5.4.5/arch/powerpc/platforms/ps3/device-init.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/platforms/ps3/device-init.c	2020-06-15 16:12:07.783778836 +0300
@@ -738,8 +738,8 @@
 	}
 	pr_debug("%s:%u: notification %s issued\n", __func__, __LINE__, op);
 
-	res = wait_event_interruptible(dev->done.wait,
-				       dev->done.done || kthread_should_stop());
+	res = swait_event_interruptible_exclusive(dev->done.wait,
+						  dev->done.done || kthread_should_stop());
 	if (kthread_should_stop())
 		res = -EINTR;
 	if (res) {
diff -Nur linux-5.4.5/arch/powerpc/platforms/pseries/iommu.c linux-5.4.5-new/arch/powerpc/platforms/pseries/iommu.c
--- linux-5.4.5/arch/powerpc/platforms/pseries/iommu.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/powerpc/platforms/pseries/iommu.c	2020-06-15 16:12:07.791778808 +0300
@@ -24,6 +24,7 @@
 #include <linux/of.h>
 #include <linux/iommu.h>
 #include <linux/rculist.h>
+#include <linux/locallock.h>
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/rtas.h>
@@ -178,6 +179,7 @@
 }
 
 static DEFINE_PER_CPU(__be64 *, tce_page);
+static DEFINE_LOCAL_IRQ_LOCK(tcp_page_lock);
 
 static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 				     long npages, unsigned long uaddr,
@@ -198,7 +200,8 @@
 		                           direction, attrs);
 	}
 
-	local_irq_save(flags);	/* to protect tcep and the page behind it */
+	/* to protect tcep and the page behind it */
+	local_lock_irqsave(tcp_page_lock, flags);
 
 	tcep = __this_cpu_read(tce_page);
 
@@ -209,7 +212,7 @@
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		/* If allocation fails, fall back to the loop implementation */
 		if (!tcep) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(tcp_page_lock, flags);
 			return tce_build_pSeriesLP(tbl, tcenum, npages, uaddr,
 					    direction, attrs);
 		}
@@ -243,7 +246,7 @@
 		tcenum += limit;
 	} while (npages > 0 && !rc);
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(tcp_page_lock, flags);
 
 	if (unlikely(rc == H_NOT_ENOUGH_RESOURCES)) {
 		ret = (int)rc;
@@ -401,13 +404,14 @@
 	u64 rc = 0;
 	long l, limit;
 
-	local_irq_disable();	/* to protect tcep and the page behind it */
+	/* to protect tcep and the page behind it */
+	local_lock_irq(tcp_page_lock);
 	tcep = __this_cpu_read(tce_page);
 
 	if (!tcep) {
 		tcep = (__be64 *)__get_free_page(GFP_ATOMIC);
 		if (!tcep) {
-			local_irq_enable();
+			local_unlock_irq(tcp_page_lock);
 			return -ENOMEM;
 		}
 		__this_cpu_write(tce_page, tcep);
@@ -453,7 +457,7 @@
 
 	/* error cleanup: caller will clear whole range */
 
-	local_irq_enable();
+	local_unlock_irq(tcp_page_lock);
 	return rc;
 }
 
diff -Nur linux-5.4.5/arch/riscv/kernel/entry.S linux-5.4.5-new/arch/riscv/kernel/entry.S
--- linux-5.4.5/arch/riscv/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/riscv/kernel/entry.S	2020-06-15 16:12:07.839778640 +0300
@@ -155,7 +155,7 @@
 	REG_L x2,  PT_SP(sp)
 	.endm
 
-#if !IS_ENABLED(CONFIG_PREEMPT)
+#if !IS_ENABLED(CONFIG_PREEMPTION)
 .set resume_kernel, restore_all
 #endif
 
@@ -269,7 +269,7 @@
 	RESTORE_ALL
 	sret
 
-#if IS_ENABLED(CONFIG_PREEMPT)
+#if IS_ENABLED(CONFIG_PREEMPTION)
 resume_kernel:
 	REG_L s0, TASK_TI_PREEMPT_COUNT(tp)
 	bnez s0, restore_all
diff -Nur linux-5.4.5/arch/s390/include/asm/preempt.h linux-5.4.5-new/arch/s390/include/asm/preempt.h
--- linux-5.4.5/arch/s390/include/asm/preempt.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/s390/include/asm/preempt.h	2020-06-15 16:12:07.887778471 +0300
@@ -130,11 +130,11 @@
 
 #endif /* CONFIG_HAVE_MARCH_Z196_FEATURES */
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 extern asmlinkage void preempt_schedule(void);
 #define __preempt_schedule() preempt_schedule()
 extern asmlinkage void preempt_schedule_notrace(void);
 #define __preempt_schedule_notrace() preempt_schedule_notrace()
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 #endif /* __ASM_PREEMPT_H */
diff -Nur linux-5.4.5/arch/s390/include/asm/spinlock_types.h linux-5.4.5-new/arch/s390/include/asm/spinlock_types.h
--- linux-5.4.5/arch/s390/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/s390/include/asm/spinlock_types.h	2020-06-15 16:12:07.887778471 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	int lock;
 } __attribute__ ((aligned (4))) arch_spinlock_t;
diff -Nur linux-5.4.5/arch/s390/Kconfig linux-5.4.5-new/arch/s390/Kconfig
--- linux-5.4.5/arch/s390/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/s390/Kconfig	2020-06-15 16:12:07.939778289 +0300
@@ -30,7 +30,7 @@
 	def_bool y
 
 config GENERIC_LOCKBREAK
-	def_bool y if PREEMPT
+	def_bool y if PREEMPTTION
 
 config PGSTE
 	def_bool y if KVM
diff -Nur linux-5.4.5/arch/s390/kernel/dumpstack.c linux-5.4.5-new/arch/s390/kernel/dumpstack.c
--- linux-5.4.5/arch/s390/kernel/dumpstack.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/s390/kernel/dumpstack.c	2020-06-15 16:12:07.919778359 +0300
@@ -194,6 +194,8 @@
 	       regs->int_code >> 17, ++die_counter);
 #ifdef CONFIG_PREEMPT
 	pr_cont("PREEMPT ");
+#elif defined(CONFIG_PREEMPT_RT)
+	pr_cont("PREEMPT_RT ");
 #endif
 	pr_cont("SMP ");
 	if (debug_pagealloc_enabled())
diff -Nur linux-5.4.5/arch/s390/kernel/entry.S linux-5.4.5-new/arch/s390/kernel/entry.S
--- linux-5.4.5/arch/s390/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/s390/kernel/entry.S	2020-06-15 16:12:07.919778359 +0300
@@ -790,7 +790,7 @@
 .Lio_work:
 	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
 	jo	.Lio_work_user		# yes -> do resched & signal
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	# check for preemptive scheduling
 	icm	%r0,15,__LC_PREEMPT_COUNT
 	jnz	.Lio_restore		# preemption is disabled
diff -Nur linux-5.4.5/arch/sh/include/asm/spinlock_types.h linux-5.4.5-new/arch/sh/include/asm/spinlock_types.h
--- linux-5.4.5/arch/sh/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sh/include/asm/spinlock_types.h	2020-06-15 16:12:08.015778022 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SH_SPINLOCK_TYPES_H
 #define __ASM_SH_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 typedef struct {
 	volatile unsigned int lock;
 } arch_spinlock_t;
diff -Nur linux-5.4.5/arch/sh/Kconfig linux-5.4.5-new/arch/sh/Kconfig
--- linux-5.4.5/arch/sh/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sh/Kconfig	2020-06-15 16:12:08.047777911 +0300
@@ -108,7 +108,7 @@
 
 config GENERIC_LOCKBREAK
 	def_bool y
-	depends on SMP && PREEMPT
+	depends on SMP && PREEMPTION
 
 config ARCH_SUSPEND_POSSIBLE
 	def_bool n
diff -Nur linux-5.4.5/arch/sh/kernel/cpu/sh5/entry.S linux-5.4.5-new/arch/sh/kernel/cpu/sh5/entry.S
--- linux-5.4.5/arch/sh/kernel/cpu/sh5/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sh/kernel/cpu/sh5/entry.S	2020-06-15 16:12:08.075777812 +0300
@@ -86,7 +86,7 @@
 	andi	r6, ~0xf0, r6;		\
 	putcon	r6, SR;
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 #  define preempt_stop()	CLI()
 #else
 #  define preempt_stop()
@@ -884,7 +884,7 @@
 
 	/* Check softirqs */
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	pta   ret_from_syscall, tr0
 	blink   tr0, ZERO
 
diff -Nur linux-5.4.5/arch/sh/kernel/entry-common.S linux-5.4.5-new/arch/sh/kernel/entry-common.S
--- linux-5.4.5/arch/sh/kernel/entry-common.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sh/kernel/entry-common.S	2020-06-15 16:12:08.091777757 +0300
@@ -41,7 +41,7 @@
  */
 #include <asm/dwarf.h>
 
-#if defined(CONFIG_PREEMPT)
+#if defined(CONFIG_PREEMPTION)
 #  define preempt_stop()	cli ; TRACE_IRQS_OFF
 #else
 #  define preempt_stop()
@@ -84,7 +84,7 @@
 	get_current_thread_info r8, r0
 	bt	resume_kernel	! Yes, it's from kernel, go back soon
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	bra	resume_userspace
 	 nop
 ENTRY(resume_kernel)
diff -Nur linux-5.4.5/arch/sh/kernel/irq.c linux-5.4.5-new/arch/sh/kernel/irq.c
--- linux-5.4.5/arch/sh/kernel/irq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sh/kernel/irq.c	2020-06-15 16:12:08.091777757 +0300
@@ -148,6 +148,7 @@
 	hardirq_ctx[cpu] = NULL;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curctx;
@@ -175,6 +176,7 @@
 		  "r5", "r6", "r7", "r8", "r9", "r15", "t", "pr"
 	);
 }
+#endif
 #else
 static inline void handle_one_irq(unsigned int irq)
 {
diff -Nur linux-5.4.5/arch/sparc/Kconfig linux-5.4.5-new/arch/sparc/Kconfig
--- linux-5.4.5/arch/sparc/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sparc/Kconfig	2020-06-15 16:12:08.267777139 +0300
@@ -277,7 +277,7 @@
 config GENERIC_LOCKBREAK
 	bool
 	default y
-	depends on SPARC64 && SMP && PREEMPT
+	depends on SPARC64 && SMP && PREEMPTION
 
 config NUMA
 	bool "NUMA support"
diff -Nur linux-5.4.5/arch/sparc/kernel/irq_64.c linux-5.4.5-new/arch/sparc/kernel/irq_64.c
--- linux-5.4.5/arch/sparc/kernel/irq_64.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sparc/kernel/irq_64.c	2020-06-15 16:12:08.231777265 +0300
@@ -854,6 +854,7 @@
 	set_irq_regs(old_regs);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	void *orig_sp, *sp = softirq_stack[smp_processor_id()];
@@ -868,6 +869,7 @@
 	__asm__ __volatile__("mov %0, %%sp"
 			     : : "r" (orig_sp));
 }
+#endif
 
 #ifdef CONFIG_HOTPLUG_CPU
 void fixup_irqs(void)
diff -Nur linux-5.4.5/arch/sparc/kernel/rtrap_64.S linux-5.4.5-new/arch/sparc/kernel/rtrap_64.S
--- linux-5.4.5/arch/sparc/kernel/rtrap_64.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/sparc/kernel/rtrap_64.S	2020-06-15 16:12:08.231777265 +0300
@@ -310,7 +310,7 @@
 		retry
 
 to_kernel:
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 		ldsw			[%g6 + TI_PRE_COUNT], %l5
 		brnz			%l5, kern_fpucheck
 		 ldx			[%g6 + TI_FLAGS], %l5
diff -Nur linux-5.4.5/arch/x86/crypto/aesni-intel_glue.c linux-5.4.5-new/arch/x86/crypto/aesni-intel_glue.c
--- linux-5.4.5/arch/x86/crypto/aesni-intel_glue.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/crypto/aesni-intel_glue.c	2020-06-15 16:12:08.715775568 +0300
@@ -387,14 +387,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -409,14 +409,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -431,14 +431,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -453,14 +453,14 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -510,18 +510,20 @@
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		kernel_fpu_begin();
 		aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			              nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 	if (walk.nbytes) {
+		kernel_fpu_begin();
 		ctr_crypt_final(ctx, &walk);
+		kernel_fpu_end();
 		err = skcipher_walk_done(&walk, 0);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
diff -Nur linux-5.4.5/arch/x86/crypto/cast5_avx_glue.c linux-5.4.5-new/arch/x86/crypto/cast5_avx_glue.c
--- linux-5.4.5/arch/x86/crypto/cast5_avx_glue.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/crypto/cast5_avx_glue.c	2020-06-15 16:12:08.715775568 +0300
@@ -46,7 +46,7 @@
 
 static int ecb_crypt(struct skcipher_request *req, bool enc)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
 	struct skcipher_walk walk;
@@ -61,7 +61,7 @@
 		u8 *wsrc = walk.src.virt.addr;
 		u8 *wdst = walk.dst.virt.addr;
 
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 
 		/* Process multi-block batch */
 		if (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {
@@ -90,10 +90,9 @@
 		} while (nbytes >= bsize);
 
 done:
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -197,7 +196,7 @@
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -205,12 +204,11 @@
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes)) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __cbc_decrypt(ctx, &walk);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -277,7 +275,7 @@
 {
 	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 	struct cast5_ctx *ctx = crypto_skcipher_ctx(tfm);
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
@@ -285,13 +283,12 @@
 	err = skcipher_walk_virt(&walk, req, false);
 
 	while ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, &walk, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, &walk, nbytes);
 		nbytes = __ctr_crypt(&walk, ctx);
+		cast5_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	cast5_fpu_end(fpu_enabled);
-
 	if (walk.nbytes) {
 		ctr_crypt_final(&walk, ctx);
 		err = skcipher_walk_done(&walk, 0);
diff -Nur linux-5.4.5/arch/x86/crypto/chacha_glue.c linux-5.4.5-new/arch/x86/crypto/chacha_glue.c
--- linux-5.4.5/arch/x86/crypto/chacha_glue.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/crypto/chacha_glue.c	2020-06-15 16:12:08.715775568 +0300
@@ -127,7 +127,6 @@
 				  const struct chacha_ctx *ctx, const u8 *iv)
 {
 	u32 *state, state_buf[16 + 2] __aligned(8);
-	int next_yield = 4096; /* bytes until next FPU yield */
 	int err = 0;
 
 	BUILD_BUG_ON(CHACHA_STATE_ALIGN != 16);
@@ -140,20 +139,14 @@
 
 		if (nbytes < walk->total) {
 			nbytes = round_down(nbytes, walk->stride);
-			next_yield -= nbytes;
 		}
 
 		chacha_dosimd(state, walk->dst.virt.addr, walk->src.virt.addr,
 			      nbytes, ctx->nrounds);
 
-		if (next_yield <= 0) {
-			/* temporarily allow preemption */
-			kernel_fpu_end();
-			kernel_fpu_begin();
-			next_yield = 4096;
-		}
-
+		kernel_fpu_end();
 		err = skcipher_walk_done(walk, walk->nbytes - nbytes);
+		kernel_fpu_begin();
 	}
 
 	return err;
diff -Nur linux-5.4.5/arch/x86/crypto/glue_helper.c linux-5.4.5-new/arch/x86/crypto/glue_helper.c
--- linux-5.4.5/arch/x86/crypto/glue_helper.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/crypto/glue_helper.c	2020-06-15 16:12:08.715775568 +0300
@@ -24,7 +24,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -37,7 +37,7 @@
 		unsigned int i;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		for (i = 0; i < gctx->num_funcs; i++) {
 			func_bytes = bsize * gctx->funcs[i].num_blocks;
 
@@ -55,10 +55,9 @@
 			if (nbytes < bsize)
 				break;
 		}
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
-
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_ecb_req_128bit);
@@ -101,7 +100,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -115,7 +114,7 @@
 		u128 last_iv;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 		/* Start of the last block. */
 		src += nbytes / bsize - 1;
 		dst += nbytes / bsize - 1;
@@ -147,10 +146,10 @@
 done:
 		u128_xor(dst, dst, (u128 *)walk.iv);
 		*(u128 *)walk.iv = last_iv;
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_cbc_decrypt_req_128bit);
@@ -161,7 +160,7 @@
 	void *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));
 	const unsigned int bsize = 128 / 8;
 	struct skcipher_walk walk;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	unsigned int nbytes;
 	int err;
 
@@ -175,7 +174,7 @@
 		le128 ctrblk;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     &walk, fpu_enabled, nbytes);
+					     &walk, false, nbytes);
 
 		be128_to_le128(&ctrblk, (be128 *)walk.iv);
 
@@ -199,11 +198,10 @@
 		}
 
 		le128_to_be128((be128 *)walk.iv, &ctrblk);
+		glue_fpu_end(fpu_enabled);
 		err = skcipher_walk_done(&walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
-
 	if (nbytes) {
 		le128 ctrblk;
 		u128 tmp;
@@ -301,8 +299,14 @@
 	tweak_fn(tweak_ctx, walk.iv, walk.iv);
 
 	while (nbytes) {
+		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+					     &walk, fpu_enabled,
+					     nbytes < bsize ? bsize : nbytes);
 		nbytes = __glue_xts_req_128bit(gctx, crypt_ctx, &walk);
 
+		glue_fpu_end(fpu_enabled);
+		fpu_enabled = false;
+
 		err = skcipher_walk_done(&walk, nbytes);
 		nbytes = walk.nbytes;
 	}
diff -Nur linux-5.4.5/arch/x86/entry/common.c linux-5.4.5-new/arch/x86/entry/common.c
--- linux-5.4.5/arch/x86/entry/common.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/entry/common.c	2020-06-15 16:12:08.739775484 +0300
@@ -130,7 +130,7 @@
 
 #define EXIT_TO_USERMODE_LOOP_FLAGS				\
 	(_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_UPROBE |	\
-	 _TIF_NEED_RESCHED | _TIF_USER_RETURN_NOTIFY | _TIF_PATCH_PENDING)
+	 _TIF_NEED_RESCHED_MASK | _TIF_USER_RETURN_NOTIFY | _TIF_PATCH_PENDING)
 
 static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
 {
@@ -145,9 +145,16 @@
 		/* We have work to do. */
 		local_irq_enable();
 
-		if (cached_flags & _TIF_NEED_RESCHED)
+		if (cached_flags & _TIF_NEED_RESCHED_MASK)
 			schedule();
 
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+		if (unlikely(current->forced_info.si_signo)) {
+			struct task_struct *t = current;
+			force_sig_info(&t->forced_info);
+			t->forced_info.si_signo = 0;
+		}
+#endif
 		if (cached_flags & _TIF_UPROBE)
 			uprobe_notify_resume(regs);
 
diff -Nur linux-5.4.5/arch/x86/entry/entry_32.S linux-5.4.5-new/arch/x86/entry/entry_32.S
--- linux-5.4.5/arch/x86/entry/entry_32.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/entry/entry_32.S	2020-06-15 16:12:08.739775484 +0300
@@ -1106,8 +1106,26 @@
 restore_all_kernel:
 #ifdef CONFIG_PREEMPTION
 	DISABLE_INTERRUPTS(CLBR_ANY)
+	# preempt count == 0 + NEED_RS set?
 	cmpl	$0, PER_CPU_VAR(__preempt_count)
+#ifndef CONFIG_PREEMPT_LAZY
 	jnz	.Lno_preempt
+#else
+	jz	test_int_off
+
+	# atleast preempt count == 0 ?
+	cmpl	$_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
+	jne	.Lno_preempt
+
+	movl	PER_CPU_VAR(current_task), %ebp
+	cmpl	$0,TASK_TI_preempt_lazy_count(%ebp)	# non-zero preempt_lazy_count ?
+	jnz	.Lno_preempt
+
+	testl	$_TIF_NEED_RESCHED_LAZY, TASK_TI_flags(%ebp)
+	jz	.Lno_preempt
+
+test_int_off:
+#endif
 	testl	$X86_EFLAGS_IF, PT_EFLAGS(%esp)	# interrupts off (exception path) ?
 	jz	.Lno_preempt
 	call	preempt_schedule_irq
diff -Nur linux-5.4.5/arch/x86/entry/entry_64.S linux-5.4.5-new/arch/x86/entry/entry_64.S
--- linux-5.4.5/arch/x86/entry/entry_64.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/entry/entry_64.S	2020-06-15 16:12:08.739775484 +0300
@@ -670,7 +670,23 @@
 	btl	$9, EFLAGS(%rsp)		/* were interrupts off? */
 	jnc	1f
 	cmpl	$0, PER_CPU_VAR(__preempt_count)
+#ifndef CONFIG_PREEMPT_LAZY
 	jnz	1f
+#else
+	jz	do_preempt_schedule_irq
+
+	# atleast preempt count == 0 ?
+	cmpl	$_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
+	jnz	1f
+
+	movq	PER_CPU_VAR(current_task), %rcx
+	cmpl	$0, TASK_TI_preempt_lazy_count(%rcx)
+	jnz	1f
+
+	btl	$TIF_NEED_RESCHED_LAZY,TASK_TI_flags(%rcx)
+	jnc	1f
+do_preempt_schedule_irq:
+#endif
 	call	preempt_schedule_irq
 1:
 #endif
@@ -1074,6 +1090,7 @@
 	jmp	2b
 	.previous
 
+#ifndef CONFIG_PREEMPT_RT
 /* Call softirq on interrupt stack. Interrupts are off. */
 ENTRY(do_softirq_own_stack)
 	pushq	%rbp
@@ -1084,6 +1101,7 @@
 	leaveq
 	ret
 ENDPROC(do_softirq_own_stack)
+#endif
 
 #ifdef CONFIG_XEN_PV
 idtentry hypervisor_callback xen_do_hypervisor_callback has_error_code=0
diff -Nur linux-5.4.5/arch/x86/entry/syscalls/syscall_64.tbl linux-5.4.5-new/arch/x86/entry/syscalls/syscall_64.tbl
--- linux-5.4.5/arch/x86/entry/syscalls/syscall_64.tbl	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/entry/syscalls/syscall_64.tbl	2020-06-12 13:01:54.821968773 +0300
@@ -357,6 +357,12 @@
 433	common	fspick			__x64_sys_fspick
 434	common	pidfd_open		__x64_sys_pidfd_open
 435	common	clone3			__x64_sys_clone3/ptregs
+436     common  socket_rtnet            __x64_sys_socket_rtnet
+437     common  bind_rtnet              __x64_sys_bind_rtnet
+438     common  recvmsg_rtnet           __x64_sys_recvmsg_rtnet
+439     common  sendto_rtnet            __x64_sys_sendto_rtnet
+440     common  recvfrom_rtnet          __x64_sys_recvfrom_rtnet
+441     common  sendmsg_rtnet           __x64_sys_sendmsg_rtnet
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff -Nur linux-5.4.5/arch/x86/include/asm/fpu/api.h linux-5.4.5-new/arch/x86/include/asm/fpu/api.h
--- linux-5.4.5/arch/x86/include/asm/fpu/api.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/include/asm/fpu/api.h	2020-06-15 16:12:08.863775050 +0300
@@ -23,6 +23,7 @@
 extern void kernel_fpu_end(void);
 extern bool irq_fpu_usable(void);
 extern void fpregs_mark_activate(void);
+extern void kernel_fpu_resched(void);
 
 /*
  * Use fpregs_lock() while editing CPU's FPU registers or fpu->state.
diff -Nur linux-5.4.5/arch/x86/include/asm/preempt.h linux-5.4.5-new/arch/x86/include/asm/preempt.h
--- linux-5.4.5/arch/x86/include/asm/preempt.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/include/asm/preempt.h	2020-06-15 16:12:08.931774811 +0300
@@ -89,17 +89,48 @@
  * a decrement which hits zero means we have no preempt_count and should
  * reschedule.
  */
-static __always_inline bool __preempt_count_dec_and_test(void)
+static __always_inline bool ____preempt_count_dec_and_test(void)
 {
 	return GEN_UNARY_RMWcc("decl", __preempt_count, e, __percpu_arg([var]));
 }
 
+static __always_inline bool __preempt_count_dec_and_test(void)
+{
+	if (____preempt_count_dec_and_test())
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if (preempt_count())
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
+}
+
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
 static __always_inline bool should_resched(int preempt_offset)
 {
+#ifdef CONFIG_PREEMPT_LAZY
+	u32 tmp;
+	tmp = raw_cpu_read_4(__preempt_count);
+	if (tmp == preempt_offset)
+		return true;
+
+	/* preempt count == 0 ? */
+	tmp &= ~PREEMPT_NEED_RESCHED;
+	if (tmp != preempt_offset)
+		return false;
+	/* XXX PREEMPT_LOCK_OFFSET */
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
 	return unlikely(raw_cpu_read_4(__preempt_count) == preempt_offset);
+#endif
 }
 
 #ifdef CONFIG_PREEMPTION
diff -Nur linux-5.4.5/arch/x86/include/asm/signal.h linux-5.4.5-new/arch/x86/include/asm/signal.h
--- linux-5.4.5/arch/x86/include/asm/signal.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/include/asm/signal.h	2020-06-15 16:12:08.931774811 +0300
@@ -28,6 +28,19 @@
 #define SA_IA32_ABI	0x02000000u
 #define SA_X32_ABI	0x01000000u
 
+/*
+ * Because some traps use the IST stack, we must keep preemption
+ * disabled while calling do_trap(), but do_trap() may call
+ * force_sig_info() which will grab the signal spin_locks for the
+ * task, which in PREEMPT_RT are mutexes.  By defining
+ * ARCH_RT_DELAYS_SIGNAL_SEND the force_sig_info() will set
+ * TIF_NOTIFY_RESUME and set up the signal to be sent on exit of the
+ * trap.
+ */
+#if defined(CONFIG_PREEMPT_RT)
+#define ARCH_RT_DELAYS_SIGNAL_SEND
+#endif
+
 #ifndef CONFIG_COMPAT
 typedef sigset_t compat_sigset_t;
 #endif
diff -Nur linux-5.4.5/arch/x86/include/asm/stackprotector.h linux-5.4.5-new/arch/x86/include/asm/stackprotector.h
--- linux-5.4.5/arch/x86/include/asm/stackprotector.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/include/asm/stackprotector.h	2020-06-15 16:12:08.931774811 +0300
@@ -60,7 +60,7 @@
  */
 static __always_inline void boot_init_stack_canary(void)
 {
-	u64 canary;
+	u64 uninitialized_var(canary);
 	u64 tsc;
 
 #ifdef CONFIG_X86_64
@@ -71,8 +71,14 @@
 	 * of randomness. The TSC only matters for very early init,
 	 * there it already has some randomness on most systems. Later
 	 * on during the bootup the random pool has true entropy too.
+	 * For preempt-rt we need to weaken the randomness a bit, as
+	 * we can't call into the random generator from atomic context
+	 * due to locking constraints. We just leave canary
+	 * uninitialized and use the TSC based randomness on top of it.
 	 */
+#ifndef CONFIG_PREEMPT_RT
 	get_random_bytes(&canary, sizeof(canary));
+#endif
 	tsc = rdtsc();
 	canary += tsc + (tsc << 32UL);
 	canary &= CANARY_MASK;
diff -Nur linux-5.4.5/arch/x86/include/asm/thread_info.h linux-5.4.5-new/arch/x86/include/asm/thread_info.h
--- linux-5.4.5/arch/x86/include/asm/thread_info.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/include/asm/thread_info.h	2020-06-15 16:12:08.931774811 +0300
@@ -56,17 +56,24 @@
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	u32			status;		/* thread synchronous flags */
+	int			preempt_lazy_count;	/* 0 => lazy preemptable
+							  <0 => BUG */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.flags		= 0,			\
+	.preempt_lazy_count = 0,		\
 }
 
 #else /* !__ASSEMBLY__ */
 
 #include <asm/asm-offsets.h>
 
+#define GET_THREAD_INFO(reg) \
+	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
+	_ASM_SUB $(THREAD_SIZE),reg ;
+
 #endif
 
 /*
@@ -92,6 +99,7 @@
 #define TIF_NOCPUID		15	/* CPUID is not accessible in userland */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
+#define TIF_NEED_RESCHED_LAZY	18	/* lazy rescheduling necessary */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
@@ -122,6 +130,7 @@
 #define _TIF_NOCPUID		(1 << TIF_NOCPUID)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
@@ -159,6 +168,8 @@
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
+
 #define STACK_WARN		(THREAD_SIZE/8)
 
 /*
diff -Nur linux-5.4.5/arch/x86/Kconfig linux-5.4.5-new/arch/x86/Kconfig
--- linux-5.4.5/arch/x86/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/Kconfig	2020-06-15 16:12:09.691772146 +0300
@@ -90,6 +90,7 @@
 	select ARCH_SUPPORTS_ACPI
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+	select ARCH_SUPPORTS_RT
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_QUEUED_RWLOCKS
 	select ARCH_USE_QUEUED_SPINLOCKS
@@ -132,8 +133,8 @@
 	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
 	select HAVE_ARCH_AUDITSYSCALL
 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
-	select HAVE_ARCH_JUMP_LABEL
-	select HAVE_ARCH_JUMP_LABEL_RELATIVE
+	select HAVE_ARCH_JUMP_LABEL		if !PREEMPT_RT
+	select HAVE_ARCH_JUMP_LABEL_RELATIVE	if !PREEMPT_RT
 	select HAVE_ARCH_KASAN			if X86_64
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS		if MMU
@@ -199,6 +200,7 @@
 	select HAVE_PCI
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_RCU_TABLE_FREE		if PARAVIRT
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RELIABLE_STACKTRACE		if X86_64 && (UNWINDER_FRAME_POINTER || UNWINDER_ORC) && STACK_VALIDATION
diff -Nur linux-5.4.5/arch/x86/kernel/apic/io_apic.c linux-5.4.5-new/arch/x86/kernel/apic/io_apic.c
--- linux-5.4.5/arch/x86/kernel/apic/io_apic.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/apic/io_apic.c	2020-06-15 16:12:08.951774741 +0300
@@ -1725,19 +1725,20 @@
 	return false;
 }
 
-static inline bool ioapic_irqd_mask(struct irq_data *data)
+static inline bool ioapic_prepare_move(struct irq_data *data)
 {
-	/* If we are moving the irq we need to mask it */
+	/* If we are moving the IRQ we need to mask it */
 	if (unlikely(irqd_is_setaffinity_pending(data))) {
-		mask_ioapic_irq(data);
+		if (!irqd_irq_masked(data))
+			mask_ioapic_irq(data);
 		return true;
 	}
 	return false;
 }
 
-static inline void ioapic_irqd_unmask(struct irq_data *data, bool masked)
+static inline void ioapic_finish_move(struct irq_data *data, bool moveit)
 {
-	if (unlikely(masked)) {
+	if (unlikely(moveit)) {
 		/* Only migrate the irq if the ack has been received.
 		 *
 		 * On rare occasions the broadcast level triggered ack gets
@@ -1766,15 +1767,17 @@
 		 */
 		if (!io_apic_level_ack_pending(data->chip_data))
 			irq_move_masked_irq(data);
-		unmask_ioapic_irq(data);
+		/* If the IRQ is masked in the core, leave it: */
+		if (!irqd_irq_masked(data))
+			unmask_ioapic_irq(data);
 	}
 }
 #else
-static inline bool ioapic_irqd_mask(struct irq_data *data)
+static inline bool ioapic_prepare_move(struct irq_data *data)
 {
 	return false;
 }
-static inline void ioapic_irqd_unmask(struct irq_data *data, bool masked)
+static inline void ioapic_finish_move(struct irq_data *data, bool moveit)
 {
 }
 #endif
@@ -1783,11 +1786,11 @@
 {
 	struct irq_cfg *cfg = irqd_cfg(irq_data);
 	unsigned long v;
-	bool masked;
+	bool moveit;
 	int i;
 
 	irq_complete_move(cfg);
-	masked = ioapic_irqd_mask(irq_data);
+	moveit = ioapic_prepare_move(irq_data);
 
 	/*
 	 * It appears there is an erratum which affects at least version 0x11
@@ -1842,7 +1845,7 @@
 		eoi_ioapic_pin(cfg->vector, irq_data->chip_data);
 	}
 
-	ioapic_irqd_unmask(irq_data, masked);
+	ioapic_finish_move(irq_data, moveit);
 }
 
 static void ioapic_ir_ack_level(struct irq_data *irq_data)
diff -Nur linux-5.4.5/arch/x86/kernel/asm-offsets.c linux-5.4.5-new/arch/x86/kernel/asm-offsets.c
--- linux-5.4.5/arch/x86/kernel/asm-offsets.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/asm-offsets.c	2020-06-15 16:12:09.087774265 +0300
@@ -38,6 +38,10 @@
 #endif
 
 	BLANK();
+#ifdef CONFIG_PREEMPT_LAZY
+	OFFSET(TASK_TI_flags, task_struct, thread_info.flags);
+	OFFSET(TASK_TI_preempt_lazy_count, task_struct, thread_info.preempt_lazy_count);
+#endif
 	OFFSET(TASK_addr_limit, task_struct, thread.addr_limit);
 
 	BLANK();
@@ -92,6 +96,7 @@
 
 	BLANK();
 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
+	DEFINE(_PREEMPT_ENABLED, PREEMPT_ENABLED);
 
 	/* TLB state for the entry code */
 	OFFSET(TLB_STATE_user_pcid_flush_mask, tlb_state, user_pcid_flush_mask);
diff -Nur linux-5.4.5/arch/x86/kernel/cpu/mshyperv.c linux-5.4.5-new/arch/x86/kernel/cpu/mshyperv.c
--- linux-5.4.5/arch/x86/kernel/cpu/mshyperv.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/cpu/mshyperv.c	2020-06-15 16:12:09.019774503 +0300
@@ -77,12 +77,13 @@
 __visible void __irq_entry hv_stimer0_vector_handler(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	entering_irq();
 	inc_irq_stat(hyperv_stimer0_count);
 	if (hv_stimer0_handler)
 		hv_stimer0_handler();
-	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0);
+	add_interrupt_randomness(HYPERV_STIMER0_VECTOR, 0, ip);
 	ack_APIC_irq();
 
 	exiting_irq();
diff -Nur linux-5.4.5/arch/x86/kernel/fpu/core.c linux-5.4.5-new/arch/x86/kernel/fpu/core.c
--- linux-5.4.5/arch/x86/kernel/fpu/core.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/fpu/core.c	2020-06-15 16:12:09.051774391 +0300
@@ -113,6 +113,18 @@
 }
 EXPORT_SYMBOL_GPL(kernel_fpu_end);
 
+void kernel_fpu_resched(void)
+{
+	WARN_ON_FPU(!this_cpu_read(in_kernel_fpu));
+
+	if (should_resched(PREEMPT_OFFSET)) {
+		kernel_fpu_end();
+		cond_resched();
+		kernel_fpu_begin();
+	}
+}
+EXPORT_SYMBOL_GPL(kernel_fpu_resched);
+
 /*
  * Save the FPU state (mark it for reload if necessary):
  *
diff -Nur linux-5.4.5/arch/x86/kernel/irq_32.c linux-5.4.5-new/arch/x86/kernel/irq_32.c
--- linux-5.4.5/arch/x86/kernel/irq_32.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/irq_32.c	2020-06-15 16:12:09.087774265 +0300
@@ -131,6 +131,7 @@
 	return 0;
 }
 
+#ifndef CONFIG_PREEMPT_RT
 void do_softirq_own_stack(void)
 {
 	struct irq_stack *irqstk;
@@ -147,6 +148,7 @@
 
 	call_on_stack(__do_softirq, isp);
 }
+#endif
 
 void handle_irq(struct irq_desc *desc, struct pt_regs *regs)
 {
diff -Nur linux-5.4.5/arch/x86/kernel/process_32.c linux-5.4.5-new/arch/x86/kernel/process_32.c
--- linux-5.4.5/arch/x86/kernel/process_32.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kernel/process_32.c	2020-06-15 16:12:09.087774265 +0300
@@ -38,6 +38,7 @@
 #include <linux/io.h>
 #include <linux/kdebug.h>
 #include <linux/syscalls.h>
+#include <linux/highmem.h>
 
 #include <asm/pgtable.h>
 #include <asm/ldt.h>
@@ -196,6 +197,35 @@
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
+#ifdef CONFIG_PREEMPT_RT
+static void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	int i;
+
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+		pte_t *ptep = kmap_pte - idx;
+
+		kpte_clear_flush(ptep, __fix_to_virt(FIX_KMAP_BEGIN + idx));
+	}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_pte(kmap_pte - idx, next_p->kmap_pte[i]);
+	}
+}
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 
 /*
  *	switch_to(x,y) should switch tasks from x to y.
@@ -266,6 +296,8 @@
 
 	switch_to_extra(prev_p, next_p);
 
+	switch_kmaps(prev_p, next_p);
+
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
diff -Nur linux-5.4.5/arch/x86/kvm/x86.c linux-5.4.5-new/arch/x86/kvm/x86.c
--- linux-5.4.5/arch/x86/kvm/x86.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/kvm/x86.c	2020-06-15 16:12:09.503772805 +0300
@@ -7207,6 +7207,14 @@
 		goto out;
 	}
 
+#ifdef CONFIG_PREEMPT_RT
+	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
+		pr_err("RT requires X86_FEATURE_CONSTANT_TSC\n");
+		r = -EOPNOTSUPP;
+		goto out;
+	}
+#endif
+
 	r = -ENOMEM;
 	x86_fpu_cache = kmem_cache_create("x86_fpu", sizeof(struct fpu),
 					  __alignof__(struct fpu), SLAB_ACCOUNT,
diff -Nur linux-5.4.5/arch/x86/mm/highmem_32.c linux-5.4.5-new/arch/x86/mm/highmem_32.c
--- linux-5.4.5/arch/x86/mm/highmem_32.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/mm/highmem_32.c	2020-06-15 16:12:09.555772623 +0300
@@ -33,10 +33,11 @@
  */
 void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 {
+	pte_t pte = mk_pte(page, prot);
 	unsigned long vaddr;
 	int idx, type;
 
-	preempt_disable();
+	preempt_disable_nort();
 	pagefault_disable();
 
 	if (!PageHighMem(page))
@@ -46,7 +47,10 @@
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 	BUG_ON(!pte_none(*(kmap_pte-idx)));
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte-idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -89,6 +93,9 @@
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 		arch_flush_lazy_mmu_mode();
@@ -101,7 +108,7 @@
 #endif
 
 	pagefault_enable();
-	preempt_enable();
+	preempt_enable_nort();
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
diff -Nur linux-5.4.5/arch/x86/mm/iomap_32.c linux-5.4.5-new/arch/x86/mm/iomap_32.c
--- linux-5.4.5/arch/x86/mm/iomap_32.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/x86/mm/iomap_32.c	2020-06-15 16:12:09.555772623 +0300
@@ -46,6 +46,7 @@
 
 void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 {
+	pte_t pte = pfn_pte(pfn, prot);
 	unsigned long vaddr;
 	int idx, type;
 
@@ -55,7 +56,12 @@
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
+	WARN_ON(!pte_none(*(kmap_pte - idx)));
+
+#ifdef CONFIG_PREEMPT_RT
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte - idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -106,6 +112,9 @@
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 	}
diff -Nur linux-5.4.5/arch/xtensa/include/asm/spinlock_types.h linux-5.4.5-new/arch/xtensa/include/asm/spinlock_types.h
--- linux-5.4.5/arch/xtensa/include/asm/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/xtensa/include/asm/spinlock_types.h	2020-06-15 16:12:09.715772062 +0300
@@ -2,10 +2,6 @@
 #ifndef __ASM_SPINLOCK_TYPES_H
 #define __ASM_SPINLOCK_TYPES_H
 
-#if !defined(__LINUX_SPINLOCK_TYPES_H) && !defined(__ASM_SPINLOCK_H)
-# error "please don't include this file directly"
-#endif
-
 #include <asm-generic/qspinlock_types.h>
 #include <asm-generic/qrwlock_types.h>
 
diff -Nur linux-5.4.5/arch/xtensa/kernel/entry.S linux-5.4.5-new/arch/xtensa/kernel/entry.S
--- linux-5.4.5/arch/xtensa/kernel/entry.S	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/xtensa/kernel/entry.S	2020-06-15 16:12:09.727772020 +0300
@@ -520,7 +520,7 @@
 	call4	schedule	# void schedule (void)
 	j	1b
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 6:
 	_bbci.l	a4, TIF_NEED_RESCHED, 4f
 
diff -Nur linux-5.4.5/arch/xtensa/kernel/traps.c linux-5.4.5-new/arch/xtensa/kernel/traps.c
--- linux-5.4.5/arch/xtensa/kernel/traps.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/arch/xtensa/kernel/traps.c	2020-06-15 16:12:09.727772020 +0300
@@ -524,12 +524,15 @@
 void die(const char * str, struct pt_regs * regs, long err)
 {
 	static int die_counter;
+	const char *pr = "";
+
+	if (IS_ENABLED(CONFIG_PREEMPTION))
+		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
 
 	console_verbose();
 	spin_lock_irq(&die_lock);
 
-	pr_info("%s: sig: %ld [#%d]%s\n", str, err, ++die_counter,
-		IS_ENABLED(CONFIG_PREEMPT) ? " PREEMPT" : "");
+	pr_info("%s: sig: %ld [#%d]%s\n", str, err, ++die_counter, pr);
 	show_regs(regs);
 	if (!user_mode(regs))
 		show_stack(NULL, (unsigned long*)regs->areg[1]);
diff -Nur linux-5.4.5/block/blk-ioc.c linux-5.4.5-new/block/blk-ioc.c
--- linux-5.4.5/block/blk-ioc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/block/blk-ioc.c	2020-06-15 16:12:09.771771866 +0300
@@ -9,6 +9,7 @@
 #include <linux/blkdev.h>
 #include <linux/slab.h>
 #include <linux/sched/task.h>
+#include <linux/delay.h>
 
 #include "blk.h"
 
@@ -115,7 +116,7 @@
 			spin_unlock(&q->queue_lock);
 		} else {
 			spin_unlock_irqrestore(&ioc->lock, flags);
-			cpu_relax();
+			cpu_chill();
 			spin_lock_irqsave_nested(&ioc->lock, flags, 1);
 		}
 	}
diff -Nur linux-5.4.5/block/blk-mq.c linux-5.4.5-new/block/blk-mq.c
--- linux-5.4.5/block/blk-mq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/block/blk-mq.c	2020-06-15 16:12:09.771771866 +0300
@@ -611,9 +611,17 @@
 		return;
 	}
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
+	/*
+	 * Avoid SMP function calls for completions because they acquire
+	 * sleeping spinlocks on RT.
+	 */
+#ifdef CONFIG_PREEMPT_RT
+	shared = true;
+#else
 	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
 		shared = cpus_share_cache(cpu, ctx->cpu);
+#endif
 
 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
 		rq->csd.func = __blk_mq_complete_request_remote;
@@ -623,7 +631,7 @@
 	} else {
 		q->mq_ops->complete(rq);
 	}
-	put_cpu();
+	put_cpu_light();
 }
 
 static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
@@ -1466,14 +1474,14 @@
 		return;
 
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
-		int cpu = get_cpu();
+		int cpu = get_cpu_light();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
 			__blk_mq_run_hw_queue(hctx);
-			put_cpu();
+			put_cpu_light();
 			return;
 		}
 
-		put_cpu();
+		put_cpu_light();
 	}
 
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
diff -Nur linux-5.4.5/block/blk-softirq.c linux-5.4.5-new/block/blk-softirq.c
--- linux-5.4.5/block/blk-softirq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/block/blk-softirq.c	2020-06-15 16:12:09.771771866 +0300
@@ -42,17 +42,13 @@
 static void trigger_softirq(void *data)
 {
 	struct request *rq = data;
-	unsigned long flags;
 	struct list_head *list;
 
-	local_irq_save(flags);
 	list = this_cpu_ptr(&blk_cpu_done);
 	list_add_tail(&rq->ipi_list, list);
 
 	if (list->next == &rq->ipi_list)
 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
-
-	local_irq_restore(flags);
 }
 
 /*
@@ -91,6 +87,7 @@
 			 this_cpu_ptr(&blk_cpu_done));
 	raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	return 0;
 }
@@ -142,6 +139,7 @@
 		goto do_local;
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 static __init int blk_softirq_init(void)
diff -Nur linux-5.4.5/crypto/cryptd.c linux-5.4.5-new/crypto/cryptd.c
--- linux-5.4.5/crypto/cryptd.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/crypto/cryptd.c	2020-06-15 16:12:09.895771431 +0300
@@ -36,6 +36,7 @@
 struct cryptd_cpu_queue {
 	struct crypto_queue queue;
 	struct work_struct work;
+	spinlock_t qlock;
 };
 
 struct cryptd_queue {
@@ -105,6 +106,7 @@
 		cpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);
 		crypto_init_queue(&cpu_queue->queue, max_cpu_qlen);
 		INIT_WORK(&cpu_queue->work, cryptd_queue_worker);
+		spin_lock_init(&cpu_queue->qlock);
 	}
 	pr_info("cryptd: max_cpu_qlen set to %d\n", max_cpu_qlen);
 	return 0;
@@ -129,8 +131,10 @@
 	struct cryptd_cpu_queue *cpu_queue;
 	refcount_t *refcnt;
 
-	cpu = get_cpu();
-	cpu_queue = this_cpu_ptr(queue->cpu_queue);
+	cpu_queue = raw_cpu_ptr(queue->cpu_queue);
+	spin_lock_bh(&cpu_queue->qlock);
+	cpu = smp_processor_id();
+
 	err = crypto_enqueue_request(&cpu_queue->queue, request);
 
 	refcnt = crypto_tfm_ctx(request->tfm);
@@ -146,7 +150,7 @@
 	refcount_inc(refcnt);
 
 out_put_cpu:
-	put_cpu();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	return err;
 }
@@ -162,16 +166,11 @@
 	cpu_queue = container_of(work, struct cryptd_cpu_queue, work);
 	/*
 	 * Only handle one request at a time to avoid hogging crypto workqueue.
-	 * preempt_disable/enable is used to prevent being preempted by
-	 * cryptd_enqueue_request(). local_bh_disable/enable is used to prevent
-	 * cryptd_enqueue_request() being accessed from software interrupts.
 	 */
-	local_bh_disable();
-	preempt_disable();
+	spin_lock_bh(&cpu_queue->qlock);
 	backlog = crypto_get_backlog(&cpu_queue->queue);
 	req = crypto_dequeue_request(&cpu_queue->queue);
-	preempt_enable();
-	local_bh_enable();
+	spin_unlock_bh(&cpu_queue->qlock);
 
 	if (!req)
 		return;
diff -Nur linux-5.4.5/Documentation/printk-ringbuffer.txt linux-5.4.5-new/Documentation/printk-ringbuffer.txt
--- linux-5.4.5/Documentation/printk-ringbuffer.txt	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/Documentation/printk-ringbuffer.txt	2020-06-15 16:12:03.519793776 +0300
@@ -0,0 +1,377 @@
+struct printk_ringbuffer
+------------------------
+John Ogness <john.ogness@linutronix.de>
+
+Overview
+~~~~~~~~
+As the name suggests, this ring buffer was implemented specifically to serve
+the needs of the printk() infrastructure. The ring buffer itself is not
+specific to printk and could be used for other purposes. _However_, the
+requirements and semantics of printk are rather unique. If you intend to use
+this ring buffer for anything other than printk, you need to be very clear on
+its features, behavior, and pitfalls.
+
+Features
+^^^^^^^^
+The printk ring buffer has the following features:
+
+- single global buffer
+- resides in initialized data section (available at early boot)
+- lockless readers
+- supports multiple writers
+- supports multiple non-consuming readers
+- safe from any context (including NMI)
+- groups bytes into variable length blocks (referenced by entries)
+- entries tagged with sequence numbers
+
+Behavior
+^^^^^^^^
+Since the printk ring buffer readers are lockless, there exists no
+synchronization between readers and writers. Basically writers are the tasks
+in control and may overwrite any and all committed data at any time and from
+any context. For this reason readers can miss entries if they are overwritten
+before the reader was able to access the data. The reader API implementation
+is such that reader access to entries is atomic, so there is no risk of
+readers having to deal with partial or corrupt data. Also, entries are
+tagged with sequence numbers so readers can recognize if entries were missed.
+
+Writing to the ring buffer consists of 2 steps. First a writer must reserve
+an entry of desired size. After this step the writer has exclusive access
+to the memory region. Once the data has been written to memory, it needs to
+be committed to the ring buffer. After this step the entry has been inserted
+into the ring buffer and assigned an appropriate sequence number.
+
+Once committed, a writer must no longer access the data directly. This is
+because the data may have been overwritten and no longer exists. If a
+writer must access the data, it should either keep a private copy before
+committing the entry or use the reader API to gain access to the data.
+
+Because of how the data backend is implemented, entries that have been
+reserved but not yet committed act as barriers, preventing future writers
+from filling the ring buffer beyond the location of the reserved but not
+yet committed entry region. For this reason it is *important* that writers
+perform both reserve and commit as quickly as possible. Also, be aware that
+preemption and local interrupts are disabled and writing to the ring buffer
+is processor-reentrant locked during the reserve/commit window. Writers in
+NMI contexts can still preempt any other writers, but as long as these
+writers do not write a large amount of data with respect to the ring buffer
+size, this should not become an issue.
+
+API
+~~~
+
+Declaration
+^^^^^^^^^^^
+The printk ring buffer can be instantiated as a static structure:
+
+ /* declare a static struct printk_ringbuffer */
+ #define DECLARE_STATIC_PRINTKRB(name, szbits, cpulockptr)
+
+The value of szbits specifies the size of the ring buffer in bits. The
+cpulockptr field is a pointer to a prb_cpulock struct that is used to
+perform processor-reentrant spin locking for the writers. It is specified
+externally because it may be used for multiple ring buffers (or other
+code) to synchronize writers without risk of deadlock.
+
+Here is an example of a declaration of a printk ring buffer specifying a
+32KB (2^15) ring buffer:
+
+....
+DECLARE_STATIC_PRINTKRB_CPULOCK(rb_cpulock);
+DECLARE_STATIC_PRINTKRB(rb, 15, &rb_cpulock);
+....
+
+If writers will be using multiple ring buffers and the ordering of that usage
+is not clear, the same prb_cpulock should be used for both ring buffers.
+
+Writer API
+^^^^^^^^^^
+The writer API consists of 2 functions. The first is to reserve an entry in
+the ring buffer, the second is to commit that data to the ring buffer. The
+reserved entry information is stored within a provided `struct prb_handle`.
+
+ /* reserve an entry */
+ char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+                   unsigned int size);
+
+ /* commit a reserved entry to the ring buffer */
+ void prb_commit(struct prb_handle *h);
+
+Here is an example of a function to write data to a ring buffer:
+
+....
+int write_data(struct printk_ringbuffer *rb, char *data, int size)
+{
+    struct prb_handle h;
+    char *buf;
+
+    buf = prb_reserve(&h, rb, size);
+    if (!buf)
+        return -1;
+    memcpy(buf, data, size);
+    prb_commit(&h);
+
+    return 0;
+}
+....
+
+Pitfalls
+++++++++
+Be aware that prb_reserve() can fail. A retry might be successful, but it
+depends entirely on whether or not the next part of the ring buffer to
+overwrite belongs to reserved but not yet committed entries of other writers.
+Writers can use the prb_inc_lost() function to allow readers to notice that a
+message was lost.
+
+Reader API
+^^^^^^^^^^
+The reader API utilizes a `struct prb_iterator` to track the reader's
+position in the ring buffer.
+
+ /* declare a pre-initialized static iterator for a ring buffer */
+ #define DECLARE_STATIC_PRINTKRB_ITER(name, rbaddr)
+
+ /* initialize iterator for a ring buffer (if static macro NOT used) */
+ void prb_iter_init(struct prb_iterator *iter,
+                    struct printk_ringbuffer *rb, u64 *seq);
+
+ /* make a deep copy of an iterator */
+ void prb_iter_copy(struct prb_iterator *dest,
+                    struct prb_iterator *src);
+
+ /* non-blocking, advance to next entry (and read the data) */
+ int prb_iter_next(struct prb_iterator *iter, char *buf,
+                   int size, u64 *seq);
+
+ /* blocking, advance to next entry (and read the data) */
+ int prb_iter_wait_next(struct prb_iterator *iter, char *buf,
+                        int size, u64 *seq);
+
+ /* position iterator at the entry seq */
+ int prb_iter_seek(struct prb_iterator *iter, u64 seq);
+
+ /* read data at current position */
+ int prb_iter_data(struct prb_iterator *iter, char *buf,
+                   int size, u64 *seq);
+
+Typically prb_iter_data() is not needed because the data can be retrieved
+directly with prb_iter_next().
+
+Here is an example of a non-blocking function that will read all the data in
+a ring buffer:
+
+....
+void read_all_data(struct printk_ringbuffer *rb, char *buf, int size)
+{
+    struct prb_iterator iter;
+    u64 prev_seq = 0;
+    u64 seq;
+    int ret;
+
+    prb_iter_init(&iter, rb, NULL);
+
+    for (;;) {
+        ret = prb_iter_next(&iter, buf, size, &seq);
+        if (ret > 0) {
+            if (seq != ++prev_seq) {
+                /* "seq - prev_seq" entries missed */
+                prev_seq = seq;
+            }
+            /* process buf here */
+        } else if (ret == 0) {
+            /* hit the end, done */
+            break;
+        } else if (ret < 0) {
+            /*
+             * iterator is invalid, a writer overtook us, reset the
+             * iterator and keep going, entries were missed
+             */
+            prb_iter_init(&iter, rb, NULL);
+        }
+    }
+}
+....
+
+Pitfalls
+++++++++
+The reader's iterator can become invalid at any time because the reader was
+overtaken by a writer. Typically the reader should reset the iterator back
+to the current oldest entry (which will be newer than the entry the reader
+was at) and continue, noting the number of entries that were missed.
+
+Utility API
+^^^^^^^^^^^
+Several functions are available as convenience for external code.
+
+ /* query the size of the data buffer */
+ int prb_buffer_size(struct printk_ringbuffer *rb);
+
+ /* skip a seq number to signify a lost record */
+ void prb_inc_lost(struct printk_ringbuffer *rb);
+
+ /* processor-reentrant spin lock */
+ void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+
+ /* processor-reentrant spin unlock */
+ void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+
+Pitfalls
+++++++++
+Although the value returned by prb_buffer_size() does represent an absolute
+upper bound, the amount of data that can be stored within the ring buffer
+is actually less because of the additional storage space of a header for each
+entry.
+
+The prb_lock() and prb_unlock() functions can be used to synchronize between
+ring buffer writers and other external activities. The function of a
+processor-reentrant spin lock is to disable preemption and local interrupts
+and synchronize against other processors. It does *not* protect against
+multiple contexts of a single processor, i.e NMI.
+
+Implementation
+~~~~~~~~~~~~~~
+This section describes several of the implementation concepts and details to
+help developers better understand the code.
+
+Entries
+^^^^^^^
+All ring buffer data is stored within a single static byte array. The reason
+for this is to ensure that any pointers to the data (past and present) will
+always point to valid memory. This is important because the lockless readers
+may be preempted for long periods of time and when they resume may be working
+with expired pointers.
+
+Entries are identified by start index and size. (The start index plus size
+is the start index of the next entry.) The start index is not simply an
+offset into the byte array, but rather a logical position (lpos) that maps
+directly to byte array offsets.
+
+For example, for a byte array of 1000, an entry may have have a start index
+of 100. Another entry may have a start index of 1100. And yet another 2100.
+All of these entry are pointing to the same memory region, but only the most
+recent entry is valid. The other entries are pointing to valid memory, but
+represent entries that have been overwritten.
+
+Note that due to overflowing, the most recent entry is not necessarily the one
+with the highest lpos value. Indeed, the printk ring buffer initializes its
+data such that an overflow happens relatively quickly in order to validate the
+handling of this situation. The implementation assumes that an lpos (unsigned
+long) will never completely wrap while a reader is preempted. If this were to
+become an issue, the seq number (which never wraps) could be used to increase
+the robustness of handling this situation.
+
+Buffer Wrapping
+^^^^^^^^^^^^^^^
+If an entry starts near the end of the byte array but would extend beyond it,
+a special terminating entry (size = -1) is inserted into the byte array and
+the real entry is placed at the beginning of the byte array. This can waste
+space at the end of the byte array, but simplifies the implementation by
+allowing writers to always work with contiguous buffers.
+
+Note that the size field is the first 4 bytes of the entry header. Also note
+that calc_next() always ensures that there are at least 4 bytes left at the
+end of the byte array to allow room for a terminating entry.
+
+Ring Buffer Pointers
+^^^^^^^^^^^^^^^^^^^^
+Three pointers (lpos values) are used to manage the ring buffer:
+
+ - _tail_: points to the oldest entry
+ - _head_: points to where the next new committed entry will be
+ - _reserve_: points to where the next new reserved entry will be
+
+These pointers always maintain a logical ordering:
+
+ tail <= head <= reserve
+
+The reserve pointer moves forward when a writer reserves a new entry. The
+head pointer moves forward when a writer commits a new entry.
+
+The reserve pointer cannot overwrite the tail pointer in a wrap situation. In
+such a situation, the tail pointer must be "pushed forward", thus
+invalidating that oldest entry. Readers identify if they are accessing a
+valid entry by ensuring their entry pointer is `>= tail && < head`.
+
+If the tail pointer is equal to the head pointer, it cannot be pushed and any
+reserve operation will fail. The only resolution is for writers to commit
+their reserved entries.
+
+Processor-Reentrant Locking
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+The purpose of the processor-reentrant locking is to limit the interruption
+scenarios of writers to 2 contexts. This allows for a simplified
+implementation where:
+
+- The reserve/commit window only exists on 1 processor at a time. A reserve
+  can never fail due to uncommitted entries of other processors.
+
+- When committing entries, it is trivial to handle the situation when
+  subsequent entries have already been committed, i.e. managing the head
+  pointer.
+
+Performance
+~~~~~~~~~~~
+Some basic tests were performed on a quad Intel(R) Xeon(R) CPU E5-2697 v4 at
+2.30GHz (36 cores / 72 threads). All tests involved writing a total of
+32,000,000 records at an average of 33 bytes each. Each writer was pinned to
+its own CPU and would write as fast as it could until a total of 32,000,000
+records were written. All tests involved 2 readers that were both pinned
+together to another CPU. Each reader would read as fast as it could and track
+how many of the 32,000,000 records it could read. All tests used a ring buffer
+of 16KB in size, which holds around 350 records (header + data for each
+entry).
+
+The only difference between the tests is the number of writers (and thus also
+the number of records per writer). As more writers are added, the time to
+write a record increases. This is because data pointers, modified via cmpxchg,
+and global data access in general become more contended.
+
+1 writer
+^^^^^^^^
+ runtime: 0m 18s
+ reader1: 16219900/32000000 (50%) records
+ reader2: 16141582/32000000 (50%) records
+
+2 writers
+^^^^^^^^^
+ runtime: 0m 32s
+ reader1: 16327957/32000000 (51%) records
+ reader2: 16313988/32000000 (50%) records
+
+4 writers
+^^^^^^^^^
+ runtime: 0m 42s
+ reader1: 16421642/32000000 (51%) records
+ reader2: 16417224/32000000 (51%) records
+
+8 writers
+^^^^^^^^^
+ runtime: 0m 43s
+ reader1: 16418300/32000000 (51%) records
+ reader2: 16432222/32000000 (51%) records
+
+16 writers
+^^^^^^^^^^
+ runtime: 0m 54s
+ reader1: 16539189/32000000 (51%) records
+ reader2: 16542711/32000000 (51%) records
+
+32 writers
+^^^^^^^^^^
+ runtime: 1m 13s
+ reader1: 16731808/32000000 (52%) records
+ reader2: 16735119/32000000 (52%) records
+
+Comments
+^^^^^^^^
+It is particularly interesting to compare/contrast the 1-writer and 32-writer
+tests. Despite the writing of the 32,000,000 records taking over 4 times
+longer, the readers (which perform no cmpxchg) were still unable to keep up.
+This shows that the memory contention between the increasing number of CPUs
+also has a dramatic effect on readers.
+
+It should also be noted that in all cases each reader was able to read >=50%
+of the records. This means that a single reader would have been able to keep
+up with the writer(s) in all cases, becoming slightly easier as more writers
+are added. This was the purpose of pinning 2 readers to 1 CPU: to observe how
+maximum reader performance changes.
diff -Nur linux-5.4.5/Documentation/RCU/checklist.txt linux-5.4.5-new/Documentation/RCU/checklist.txt
--- linux-5.4.5/Documentation/RCU/checklist.txt	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/checklist.txt	2020-06-15 16:12:03.711793104 +0300
@@ -210,8 +210,8 @@
 	the rest of the system.
 
 7.	As of v4.20, a given kernel implements only one RCU flavor,
-	which is RCU-sched for PREEMPT=n and RCU-preempt for PREEMPT=y.
-	If the updater uses call_rcu() or synchronize_rcu(),
+	which is RCU-sched for PREEMPTION=n and RCU-preempt for
+	PREEMPTION=y. If the updater uses call_rcu() or synchronize_rcu(),
 	then the corresponding readers my use rcu_read_lock() and
 	rcu_read_unlock(), rcu_read_lock_bh() and rcu_read_unlock_bh(),
 	or any pair of primitives that disables and re-enables preemption,
diff -Nur linux-5.4.5/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html linux-5.4.5-new/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html
--- linux-5.4.5/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html	2020-06-15 16:12:03.703793132 +0300
@@ -56,8 +56,8 @@
 RCU-preempt Expedited Grace Periods</a></h2>
 
 <p>
-<tt>CONFIG_PREEMPT=y</tt> kernels implement RCU-preempt.
-The overall flow of the handling of a given CPU by an RCU-preempt
+<tt>CONFIG_PREEMPT=y</tt> and <tt>CONFIG_PREEMPT_RT=y</tt> kernels implement
+RCU-preempt. The overall flow of the handling of a given CPU by an RCU-preempt
 expedited grace period is shown in the following diagram:
 
 <p><img src="ExpRCUFlow.svg" alt="ExpRCUFlow.svg" width="55%">
@@ -140,8 +140,8 @@
 RCU-sched Expedited Grace Periods</a></h2>
 
 <p>
-<tt>CONFIG_PREEMPT=n</tt> kernels implement RCU-sched.
-The overall flow of the handling of a given CPU by an RCU-sched
+<tt>CONFIG_PREEMPT=n</tt> and <tt>CONFIG_PREEMPT_RT=n</tt> kernels implement
+RCU-sched. The overall flow of the handling of a given CPU by an RCU-sched
 expedited grace period is shown in the following diagram:
 
 <p><img src="ExpSchedFlow.svg" alt="ExpSchedFlow.svg" width="55%">
diff -Nur linux-5.4.5/Documentation/RCU/Design/Requirements/Requirements.html linux-5.4.5-new/Documentation/RCU/Design/Requirements/Requirements.html
--- linux-5.4.5/Documentation/RCU/Design/Requirements/Requirements.html	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/Design/Requirements/Requirements.html	2020-06-15 16:12:03.707793118 +0300
@@ -106,7 +106,7 @@
 Production-quality implementations of <tt>rcu_read_lock()</tt> and
 <tt>rcu_read_unlock()</tt> are extremely lightweight, and in
 fact have exactly zero overhead in Linux kernels built for production
-use with <tt>CONFIG_PREEMPT=n</tt>.
+use with <tt>CONFIG_PREEMPTION=n</tt>.
 
 <p>
 This guarantee allows ordering to be enforced with extremely low
@@ -1499,7 +1499,7 @@
 However, as I learned from Matt Mackall's
 <a href="http://elinux.org/Linux_Tiny-FAQ">bloatwatch</a>
 efforts, memory footprint is critically important on single-CPU systems with
-non-preemptible (<tt>CONFIG_PREEMPT=n</tt>) kernels, and thus
+non-preemptible (<tt>CONFIG_PREEMPTION=n</tt>) kernels, and thus
 <a href="https://lkml.kernel.org/g/20090113221724.GA15307@linux.vnet.ibm.com">tiny RCU</a>
 was born.
 Josh Triplett has since taken over the small-memory banner with his
@@ -1887,7 +1887,7 @@
 <p>
 Implementations of RCU for which <tt>rcu_read_lock()</tt>
 and <tt>rcu_read_unlock()</tt> generate no code, such as
-Linux-kernel RCU when <tt>CONFIG_PREEMPT=n</tt>, can be
+Linux-kernel RCU when <tt>CONFIG_PREEMPTION=n</tt>, can be
 nested arbitrarily deeply.
 After all, there is no overhead.
 Except that if all these instances of <tt>rcu_read_lock()</tt>
@@ -2229,7 +2229,7 @@
 <p>
 However, once the scheduler has spawned its first kthread, this early
 boot trick fails for <tt>synchronize_rcu()</tt> (as well as for
-<tt>synchronize_rcu_expedited()</tt>) in <tt>CONFIG_PREEMPT=y</tt>
+<tt>synchronize_rcu_expedited()</tt>) in <tt>CONFIG_PREEMPTION=y</tt>
 kernels.
 The reason is that an RCU read-side critical section might be preempted,
 which means that a subsequent <tt>synchronize_rcu()</tt> really does have
@@ -2568,7 +2568,7 @@
 
 <p>
 If the compiler did make this transformation in a
-<tt>CONFIG_PREEMPT=n</tt> kernel build, and if <tt>get_user()</tt> did
+<tt>CONFIG_PREEMPTION=n</tt> kernel build, and if <tt>get_user()</tt> did
 page fault, the result would be a quiescent state in the middle
 of an RCU read-side critical section.
 This misplaced quiescent state could result in line&nbsp;4 being
@@ -2906,7 +2906,7 @@
 The real-time-latency response requirements are such that the
 traditional approach of disabling preemption across RCU
 read-side critical sections is inappropriate.
-Kernels built with <tt>CONFIG_PREEMPT=y</tt> therefore
+Kernels built with <tt>CONFIG_PREEMPTION=y</tt> therefore
 use an RCU implementation that allows RCU read-side critical
 sections to be preempted.
 This requirement made its presence known after users made it
@@ -3064,7 +3064,7 @@
 <tt>rcu_barrier_bh()</tt>, and
 <tt>rcu_read_lock_bh_held()</tt>.
 However, the update-side APIs are now simple wrappers for other RCU
-flavors, namely RCU-sched in CONFIG_PREEMPT=n kernels and RCU-preempt
+flavors, namely RCU-sched in CONFIG_PREEMPTION=n kernels and RCU-preempt
 otherwise.
 
 <h3><a name="Sched Flavor">Sched Flavor (Historical)</a></h3>
@@ -3088,12 +3088,12 @@
 Therefore, <i>RCU-sched</i> was created, which follows &ldquo;classic&rdquo;
 RCU in that an RCU-sched grace period waits for for pre-existing
 interrupt and NMI handlers.
-In kernels built with <tt>CONFIG_PREEMPT=n</tt>, the RCU and RCU-sched
+In kernels built with <tt>CONFIG_PREEMPTION=n</tt>, the RCU and RCU-sched
 APIs have identical implementations, while kernels built with
-<tt>CONFIG_PREEMPT=y</tt> provide a separate implementation for each.
+<tt>CONFIG_PREEMPTION=y</tt> provide a separate implementation for each.
 
 <p>
-Note well that in <tt>CONFIG_PREEMPT=y</tt> kernels,
+Note well that in <tt>CONFIG_PREEMPTION=y</tt> kernels,
 <tt>rcu_read_lock_sched()</tt> and <tt>rcu_read_unlock_sched()</tt>
 disable and re-enable preemption, respectively.
 This means that if there was a preemption attempt during the
@@ -3302,12 +3302,12 @@
 <tt>call_rcu_tasks()</tt>,
 <tt>synchronize_rcu_tasks()</tt>, and
 <tt>rcu_barrier_tasks()</tt>.
-In <tt>CONFIG_PREEMPT=n</tt> kernels, trampolines cannot be preempted,
+In <tt>CONFIG_PREEMPTION=n</tt> kernels, trampolines cannot be preempted,
 so these APIs map to
 <tt>call_rcu()</tt>,
 <tt>synchronize_rcu()</tt>, and
 <tt>rcu_barrier()</tt>, respectively.
-In <tt>CONFIG_PREEMPT=y</tt> kernels, trampolines can be preempted,
+In <tt>CONFIG_PREEMPTION=y</tt> kernels, trampolines can be preempted,
 and these three APIs are therefore implemented by separate functions
 that check for voluntary context switches.
 
diff -Nur linux-5.4.5/Documentation/RCU/rcubarrier.txt linux-5.4.5-new/Documentation/RCU/rcubarrier.txt
--- linux-5.4.5/Documentation/RCU/rcubarrier.txt	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/rcubarrier.txt	2020-06-15 16:12:03.711793104 +0300
@@ -6,8 +6,8 @@
 of as a replacement for read-writer locking (among other things), but with
 very low-overhead readers that are immune to deadlock, priority inversion,
 and unbounded latency. RCU read-side critical sections are delimited
-by rcu_read_lock() and rcu_read_unlock(), which, in non-CONFIG_PREEMPT
-kernels, generate no code whatsoever.
+by rcu_read_lock() and rcu_read_unlock(), which, in
+non-CONFIG_PREEMPTION kernels, generate no code whatsoever.
 
 This means that RCU writers are unaware of the presence of concurrent
 readers, so that RCU updates to shared data must be undertaken quite
@@ -303,10 +303,10 @@
 	to smp_call_function() and further to smp_call_function_on_cpu(),
 	causing this latter to spin until the cross-CPU invocation of
 	rcu_barrier_func() has completed. This by itself would prevent
-	a grace period from completing on non-CONFIG_PREEMPT kernels,
+	a grace period from completing on non-CONFIG_PREEMPTION kernels,
 	since each CPU must undergo a context switch (or other quiescent
 	state) before the grace period can complete. However, this is
-	of no use in CONFIG_PREEMPT kernels.
+	of no use in CONFIG_PREEMPTION kernels.
 
 	Therefore, on_each_cpu() disables preemption across its call
 	to smp_call_function() and also across the local call to
diff -Nur linux-5.4.5/Documentation/RCU/stallwarn.txt linux-5.4.5-new/Documentation/RCU/stallwarn.txt
--- linux-5.4.5/Documentation/RCU/stallwarn.txt	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/stallwarn.txt	2020-06-15 16:12:03.711793104 +0300
@@ -20,7 +20,7 @@
 
 o	A CPU looping with bottom halves disabled.
 
-o	For !CONFIG_PREEMPT kernels, a CPU looping anywhere in the kernel
+o	For !CONFIG_PREEMPTION kernels, a CPU looping anywhere in the kernel
 	without invoking schedule().  If the looping in the kernel is
 	really expected and desirable behavior, you might need to add
 	some calls to cond_resched().
@@ -39,7 +39,7 @@
 	result in the "rcu_.*kthread starved for" console-log message,
 	which will include additional debugging information.
 
-o	A CPU-bound real-time task in a CONFIG_PREEMPT kernel, which might
+o	A CPU-bound real-time task in a CONFIG_PREEMPTION kernel, which might
 	happen to preempt a low-priority task in the middle of an RCU
 	read-side critical section.   This is especially damaging if
 	that low-priority task is not permitted to run on any other CPU,
diff -Nur linux-5.4.5/Documentation/RCU/whatisRCU.txt linux-5.4.5-new/Documentation/RCU/whatisRCU.txt
--- linux-5.4.5/Documentation/RCU/whatisRCU.txt	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/RCU/whatisRCU.txt	2020-06-15 16:12:03.711793104 +0300
@@ -648,9 +648,10 @@
 
 This section presents a "toy" RCU implementation that is based on
 "classic RCU".  It is also short on performance (but only for updates) and
-on features such as hotplug CPU and the ability to run in CONFIG_PREEMPT
-kernels.  The definitions of rcu_dereference() and rcu_assign_pointer()
-are the same as those shown in the preceding section, so they are omitted.
+on features such as hotplug CPU and the ability to run in
+CONFIG_PREEMPTION kernels. The definitions of rcu_dereference() and
+rcu_assign_pointer() are the same as those shown in the preceding
+section, so they are omitted.
 
 	void rcu_read_lock(void) { }
 
diff -Nur linux-5.4.5/Documentation/trace/ftrace-uses.rst linux-5.4.5-new/Documentation/trace/ftrace-uses.rst
--- linux-5.4.5/Documentation/trace/ftrace-uses.rst	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/Documentation/trace/ftrace-uses.rst	2020-06-15 16:12:04.991788621 +0300
@@ -146,7 +146,7 @@
 	itself or any nested functions that those functions call.
 
 	If this flag is set, it is possible that the callback will also
-	be called with preemption enabled (when CONFIG_PREEMPT is set),
+	be called with preemption enabled (when CONFIG_PREEMPTION is set),
 	but this is not guaranteed.
 
 FTRACE_OPS_FL_IPMODIFY
diff -Nur linux-5.4.5/drivers/block/zram/zcomp.c linux-5.4.5-new/drivers/block/zram/zcomp.c
--- linux-5.4.5/drivers/block/zram/zcomp.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/block/zram/zcomp.c	2020-06-15 16:12:10.371769761 +0300
@@ -113,12 +113,20 @@
 
 struct zcomp_strm *zcomp_stream_get(struct zcomp *comp)
 {
-	return *get_cpu_ptr(comp->stream);
+	struct zcomp_strm *zstrm;
+
+	zstrm = *get_local_ptr(comp->stream);
+	spin_lock(&zstrm->zcomp_lock);
+	return zstrm;
 }
 
 void zcomp_stream_put(struct zcomp *comp)
 {
-	put_cpu_ptr(comp->stream);
+	struct zcomp_strm *zstrm;
+
+	zstrm = *this_cpu_ptr(comp->stream);
+	spin_unlock(&zstrm->zcomp_lock);
+	put_local_ptr(zstrm);
 }
 
 int zcomp_compress(struct zcomp_strm *zstrm,
@@ -168,6 +176,7 @@
 		pr_err("Can't allocate a compression stream\n");
 		return -ENOMEM;
 	}
+	spin_lock_init(&zstrm->zcomp_lock);
 	*per_cpu_ptr(comp->stream, cpu) = zstrm;
 	return 0;
 }
diff -Nur linux-5.4.5/drivers/block/zram/zcomp.h linux-5.4.5-new/drivers/block/zram/zcomp.h
--- linux-5.4.5/drivers/block/zram/zcomp.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/block/zram/zcomp.h	2020-06-15 16:12:10.371769761 +0300
@@ -10,6 +10,7 @@
 	/* compression/decompression buffer */
 	void *buffer;
 	struct crypto_comp *tfm;
+	spinlock_t zcomp_lock;
 };
 
 /* dynamic per-device compression frontend */
diff -Nur linux-5.4.5/drivers/block/zram/zram_drv.c linux-5.4.5-new/drivers/block/zram/zram_drv.c
--- linux-5.4.5/drivers/block/zram/zram_drv.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/block/zram/zram_drv.c	2020-06-15 16:12:10.371769761 +0300
@@ -55,6 +55,40 @@
 static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
 				u32 index, int offset, struct bio *bio);
 
+#ifdef CONFIG_PREEMPT_RT
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages)
+{
+	size_t index;
+
+	for (index = 0; index < num_pages; index++)
+		spin_lock_init(&zram->table[index].lock);
+}
+
+static int zram_slot_trylock(struct zram *zram, u32 index)
+{
+	int ret;
+
+	ret = spin_trylock(&zram->table[index].lock);
+	if (ret)
+		__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+	return ret;
+}
+
+static void zram_slot_lock(struct zram *zram, u32 index)
+{
+	spin_lock(&zram->table[index].lock);
+	__set_bit(ZRAM_LOCK, &zram->table[index].flags);
+}
+
+static void zram_slot_unlock(struct zram *zram, u32 index)
+{
+	__clear_bit(ZRAM_LOCK, &zram->table[index].flags);
+	spin_unlock(&zram->table[index].lock);
+}
+
+#else
+
+static void zram_meta_init_table_locks(struct zram *zram, size_t num_pages) { }
 
 static int zram_slot_trylock(struct zram *zram, u32 index)
 {
@@ -70,6 +104,7 @@
 {
 	bit_spin_unlock(ZRAM_LOCK, &zram->table[index].flags);
 }
+#endif
 
 static inline bool init_done(struct zram *zram)
 {
@@ -1155,6 +1190,7 @@
 
 	if (!huge_class_size)
 		huge_class_size = zs_huge_class_size(zram->mem_pool);
+	zram_meta_init_table_locks(zram, num_pages);
 	return true;
 }
 
@@ -1217,6 +1253,7 @@
 	unsigned long handle;
 	unsigned int size;
 	void *src, *dst;
+	struct zcomp_strm *zstrm;
 
 	zram_slot_lock(zram, index);
 	if (zram_test_flag(zram, index, ZRAM_WB)) {
@@ -1247,6 +1284,7 @@
 
 	size = zram_get_obj_size(zram, index);
 
+	zstrm = zcomp_stream_get(zram->comp);
 	src = zs_map_object(zram->mem_pool, handle, ZS_MM_RO);
 	if (size == PAGE_SIZE) {
 		dst = kmap_atomic(page);
@@ -1254,14 +1292,13 @@
 		kunmap_atomic(dst);
 		ret = 0;
 	} else {
-		struct zcomp_strm *zstrm = zcomp_stream_get(zram->comp);
 
 		dst = kmap_atomic(page);
 		ret = zcomp_decompress(zstrm, src, size, dst);
 		kunmap_atomic(dst);
-		zcomp_stream_put(zram->comp);
 	}
 	zs_unmap_object(zram->mem_pool, handle);
+	zcomp_stream_put(zram->comp);
 	zram_slot_unlock(zram, index);
 
 	/* Should NEVER happen. Return bio error if it does. */
diff -Nur linux-5.4.5/drivers/block/zram/zram_drv.h linux-5.4.5-new/drivers/block/zram/zram_drv.h
--- linux-5.4.5/drivers/block/zram/zram_drv.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/block/zram/zram_drv.h	2020-06-15 16:12:10.371769761 +0300
@@ -63,6 +63,7 @@
 		unsigned long element;
 	};
 	unsigned long flags;
+	spinlock_t lock;
 #ifdef CONFIG_ZRAM_MEMORY_TRACKING
 	ktime_t ac_time;
 #endif
diff -Nur linux-5.4.5/drivers/char/random.c linux-5.4.5-new/drivers/char/random.c
--- linux-5.4.5/drivers/char/random.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/char/random.c	2020-06-15 16:12:10.487769354 +0300
@@ -1305,28 +1305,27 @@
 	return *ptr;
 }
 
-void add_interrupt_randomness(int irq, int irq_flags)
+void add_interrupt_randomness(int irq, int irq_flags, __u64 ip)
 {
 	struct entropy_store	*r;
 	struct fast_pool	*fast_pool = this_cpu_ptr(&irq_randomness);
-	struct pt_regs		*regs = get_irq_regs();
 	unsigned long		now = jiffies;
 	cycles_t		cycles = random_get_entropy();
 	__u32			c_high, j_high;
-	__u64			ip;
 	unsigned long		seed;
 	int			credit = 0;
 
 	if (cycles == 0)
-		cycles = get_reg(fast_pool, regs);
+		cycles = get_reg(fast_pool, NULL);
 	c_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;
 	j_high = (sizeof(now) > 4) ? now >> 32 : 0;
 	fast_pool->pool[0] ^= cycles ^ j_high ^ irq;
 	fast_pool->pool[1] ^= now ^ c_high;
-	ip = regs ? instruction_pointer(regs) : _RET_IP_;
+	if (!ip)
+		ip = _RET_IP_;
 	fast_pool->pool[2] ^= ip;
 	fast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :
-		get_reg(fast_pool, regs);
+		get_reg(fast_pool, NULL);
 
 	fast_mix(fast_pool);
 	add_interrupt_bench(cycles);
diff -Nur linux-5.4.5/drivers/char/tpm/tpm-dev-common.c linux-5.4.5-new/drivers/char/tpm/tpm-dev-common.c
--- linux-5.4.5/drivers/char/tpm/tpm-dev-common.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/char/tpm/tpm-dev-common.c	2020-06-15 16:12:10.483769368 +0300
@@ -20,7 +20,6 @@
 #include "tpm-dev.h"
 
 static struct workqueue_struct *tpm_dev_wq;
-static DEFINE_MUTEX(tpm_dev_wq_lock);
 
 static ssize_t tpm_dev_transmit(struct tpm_chip *chip, struct tpm_space *space,
 				u8 *buf, size_t bufsiz)
diff -Nur linux-5.4.5/drivers/char/tpm/tpm_tis.c linux-5.4.5-new/drivers/char/tpm/tpm_tis.c
--- linux-5.4.5/drivers/char/tpm/tpm_tis.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/char/tpm/tpm_tis.c	2020-06-15 16:12:10.483769368 +0300
@@ -49,6 +49,31 @@
 	return container_of(data, struct tpm_tis_tcg_phy, priv);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Flushes previous write operations to chip so that a subsequent
+ * ioread*()s won't stall a cpu.
+ */
+static inline void tpm_tis_flush(void __iomem *iobase)
+{
+	ioread8(iobase + TPM_ACCESS(0));
+}
+#else
+#define tpm_tis_flush(iobase) do { } while (0)
+#endif
+
+static inline void tpm_tis_iowrite8(u8 b, void __iomem *iobase, u32 addr)
+{
+	iowrite8(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
+static inline void tpm_tis_iowrite32(u32 b, void __iomem *iobase, u32 addr)
+{
+	iowrite32(b, iobase + addr);
+	tpm_tis_flush(iobase);
+}
+
 static bool interrupts = true;
 module_param(interrupts, bool, 0444);
 MODULE_PARM_DESC(interrupts, "Enable interrupts");
@@ -146,7 +171,7 @@
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
 	while (len--)
-		iowrite8(*value++, phy->iobase + addr);
+		tpm_tis_iowrite8(*value++, phy->iobase, addr);
 
 	return 0;
 }
@@ -173,7 +198,7 @@
 {
 	struct tpm_tis_tcg_phy *phy = to_tpm_tis_tcg_phy(data);
 
-	iowrite32(value, phy->iobase + addr);
+	tpm_tis_iowrite32(value, phy->iobase, addr);
 
 	return 0;
 }
diff -Nur linux-5.4.5/drivers/clocksource/Kconfig linux-5.4.5-new/drivers/clocksource/Kconfig
--- linux-5.4.5/drivers/clocksource/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/clocksource/Kconfig	2020-06-15 16:12:10.795768273 +0300
@@ -434,6 +434,13 @@
 	help
 	  Support for Timer Counter Blocks on Atmel SoCs.
 
+config ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
+	bool "TC Block use 32 KiHz clock"
+	depends on ATMEL_TCB_CLKSRC
+	default y
+	help
+	  Select this to use 32 KiHz base clock rate as TC block clock.
+
 config CLKSRC_EXYNOS_MCT
 	bool "Exynos multi core timer driver" if COMPILE_TEST
 	depends on ARM || ARM64
diff -Nur linux-5.4.5/drivers/clocksource/timer-atmel-tcb.c linux-5.4.5-new/drivers/clocksource/timer-atmel-tcb.c
--- linux-5.4.5/drivers/clocksource/timer-atmel-tcb.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/clocksource/timer-atmel-tcb.c	2020-06-15 16:12:10.799768260 +0300
@@ -28,8 +28,7 @@
  *     this 32 bit free-running counter. the second channel is not used.
  *
  *   - The third channel may be used to provide a 16-bit clockevent
- *     source, used in either periodic or oneshot mode.  This runs
- *     at 32 KiHZ, and can handle delays of up to two seconds.
+ *     source, used in either periodic or oneshot mode.
  *
  * REVISIT behavior during system suspend states... we should disable
  * all clocks and save the power.  Easily done for clockevent devices,
@@ -143,6 +142,8 @@
 struct tc_clkevt_device {
 	struct clock_event_device	clkevt;
 	struct clk			*clk;
+	bool				clk_enabled;
+	u32				freq;
 	void __iomem			*regs;
 };
 
@@ -151,15 +152,26 @@
 	return container_of(clkevt, struct tc_clkevt_device, clkevt);
 }
 
-/* For now, we always use the 32K clock ... this optimizes for NO_HZ,
- * because using one of the divided clocks would usually mean the
- * tick rate can never be less than several dozen Hz (vs 0.5 Hz).
- *
- * A divided clock could be good for high resolution timers, since
- * 30.5 usec resolution can seem "low".
- */
 static u32 timer_clock;
 
+static void tc_clk_disable(struct clock_event_device *d)
+{
+	struct tc_clkevt_device *tcd = to_tc_clkevt(d);
+
+	clk_disable(tcd->clk);
+	tcd->clk_enabled = false;
+}
+
+static void tc_clk_enable(struct clock_event_device *d)
+{
+	struct tc_clkevt_device *tcd = to_tc_clkevt(d);
+
+	if (tcd->clk_enabled)
+		return;
+	clk_enable(tcd->clk);
+	tcd->clk_enabled = true;
+}
+
 static int tc_shutdown(struct clock_event_device *d)
 {
 	struct tc_clkevt_device *tcd = to_tc_clkevt(d);
@@ -167,8 +179,14 @@
 
 	writel(0xff, regs + ATMEL_TC_REG(2, IDR));
 	writel(ATMEL_TC_CLKDIS, regs + ATMEL_TC_REG(2, CCR));
+	return 0;
+}
+
+static int tc_shutdown_clk_off(struct clock_event_device *d)
+{
+	tc_shutdown(d);
 	if (!clockevent_state_detached(d))
-		clk_disable(tcd->clk);
+		tc_clk_disable(d);
 
 	return 0;
 }
@@ -181,9 +199,9 @@
 	if (clockevent_state_oneshot(d) || clockevent_state_periodic(d))
 		tc_shutdown(d);
 
-	clk_enable(tcd->clk);
+	tc_clk_enable(d);
 
-	/* slow clock, count up to RC, then irq and stop */
+	/* count up to RC, then irq and stop */
 	writel(timer_clock | ATMEL_TC_CPCSTOP | ATMEL_TC_WAVE |
 		     ATMEL_TC_WAVESEL_UP_AUTO, regs + ATMEL_TC_REG(2, CMR));
 	writel(ATMEL_TC_CPCS, regs + ATMEL_TC_REG(2, IER));
@@ -203,12 +221,12 @@
 	/* By not making the gentime core emulate periodic mode on top
 	 * of oneshot, we get lower overhead and improved accuracy.
 	 */
-	clk_enable(tcd->clk);
+	tc_clk_enable(d);
 
-	/* slow clock, count up to RC, then irq and restart */
+	/* count up to RC, then irq and restart */
 	writel(timer_clock | ATMEL_TC_WAVE | ATMEL_TC_WAVESEL_UP_AUTO,
 		     regs + ATMEL_TC_REG(2, CMR));
-	writel((32768 + HZ / 2) / HZ, tcaddr + ATMEL_TC_REG(2, RC));
+	writel((tcd->freq + HZ / 2) / HZ, tcaddr + ATMEL_TC_REG(2, RC));
 
 	/* Enable clock and interrupts on RC compare */
 	writel(ATMEL_TC_CPCS, regs + ATMEL_TC_REG(2, IER));
@@ -234,9 +252,13 @@
 		.features		= CLOCK_EVT_FEAT_PERIODIC |
 					  CLOCK_EVT_FEAT_ONESHOT,
 		/* Should be lower than at91rm9200's system timer */
+#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
 		.rating			= 125,
+#else
+		.rating			= 200,
+#endif
 		.set_next_event		= tc_next_event,
-		.set_state_shutdown	= tc_shutdown,
+		.set_state_shutdown	= tc_shutdown_clk_off,
 		.set_state_periodic	= tc_set_periodic,
 		.set_state_oneshot	= tc_set_oneshot,
 	},
@@ -256,8 +278,11 @@
 	return IRQ_NONE;
 }
 
-static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
+static const u8 atmel_tcb_divisors[5] = { 2, 8, 32, 128, 0, };
+
+static int __init setup_clkevents(struct atmel_tc *tc, int divisor_idx)
 {
+	unsigned divisor = atmel_tcb_divisors[divisor_idx];
 	int ret;
 	struct clk *t2_clk = tc->clk[2];
 	int irq = tc->irq[2];
@@ -278,7 +303,11 @@
 	clkevt.regs = tc->regs;
 	clkevt.clk = t2_clk;
 
-	timer_clock = clk32k_divisor_idx;
+	timer_clock = divisor_idx;
+	if (!divisor)
+		clkevt.freq = 32768;
+	else
+		clkevt.freq = clk_get_rate(t2_clk) / divisor;
 
 	clkevt.clkevt.cpumask = cpumask_of(0);
 
@@ -289,7 +318,7 @@
 		return ret;
 	}
 
-	clockevents_config_and_register(&clkevt.clkevt, 32768, 1, 0xffff);
+	clockevents_config_and_register(&clkevt.clkevt, clkevt.freq, 1, 0xffff);
 
 	return ret;
 }
@@ -346,8 +375,6 @@
 	writel(ATMEL_TC_SYNC, tcaddr + ATMEL_TC_BCR);
 }
 
-static const u8 atmel_tcb_divisors[5] = { 2, 8, 32, 128, 0, };
-
 static const struct of_device_id atmel_tcb_of_match[] = {
 	{ .compatible = "atmel,at91rm9200-tcb", .data = (void *)16, },
 	{ .compatible = "atmel,at91sam9x5-tcb", .data = (void *)32, },
@@ -467,7 +494,11 @@
 		goto err_disable_t1;
 
 	/* channel 2:  periodic and oneshot timer support */
+#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
 	ret = setup_clkevents(&tc, clk32k_divisor_idx);
+#else
+	ret = setup_clkevents(&tc, best_divisor_idx);
+#endif
 	if (ret)
 		goto err_unregister_clksrc;
 
diff -Nur linux-5.4.5/drivers/connector/cn_proc.c linux-5.4.5-new/drivers/connector/cn_proc.c
--- linux-5.4.5/drivers/connector/cn_proc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/connector/cn_proc.c	2020-06-15 16:12:10.799768260 +0300
@@ -18,6 +18,7 @@
 #include <linux/pid_namespace.h>
 
 #include <linux/cn_proc.h>
+#include <linux/locallock.h>
 
 /*
  * Size of a cn_msg followed by a proc_event structure.  Since the
@@ -40,10 +41,11 @@
 
 /* proc_event_counts is used as the sequence number of the netlink message */
 static DEFINE_PER_CPU(__u32, proc_event_counts) = { 0 };
+static DEFINE_LOCAL_IRQ_LOCK(send_msg_lock);
 
 static inline void send_msg(struct cn_msg *msg)
 {
-	preempt_disable();
+	local_lock(send_msg_lock);
 
 	msg->seq = __this_cpu_inc_return(proc_event_counts) - 1;
 	((struct proc_event *)msg->data)->cpu = smp_processor_id();
@@ -56,7 +58,7 @@
 	 */
 	cn_netlink_send(msg, 0, CN_IDX_PROC, GFP_NOWAIT);
 
-	preempt_enable();
+	local_unlock(send_msg_lock);
 }
 
 void proc_fork_connector(struct task_struct *task)
diff -Nur linux-5.4.5/drivers/dma-buf/dma-buf.c linux-5.4.5-new/drivers/dma-buf/dma-buf.c
--- linux-5.4.5/drivers/dma-buf/dma-buf.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/dma-buf/dma-buf.c	2020-06-15 16:12:11.071767306 +0300
@@ -214,7 +214,7 @@
 		return 0;
 
 retry:
-	seq = read_seqcount_begin(&resv->seq);
+	seq = read_seqbegin(&resv->seq);
 	rcu_read_lock();
 
 	fobj = rcu_dereference(resv->fence);
@@ -223,7 +223,7 @@
 	else
 		shared_count = 0;
 	fence_excl = rcu_dereference(resv->fence_excl);
-	if (read_seqcount_retry(&resv->seq, seq)) {
+	if (read_seqretry(&resv->seq, seq)) {
 		rcu_read_unlock();
 		goto retry;
 	}
@@ -1189,12 +1189,12 @@
 
 		robj = buf_obj->resv;
 		while (true) {
-			seq = read_seqcount_begin(&robj->seq);
+			seq = read_seqbegin(&robj->seq);
 			rcu_read_lock();
 			fobj = rcu_dereference(robj->fence);
 			shared_count = fobj ? fobj->shared_count : 0;
 			fence = rcu_dereference(robj->fence_excl);
-			if (!read_seqcount_retry(&robj->seq, seq))
+			if (!read_seqretry(&robj->seq, seq))
 				break;
 			rcu_read_unlock();
 		}
diff -Nur linux-5.4.5/drivers/dma-buf/dma-resv.c linux-5.4.5-new/drivers/dma-buf/dma-resv.c
--- linux-5.4.5/drivers/dma-buf/dma-resv.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/dma-buf/dma-resv.c	2020-06-15 16:12:11.071767306 +0300
@@ -49,12 +49,6 @@
 DEFINE_WD_CLASS(reservation_ww_class);
 EXPORT_SYMBOL(reservation_ww_class);
 
-struct lock_class_key reservation_seqcount_class;
-EXPORT_SYMBOL(reservation_seqcount_class);
-
-const char reservation_seqcount_string[] = "reservation_seqcount";
-EXPORT_SYMBOL(reservation_seqcount_string);
-
 /**
  * dma_resv_list_alloc - allocate fence list
  * @shared_max: number of fences we need space for
@@ -103,8 +97,7 @@
 {
 	ww_mutex_init(&obj->lock, &reservation_ww_class);
 
-	__seqcount_init(&obj->seq, reservation_seqcount_string,
-			&reservation_seqcount_class);
+	seqlock_init(&obj->seq);
 	RCU_INIT_POINTER(obj->fence, NULL);
 	RCU_INIT_POINTER(obj->fence_excl, NULL);
 }
@@ -234,8 +227,7 @@
 	fobj = dma_resv_get_list(obj);
 	count = fobj->shared_count;
 
-	preempt_disable();
-	write_seqcount_begin(&obj->seq);
+	write_seqlock(&obj->seq);
 
 	for (i = 0; i < count; ++i) {
 
@@ -255,8 +247,7 @@
 	/* pointer update must be visible before we extend the shared_count */
 	smp_store_mb(fobj->shared_count, count);
 
-	write_seqcount_end(&obj->seq);
-	preempt_enable();
+	write_sequnlock(&obj->seq);
 	dma_fence_put(old);
 }
 EXPORT_SYMBOL(dma_resv_add_shared_fence);
@@ -283,14 +274,12 @@
 	if (fence)
 		dma_fence_get(fence);
 
-	preempt_disable();
-	write_seqcount_begin(&obj->seq);
-	/* write_seqcount_begin provides the necessary memory barrier */
+	write_seqlock(&obj->seq);
+	/* write_seqlock provides the necessary memory barrier */
 	RCU_INIT_POINTER(obj->fence_excl, fence);
 	if (old)
 		old->shared_count = 0;
-	write_seqcount_end(&obj->seq);
-	preempt_enable();
+	write_sequnlock(&obj->seq);
 
 	/* inplace update, no shared fences */
 	while (i--)
@@ -368,13 +357,11 @@
 	src_list = dma_resv_get_list(dst);
 	old = dma_resv_get_excl(dst);
 
-	preempt_disable();
-	write_seqcount_begin(&dst->seq);
-	/* write_seqcount_begin provides the necessary memory barrier */
+	write_seqlock(&dst->seq);
+	/* write_seqlock provides the necessary memory barrier */
 	RCU_INIT_POINTER(dst->fence_excl, new);
 	RCU_INIT_POINTER(dst->fence, dst_list);
-	write_seqcount_end(&dst->seq);
-	preempt_enable();
+	write_sequnlock(&dst->seq);
 
 	dma_resv_list_free(src_list);
 	dma_fence_put(old);
@@ -414,7 +401,7 @@
 		shared_count = i = 0;
 
 		rcu_read_lock();
-		seq = read_seqcount_begin(&obj->seq);
+		seq = read_seqbegin(&obj->seq);
 
 		fence_excl = rcu_dereference(obj->fence_excl);
 		if (fence_excl && !dma_fence_get_rcu(fence_excl))
@@ -456,7 +443,7 @@
 			}
 		}
 
-		if (i != shared_count || read_seqcount_retry(&obj->seq, seq)) {
+		if (i != shared_count || read_seqretry(&obj->seq, seq)) {
 			while (i--)
 				dma_fence_put(shared[i]);
 			dma_fence_put(fence_excl);
@@ -507,7 +494,7 @@
 
 retry:
 	shared_count = 0;
-	seq = read_seqcount_begin(&obj->seq);
+	seq = read_seqbegin(&obj->seq);
 	rcu_read_lock();
 	i = -1;
 
@@ -553,7 +540,7 @@
 
 	rcu_read_unlock();
 	if (fence) {
-		if (read_seqcount_retry(&obj->seq, seq)) {
+		if (read_seqretry(&obj->seq, seq)) {
 			dma_fence_put(fence);
 			goto retry;
 		}
@@ -607,7 +594,7 @@
 retry:
 	ret = true;
 	shared_count = 0;
-	seq = read_seqcount_begin(&obj->seq);
+	seq = read_seqbegin(&obj->seq);
 
 	if (test_all) {
 		unsigned i;
@@ -627,7 +614,7 @@
 				break;
 		}
 
-		if (read_seqcount_retry(&obj->seq, seq))
+		if (read_seqretry(&obj->seq, seq))
 			goto retry;
 	}
 
@@ -639,7 +626,7 @@
 			if (ret < 0)
 				goto retry;
 
-			if (read_seqcount_retry(&obj->seq, seq))
+			if (read_seqretry(&obj->seq, seq))
 				goto retry;
 		}
 	}
diff -Nur linux-5.4.5/drivers/firmware/efi/efi.c linux-5.4.5-new/drivers/firmware/efi/efi.c
--- linux-5.4.5/drivers/firmware/efi/efi.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/firmware/efi/efi.c	2020-06-15 16:12:11.219766786 +0300
@@ -68,7 +68,7 @@
 
 struct workqueue_struct *efi_rts_wq;
 
-static bool disable_runtime;
+static bool disable_runtime = IS_ENABLED(CONFIG_PREEMPT_RT);
 static int __init setup_noefi(char *arg)
 {
 	disable_runtime = true;
@@ -94,6 +94,9 @@
 	if (parse_option_str(str, "noruntime"))
 		disable_runtime = true;
 
+	if (parse_option_str(str, "runtime"))
+		disable_runtime = false;
+
 	return 0;
 }
 early_param("efi", parse_efi_cmdline);
diff -Nur linux-5.4.5/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c linux-5.4.5-new/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
--- linux-5.4.5/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c	2020-06-15 16:12:11.399766155 +0300
@@ -252,11 +252,9 @@
 	new->shared_count = k;
 
 	/* Install the new fence list, seqcount provides the barriers */
-	preempt_disable();
-	write_seqcount_begin(&resv->seq);
+	write_seqlock(&resv->seq);
 	RCU_INIT_POINTER(resv->fence, new);
-	write_seqcount_end(&resv->seq);
-	preempt_enable();
+	write_sequnlock(&resv->seq);
 
 	/* Drop the references to the removed fences or move them to ef_list */
 	for (i = j, k = 0; i < old->shared_count; ++i) {
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/display/intel_sprite.c linux-5.4.5-new/drivers/gpu/drm/i915/display/intel_sprite.c
--- linux-5.4.5/drivers/gpu/drm/i915/display/intel_sprite.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/display/intel_sprite.c	2020-06-15 16:12:14.327755878 +0300
@@ -38,6 +38,7 @@
 #include <drm/drm_plane_helper.h>
 #include <drm/drm_rect.h>
 #include <drm/i915_drm.h>
+#include <linux/locallock.h>
 
 #include "i915_drv.h"
 #include "i915_trace.h"
@@ -80,6 +81,8 @@
 #define VBLANK_EVASION_TIME_US 100
 #endif
 
+static DEFINE_LOCAL_IRQ_LOCK(pipe_update_lock);
+
 /**
  * intel_pipe_update_start() - start update of a set of display registers
  * @new_crtc_state: the new crtc state
@@ -129,7 +132,7 @@
 		DRM_ERROR("PSR idle timed out 0x%x, atomic update may fail\n",
 			  psr_status);
 
-	local_irq_disable();
+	local_lock_irq(pipe_update_lock);
 
 	crtc->debug.min_vbl = min;
 	crtc->debug.max_vbl = max;
@@ -153,11 +156,11 @@
 			break;
 		}
 
-		local_irq_enable();
+		local_unlock_irq(pipe_update_lock);
 
 		timeout = schedule_timeout(timeout);
 
-		local_irq_disable();
+		local_lock_irq(pipe_update_lock);
 	}
 
 	finish_wait(wq, &wait);
@@ -190,7 +193,7 @@
 	return;
 
 irq_disable:
-	local_irq_disable();
+	local_lock_irq(pipe_update_lock);
 }
 
 /**
@@ -226,7 +229,7 @@
 		new_crtc_state->base.event = NULL;
 	}
 
-	local_irq_enable();
+	local_unlock_irq(pipe_update_lock);
 
 	if (intel_vgpu_active(dev_priv))
 		return;
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/gem/i915_gem_busy.c linux-5.4.5-new/drivers/gpu/drm/i915/gem/i915_gem_busy.c
--- linux-5.4.5/drivers/gpu/drm/i915/gem/i915_gem_busy.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/gem/i915_gem_busy.c	2020-06-15 16:12:14.531755163 +0300
@@ -75,7 +75,6 @@
 
 	return __busy_set_if_active(fence, __busy_write_id);
 }
-
 int
 i915_gem_busy_ioctl(struct drm_device *dev, void *data,
 		    struct drm_file *file)
@@ -110,7 +109,8 @@
 	 *
 	 */
 retry:
-	seq = raw_read_seqcount(&obj->base.resv->seq);
+	/* XXX raw_read_seqcount() does not wait for the WRTIE to finish */
+	seq = read_seqbegin(&obj->base.resv->seq);
 
 	/* Translate the exclusive fence to the READ *and* WRITE engine */
 	args->busy =
@@ -129,7 +129,7 @@
 		}
 	}
 
-	if (args->busy && read_seqcount_retry(&obj->base.resv->seq, seq))
+	if (args->busy && read_seqretry(&obj->base.resv->seq, seq))
 		goto retry;
 
 	err = 0;
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/gt/intel_breadcrumbs.c linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_breadcrumbs.c
--- linux-5.4.5/drivers/gpu/drm/i915/gt/intel_breadcrumbs.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_breadcrumbs.c	2020-06-15 16:12:14.663754699 +0300
@@ -120,7 +120,6 @@
 	struct dma_fence_cb *cur, *tmp;
 
 	lockdep_assert_held(fence->lock);
-	lockdep_assert_irqs_disabled();
 
 	list_for_each_entry_safe(cur, tmp, list, node) {
 		INIT_LIST_HEAD(&cur->node);
@@ -134,9 +133,10 @@
 	const ktime_t timestamp = ktime_get();
 	struct intel_context *ce, *cn;
 	struct list_head *pos, *next;
+	unsigned long flags;
 	LIST_HEAD(signal);
 
-	spin_lock(&b->irq_lock);
+	spin_lock_irqsave(&b->irq_lock, flags);
 
 	if (b->irq_armed && list_empty(&b->signalers))
 		__intel_breadcrumbs_disarm_irq(b);
@@ -182,30 +182,23 @@
 		}
 	}
 
-	spin_unlock(&b->irq_lock);
+	spin_unlock_irqrestore(&b->irq_lock, flags);
 
 	list_for_each_safe(pos, next, &signal) {
 		struct i915_request *rq =
 			list_entry(pos, typeof(*rq), signal_link);
 		struct list_head cb_list;
 
-		spin_lock(&rq->lock);
+		spin_lock_irqsave(&rq->lock, flags);
 		list_replace(&rq->fence.cb_list, &cb_list);
 		__dma_fence_signal__timestamp(&rq->fence, timestamp);
 		__dma_fence_signal__notify(&rq->fence, &cb_list);
-		spin_unlock(&rq->lock);
+		spin_unlock_irqrestore(&rq->lock, flags);
 
 		i915_request_put(rq);
 	}
 }
 
-void intel_engine_signal_breadcrumbs(struct intel_engine_cs *engine)
-{
-	local_irq_disable();
-	intel_engine_breadcrumbs_irq(engine);
-	local_irq_enable();
-}
-
 static void signal_irq_work(struct irq_work *work)
 {
 	struct intel_engine_cs *engine =
@@ -275,7 +268,6 @@
 bool i915_request_enable_breadcrumb(struct i915_request *rq)
 {
 	lockdep_assert_held(&rq->lock);
-	lockdep_assert_irqs_disabled();
 
 	if (test_bit(I915_FENCE_FLAG_ACTIVE, &rq->fence.flags)) {
 		struct intel_breadcrumbs *b = &rq->engine->breadcrumbs;
@@ -325,7 +317,6 @@
 	struct intel_breadcrumbs *b = &rq->engine->breadcrumbs;
 
 	lockdep_assert_held(&rq->lock);
-	lockdep_assert_irqs_disabled();
 
 	/*
 	 * We must wait for b->irq_lock so that we know the interrupt handler
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/gt/intel_engine.h linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_engine.h
--- linux-5.4.5/drivers/gpu/drm/i915/gt/intel_engine.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_engine.h	2020-06-15 16:12:14.663754699 +0300
@@ -349,7 +349,6 @@
 void intel_engine_init_breadcrumbs(struct intel_engine_cs *engine);
 void intel_engine_fini_breadcrumbs(struct intel_engine_cs *engine);
 
-void intel_engine_signal_breadcrumbs(struct intel_engine_cs *engine);
 void intel_engine_disarm_breadcrumbs(struct intel_engine_cs *engine);
 
 static inline void
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/gt/intel_hangcheck.c linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_hangcheck.c
--- linux-5.4.5/drivers/gpu/drm/i915/gt/intel_hangcheck.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_hangcheck.c	2020-06-15 16:12:14.663754699 +0300
@@ -283,7 +283,7 @@
 	for_each_engine(engine, gt->i915, id) {
 		struct hangcheck hc;
 
-		intel_engine_signal_breadcrumbs(engine);
+		intel_engine_breadcrumbs_irq(engine);
 
 		hangcheck_load_sample(engine, &hc);
 		hangcheck_accumulate_sample(engine, &hc);
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/gt/intel_reset.c linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_reset.c
--- linux-5.4.5/drivers/gpu/drm/i915/gt/intel_reset.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/gt/intel_reset.c	2020-06-15 16:12:14.663754699 +0300
@@ -695,7 +695,7 @@
 	engine->reset.finish(engine);
 	intel_uncore_forcewake_put(engine->uncore, FORCEWAKE_ALL);
 
-	intel_engine_signal_breadcrumbs(engine);
+	intel_engine_breadcrumbs_irq(engine);
 }
 
 static void reset_finish(struct intel_gt *gt, intel_engine_mask_t awake)
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/i915_irq.c linux-5.4.5-new/drivers/gpu/drm/i915/i915_irq.c
--- linux-5.4.5/drivers/gpu/drm/i915/i915_irq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/i915_irq.c	2020-06-15 16:12:14.831754109 +0300
@@ -983,6 +983,7 @@
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -1034,6 +1035,7 @@
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/i915_request.c linux-5.4.5-new/drivers/gpu/drm/i915/i915_request.c
--- linux-5.4.5/drivers/gpu/drm/i915/i915_request.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/i915_request.c	2020-06-15 16:12:14.835754095 +0300
@@ -205,14 +205,14 @@
 	 * check that the rq still belongs to the newly locked engine.
 	 */
 	locked = READ_ONCE(rq->engine);
-	spin_lock(&locked->active.lock);
+	spin_lock_irq(&locked->active.lock);
 	while (unlikely(locked != (engine = READ_ONCE(rq->engine)))) {
 		spin_unlock(&locked->active.lock);
 		spin_lock(&engine->active.lock);
 		locked = engine;
 	}
 	list_del(&rq->sched.link);
-	spin_unlock(&locked->active.lock);
+	spin_unlock_irq(&locked->active.lock);
 }
 
 static bool i915_request_retire(struct i915_request *rq)
@@ -272,8 +272,6 @@
 		active->retire(active, rq);
 	}
 
-	local_irq_disable();
-
 	/*
 	 * We only loosely track inflight requests across preemption,
 	 * and so we may find ourselves attempting to retire a _completed_
@@ -282,7 +280,7 @@
 	 */
 	remove_from_engine(rq);
 
-	spin_lock(&rq->lock);
+	spin_lock_irq(&rq->lock);
 	i915_request_mark_complete(rq);
 	if (!i915_request_signaled(rq))
 		dma_fence_signal_locked(&rq->fence);
@@ -297,9 +295,7 @@
 		__notify_execute_cb(rq);
 	}
 	GEM_BUG_ON(!list_empty(&rq->execute_cb));
-	spin_unlock(&rq->lock);
-
-	local_irq_enable();
+	spin_unlock_irq(&rq->lock);
 
 	remove_from_client(rq);
 	list_del(&rq->link);
diff -Nur linux-5.4.5/drivers/gpu/drm/i915/i915_trace.h linux-5.4.5-new/drivers/gpu/drm/i915/i915_trace.h
--- linux-5.4.5/drivers/gpu/drm/i915/i915_trace.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/i915/i915_trace.h	2020-06-15 16:12:14.835754095 +0300
@@ -2,6 +2,10 @@
 #if !defined(_I915_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
 #define _I915_TRACE_H_
 
+#ifdef CONFIG_PREEMPT_RT
+#define NOTRACE
+#endif
+
 #include <linux/stringify.h>
 #include <linux/types.h>
 #include <linux/tracepoint.h>
@@ -721,7 +725,7 @@
 	    TP_ARGS(rq)
 );
 
-#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS)
+#if defined(CONFIG_DRM_I915_LOW_LEVEL_TRACEPOINTS) && !defined(NOTRACE)
 DEFINE_EVENT(i915_request, i915_request_submit,
 	     TP_PROTO(struct i915_request *rq),
 	     TP_ARGS(rq)
diff -Nur linux-5.4.5/drivers/gpu/drm/Kconfig linux-5.4.5-new/drivers/gpu/drm/Kconfig
--- linux-5.4.5/drivers/gpu/drm/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/Kconfig	2020-06-15 16:12:13.831757620 +0300
@@ -397,7 +397,7 @@
 
 config DRM_I810
 	tristate "Intel I810"
-	# !PREEMPT because of missing ioctl locking
+	# !PREEMPTION because of missing ioctl locking
 	depends on DRM && AGP && AGP_INTEL && (!PREEMPTION || BROKEN)
 	help
 	  Choose this option if you have an Intel I810 graphics card.  If M is
diff -Nur linux-5.4.5/drivers/gpu/drm/radeon/radeon_display.c linux-5.4.5-new/drivers/gpu/drm/radeon/radeon_display.c
--- linux-5.4.5/drivers/gpu/drm/radeon/radeon_display.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/gpu/drm/radeon/radeon_display.c	2020-06-15 16:12:15.503751750 +0300
@@ -1819,6 +1819,7 @@
 	struct radeon_device *rdev = dev->dev_private;
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -1911,6 +1912,7 @@
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	/* Decode into vertical and horizontal scanout position. */
 	*vpos = position & 0x1fff;
diff -Nur linux-5.4.5/drivers/hv/hyperv_vmbus.h linux-5.4.5-new/drivers/hv/hyperv_vmbus.h
--- linux-5.4.5/drivers/hv/hyperv_vmbus.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/hv/hyperv_vmbus.h	2020-06-15 16:12:15.807750682 +0300
@@ -18,6 +18,7 @@
 #include <linux/atomic.h>
 #include <linux/hyperv.h>
 #include <linux/interrupt.h>
+#include <linux/irq.h>
 
 #include "hv_trace.h"
 
diff -Nur linux-5.4.5/drivers/hv/vmbus_drv.c linux-5.4.5-new/drivers/hv/vmbus_drv.c
--- linux-5.4.5/drivers/hv/vmbus_drv.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/hv/vmbus_drv.c	2020-06-15 16:12:15.807750682 +0300
@@ -22,6 +22,7 @@
 #include <linux/clockchips.h>
 #include <linux/cpu.h>
 #include <linux/sched/task_stack.h>
+#include <linux/irq.h>
 
 #include <asm/mshyperv.h>
 #include <linux/delay.h>
@@ -1199,6 +1200,8 @@
 	void *page_addr = hv_cpu->synic_event_page;
 	struct hv_message *msg;
 	union hv_synic_event_flags *event;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 	bool handled = false;
 
 	if (unlikely(page_addr == NULL))
@@ -1243,7 +1246,7 @@
 			tasklet_schedule(&hv_cpu->msg_dpc);
 	}
 
-	add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0);
+	add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0, ip);
 }
 
 /*
diff -Nur linux-5.4.5/drivers/Kconfig linux-5.4.5-new/drivers/Kconfig
--- linux-5.4.5/drivers/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/Kconfig	2020-06-15 16:12:10.011771024 +0300
@@ -9,7 +9,6 @@
 source "drivers/pcmcia/Kconfig"
 source "drivers/rapidio/Kconfig"
 
-
 source "drivers/base/Kconfig"
 
 source "drivers/bus/Kconfig"
diff -Nur linux-5.4.5/drivers/leds/trigger/Kconfig linux-5.4.5-new/drivers/leds/trigger/Kconfig
--- linux-5.4.5/drivers/leds/trigger/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/leds/trigger/Kconfig	2020-06-15 16:12:16.647747731 +0300
@@ -64,6 +64,7 @@
 
 config LEDS_TRIGGER_CPU
 	bool "LED CPU Trigger"
+	depends on !PREEMPT_RT
 	help
 	  This allows LEDs to be controlled by active CPUs. This shows
 	  the active CPUs across an array of LEDs so you can see which
diff -Nur linux-5.4.5/drivers/md/bcache/Kconfig linux-5.4.5-new/drivers/md/bcache/Kconfig
--- linux-5.4.5/drivers/md/bcache/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/md/bcache/Kconfig	2020-06-15 16:12:16.699747549 +0300
@@ -2,6 +2,7 @@
 
 config BCACHE
 	tristate "Block device as cache"
+	depends on !PREEMPT_RT
 	select CRC64
 	help
 	Allows a block device to be used as cache for other devices; uses
diff -Nur linux-5.4.5/drivers/md/raid5.c linux-5.4.5-new/drivers/md/raid5.c
--- linux-5.4.5/drivers/md/raid5.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/md/raid5.c	2020-06-15 16:12:16.739747409 +0300
@@ -2058,8 +2058,9 @@
 	struct raid5_percpu *percpu;
 	unsigned long cpu;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	percpu = per_cpu_ptr(conf->percpu, cpu);
+	spin_lock(&percpu->lock);
 	if (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {
 		ops_run_biofill(sh);
 		overlap_clear++;
@@ -2118,7 +2119,8 @@
 			if (test_and_clear_bit(R5_Overlap, &dev->flags))
 				wake_up(&sh->raid_conf->wait_for_overlap);
 		}
-	put_cpu();
+	spin_unlock(&percpu->lock);
+	put_cpu_light();
 }
 
 static void free_stripe(struct kmem_cache *sc, struct stripe_head *sh)
@@ -6821,6 +6823,7 @@
 			__func__, cpu);
 		return -ENOMEM;
 	}
+	spin_lock_init(&per_cpu_ptr(conf->percpu, cpu)->lock);
 	return 0;
 }
 
diff -Nur linux-5.4.5/drivers/md/raid5.h linux-5.4.5-new/drivers/md/raid5.h
--- linux-5.4.5/drivers/md/raid5.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/md/raid5.h	2020-06-15 16:12:16.743747394 +0300
@@ -634,6 +634,7 @@
 	int			recovery_disabled;
 	/* per cpu variables */
 	struct raid5_percpu {
+		spinlock_t	lock;		/* Protection for -RT */
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
 		void		*scribble;  /* space for constructing buffer
 					     * lists and performing address
diff -Nur linux-5.4.5/drivers/media/platform/Kconfig linux-5.4.5-new/drivers/media/platform/Kconfig
--- linux-5.4.5/drivers/media/platform/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/media/platform/Kconfig	2020-06-15 16:12:17.219745722 +0300
@@ -585,7 +585,7 @@
 
 config CEC_GPIO
 	tristate "Generic GPIO-based CEC driver"
-	depends on PREEMPT || COMPILE_TEST
+	depends on PREEMPTION || COMPILE_TEST
 	select CEC_CORE
 	select CEC_PIN
 	select GPIOLIB
diff -Nur linux-5.4.5/drivers/net/wireless/intersil/orinoco/orinoco_usb.c linux-5.4.5-new/drivers/net/wireless/intersil/orinoco/orinoco_usb.c
--- linux-5.4.5/drivers/net/wireless/intersil/orinoco/orinoco_usb.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/net/wireless/intersil/orinoco/orinoco_usb.c	2020-06-15 16:12:19.191738793 +0300
@@ -693,8 +693,8 @@
 			while (!ctx->done.done && msecs--)
 				udelay(1000);
 		} else {
-			wait_event_interruptible(ctx->done.wait,
-						 ctx->done.done);
+			swait_event_interruptible_exclusive(ctx->done.wait,
+							    ctx->done.done);
 		}
 		break;
 	default:
diff -Nur linux-5.4.5/drivers/of/base.c linux-5.4.5-new/drivers/of/base.c
--- linux-5.4.5/drivers/of/base.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/of/base.c	2020-06-15 16:12:19.535737584 +0300
@@ -123,115 +123,38 @@
 }
 #endif
 
-/*
- * Assumptions behind phandle_cache implementation:
- *   - phandle property values are in a contiguous range of 1..n
- *
- * If the assumptions do not hold, then
- *   - the phandle lookup overhead reduction provided by the cache
- *     will likely be less
- */
+#define OF_PHANDLE_CACHE_BITS	7
+#define OF_PHANDLE_CACHE_SZ	BIT(OF_PHANDLE_CACHE_BITS)
 
-static struct device_node **phandle_cache;
-static u32 phandle_cache_mask;
+static struct device_node *phandle_cache[OF_PHANDLE_CACHE_SZ];
 
-/*
- * Caller must hold devtree_lock.
- */
-static void __of_free_phandle_cache(void)
+static u32 of_phandle_cache_hash(phandle handle)
 {
-	u32 cache_entries = phandle_cache_mask + 1;
-	u32 k;
-
-	if (!phandle_cache)
-		return;
-
-	for (k = 0; k < cache_entries; k++)
-		of_node_put(phandle_cache[k]);
-
-	kfree(phandle_cache);
-	phandle_cache = NULL;
+	return hash_32(handle, OF_PHANDLE_CACHE_BITS);
 }
 
-int of_free_phandle_cache(void)
-{
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&devtree_lock, flags);
-
-	__of_free_phandle_cache();
-
-	raw_spin_unlock_irqrestore(&devtree_lock, flags);
-
-	return 0;
-}
-#if !defined(CONFIG_MODULES)
-late_initcall_sync(of_free_phandle_cache);
-#endif
-
 /*
  * Caller must hold devtree_lock.
  */
-void __of_free_phandle_cache_entry(phandle handle)
+void __of_phandle_cache_inv_entry(phandle handle)
 {
-	phandle masked_handle;
+	u32 handle_hash;
 	struct device_node *np;
 
 	if (!handle)
 		return;
 
-	masked_handle = handle & phandle_cache_mask;
-
-	if (phandle_cache) {
-		np = phandle_cache[masked_handle];
-		if (np && handle == np->phandle) {
-			of_node_put(np);
-			phandle_cache[masked_handle] = NULL;
-		}
-	}
-}
-
-void of_populate_phandle_cache(void)
-{
-	unsigned long flags;
-	u32 cache_entries;
-	struct device_node *np;
-	u32 phandles = 0;
-
-	raw_spin_lock_irqsave(&devtree_lock, flags);
-
-	__of_free_phandle_cache();
+	handle_hash = of_phandle_cache_hash(handle);
 
-	for_each_of_allnodes(np)
-		if (np->phandle && np->phandle != OF_PHANDLE_ILLEGAL)
-			phandles++;
-
-	if (!phandles)
-		goto out;
-
-	cache_entries = roundup_pow_of_two(phandles);
-	phandle_cache_mask = cache_entries - 1;
-
-	phandle_cache = kcalloc(cache_entries, sizeof(*phandle_cache),
-				GFP_ATOMIC);
-	if (!phandle_cache)
-		goto out;
-
-	for_each_of_allnodes(np)
-		if (np->phandle && np->phandle != OF_PHANDLE_ILLEGAL) {
-			of_node_get(np);
-			phandle_cache[np->phandle & phandle_cache_mask] = np;
-		}
-
-out:
-	raw_spin_unlock_irqrestore(&devtree_lock, flags);
+	np = phandle_cache[handle_hash];
+	if (np && handle == np->phandle)
+		phandle_cache[handle_hash] = NULL;
 }
 
 void __init of_core_init(void)
 {
 	struct device_node *np;
 
-	of_populate_phandle_cache();
 
 	/* Create the kset, and register existing nodes */
 	mutex_lock(&of_mutex);
@@ -241,8 +164,11 @@
 		pr_err("failed to register existing nodes\n");
 		return;
 	}
-	for_each_of_allnodes(np)
+	for_each_of_allnodes(np) {
 		__of_attach_node_sysfs(np);
+		if (np->phandle && !phandle_cache[of_phandle_cache_hash(np->phandle)])
+			phandle_cache[of_phandle_cache_hash(np->phandle)] = np;
+	}
 	mutex_unlock(&of_mutex);
 
 	/* Symlink in /proc as required by userspace ABI */
@@ -1223,36 +1149,29 @@
 {
 	struct device_node *np = NULL;
 	unsigned long flags;
-	phandle masked_handle;
+	u32 handle_hash;
 
 	if (!handle)
 		return NULL;
 
-	raw_spin_lock_irqsave(&devtree_lock, flags);
+	handle_hash = of_phandle_cache_hash(handle);
 
-	masked_handle = handle & phandle_cache_mask;
+	raw_spin_lock_irqsave(&devtree_lock, flags);
 
-	if (phandle_cache) {
-		if (phandle_cache[masked_handle] &&
-		    handle == phandle_cache[masked_handle]->phandle)
-			np = phandle_cache[masked_handle];
-		if (np && of_node_check_flag(np, OF_DETACHED)) {
-			WARN_ON(1); /* did not uncache np on node removal */
-			of_node_put(np);
-			phandle_cache[masked_handle] = NULL;
-			np = NULL;
-		}
+	if (phandle_cache[handle_hash] &&
+	    handle == phandle_cache[handle_hash]->phandle)
+		np = phandle_cache[handle_hash];
+	if (np && of_node_check_flag(np, OF_DETACHED)) {
+		WARN_ON(1); /* did not uncache np on node removal */
+		phandle_cache[handle_hash] = NULL;
+		np = NULL;
 	}
 
 	if (!np) {
 		for_each_of_allnodes(np)
 			if (np->phandle == handle &&
 			    !of_node_check_flag(np, OF_DETACHED)) {
-				if (phandle_cache) {
-					/* will put when removed from cache */
-					of_node_get(np);
-					phandle_cache[masked_handle] = np;
-				}
+				phandle_cache[handle_hash] = np;
 				break;
 			}
 	}
diff -Nur linux-5.4.5/drivers/of/dynamic.c linux-5.4.5-new/drivers/of/dynamic.c
--- linux-5.4.5/drivers/of/dynamic.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/of/dynamic.c	2020-06-15 16:12:19.535737584 +0300
@@ -276,7 +276,7 @@
 	of_node_set_flag(np, OF_DETACHED);
 
 	/* race with of_find_node_by_phandle() prevented by devtree_lock */
-	__of_free_phandle_cache_entry(np->phandle);
+	__of_phandle_cache_inv_entry(np->phandle);
 }
 
 /**
diff -Nur linux-5.4.5/drivers/of/of_private.h linux-5.4.5-new/drivers/of/of_private.h
--- linux-5.4.5/drivers/of/of_private.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/of/of_private.h	2020-06-15 16:12:19.535737584 +0300
@@ -85,14 +85,12 @@
 #endif
 
 #if defined(CONFIG_OF_DYNAMIC)
-void __of_free_phandle_cache_entry(phandle handle);
+void __of_phandle_cache_inv_entry(phandle handle);
 #endif
 
 #if defined(CONFIG_OF_OVERLAY)
 void of_overlay_mutex_lock(void);
 void of_overlay_mutex_unlock(void);
-int of_free_phandle_cache(void);
-void of_populate_phandle_cache(void);
 #else
 static inline void of_overlay_mutex_lock(void) {};
 static inline void of_overlay_mutex_unlock(void) {};
diff -Nur linux-5.4.5/drivers/of/overlay.c linux-5.4.5-new/drivers/of/overlay.c
--- linux-5.4.5/drivers/of/overlay.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/of/overlay.c	2020-06-15 16:12:19.535737584 +0300
@@ -971,8 +971,6 @@
 		goto err_free_overlay_changeset;
 	}
 
-	of_populate_phandle_cache();
-
 	ret = __of_changeset_apply_notify(&ovcs->cset);
 	if (ret)
 		pr_err("overlay apply changeset entry notify error %d\n", ret);
@@ -1215,17 +1213,9 @@
 
 	list_del(&ovcs->ovcs_list);
 
-	/*
-	 * Disable phandle cache.  Avoids race condition that would arise
-	 * from removing cache entry when the associated node is deleted.
-	 */
-	of_free_phandle_cache();
-
 	ret_apply = 0;
 	ret = __of_changeset_revert_entries(&ovcs->cset, &ret_apply);
 
-	of_populate_phandle_cache();
-
 	if (ret) {
 		if (ret_apply)
 			devicetree_state_flags |= DTSF_REVERT_FAIL;
diff -Nur linux-5.4.5/drivers/pci/switch/switchtec.c linux-5.4.5-new/drivers/pci/switch/switchtec.c
--- linux-5.4.5/drivers/pci/switch/switchtec.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/pci/switch/switchtec.c	2020-06-15 16:12:19.611737317 +0300
@@ -52,10 +52,11 @@
 
 	enum mrpc_state state;
 
-	struct completion comp;
+	wait_queue_head_t cmd_comp;
 	struct kref kref;
 	struct list_head list;
 
+	bool cmd_done;
 	u32 cmd;
 	u32 status;
 	u32 return_code;
@@ -77,7 +78,7 @@
 	stuser->stdev = stdev;
 	kref_init(&stuser->kref);
 	INIT_LIST_HEAD(&stuser->list);
-	init_completion(&stuser->comp);
+	init_waitqueue_head(&stuser->cmd_comp);
 	stuser->event_cnt = atomic_read(&stdev->event_cnt);
 
 	dev_dbg(&stdev->dev, "%s: %p\n", __func__, stuser);
@@ -175,7 +176,7 @@
 	kref_get(&stuser->kref);
 	stuser->read_len = sizeof(stuser->data);
 	stuser_set_state(stuser, MRPC_QUEUED);
-	init_completion(&stuser->comp);
+	stuser->cmd_done = false;
 	list_add_tail(&stuser->list, &stdev->mrpc_queue);
 
 	mrpc_cmd_submit(stdev);
@@ -222,7 +223,8 @@
 		memcpy_fromio(stuser->data, &stdev->mmio_mrpc->output_data,
 			      stuser->read_len);
 out:
-	complete_all(&stuser->comp);
+	stuser->cmd_done = true;
+	wake_up_interruptible(&stuser->cmd_comp);
 	list_del_init(&stuser->list);
 	stuser_put(stuser);
 	stdev->mrpc_busy = 0;
@@ -494,10 +496,11 @@
 	mutex_unlock(&stdev->mrpc_mutex);
 
 	if (filp->f_flags & O_NONBLOCK) {
-		if (!try_wait_for_completion(&stuser->comp))
+		if (!READ_ONCE(stuser->cmd_done))
 			return -EAGAIN;
 	} else {
-		rc = wait_for_completion_interruptible(&stuser->comp);
+		rc = wait_event_interruptible(stuser->cmd_comp,
+					      stuser->cmd_done);
 		if (rc < 0)
 			return rc;
 	}
@@ -545,7 +548,7 @@
 	struct switchtec_dev *stdev = stuser->stdev;
 	__poll_t ret = 0;
 
-	poll_wait(filp, &stuser->comp.wait, wait);
+	poll_wait(filp, &stuser->cmd_comp, wait);
 	poll_wait(filp, &stdev->event_wq, wait);
 
 	if (lock_mutex_and_test_alive(stdev))
@@ -553,7 +556,7 @@
 
 	mutex_unlock(&stdev->mrpc_mutex);
 
-	if (try_wait_for_completion(&stuser->comp))
+	if (READ_ONCE(stuser->cmd_done))
 		ret |= EPOLLIN | EPOLLRDNORM;
 
 	if (stuser->event_cnt != atomic_read(&stdev->event_cnt))
@@ -1106,7 +1109,8 @@
 
 	/* Wake up and kill any users waiting on an MRPC request */
 	list_for_each_entry_safe(stuser, tmpuser, &stdev->mrpc_queue, list) {
-		complete_all(&stuser->comp);
+		stuser->cmd_done = true;
+		wake_up_interruptible(&stuser->cmd_comp);
 		list_del_init(&stuser->list);
 		stuser_put(stuser);
 	}
diff -Nur linux-5.4.5/drivers/scsi/fcoe/fcoe.c linux-5.4.5-new/drivers/scsi/fcoe/fcoe.c
--- linux-5.4.5/drivers/scsi/fcoe/fcoe.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/scsi/fcoe/fcoe.c	2020-06-15 16:12:20.351734716 +0300
@@ -1452,11 +1452,11 @@
 static int fcoe_alloc_paged_crc_eof(struct sk_buff *skb, int tlen)
 {
 	struct fcoe_percpu_s *fps;
-	int rc;
+	int rc, cpu = get_cpu_light();
 
-	fps = &get_cpu_var(fcoe_percpu);
+	fps = &per_cpu(fcoe_percpu, cpu);
 	rc = fcoe_get_paged_crc_eof(skb, tlen, fps);
-	put_cpu_var(fcoe_percpu);
+	put_cpu_light();
 
 	return rc;
 }
@@ -1641,11 +1641,11 @@
 		return 0;
 	}
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	stats->InvalidCRCCount++;
 	if (stats->InvalidCRCCount < 5)
 		printk(KERN_WARNING "fcoe: dropping frame with CRC error\n");
-	put_cpu();
+	put_cpu_light();
 	return -EINVAL;
 }
 
@@ -1686,7 +1686,7 @@
 	 */
 	hp = (struct fcoe_hdr *) skb_network_header(skb);
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	if (unlikely(FC_FCOE_DECAPS_VER(hp) != FC_FCOE_VER)) {
 		if (stats->ErrorFrames < 5)
 			printk(KERN_WARNING "fcoe: FCoE version "
@@ -1718,13 +1718,13 @@
 		goto drop;
 
 	if (!fcoe_filter_frames(lport, fp)) {
-		put_cpu();
+		put_cpu_light();
 		fc_exch_recv(lport, fp);
 		return;
 	}
 drop:
 	stats->ErrorFrames++;
-	put_cpu();
+	put_cpu_light();
 	kfree_skb(skb);
 }
 
diff -Nur linux-5.4.5/drivers/scsi/fcoe/fcoe_ctlr.c linux-5.4.5-new/drivers/scsi/fcoe/fcoe_ctlr.c
--- linux-5.4.5/drivers/scsi/fcoe/fcoe_ctlr.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/scsi/fcoe/fcoe_ctlr.c	2020-06-15 16:12:20.351734716 +0300
@@ -826,7 +826,7 @@
 
 	INIT_LIST_HEAD(&del_list);
 
-	stats = per_cpu_ptr(fip->lp->stats, get_cpu());
+	stats = per_cpu_ptr(fip->lp->stats, get_cpu_light());
 
 	list_for_each_entry_safe(fcf, next, &fip->fcfs, list) {
 		deadline = fcf->time + fcf->fka_period + fcf->fka_period / 2;
@@ -862,7 +862,7 @@
 				sel_time = fcf->time;
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 
 	list_for_each_entry_safe(fcf, next, &del_list, list) {
 		/* Removes fcf from current list */
diff -Nur linux-5.4.5/drivers/scsi/libfc/fc_exch.c linux-5.4.5-new/drivers/scsi/libfc/fc_exch.c
--- linux-5.4.5/drivers/scsi/libfc/fc_exch.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/scsi/libfc/fc_exch.c	2020-06-15 16:12:20.399734547 +0300
@@ -821,10 +821,10 @@
 	}
 	memset(ep, 0, sizeof(*ep));
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = per_cpu_ptr(mp->pool, cpu);
 	spin_lock_bh(&pool->lock);
-	put_cpu();
+	put_cpu_light();
 
 	/* peek cache of free slot */
 	if (pool->left != FC_XID_UNKNOWN) {
diff -Nur linux-5.4.5/drivers/thermal/intel/x86_pkg_temp_thermal.c linux-5.4.5-new/drivers/thermal/intel/x86_pkg_temp_thermal.c
--- linux-5.4.5/drivers/thermal/intel/x86_pkg_temp_thermal.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/thermal/intel/x86_pkg_temp_thermal.c	2020-06-15 16:12:21.427730933 +0300
@@ -63,7 +63,7 @@
 /* Array of zone pointers */
 static struct zone_device **zones;
 /* Serializes interrupt notification, work and hotplug */
-static DEFINE_SPINLOCK(pkg_temp_lock);
+static DEFINE_RAW_SPINLOCK(pkg_temp_lock);
 /* Protects zone operation in the work function against hotplug removal */
 static DEFINE_MUTEX(thermal_zone_mutex);
 
@@ -266,12 +266,12 @@
 	u64 msr_val, wr_val;
 
 	mutex_lock(&thermal_zone_mutex);
-	spin_lock_irq(&pkg_temp_lock);
+	raw_spin_lock_irq(&pkg_temp_lock);
 	++pkg_work_cnt;
 
 	zonedev = pkg_temp_thermal_get_dev(cpu);
 	if (!zonedev) {
-		spin_unlock_irq(&pkg_temp_lock);
+		raw_spin_unlock_irq(&pkg_temp_lock);
 		mutex_unlock(&thermal_zone_mutex);
 		return;
 	}
@@ -285,7 +285,7 @@
 	}
 
 	enable_pkg_thres_interrupt();
-	spin_unlock_irq(&pkg_temp_lock);
+	raw_spin_unlock_irq(&pkg_temp_lock);
 
 	/*
 	 * If tzone is not NULL, then thermal_zone_mutex will prevent the
@@ -310,7 +310,7 @@
 	struct zone_device *zonedev;
 	unsigned long flags;
 
-	spin_lock_irqsave(&pkg_temp_lock, flags);
+	raw_spin_lock_irqsave(&pkg_temp_lock, flags);
 	++pkg_interrupt_cnt;
 
 	disable_pkg_thres_interrupt();
@@ -322,7 +322,7 @@
 		pkg_thermal_schedule_work(zonedev->cpu, &zonedev->work);
 	}
 
-	spin_unlock_irqrestore(&pkg_temp_lock, flags);
+	raw_spin_unlock_irqrestore(&pkg_temp_lock, flags);
 	return 0;
 }
 
@@ -368,9 +368,9 @@
 	      zonedev->msr_pkg_therm_high);
 
 	cpumask_set_cpu(cpu, &zonedev->cpumask);
-	spin_lock_irq(&pkg_temp_lock);
+	raw_spin_lock_irq(&pkg_temp_lock);
 	zones[id] = zonedev;
-	spin_unlock_irq(&pkg_temp_lock);
+	raw_spin_unlock_irq(&pkg_temp_lock);
 	return 0;
 }
 
@@ -407,7 +407,7 @@
 	}
 
 	/* Protect against work and interrupts */
-	spin_lock_irq(&pkg_temp_lock);
+	raw_spin_lock_irq(&pkg_temp_lock);
 
 	/*
 	 * Check whether this cpu was the current target and store the new
@@ -439,9 +439,9 @@
 		 * To cancel the work we need to drop the lock, otherwise
 		 * we might deadlock if the work needs to be flushed.
 		 */
-		spin_unlock_irq(&pkg_temp_lock);
+		raw_spin_unlock_irq(&pkg_temp_lock);
 		cancel_delayed_work_sync(&zonedev->work);
-		spin_lock_irq(&pkg_temp_lock);
+		raw_spin_lock_irq(&pkg_temp_lock);
 		/*
 		 * If this is not the last cpu in the package and the work
 		 * did not run after we dropped the lock above, then we
@@ -452,7 +452,7 @@
 			pkg_thermal_schedule_work(target, &zonedev->work);
 	}
 
-	spin_unlock_irq(&pkg_temp_lock);
+	raw_spin_unlock_irq(&pkg_temp_lock);
 
 	/* Final cleanup if this is the last cpu */
 	if (lastcpu)
diff -Nur linux-5.4.5/drivers/tty/serial/8250/8250_core.c linux-5.4.5-new/drivers/tty/serial/8250/8250_core.c
--- linux-5.4.5/drivers/tty/serial/8250/8250_core.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/8250/8250_core.c	2020-06-15 16:12:21.503730665 +0300
@@ -55,7 +55,16 @@
 
 static unsigned int skip_txen_test; /* force skip of txen test at init time */
 
-#define PASS_LIMIT	512
+/*
+ * On -rt we can have a more delays, and legitimately
+ * so - so don't drop work spuriously and spam the
+ * syslog:
+ */
+#ifdef CONFIG_PREEMPT_RT
+# define PASS_LIMIT	1000000
+#else
+# define PASS_LIMIT	512
+#endif
 
 #include <asm/serial.h>
 /*
@@ -266,7 +275,7 @@
 static void serial8250_backup_timeout(struct timer_list *t)
 {
 	struct uart_8250_port *up = from_timer(up, t, timer);
-	unsigned int iir, ier = 0, lsr;
+	unsigned int iir, lsr;
 	unsigned long flags;
 
 	spin_lock_irqsave(&up->port.lock, flags);
@@ -275,10 +284,8 @@
 	 * Must disable interrupts or else we risk racing with the interrupt
 	 * based handler.
 	 */
-	if (up->port.irq) {
-		ier = serial_in(up, UART_IER);
-		serial_out(up, UART_IER, 0);
-	}
+	if (up->port.irq)
+		clear_ier(up);
 
 	iir = serial_in(up, UART_IIR);
 
@@ -301,7 +308,7 @@
 		serial8250_tx_chars(up);
 
 	if (up->port.irq)
-		serial_out(up, UART_IER, ier);
+		restore_ier(up);
 
 	spin_unlock_irqrestore(&up->port.lock, flags);
 
@@ -579,6 +586,14 @@
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
+static void univ8250_console_write_atomic(struct console *co, const char *s,
+					  unsigned int count)
+{
+	struct uart_8250_port *up = &serial8250_ports[co->index];
+
+	serial8250_console_write_atomic(up, s, count);
+}
+
 static void univ8250_console_write(struct console *co, const char *s,
 				   unsigned int count)
 {
@@ -664,6 +679,7 @@
 
 static struct console univ8250_console = {
 	.name		= "ttyS",
+	.write_atomic	= univ8250_console_write_atomic,
 	.write		= univ8250_console_write,
 	.device		= uart_console_device,
 	.setup		= univ8250_console_setup,
diff -Nur linux-5.4.5/drivers/tty/serial/8250/8250_dma.c linux-5.4.5-new/drivers/tty/serial/8250/8250_dma.c
--- linux-5.4.5/drivers/tty/serial/8250/8250_dma.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/8250/8250_dma.c	2020-06-15 16:12:21.503730665 +0300
@@ -35,7 +35,7 @@
 
 	ret = serial8250_tx_dma(p);
 	if (ret)
-		serial8250_set_THRI(p);
+		serial8250_set_THRI_sier(p);
 
 	spin_unlock_irqrestore(&p->port.lock, flags);
 }
@@ -98,7 +98,7 @@
 	dma_async_issue_pending(dma->txchan);
 	if (dma->tx_err) {
 		dma->tx_err = 0;
-		serial8250_clear_THRI(p);
+		serial8250_clear_THRI_sier(p);
 	}
 	return 0;
 err:
diff -Nur linux-5.4.5/drivers/tty/serial/8250/8250.h linux-5.4.5-new/drivers/tty/serial/8250/8250.h
--- linux-5.4.5/drivers/tty/serial/8250/8250.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/8250/8250.h	2020-06-15 16:12:21.503730665 +0300
@@ -96,6 +96,10 @@
 #define SERIAL8250_SHARE_IRQS 0
 #endif
 
+void set_ier(struct uart_8250_port *up, unsigned char ier);
+void clear_ier(struct uart_8250_port *up);
+void restore_ier(struct uart_8250_port *up);
+
 #define SERIAL8250_PORT_FLAGS(_base, _irq, _flags)		\
 	{							\
 		.iobase		= _base,			\
@@ -139,6 +143,15 @@
 	return true;
 }
 
+static inline bool serial8250_set_THRI_sier(struct uart_8250_port *up)
+{
+	if (up->ier & UART_IER_THRI)
+		return false;
+	up->ier |= UART_IER_THRI;
+	set_ier(up, up->ier);
+	return true;
+}
+
 static inline bool serial8250_clear_THRI(struct uart_8250_port *up)
 {
 	if (!(up->ier & UART_IER_THRI))
@@ -148,6 +161,15 @@
 	return true;
 }
 
+static inline bool serial8250_clear_THRI_sier(struct uart_8250_port *up)
+{
+	if (!(up->ier & UART_IER_THRI))
+		return false;
+	up->ier &= ~UART_IER_THRI;
+	set_ier(up, up->ier);
+	return true;
+}
+
 struct uart_8250_port *serial8250_get_port(int line);
 
 void serial8250_rpm_get(struct uart_8250_port *p);
diff -Nur linux-5.4.5/drivers/tty/serial/8250/8250_port.c linux-5.4.5-new/drivers/tty/serial/8250/8250_port.c
--- linux-5.4.5/drivers/tty/serial/8250/8250_port.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/8250/8250_port.c	2020-06-15 16:12:21.503730665 +0300
@@ -721,7 +721,7 @@
 			serial_out(p, UART_EFR, UART_EFR_ECB);
 			serial_out(p, UART_LCR, 0);
 		}
-		serial_out(p, UART_IER, sleep ? UART_IERX_SLEEP : 0);
+		set_ier(p, sleep ? UART_IERX_SLEEP : 0);
 		if (p->capabilities & UART_CAP_EFR) {
 			serial_out(p, UART_LCR, UART_LCR_CONF_MODE_B);
 			serial_out(p, UART_EFR, efr);
@@ -1390,7 +1390,7 @@
 
 	up->ier &= ~(UART_IER_RLSI | UART_IER_RDI);
 	up->port.read_status_mask &= ~UART_LSR_DR;
-	serial_port_out(port, UART_IER, up->ier);
+	set_ier(up, up->ier);
 
 	serial8250_rpm_put(up);
 }
@@ -1408,7 +1408,7 @@
 		serial8250_clear_and_reinit_fifos(p);
 
 		p->ier |= UART_IER_RLSI | UART_IER_RDI;
-		serial_port_out(&p->port, UART_IER, p->ier);
+		set_ier(p, p->ier);
 	}
 }
 static enum hrtimer_restart serial8250_em485_handle_stop_tx(struct hrtimer *t)
@@ -1459,7 +1459,7 @@
 
 static inline void __do_stop_tx(struct uart_8250_port *p)
 {
-	if (serial8250_clear_THRI(p))
+	if (serial8250_clear_THRI_sier(p))
 		serial8250_rpm_put_tx(p);
 }
 
@@ -1509,7 +1509,7 @@
 	if (up->dma && !up->dma->tx_dma(up))
 		return;
 
-	if (serial8250_set_THRI(up)) {
+	if (serial8250_set_THRI_sier(up)) {
 		if (up->bugs & UART_BUG_TXEN) {
 			unsigned char lsr;
 
@@ -1616,7 +1616,7 @@
 	mctrl_gpio_disable_ms(up->gpios);
 
 	up->ier &= ~UART_IER_MSI;
-	serial_port_out(port, UART_IER, up->ier);
+	set_ier(up, up->ier);
 }
 
 static void serial8250_enable_ms(struct uart_port *port)
@@ -1632,7 +1632,7 @@
 	up->ier |= UART_IER_MSI;
 
 	serial8250_rpm_get(up);
-	serial_port_out(port, UART_IER, up->ier);
+	set_ier(up, up->ier);
 	serial8250_rpm_put(up);
 }
 
@@ -1991,6 +1991,54 @@
 	}
 }
 
+static atomic_t ier_counter = ATOMIC_INIT(0);
+static atomic_t ier_value = ATOMIC_INIT(0);
+
+void set_ier(struct uart_8250_port *up, unsigned char ier)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+
+	console_atomic_lock(&flags);
+	if (atomic_read(&ier_counter) > 0)
+		atomic_set(&ier_value, ier);
+	else
+		serial_port_out(port, UART_IER, ier);
+	console_atomic_unlock(flags);
+}
+
+void clear_ier(struct uart_8250_port *up)
+{
+	struct uart_port *port = &up->port;
+	unsigned int ier_cleared = 0;
+	unsigned int flags;
+	unsigned int ier;
+
+	console_atomic_lock(&flags);
+	atomic_inc(&ier_counter);
+	ier = serial_port_in(port, UART_IER);
+	if (up->capabilities & UART_CAP_UUE)
+		ier_cleared = UART_IER_UUE;
+	if (ier != ier_cleared) {
+		serial_port_out(port, UART_IER, ier_cleared);
+		atomic_set(&ier_value, ier);
+	}
+	console_atomic_unlock(flags);
+}
+EXPORT_SYMBOL_GPL(clear_ier);
+
+void restore_ier(struct uart_8250_port *up)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+
+	console_atomic_lock(&flags);
+	if (atomic_fetch_dec(&ier_counter) == 1)
+		serial_port_out(port, UART_IER, atomic_read(&ier_value));
+	console_atomic_unlock(flags);
+}
+EXPORT_SYMBOL_GPL(restore_ier);
+
 #ifdef CONFIG_CONSOLE_POLL
 /*
  * Console polling routines for writing and reading from the uart while
@@ -2022,18 +2070,10 @@
 static void serial8250_put_poll_char(struct uart_port *port,
 			 unsigned char c)
 {
-	unsigned int ier;
 	struct uart_8250_port *up = up_to_u8250p(port);
 
 	serial8250_rpm_get(up);
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	clear_ier(up);
 
 	wait_for_xmitr(up, BOTH_EMPTY);
 	/*
@@ -2046,7 +2086,7 @@
 	 *	and restore the IER
 	 */
 	wait_for_xmitr(up, BOTH_EMPTY);
-	serial_port_out(port, UART_IER, ier);
+	restore_ier(up);
 	serial8250_rpm_put(up);
 }
 
@@ -2354,7 +2394,7 @@
 	 */
 	spin_lock_irqsave(&port->lock, flags);
 	up->ier = 0;
-	serial_port_out(port, UART_IER, 0);
+	set_ier(up, 0);
 	spin_unlock_irqrestore(&port->lock, flags);
 
 	synchronize_irq(port->irq);
@@ -2639,7 +2679,7 @@
 	if (up->capabilities & UART_CAP_RTOIE)
 		up->ier |= UART_IER_RTOIE;
 
-	serial_port_out(port, UART_IER, up->ier);
+	set_ier(up, up->ier);
 
 	if (up->capabilities & UART_CAP_EFR) {
 		unsigned char efr = 0;
@@ -3103,7 +3143,7 @@
 
 #ifdef CONFIG_SERIAL_8250_CONSOLE
 
-static void serial8250_console_putchar(struct uart_port *port, int ch)
+static void serial8250_console_putchar_locked(struct uart_port *port, int ch)
 {
 	struct uart_8250_port *up = up_to_u8250p(port);
 
@@ -3111,6 +3151,18 @@
 	serial_port_out(port, UART_TX, ch);
 }
 
+static void serial8250_console_putchar(struct uart_port *port, int ch)
+{
+	struct uart_8250_port *up = up_to_u8250p(port);
+	unsigned int flags;
+
+	wait_for_xmitr(up, UART_LSR_THRE);
+
+	console_atomic_lock(&flags);
+	serial8250_console_putchar_locked(port, ch);
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Restore serial console when h/w power-off detected
  */
@@ -3132,6 +3184,31 @@
 	serial8250_out_MCR(up, UART_MCR_DTR | UART_MCR_RTS);
 }
 
+void serial8250_console_write_atomic(struct uart_8250_port *up,
+				     const char *s, unsigned int count)
+{
+	struct uart_port *port = &up->port;
+	unsigned int flags;
+
+	console_atomic_lock(&flags);
+
+	touch_nmi_watchdog();
+
+	clear_ier(up);
+
+	if (atomic_fetch_inc(&up->console_printing)) {
+		uart_console_write(port, "\n", 1,
+				   serial8250_console_putchar_locked);
+	}
+	uart_console_write(port, s, count, serial8250_console_putchar_locked);
+	atomic_dec(&up->console_printing);
+
+	wait_for_xmitr(up, BOTH_EMPTY);
+	restore_ier(up);
+
+	console_atomic_unlock(flags);
+}
+
 /*
  *	Print a string to the serial port trying not to disturb
  *	any possible real use of the port...
@@ -3143,27 +3220,13 @@
 {
 	struct uart_port *port = &up->port;
 	unsigned long flags;
-	unsigned int ier;
-	int locked = 1;
 
 	touch_nmi_watchdog();
 
 	serial8250_rpm_get(up);
+	spin_lock_irqsave(&port->lock, flags);
 
-	if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
-	else
-		spin_lock_irqsave(&port->lock, flags);
-
-	/*
-	 *	First save the IER then disable the interrupts
-	 */
-	ier = serial_port_in(port, UART_IER);
-
-	if (up->capabilities & UART_CAP_UUE)
-		serial_port_out(port, UART_IER, UART_IER_UUE);
-	else
-		serial_port_out(port, UART_IER, 0);
+	clear_ier(up);
 
 	/* check scratch reg to see if port powered off during system sleep */
 	if (up->canary && (up->canary != serial_port_in(port, UART_SCR))) {
@@ -3171,14 +3234,16 @@
 		up->canary = 0;
 	}
 
+	atomic_inc(&up->console_printing);
 	uart_console_write(port, s, count, serial8250_console_putchar);
+	atomic_dec(&up->console_printing);
 
 	/*
 	 *	Finally, wait for transmitter to become empty
 	 *	and restore the IER
 	 */
 	wait_for_xmitr(up, BOTH_EMPTY);
-	serial_port_out(port, UART_IER, ier);
+	restore_ier(up);
 
 	/*
 	 *	The receive handling will happen properly because the
@@ -3190,8 +3255,7 @@
 	if (up->msr_saved_flags)
 		serial8250_modem_status(up);
 
-	if (locked)
-		spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 	serial8250_rpm_put(up);
 }
 
@@ -3212,6 +3276,7 @@
 
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe)
 {
+	struct uart_8250_port *up = up_to_u8250p(port);
 	int baud = 9600;
 	int bits = 8;
 	int parity = 'n';
@@ -3220,6 +3285,8 @@
 	if (!port->iobase && !port->membase)
 		return -ENODEV;
 
+	atomic_set(&up->console_printing, 0);
+
 	if (options)
 		uart_parse_options(options, &baud, &parity, &bits, &flow);
 	else if (probe)
diff -Nur linux-5.4.5/drivers/tty/serial/amba-pl011.c linux-5.4.5-new/drivers/tty/serial/amba-pl011.c
--- linux-5.4.5/drivers/tty/serial/amba-pl011.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/amba-pl011.c	2020-06-15 16:12:21.567730440 +0300
@@ -2209,18 +2209,24 @@
 {
 	struct uart_amba_port *uap = amba_ports[co->index];
 	unsigned int old_cr = 0, new_cr;
-	unsigned long flags;
+	unsigned long flags = 0;
 	int locked = 1;
 
 	clk_enable(uap->clk);
 
-	local_irq_save(flags);
+	/*
+	 * local_irq_save(flags);
+	 *
+	 * This local_irq_save() is nonsense. If we come in via sysrq
+	 * handling then interrupts are already disabled. Aside of
+	 * that the port.sysrq check is racy on SMP regardless.
+	*/
 	if (uap->port.sysrq)
 		locked = 0;
 	else if (oops_in_progress)
-		locked = spin_trylock(&uap->port.lock);
+		locked = spin_trylock_irqsave(&uap->port.lock, flags);
 	else
-		spin_lock(&uap->port.lock);
+		spin_lock_irqsave(&uap->port.lock, flags);
 
 	/*
 	 *	First save the CR then disable the interrupts
@@ -2246,8 +2252,7 @@
 		pl011_write(old_cr, uap, REG_CR);
 
 	if (locked)
-		spin_unlock(&uap->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&uap->port.lock, flags);
 
 	clk_disable(uap->clk);
 }
diff -Nur linux-5.4.5/drivers/tty/serial/omap-serial.c linux-5.4.5-new/drivers/tty/serial/omap-serial.c
--- linux-5.4.5/drivers/tty/serial/omap-serial.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/tty/serial/omap-serial.c	2020-06-15 16:12:21.567730440 +0300
@@ -1307,13 +1307,10 @@
 
 	pm_runtime_get_sync(up->dev);
 
-	local_irq_save(flags);
-	if (up->port.sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock(&up->port.lock);
+	if (up->port.sysrq || oops_in_progress)
+		locked = spin_trylock_irqsave(&up->port.lock, flags);
 	else
-		spin_lock(&up->port.lock);
+		spin_lock_irqsave(&up->port.lock, flags);
 
 	/*
 	 * First save the IER then disable the interrupts
@@ -1342,8 +1339,7 @@
 	pm_runtime_mark_last_busy(up->dev);
 	pm_runtime_put_autosuspend(up->dev);
 	if (locked)
-		spin_unlock(&up->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&up->port.lock, flags);
 }
 
 static int __init
diff -Nur linux-5.4.5/drivers/usb/gadget/function/f_fs.c linux-5.4.5-new/drivers/usb/gadget/function/f_fs.c
--- linux-5.4.5/drivers/usb/gadget/function/f_fs.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/usb/gadget/function/f_fs.c	2020-06-15 16:12:21.755729779 +0300
@@ -1715,7 +1715,7 @@
 		pr_info("%s(): freeing\n", __func__);
 		ffs_data_clear(ffs);
 		BUG_ON(waitqueue_active(&ffs->ev.waitq) ||
-		       waitqueue_active(&ffs->ep0req_completion.wait) ||
+		       swait_active(&ffs->ep0req_completion.wait) ||
 		       waitqueue_active(&ffs->wait));
 		destroy_workqueue(ffs->io_completion_wq);
 		kfree(ffs->dev_name);
diff -Nur linux-5.4.5/drivers/usb/gadget/legacy/inode.c linux-5.4.5-new/drivers/usb/gadget/legacy/inode.c
--- linux-5.4.5/drivers/usb/gadget/legacy/inode.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/usb/gadget/legacy/inode.c	2020-06-15 16:12:21.763729751 +0300
@@ -344,7 +344,7 @@
 	spin_unlock_irq (&epdata->dev->lock);
 
 	if (likely (value == 0)) {
-		value = wait_event_interruptible (done.wait, done.done);
+		value = swait_event_interruptible_exclusive(done.wait, done.done);
 		if (value != 0) {
 			spin_lock_irq (&epdata->dev->lock);
 			if (likely (epdata->ep != NULL)) {
@@ -353,7 +353,7 @@
 				usb_ep_dequeue (epdata->ep, epdata->req);
 				spin_unlock_irq (&epdata->dev->lock);
 
-				wait_event (done.wait, done.done);
+				swait_event_exclusive(done.wait, done.done);
 				if (epdata->status == -ECONNRESET)
 					epdata->status = -EINTR;
 			} else {
diff -Nur linux-5.4.5/drivers/video/backlight/Kconfig linux-5.4.5-new/drivers/video/backlight/Kconfig
--- linux-5.4.5/drivers/video/backlight/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/video/backlight/Kconfig	2020-06-15 16:12:22.047728752 +0300
@@ -99,7 +99,7 @@
 
 config LCD_HP700
 	tristate "HP Jornada 700 series LCD Driver"
-	depends on SA1100_JORNADA720_SSP && !PREEMPT
+	depends on SA1100_JORNADA720_SSP && !PREEMPTION
 	default y
 	help
 	  If you have an HP Jornada 700 series handheld (710/720/728)
@@ -228,7 +228,7 @@
 
 config BACKLIGHT_HP700
 	tristate "HP Jornada 700 series Backlight Driver"
-	depends on SA1100_JORNADA720_SSP && !PREEMPT
+	depends on SA1100_JORNADA720_SSP && !PREEMPTION
 	default y
 	help
 	  If you have an HP Jornada 700 series,
diff -Nur linux-5.4.5/drivers/watchdog/watchdog_dev.c linux-5.4.5-new/drivers/watchdog/watchdog_dev.c
--- linux-5.4.5/drivers/watchdog/watchdog_dev.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/watchdog/watchdog_dev.c	2020-06-15 16:12:22.427727416 +0300
@@ -158,7 +158,7 @@
 		ktime_t t = watchdog_next_keepalive(wdd);
 
 		if (t > 0)
-			hrtimer_start(&wd_data->timer, t, HRTIMER_MODE_REL);
+			hrtimer_start(&wd_data->timer, t, HRTIMER_MODE_REL_HARD);
 	} else {
 		hrtimer_cancel(&wd_data->timer);
 	}
@@ -177,7 +177,7 @@
 	if (ktime_after(earliest_keepalive, now)) {
 		hrtimer_start(&wd_data->timer,
 			      ktime_sub(earliest_keepalive, now),
-			      HRTIMER_MODE_REL);
+			      HRTIMER_MODE_REL_HARD);
 		return 0;
 	}
 
@@ -971,7 +971,7 @@
 		return -ENODEV;
 
 	kthread_init_work(&wd_data->work, watchdog_ping_work);
-	hrtimer_init(&wd_data->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	hrtimer_init(&wd_data->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
 	wd_data->timer.function = watchdog_timer_expired;
 
 	if (wdd->id == 0) {
@@ -1019,7 +1019,7 @@
 		__module_get(wdd->ops->owner);
 		kref_get(&wd_data->kref);
 		if (handle_boot_enabled)
-			hrtimer_start(&wd_data->timer, 0, HRTIMER_MODE_REL);
+			hrtimer_start(&wd_data->timer, 0, HRTIMER_MODE_REL_HARD);
 		else
 			pr_info("watchdog%d running and kernel based pre-userspace handler disabled\n",
 				wdd->id);
diff -Nur linux-5.4.5/drivers/xen/preempt.c linux-5.4.5-new/drivers/xen/preempt.c
--- linux-5.4.5/drivers/xen/preempt.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/drivers/xen/preempt.c	2020-06-15 16:12:22.447727345 +0300
@@ -8,7 +8,7 @@
 #include <linux/sched.h>
 #include <xen/xen-ops.h>
 
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 
 /*
  * Some hypercalls issued by the toolstack can take many 10s of
@@ -37,4 +37,4 @@
 		__this_cpu_write(xen_in_preemptible_hcall, true);
 	}
 }
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
diff -Nur linux-5.4.5/fs/afs/dir_silly.c linux-5.4.5-new/fs/afs/dir_silly.c
--- linux-5.4.5/fs/afs/dir_silly.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/afs/dir_silly.c	2020-06-15 16:12:22.471727261 +0300
@@ -202,7 +202,7 @@
 	struct dentry *alias;
 	int ret;
 
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	_enter("%p{%pd},%llx", dentry, dentry, vnode->fid.vnode);
 
diff -Nur linux-5.4.5/fs/btrfs/volumes.h linux-5.4.5-new/fs/btrfs/volumes.h
--- linux-5.4.5/fs/btrfs/volumes.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/btrfs/volumes.h	2020-06-15 16:12:22.679726530 +0300
@@ -179,7 +179,7 @@
 	write_seqcount_end(&dev->data_seqcount);			\
 	preempt_enable();						\
 }
-#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)
+#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPTION)
 #define BTRFS_DEVICE_GETSET_FUNCS(name)					\
 static inline u64							\
 btrfs_device_get_##name(const struct btrfs_device *dev)			\
diff -Nur linux-5.4.5/fs/buffer.c linux-5.4.5-new/fs/buffer.c
--- linux-5.4.5/fs/buffer.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/buffer.c	2020-06-15 16:12:22.507727134 +0300
@@ -275,8 +275,7 @@
 	 * decide that the page is now completely done.
 	 */
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	spin_lock_irqsave(&first->b_uptodate_lock, flags);
 	clear_buffer_async_read(bh);
 	unlock_buffer(bh);
 	tmp = bh;
@@ -289,8 +288,7 @@
 		}
 		tmp = tmp->b_this_page;
 	} while (tmp != bh);
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 
 	/*
 	 * If none of the buffers had errors and they are all
@@ -302,8 +300,7 @@
 	return;
 
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 	return;
 }
 
@@ -331,8 +328,7 @@
 	}
 
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	spin_lock_irqsave(&first->b_uptodate_lock, flags);
 
 	clear_buffer_async_write(bh);
 	unlock_buffer(bh);
@@ -344,14 +340,12 @@
 		}
 		tmp = tmp->b_this_page;
 	}
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 	end_page_writeback(page);
 	return;
 
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 	return;
 }
 EXPORT_SYMBOL(end_buffer_async_write);
@@ -3368,6 +3362,7 @@
 	struct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);
 	if (ret) {
 		INIT_LIST_HEAD(&ret->b_assoc_buffers);
+		spin_lock_init(&ret->b_uptodate_lock);
 		preempt_disable();
 		__this_cpu_inc(bh_accounting.nr);
 		recalc_bh_state();
diff -Nur linux-5.4.5/fs/cifs/readdir.c linux-5.4.5-new/fs/cifs/readdir.c
--- linux-5.4.5/fs/cifs/readdir.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/cifs/readdir.c	2020-06-15 16:12:22.719726388 +0300
@@ -80,7 +80,7 @@
 	struct inode *inode;
 	struct super_block *sb = parent->d_sb;
 	struct cifs_sb_info *cifs_sb = CIFS_SB(sb);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	cifs_dbg(FYI, "%s: for %s\n", __func__, name->name);
 
diff -Nur linux-5.4.5/fs/dcache.c linux-5.4.5-new/fs/dcache.c
--- linux-5.4.5/fs/dcache.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/dcache.c	2020-06-15 16:12:22.507727134 +0300
@@ -2482,9 +2482,10 @@
 static inline unsigned start_dir_add(struct inode *dir)
 {
 
+	preempt_disable_rt();
 	for (;;) {
-		unsigned n = dir->i_dir_seq;
-		if (!(n & 1) && cmpxchg(&dir->i_dir_seq, n, n + 1) == n)
+		unsigned n = dir->__i_dir_seq;
+		if (!(n & 1) && cmpxchg(&dir->__i_dir_seq, n, n + 1) == n)
 			return n;
 		cpu_relax();
 	}
@@ -2492,26 +2493,30 @@
 
 static inline void end_dir_add(struct inode *dir, unsigned n)
 {
-	smp_store_release(&dir->i_dir_seq, n + 2);
+	smp_store_release(&dir->__i_dir_seq, n + 2);
+	preempt_enable_rt();
 }
 
 static void d_wait_lookup(struct dentry *dentry)
 {
-	if (d_in_lookup(dentry)) {
-		DECLARE_WAITQUEUE(wait, current);
-		add_wait_queue(dentry->d_wait, &wait);
-		do {
-			set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock(&dentry->d_lock);
-			schedule();
-			spin_lock(&dentry->d_lock);
-		} while (d_in_lookup(dentry));
-	}
+	struct swait_queue __wait;
+
+	if (!d_in_lookup(dentry))
+		return;
+
+	INIT_LIST_HEAD(&__wait.task_list);
+	do {
+		prepare_to_swait_exclusive(dentry->d_wait, &__wait, TASK_UNINTERRUPTIBLE);
+		spin_unlock(&dentry->d_lock);
+		schedule();
+		spin_lock(&dentry->d_lock);
+	} while (d_in_lookup(dentry));
+	finish_swait(dentry->d_wait, &__wait);
 }
 
 struct dentry *d_alloc_parallel(struct dentry *parent,
 				const struct qstr *name,
-				wait_queue_head_t *wq)
+				struct swait_queue_head *wq)
 {
 	unsigned int hash = name->hash;
 	struct hlist_bl_head *b = in_lookup_hash(parent, hash);
@@ -2525,7 +2530,7 @@
 
 retry:
 	rcu_read_lock();
-	seq = smp_load_acquire(&parent->d_inode->i_dir_seq);
+	seq = smp_load_acquire(&parent->d_inode->__i_dir_seq);
 	r_seq = read_seqbegin(&rename_lock);
 	dentry = __d_lookup_rcu(parent, name, &d_seq);
 	if (unlikely(dentry)) {
@@ -2553,7 +2558,7 @@
 	}
 
 	hlist_bl_lock(b);
-	if (unlikely(READ_ONCE(parent->d_inode->i_dir_seq) != seq)) {
+	if (unlikely(READ_ONCE(parent->d_inode->__i_dir_seq) != seq)) {
 		hlist_bl_unlock(b);
 		rcu_read_unlock();
 		goto retry;
@@ -2626,7 +2631,7 @@
 	hlist_bl_lock(b);
 	dentry->d_flags &= ~DCACHE_PAR_LOOKUP;
 	__hlist_bl_del(&dentry->d_u.d_in_lookup_hash);
-	wake_up_all(dentry->d_wait);
+	swake_up_all(dentry->d_wait);
 	dentry->d_wait = NULL;
 	hlist_bl_unlock(b);
 	INIT_HLIST_NODE(&dentry->d_u.d_alias);
@@ -3139,6 +3144,8 @@
 
 static void __init dcache_init_early(void)
 {
+	unsigned int loop;
+
 	/* If hashes are distributed across NUMA nodes, defer
 	 * hash allocation until vmalloc space is available.
 	 */
@@ -3155,11 +3162,16 @@
 					NULL,
 					0,
 					0);
+
+	for (loop = 0; loop < (1U << d_hash_shift); loop++)
+		INIT_HLIST_BL_HEAD(dentry_hashtable + loop);
+
 	d_hash_shift = 32 - d_hash_shift;
 }
 
 static void __init dcache_init(void)
 {
+	unsigned int loop;
 	/*
 	 * A constructor could be added for stable state like the lists,
 	 * but it is probably not worth it because of the cache nature
@@ -3183,6 +3195,10 @@
 					NULL,
 					0,
 					0);
+
+	for (loop = 0; loop < (1U << d_hash_shift); loop++)
+		INIT_HLIST_BL_HEAD(dentry_hashtable + loop);
+
 	d_hash_shift = 32 - d_hash_shift;
 }
 
diff -Nur linux-5.4.5/fs/eventpoll.c linux-5.4.5-new/fs/eventpoll.c
--- linux-5.4.5/fs/eventpoll.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/eventpoll.c	2020-06-15 16:12:22.507727134 +0300
@@ -567,12 +567,12 @@
 
 static void ep_poll_safewake(wait_queue_head_t *wq)
 {
-	int this_cpu = get_cpu();
+	int this_cpu = get_cpu_light();
 
 	ep_call_nested(&poll_safewake_ncalls,
 		       ep_poll_wakeup_proc, NULL, wq, (void *) (long) this_cpu);
 
-	put_cpu();
+	put_cpu_light();
 }
 
 #else
diff -Nur linux-5.4.5/fs/ext4/page-io.c linux-5.4.5-new/fs/ext4/page-io.c
--- linux-5.4.5/fs/ext4/page-io.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/ext4/page-io.c	2020-06-15 16:12:22.923725671 +0300
@@ -87,11 +87,10 @@
 		}
 		bh = head = page_buffers(page);
 		/*
-		 * We check all buffers in the page under BH_Uptodate_Lock
+		 * We check all buffers in the page under b_uptodate_lock
 		 * to avoid races with other end io clearing async_write flags
 		 */
-		local_irq_save(flags);
-		bit_spin_lock(BH_Uptodate_Lock, &head->b_state);
+		spin_lock_irqsave(&head->b_uptodate_lock, flags);
 		do {
 			if (bh_offset(bh) < bio_start ||
 			    bh_offset(bh) + bh->b_size > bio_end) {
@@ -103,8 +102,7 @@
 			if (bio->bi_status)
 				buffer_io_error(bh);
 		} while ((bh = bh->b_this_page) != head);
-		bit_spin_unlock(BH_Uptodate_Lock, &head->b_state);
-		local_irq_restore(flags);
+		spin_unlock_irqrestore(&head->b_uptodate_lock, flags);
 		if (!under_io) {
 			fscrypt_free_bounce_page(bounce_page);
 			end_page_writeback(page);
diff -Nur linux-5.4.5/fs/fscache/cookie.c linux-5.4.5-new/fs/fscache/cookie.c
--- linux-5.4.5/fs/fscache/cookie.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/fscache/cookie.c	2020-06-15 16:12:23.011725362 +0300
@@ -958,3 +958,11 @@
 	return -ESTALE;
 }
 EXPORT_SYMBOL(__fscache_check_consistency);
+
+void __init fscache_cookie_init(void)
+{
+	int i;
+
+	for (i = 0; i < (1 << fscache_cookie_hash_shift) - 1; i++)
+		INIT_HLIST_BL_HEAD(&fscache_cookie_hash[i]);
+}
diff -Nur linux-5.4.5/fs/fscache/main.c linux-5.4.5-new/fs/fscache/main.c
--- linux-5.4.5/fs/fscache/main.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/fscache/main.c	2020-06-15 16:12:23.011725362 +0300
@@ -145,6 +145,7 @@
 		ret = -ENOMEM;
 		goto error_cookie_jar;
 	}
+	fscache_cookie_init();
 
 	fscache_root = kobject_create_and_add("fscache", kernel_kobj);
 	if (!fscache_root)
diff -Nur linux-5.4.5/fs/fuse/readdir.c linux-5.4.5-new/fs/fuse/readdir.c
--- linux-5.4.5/fs/fuse/readdir.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/fuse/readdir.c	2020-06-15 16:12:23.015725348 +0300
@@ -158,7 +158,7 @@
 	struct inode *dir = d_inode(parent);
 	struct fuse_conn *fc;
 	struct inode *inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (!o->nodeid) {
 		/*
diff -Nur linux-5.4.5/fs/inode.c linux-5.4.5-new/fs/inode.c
--- linux-5.4.5/fs/inode.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/inode.c	2020-06-15 16:12:22.511727120 +0300
@@ -156,7 +156,7 @@
 	inode->i_bdev = NULL;
 	inode->i_cdev = NULL;
 	inode->i_link = NULL;
-	inode->i_dir_seq = 0;
+	inode->__i_dir_seq = 0;
 	inode->i_rdev = 0;
 	inode->dirtied_when = 0;
 
diff -Nur linux-5.4.5/fs/jbd2/commit.c linux-5.4.5-new/fs/jbd2/commit.c
--- linux-5.4.5/fs/jbd2/commit.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/jbd2/commit.c	2020-06-15 16:12:23.075725137 +0300
@@ -482,10 +482,10 @@
 		if (jh->b_committed_data) {
 			struct buffer_head *bh = jh2bh(jh);
 
-			jbd_lock_bh_state(bh);
+			spin_lock(&jh->b_state_lock);
 			jbd2_free(jh->b_committed_data, bh->b_size);
 			jh->b_committed_data = NULL;
-			jbd_unlock_bh_state(bh);
+			spin_unlock(&jh->b_state_lock);
 		}
 		jbd2_journal_refile_buffer(journal, jh);
 	}
@@ -918,6 +918,7 @@
 		transaction_t *cp_transaction;
 		struct buffer_head *bh;
 		int try_to_free = 0;
+		bool drop_ref;
 
 		jh = commit_transaction->t_forget;
 		spin_unlock(&journal->j_list_lock);
@@ -927,7 +928,7 @@
 		 * done with it.
 		 */
 		get_bh(bh);
-		jbd_lock_bh_state(bh);
+		spin_lock(&jh->b_state_lock);
 		J_ASSERT_JH(jh,	jh->b_transaction == commit_transaction);
 
 		/*
@@ -1022,8 +1023,10 @@
 				try_to_free = 1;
 		}
 		JBUFFER_TRACE(jh, "refile or unfile buffer");
-		__jbd2_journal_refile_buffer(jh);
-		jbd_unlock_bh_state(bh);
+		drop_ref = __jbd2_journal_refile_buffer(jh);
+		spin_unlock(&jh->b_state_lock);
+		if (drop_ref)
+			jbd2_journal_put_journal_head(jh);
 		if (try_to_free)
 			release_buffer_page(bh);	/* Drops bh reference */
 		else
diff -Nur linux-5.4.5/fs/jbd2/journal.c linux-5.4.5-new/fs/jbd2/journal.c
--- linux-5.4.5/fs/jbd2/journal.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/jbd2/journal.c	2020-06-15 16:12:23.075725137 +0300
@@ -363,7 +363,7 @@
 	/* keep subsequent assertions sane */
 	atomic_set(&new_bh->b_count, 1);
 
-	jbd_lock_bh_state(bh_in);
+	spin_lock(&jh_in->b_state_lock);
 repeat:
 	/*
 	 * If a new transaction has already done a buffer copy-out, then
@@ -405,13 +405,13 @@
 	if (need_copy_out && !done_copy_out) {
 		char *tmp;
 
-		jbd_unlock_bh_state(bh_in);
+		spin_unlock(&jh_in->b_state_lock);
 		tmp = jbd2_alloc(bh_in->b_size, GFP_NOFS);
 		if (!tmp) {
 			brelse(new_bh);
 			return -ENOMEM;
 		}
-		jbd_lock_bh_state(bh_in);
+		spin_lock(&jh_in->b_state_lock);
 		if (jh_in->b_frozen_data) {
 			jbd2_free(tmp, bh_in->b_size);
 			goto repeat;
@@ -464,7 +464,7 @@
 	__jbd2_journal_file_buffer(jh_in, transaction, BJ_Shadow);
 	spin_unlock(&journal->j_list_lock);
 	set_buffer_shadow(bh_in);
-	jbd_unlock_bh_state(bh_in);
+	spin_unlock(&jh_in->b_state_lock);
 
 	return do_escape | (done_copy_out << 1);
 }
@@ -2410,6 +2410,8 @@
 		ret = kmem_cache_zalloc(jbd2_journal_head_cache,
 				GFP_NOFS | __GFP_NOFAIL);
 	}
+	if (ret)
+		spin_lock_init(&ret->b_state_lock);
 	return ret;
 }
 
@@ -2529,17 +2531,23 @@
 	J_ASSERT_BH(bh, buffer_jbd(bh));
 	J_ASSERT_BH(bh, jh2bh(jh) == bh);
 	BUFFER_TRACE(bh, "remove journal_head");
+
+	/* Unlink before dropping the lock */
+	bh->b_private = NULL;
+	jh->b_bh = NULL;	/* debug, really */
+	clear_buffer_jbd(bh);
+}
+
+static void journal_release_journal_head(struct journal_head *jh, size_t b_size)
+{
 	if (jh->b_frozen_data) {
 		printk(KERN_WARNING "%s: freeing b_frozen_data\n", __func__);
-		jbd2_free(jh->b_frozen_data, bh->b_size);
+		jbd2_free(jh->b_frozen_data, b_size);
 	}
 	if (jh->b_committed_data) {
 		printk(KERN_WARNING "%s: freeing b_committed_data\n", __func__);
-		jbd2_free(jh->b_committed_data, bh->b_size);
+		jbd2_free(jh->b_committed_data, b_size);
 	}
-	bh->b_private = NULL;
-	jh->b_bh = NULL;	/* debug, really */
-	clear_buffer_jbd(bh);
 	journal_free_journal_head(jh);
 }
 
@@ -2557,9 +2565,11 @@
 	if (!jh->b_jcount) {
 		__journal_remove_journal_head(bh);
 		jbd_unlock_bh_journal_head(bh);
+		journal_release_journal_head(jh, bh->b_size);
 		__brelse(bh);
-	} else
+	} else {
 		jbd_unlock_bh_journal_head(bh);
+	}
 }
 
 /*
diff -Nur linux-5.4.5/fs/jbd2/transaction.c linux-5.4.5-new/fs/jbd2/transaction.c
--- linux-5.4.5/fs/jbd2/transaction.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/jbd2/transaction.c	2020-06-15 16:12:23.075725137 +0300
@@ -879,7 +879,7 @@
 
  	start_lock = jiffies;
 	lock_buffer(bh);
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 
 	/* If it takes too long to lock the buffer, trace it */
 	time_lock = jbd2_time_diff(start_lock, jiffies);
@@ -929,7 +929,7 @@
 
 	error = -EROFS;
 	if (is_handle_aborted(handle)) {
-		jbd_unlock_bh_state(bh);
+		spin_unlock(&jh->b_state_lock);
 		goto out;
 	}
 	error = 0;
@@ -993,7 +993,7 @@
 	 */
 	if (buffer_shadow(bh)) {
 		JBUFFER_TRACE(jh, "on shadow: sleep");
-		jbd_unlock_bh_state(bh);
+		spin_unlock(&jh->b_state_lock);
 		wait_on_bit_io(&bh->b_state, BH_Shadow, TASK_UNINTERRUPTIBLE);
 		goto repeat;
 	}
@@ -1014,7 +1014,7 @@
 		JBUFFER_TRACE(jh, "generate frozen data");
 		if (!frozen_buffer) {
 			JBUFFER_TRACE(jh, "allocate memory for buffer");
-			jbd_unlock_bh_state(bh);
+			spin_unlock(&jh->b_state_lock);
 			frozen_buffer = jbd2_alloc(jh2bh(jh)->b_size,
 						   GFP_NOFS | __GFP_NOFAIL);
 			goto repeat;
@@ -1033,7 +1033,7 @@
 	jh->b_next_transaction = transaction;
 
 done:
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
 
 	/*
 	 * If we are about to journal a buffer, then any revoke pending on it is
@@ -1172,7 +1172,7 @@
 	 * that case: the transaction must have deleted the buffer for it to be
 	 * reused here.
 	 */
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 	J_ASSERT_JH(jh, (jh->b_transaction == transaction ||
 		jh->b_transaction == NULL ||
 		(jh->b_transaction == journal->j_committing_transaction &&
@@ -1207,7 +1207,7 @@
 		jh->b_next_transaction = transaction;
 		spin_unlock(&journal->j_list_lock);
 	}
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
 
 	/*
 	 * akpm: I added this.  ext3_alloc_branch can pick up new indirect
@@ -1275,13 +1275,13 @@
 		committed_data = jbd2_alloc(jh2bh(jh)->b_size,
 					    GFP_NOFS|__GFP_NOFAIL);
 
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 	if (!jh->b_committed_data) {
 		/* Copy out the current buffer contents into the
 		 * preserved, committed copy. */
 		JBUFFER_TRACE(jh, "generate b_committed data");
 		if (!committed_data) {
-			jbd_unlock_bh_state(bh);
+			spin_unlock(&jh->b_state_lock);
 			goto repeat;
 		}
 
@@ -1289,7 +1289,7 @@
 		committed_data = NULL;
 		memcpy(jh->b_committed_data, bh->b_data, bh->b_size);
 	}
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
 out:
 	jbd2_journal_put_journal_head(jh);
 	if (unlikely(committed_data))
@@ -1390,16 +1390,16 @@
 	 */
 	if (jh->b_transaction != transaction &&
 	    jh->b_next_transaction != transaction) {
-		jbd_lock_bh_state(bh);
+		spin_lock(&jh->b_state_lock);
 		J_ASSERT_JH(jh, jh->b_transaction == transaction ||
 				jh->b_next_transaction == transaction);
-		jbd_unlock_bh_state(bh);
+		spin_unlock(&jh->b_state_lock);
 	}
 	if (jh->b_modified == 1) {
 		/* If it's in our transaction it must be in BJ_Metadata list. */
 		if (jh->b_transaction == transaction &&
 		    jh->b_jlist != BJ_Metadata) {
-			jbd_lock_bh_state(bh);
+			spin_lock(&jh->b_state_lock);
 			if (jh->b_transaction == transaction &&
 			    jh->b_jlist != BJ_Metadata)
 				pr_err("JBD2: assertion failure: h_type=%u "
@@ -1409,13 +1409,13 @@
 				       jh->b_jlist);
 			J_ASSERT_JH(jh, jh->b_transaction != transaction ||
 					jh->b_jlist == BJ_Metadata);
-			jbd_unlock_bh_state(bh);
+			spin_unlock(&jh->b_state_lock);
 		}
 		goto out;
 	}
 
 	journal = transaction->t_journal;
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 
 	if (jh->b_modified == 0) {
 		/*
@@ -1501,7 +1501,7 @@
 	__jbd2_journal_file_buffer(jh, transaction, BJ_Metadata);
 	spin_unlock(&journal->j_list_lock);
 out_unlock_bh:
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
 out:
 	JBUFFER_TRACE(jh, "exit");
 	return ret;
@@ -1539,18 +1539,20 @@
 
 	BUFFER_TRACE(bh, "entry");
 
-	jbd_lock_bh_state(bh);
+	jh = jbd2_journal_grab_journal_head(bh);
+	if (!jh) {
+		__bforget(bh);
+		return 0;
+	}
 
-	if (!buffer_jbd(bh))
-		goto not_jbd;
-	jh = bh2jh(bh);
+	spin_lock(&jh->b_state_lock);
 
 	/* Critical error: attempting to delete a bitmap buffer, maybe?
 	 * Don't do any jbd operations, and return an error. */
 	if (!J_EXPECT_JH(jh, !jh->b_committed_data,
 			 "inconsistent data on disk")) {
 		err = -EIO;
-		goto not_jbd;
+		goto drop;
 	}
 
 	/* keep track of whether or not this transaction modified us */
@@ -1598,10 +1600,7 @@
 			__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);
 		} else {
 			__jbd2_journal_unfile_buffer(jh);
-			if (!buffer_jbd(bh)) {
-				spin_unlock(&journal->j_list_lock);
-				goto not_jbd;
-			}
+			jbd2_journal_put_journal_head(jh);
 		}
 		spin_unlock(&journal->j_list_lock);
 	} else if (jh->b_transaction) {
@@ -1643,7 +1642,7 @@
 		if (!jh->b_cp_transaction) {
 			JBUFFER_TRACE(jh, "belongs to none transaction");
 			spin_unlock(&journal->j_list_lock);
-			goto not_jbd;
+			goto drop;
 		}
 
 		/*
@@ -1653,7 +1652,7 @@
 		if (!buffer_dirty(bh)) {
 			__jbd2_journal_remove_checkpoint(jh);
 			spin_unlock(&journal->j_list_lock);
-			goto not_jbd;
+			goto drop;
 		}
 
 		/*
@@ -1666,20 +1665,15 @@
 		__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);
 		spin_unlock(&journal->j_list_lock);
 	}
-
-	jbd_unlock_bh_state(bh);
-	__brelse(bh);
 drop:
+	__brelse(bh);
+	spin_unlock(&jh->b_state_lock);
+	jbd2_journal_put_journal_head(jh);
 	if (drop_reserve) {
 		/* no need to reserve log space for this block -bzzz */
 		handle->h_buffer_credits++;
 	}
 	return err;
-
-not_jbd:
-	jbd_unlock_bh_state(bh);
-	__bforget(bh);
-	goto drop;
 }
 
 /**
@@ -1878,7 +1872,7 @@
  *
  * j_list_lock is held.
  *
- * jbd_lock_bh_state(jh2bh(jh)) is held.
+ * jh->b_state_lock is held.
  */
 
 static inline void
@@ -1902,7 +1896,7 @@
  *
  * Called with j_list_lock held, and the journal may not be locked.
  *
- * jbd_lock_bh_state(jh2bh(jh)) is held.
+ * jh->b_state_lock is held.
  */
 
 static inline void
@@ -1934,7 +1928,7 @@
 	transaction_t *transaction;
 	struct buffer_head *bh = jh2bh(jh);
 
-	J_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));
+	lockdep_assert_held(&jh->b_state_lock);
 	transaction = jh->b_transaction;
 	if (transaction)
 		assert_spin_locked(&transaction->t_journal->j_list_lock);
@@ -1971,17 +1965,15 @@
 }
 
 /*
- * Remove buffer from all transactions.
+ * Remove buffer from all transactions. The caller is responsible for dropping
+ * the jh reference that belonged to the transaction.
  *
  * Called with bh_state lock and j_list_lock
- *
- * jh and bh may be already freed when this function returns.
  */
 static void __jbd2_journal_unfile_buffer(struct journal_head *jh)
 {
 	__jbd2_journal_temp_unlink_buffer(jh);
 	jh->b_transaction = NULL;
-	jbd2_journal_put_journal_head(jh);
 }
 
 void jbd2_journal_unfile_buffer(journal_t *journal, struct journal_head *jh)
@@ -1990,18 +1982,19 @@
 
 	/* Get reference so that buffer cannot be freed before we unlock it */
 	get_bh(bh);
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 	spin_lock(&journal->j_list_lock);
 	__jbd2_journal_unfile_buffer(jh);
 	spin_unlock(&journal->j_list_lock);
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
+	jbd2_journal_put_journal_head(jh);
 	__brelse(bh);
 }
 
 /*
  * Called from jbd2_journal_try_to_free_buffers().
  *
- * Called under jbd_lock_bh_state(bh)
+ * Called under jh->b_state_lock
  */
 static void
 __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)
@@ -2088,10 +2081,10 @@
 		if (!jh)
 			continue;
 
-		jbd_lock_bh_state(bh);
+		spin_lock(&jh->b_state_lock);
 		__journal_try_to_free_buffer(journal, bh);
+		spin_unlock(&jh->b_state_lock);
 		jbd2_journal_put_journal_head(jh);
-		jbd_unlock_bh_state(bh);
 		if (buffer_jbd(bh))
 			goto busy;
 	} while ((bh = bh->b_this_page) != head);
@@ -2112,7 +2105,7 @@
  *
  * Called under j_list_lock.
  *
- * Called under jbd_lock_bh_state(bh).
+ * Called under jh->b_state_lock.
  */
 static int __dispose_buffer(struct journal_head *jh, transaction_t *transaction)
 {
@@ -2133,6 +2126,7 @@
 	} else {
 		JBUFFER_TRACE(jh, "on running transaction");
 		__jbd2_journal_unfile_buffer(jh);
+		jbd2_journal_put_journal_head(jh);
 	}
 	return may_free;
 }
@@ -2199,18 +2193,15 @@
 	 * holding the page lock. --sct
 	 */
 
-	if (!buffer_jbd(bh))
+	jh = jbd2_journal_grab_journal_head(bh);
+	if (!jh)
 		goto zap_buffer_unlocked;
 
 	/* OK, we have data buffer in journaled mode */
 	write_lock(&journal->j_state_lock);
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 	spin_lock(&journal->j_list_lock);
 
-	jh = jbd2_journal_grab_journal_head(bh);
-	if (!jh)
-		goto zap_buffer_no_jh;
-
 	/*
 	 * We cannot remove the buffer from checkpoint lists until the
 	 * transaction adding inode to orphan list (let's call it T)
@@ -2289,10 +2280,10 @@
 		 * for commit and try again.
 		 */
 		if (partial_page) {
-			jbd2_journal_put_journal_head(jh);
 			spin_unlock(&journal->j_list_lock);
-			jbd_unlock_bh_state(bh);
+			spin_unlock(&jh->b_state_lock);
 			write_unlock(&journal->j_state_lock);
+			jbd2_journal_put_journal_head(jh);
 			return -EBUSY;
 		}
 		/*
@@ -2304,10 +2295,10 @@
 		set_buffer_freed(bh);
 		if (journal->j_running_transaction && buffer_jbddirty(bh))
 			jh->b_next_transaction = journal->j_running_transaction;
-		jbd2_journal_put_journal_head(jh);
 		spin_unlock(&journal->j_list_lock);
-		jbd_unlock_bh_state(bh);
+		spin_unlock(&jh->b_state_lock);
 		write_unlock(&journal->j_state_lock);
+		jbd2_journal_put_journal_head(jh);
 		return 0;
 	} else {
 		/* Good, the buffer belongs to the running transaction.
@@ -2331,11 +2322,10 @@
 	 * here.
 	 */
 	jh->b_modified = 0;
-	jbd2_journal_put_journal_head(jh);
-zap_buffer_no_jh:
 	spin_unlock(&journal->j_list_lock);
-	jbd_unlock_bh_state(bh);
+	spin_unlock(&jh->b_state_lock);
 	write_unlock(&journal->j_state_lock);
+	jbd2_journal_put_journal_head(jh);
 zap_buffer_unlocked:
 	clear_buffer_dirty(bh);
 	J_ASSERT_BH(bh, !buffer_jbddirty(bh));
@@ -2422,7 +2412,7 @@
 	int was_dirty = 0;
 	struct buffer_head *bh = jh2bh(jh);
 
-	J_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));
+	lockdep_assert_held(&jh->b_state_lock);
 	assert_spin_locked(&transaction->t_journal->j_list_lock);
 
 	J_ASSERT_JH(jh, jh->b_jlist < BJ_Types);
@@ -2484,11 +2474,11 @@
 void jbd2_journal_file_buffer(struct journal_head *jh,
 				transaction_t *transaction, int jlist)
 {
-	jbd_lock_bh_state(jh2bh(jh));
+	spin_lock(&jh->b_state_lock);
 	spin_lock(&transaction->t_journal->j_list_lock);
 	__jbd2_journal_file_buffer(jh, transaction, jlist);
 	spin_unlock(&transaction->t_journal->j_list_lock);
-	jbd_unlock_bh_state(jh2bh(jh));
+	spin_unlock(&jh->b_state_lock);
 }
 
 /*
@@ -2498,23 +2488,25 @@
  * buffer on that transaction's metadata list.
  *
  * Called under j_list_lock
- * Called under jbd_lock_bh_state(jh2bh(jh))
+ * Called under jh->b_state_lock
  *
- * jh and bh may be already free when this function returns
+ * When this function returns true, there's no next transaction to refile to
+ * and the caller has to drop jh reference through
+ * jbd2_journal_put_journal_head().
  */
-void __jbd2_journal_refile_buffer(struct journal_head *jh)
+bool __jbd2_journal_refile_buffer(struct journal_head *jh)
 {
 	int was_dirty, jlist;
 	struct buffer_head *bh = jh2bh(jh);
 
-	J_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));
+	lockdep_assert_held(&jh->b_state_lock);
 	if (jh->b_transaction)
 		assert_spin_locked(&jh->b_transaction->t_journal->j_list_lock);
 
 	/* If the buffer is now unused, just drop it. */
 	if (jh->b_next_transaction == NULL) {
 		__jbd2_journal_unfile_buffer(jh);
-		return;
+		return true;
 	}
 
 	/*
@@ -2542,6 +2534,7 @@
 
 	if (was_dirty)
 		set_buffer_jbddirty(bh);
+	return false;
 }
 
 /*
@@ -2552,16 +2545,15 @@
  */
 void jbd2_journal_refile_buffer(journal_t *journal, struct journal_head *jh)
 {
-	struct buffer_head *bh = jh2bh(jh);
+	bool drop;
 
-	/* Get reference so that buffer cannot be freed before we unlock it */
-	get_bh(bh);
-	jbd_lock_bh_state(bh);
+	spin_lock(&jh->b_state_lock);
 	spin_lock(&journal->j_list_lock);
-	__jbd2_journal_refile_buffer(jh);
-	jbd_unlock_bh_state(bh);
+	drop = __jbd2_journal_refile_buffer(jh);
+	spin_unlock(&jh->b_state_lock);
 	spin_unlock(&journal->j_list_lock);
-	__brelse(bh);
+	if (drop)
+		jbd2_journal_put_journal_head(jh);
 }
 
 /*
diff -Nur linux-5.4.5/fs/namei.c linux-5.4.5-new/fs/namei.c
--- linux-5.4.5/fs/namei.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/namei.c	2020-06-15 16:12:22.511727120 +0300
@@ -1637,7 +1637,7 @@
 {
 	struct dentry *dentry, *old;
 	struct inode *inode = dir->d_inode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	/* Don't go there if it's already dead */
 	if (unlikely(IS_DEADDIR(inode)))
@@ -3125,7 +3125,7 @@
 	struct dentry *dentry;
 	int error, create_error = 0;
 	umode_t mode = op->mode;
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 
 	if (unlikely(IS_DEADDIR(dir_inode)))
 		return -ENOENT;
diff -Nur linux-5.4.5/fs/namespace.c linux-5.4.5-new/fs/namespace.c
--- linux-5.4.5/fs/namespace.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/namespace.c	2020-06-15 16:12:22.511727120 +0300
@@ -14,6 +14,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/namei.h>
+#include <linux/delay.h>
 #include <linux/security.h>
 #include <linux/cred.h>
 #include <linux/idr.h>
@@ -321,8 +322,11 @@
 	 * incremented count after it has set MNT_WRITE_HOLD.
 	 */
 	smp_mb();
-	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
-		cpu_relax();
+	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD) {
+		preempt_enable();
+		cpu_chill();
+		preempt_disable();
+	}
 	/*
 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
 	 * be set to match its requirements. So we must not load that until
diff -Nur linux-5.4.5/fs/nfs/delegation.c linux-5.4.5-new/fs/nfs/delegation.c
--- linux-5.4.5/fs/nfs/delegation.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/delegation.c	2020-06-15 16:12:23.367724109 +0300
@@ -162,11 +162,11 @@
 		sp = state->owner;
 		/* Block nfs4_proc_unlck */
 		mutex_lock(&sp->so_delegreturn_mutex);
-		seq = raw_seqcount_begin(&sp->so_reclaim_seqcount);
+		seq = read_seqbegin(&sp->so_reclaim_seqlock);
 		err = nfs4_open_delegation_recall(ctx, state, stateid);
 		if (!err)
 			err = nfs_delegation_claim_locks(state, stateid);
-		if (!err && read_seqcount_retry(&sp->so_reclaim_seqcount, seq))
+		if (!err && read_seqretry(&sp->so_reclaim_seqlock, seq))
 			err = -EAGAIN;
 		mutex_unlock(&sp->so_delegreturn_mutex);
 		put_nfs_open_context(ctx);
diff -Nur linux-5.4.5/fs/nfs/dir.c linux-5.4.5-new/fs/nfs/dir.c
--- linux-5.4.5/fs/nfs/dir.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/dir.c	2020-06-15 16:12:23.367724109 +0300
@@ -445,7 +445,7 @@
 void nfs_prime_dcache(struct dentry *parent, struct nfs_entry *entry)
 {
 	struct qstr filename = QSTR_INIT(entry->name, entry->len);
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct dentry *dentry;
 	struct dentry *alias;
 	struct inode *dir = d_inode(parent);
@@ -1496,7 +1496,7 @@
 		    struct file *file, unsigned open_flags,
 		    umode_t mode)
 {
-	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+	DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 	struct nfs_open_context *ctx;
 	struct dentry *res;
 	struct iattr attr = { .ia_valid = ATTR_OPEN };
@@ -1825,7 +1825,11 @@
 
 	trace_nfs_rmdir_enter(dir, dentry);
 	if (d_really_is_positive(dentry)) {
+#ifdef CONFIG_PREEMPT_RT
+		down(&NFS_I(d_inode(dentry))->rmdir_sem);
+#else
 		down_write(&NFS_I(d_inode(dentry))->rmdir_sem);
+#endif
 		error = NFS_PROTO(dir)->rmdir(dir, &dentry->d_name);
 		/* Ensure the VFS deletes this inode */
 		switch (error) {
@@ -1835,7 +1839,11 @@
 		case -ENOENT:
 			nfs_dentry_handle_enoent(dentry);
 		}
+#ifdef CONFIG_PREEMPT_RT
+		up(&NFS_I(d_inode(dentry))->rmdir_sem);
+#else
 		up_write(&NFS_I(d_inode(dentry))->rmdir_sem);
+#endif
 	} else
 		error = NFS_PROTO(dir)->rmdir(dir, &dentry->d_name);
 	trace_nfs_rmdir_exit(dir, dentry, error);
diff -Nur linux-5.4.5/fs/nfs/inode.c linux-5.4.5-new/fs/nfs/inode.c
--- linux-5.4.5/fs/nfs/inode.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/inode.c	2020-06-15 16:12:23.367724109 +0300
@@ -2105,7 +2105,11 @@
 	atomic_long_set(&nfsi->nrequests, 0);
 	atomic_long_set(&nfsi->commit_info.ncommit, 0);
 	atomic_set(&nfsi->commit_info.rpcs_out, 0);
+#ifdef CONFIG_PREEMPT_RT
+	sema_init(&nfsi->rmdir_sem, 1);
+#else
 	init_rwsem(&nfsi->rmdir_sem);
+#endif
 	mutex_init(&nfsi->commit_mutex);
 	nfs4_init_once(nfsi);
 }
diff -Nur linux-5.4.5/fs/nfs/nfs4_fs.h linux-5.4.5-new/fs/nfs/nfs4_fs.h
--- linux-5.4.5/fs/nfs/nfs4_fs.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/nfs4_fs.h	2020-06-15 16:12:23.371724095 +0300
@@ -115,7 +115,7 @@
 	unsigned long	     so_flags;
 	struct list_head     so_states;
 	struct nfs_seqid_counter so_seqid;
-	seqcount_t	     so_reclaim_seqcount;
+	seqlock_t	     so_reclaim_seqlock;
 	struct mutex	     so_delegreturn_mutex;
 };
 
diff -Nur linux-5.4.5/fs/nfs/nfs4proc.c linux-5.4.5-new/fs/nfs/nfs4proc.c
--- linux-5.4.5/fs/nfs/nfs4proc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/nfs4proc.c	2020-06-15 16:12:23.371724095 +0300
@@ -2956,7 +2956,7 @@
 	unsigned int seq;
 	int ret;
 
-	seq = raw_seqcount_begin(&sp->so_reclaim_seqcount);
+	seq = raw_seqcount_begin(&sp->so_reclaim_seqlock.seqcount);
 
 	ret = _nfs4_proc_open(opendata, ctx);
 	if (ret != 0)
@@ -2998,7 +2998,7 @@
 
 	if (d_inode(dentry) == state->inode) {
 		nfs_inode_attach_open_context(ctx);
-		if (read_seqcount_retry(&sp->so_reclaim_seqcount, seq))
+		if (read_seqretry(&sp->so_reclaim_seqlock, seq))
 			nfs4_schedule_stateid_recovery(server, state);
 	}
 
diff -Nur linux-5.4.5/fs/nfs/nfs4state.c linux-5.4.5-new/fs/nfs/nfs4state.c
--- linux-5.4.5/fs/nfs/nfs4state.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/nfs4state.c	2020-06-15 16:12:23.371724095 +0300
@@ -510,7 +510,7 @@
 	nfs4_init_seqid_counter(&sp->so_seqid);
 	atomic_set(&sp->so_count, 1);
 	INIT_LIST_HEAD(&sp->so_lru);
-	seqcount_init(&sp->so_reclaim_seqcount);
+	seqlock_init(&sp->so_reclaim_seqlock);
 	mutex_init(&sp->so_delegreturn_mutex);
 	return sp;
 }
@@ -1618,8 +1618,12 @@
 	 * recovering after a network partition or a reboot from a
 	 * server that doesn't support a grace period.
 	 */
+#ifdef CONFIG_PREEMPT_RT
+	write_seqlock(&sp->so_reclaim_seqlock);
+#else
+	write_seqcount_begin(&sp->so_reclaim_seqlock.seqcount);
+#endif
 	spin_lock(&sp->so_lock);
-	raw_write_seqcount_begin(&sp->so_reclaim_seqcount);
 restart:
 	list_for_each_entry(state, &sp->so_states, open_states) {
 		if (!test_and_clear_bit(ops->state_flag_bit, &state->flags))
@@ -1680,14 +1684,20 @@
 		spin_lock(&sp->so_lock);
 		goto restart;
 	}
-	raw_write_seqcount_end(&sp->so_reclaim_seqcount);
 	spin_unlock(&sp->so_lock);
+#ifdef CONFIG_PREEMPT_RT
+	write_sequnlock(&sp->so_reclaim_seqlock);
+#else
+	write_seqcount_end(&sp->so_reclaim_seqlock.seqcount);
+#endif
 	return 0;
 out_err:
 	nfs4_put_open_state(state);
-	spin_lock(&sp->so_lock);
-	raw_write_seqcount_end(&sp->so_reclaim_seqcount);
-	spin_unlock(&sp->so_lock);
+#ifdef CONFIG_PREEMPT_RT
+	write_sequnlock(&sp->so_reclaim_seqlock);
+#else
+	write_seqcount_end(&sp->so_reclaim_seqlock.seqcount);
+#endif
 	return status;
 }
 
diff -Nur linux-5.4.5/fs/nfs/unlink.c linux-5.4.5-new/fs/nfs/unlink.c
--- linux-5.4.5/fs/nfs/unlink.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/nfs/unlink.c	2020-06-15 16:12:23.371724095 +0300
@@ -13,7 +13,7 @@
 #include <linux/sunrpc/clnt.h>
 #include <linux/nfs_fs.h>
 #include <linux/sched.h>
-#include <linux/wait.h>
+#include <linux/swait.h>
 #include <linux/namei.h>
 #include <linux/fsnotify.h>
 
@@ -53,6 +53,29 @@
 		rpc_restart_call_prepare(task);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+static void nfs_down_anon(struct semaphore *sema)
+{
+	down(sema);
+}
+
+static void nfs_up_anon(struct semaphore *sema)
+{
+	up(sema);
+}
+
+#else
+static void nfs_down_anon(struct rw_semaphore *rwsem)
+{
+	down_read_non_owner(rwsem);
+}
+
+static void nfs_up_anon(struct rw_semaphore *rwsem)
+{
+	up_read_non_owner(rwsem);
+}
+#endif
+
 /**
  * nfs_async_unlink_release - Release the sillydelete data.
  * @calldata: struct nfs_unlinkdata to release
@@ -66,7 +89,7 @@
 	struct dentry *dentry = data->dentry;
 	struct super_block *sb = dentry->d_sb;
 
-	up_read_non_owner(&NFS_I(d_inode(dentry->d_parent))->rmdir_sem);
+	nfs_up_anon(&NFS_I(d_inode(dentry->d_parent))->rmdir_sem);
 	d_lookup_done(dentry);
 	nfs_free_unlinkdata(data);
 	dput(dentry);
@@ -119,10 +142,10 @@
 	struct inode *dir = d_inode(dentry->d_parent);
 	struct dentry *alias;
 
-	down_read_non_owner(&NFS_I(dir)->rmdir_sem);
+	nfs_down_anon(&NFS_I(dir)->rmdir_sem);
 	alias = d_alloc_parallel(dentry->d_parent, &data->args.name, &data->wq);
 	if (IS_ERR(alias)) {
-		up_read_non_owner(&NFS_I(dir)->rmdir_sem);
+		nfs_up_anon(&NFS_I(dir)->rmdir_sem);
 		return 0;
 	}
 	if (!d_in_lookup(alias)) {
@@ -144,7 +167,7 @@
 			ret = 0;
 		spin_unlock(&alias->d_lock);
 		dput(alias);
-		up_read_non_owner(&NFS_I(dir)->rmdir_sem);
+		nfs_up_anon(&NFS_I(dir)->rmdir_sem);
 		/*
 		 * If we'd displaced old cached devname, free it.  At that
 		 * point dentry is definitely not a root, so we won't need
@@ -180,7 +203,7 @@
 
 	data->cred = get_current_cred();
 	data->res.dir_attr = &data->dir_attr;
-	init_waitqueue_head(&data->wq);
+	init_swait_queue_head(&data->wq);
 
 	status = -EBUSY;
 	spin_lock(&dentry->d_lock);
diff -Nur linux-5.4.5/fs/ntfs/aops.c linux-5.4.5-new/fs/ntfs/aops.c
--- linux-5.4.5/fs/ntfs/aops.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/ntfs/aops.c	2020-06-15 16:12:23.871722336 +0300
@@ -92,8 +92,7 @@
 				"0x%llx.", (unsigned long long)bh->b_blocknr);
 	}
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	spin_lock_irqsave(&first->b_uptodate_lock, flags);
 	clear_buffer_async_read(bh);
 	unlock_buffer(bh);
 	tmp = bh;
@@ -108,8 +107,7 @@
 		}
 		tmp = tmp->b_this_page;
 	} while (tmp != bh);
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 	/*
 	 * If none of the buffers had errors then we can set the page uptodate,
 	 * but we first have to perform the post read mst fixups, if the
@@ -142,8 +140,7 @@
 	unlock_page(page);
 	return;
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	spin_unlock_irqrestore(&first->b_uptodate_lock, flags);
 	return;
 }
 
diff -Nur linux-5.4.5/fs/ocfs2/suballoc.c linux-5.4.5-new/fs/ocfs2/suballoc.c
--- linux-5.4.5/fs/ocfs2/suballoc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/ocfs2/suballoc.c	2020-06-15 16:12:23.911722196 +0300
@@ -1252,6 +1252,7 @@
 					 int nr)
 {
 	struct ocfs2_group_desc *bg = (struct ocfs2_group_desc *) bg_bh->b_data;
+	struct journal_head *jh;
 	int ret;
 
 	if (ocfs2_test_bit(nr, (unsigned long *)bg->bg_bitmap))
@@ -1260,13 +1261,14 @@
 	if (!buffer_jbd(bg_bh))
 		return 1;
 
-	jbd_lock_bh_state(bg_bh);
-	bg = (struct ocfs2_group_desc *) bh2jh(bg_bh)->b_committed_data;
+	jh = bh2jh(bg_bh);
+	spin_lock(&jh->b_state_lock);
+	bg = (struct ocfs2_group_desc *) jh->b_committed_data;
 	if (bg)
 		ret = !ocfs2_test_bit(nr, (unsigned long *)bg->bg_bitmap);
 	else
 		ret = 1;
-	jbd_unlock_bh_state(bg_bh);
+	spin_unlock(&jh->b_state_lock);
 
 	return ret;
 }
@@ -2387,6 +2389,7 @@
 	int status;
 	unsigned int tmp;
 	struct ocfs2_group_desc *undo_bg = NULL;
+	struct journal_head *jh;
 
 	/* The caller got this descriptor from
 	 * ocfs2_read_group_descriptor().  Any corruption is a code bug. */
@@ -2405,10 +2408,10 @@
 		goto bail;
 	}
 
+	jh = bh2jh(group_bh);
 	if (undo_fn) {
-		jbd_lock_bh_state(group_bh);
-		undo_bg = (struct ocfs2_group_desc *)
-					bh2jh(group_bh)->b_committed_data;
+		spin_lock(&jh->b_state_lock);
+		undo_bg = (struct ocfs2_group_desc *) jh->b_committed_data;
 		BUG_ON(!undo_bg);
 	}
 
@@ -2423,7 +2426,7 @@
 	le16_add_cpu(&bg->bg_free_bits_count, num_bits);
 	if (le16_to_cpu(bg->bg_free_bits_count) > le16_to_cpu(bg->bg_bits)) {
 		if (undo_fn)
-			jbd_unlock_bh_state(group_bh);
+			spin_unlock(&jh->b_state_lock);
 		return ocfs2_error(alloc_inode->i_sb, "Group descriptor # %llu has bit count %u but claims %u are freed. num_bits %d\n",
 				   (unsigned long long)le64_to_cpu(bg->bg_blkno),
 				   le16_to_cpu(bg->bg_bits),
@@ -2432,7 +2435,7 @@
 	}
 
 	if (undo_fn)
-		jbd_unlock_bh_state(group_bh);
+		spin_unlock(&jh->b_state_lock);
 
 	ocfs2_journal_dirty(handle, group_bh);
 bail:
diff -Nur linux-5.4.5/fs/proc/base.c linux-5.4.5-new/fs/proc/base.c
--- linux-5.4.5/fs/proc/base.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/proc/base.c	2020-06-15 16:12:23.943722083 +0300
@@ -1891,7 +1891,7 @@
 
 	child = d_hash_and_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			goto end_instantiate;
diff -Nur linux-5.4.5/fs/proc/kmsg.c linux-5.4.5-new/fs/proc/kmsg.c
--- linux-5.4.5/fs/proc/kmsg.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/proc/kmsg.c	2020-06-15 16:12:23.943722083 +0300
@@ -18,8 +18,6 @@
 #include <linux/uaccess.h>
 #include <asm/io.h>
 
-extern wait_queue_head_t log_wait;
-
 static int kmsg_open(struct inode * inode, struct file * file)
 {
 	return do_syslog(SYSLOG_ACTION_OPEN, NULL, 0, SYSLOG_FROM_PROC);
@@ -42,7 +40,7 @@
 
 static __poll_t kmsg_poll(struct file *file, poll_table *wait)
 {
-	poll_wait(file, &log_wait, wait);
+	poll_wait(file, printk_wait_queue(), wait);
 	if (do_syslog(SYSLOG_ACTION_SIZE_UNREAD, NULL, 0, SYSLOG_FROM_PROC))
 		return EPOLLIN | EPOLLRDNORM;
 	return 0;
diff -Nur linux-5.4.5/fs/proc/proc_sysctl.c linux-5.4.5-new/fs/proc/proc_sysctl.c
--- linux-5.4.5/fs/proc/proc_sysctl.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/proc/proc_sysctl.c	2020-06-15 16:12:23.943722083 +0300
@@ -702,7 +702,7 @@
 
 	child = d_lookup(dir, &qname);
 	if (!child) {
-		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);
+		DECLARE_SWAIT_QUEUE_HEAD_ONSTACK(wq);
 		child = d_alloc_parallel(dir, &qname, &wq);
 		if (IS_ERR(child))
 			return false;
diff -Nur linux-5.4.5/fs/squashfs/decompressor_multi_percpu.c linux-5.4.5-new/fs/squashfs/decompressor_multi_percpu.c
--- linux-5.4.5/fs/squashfs/decompressor_multi_percpu.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/squashfs/decompressor_multi_percpu.c	2020-06-15 16:12:24.047721717 +0300
@@ -8,6 +8,7 @@
 #include <linux/slab.h>
 #include <linux/percpu.h>
 #include <linux/buffer_head.h>
+#include <linux/locallock.h>
 
 #include "squashfs_fs.h"
 #include "squashfs_fs_sb.h"
@@ -23,6 +24,8 @@
 	void		*stream;
 };
 
+static DEFINE_LOCAL_IRQ_LOCK(stream_lock);
+
 void *squashfs_decompressor_create(struct squashfs_sb_info *msblk,
 						void *comp_opts)
 {
@@ -77,10 +80,15 @@
 {
 	struct squashfs_stream __percpu *percpu =
 			(struct squashfs_stream __percpu *) msblk->stream;
-	struct squashfs_stream *stream = get_cpu_ptr(percpu);
-	int res = msblk->decompressor->decompress(msblk, stream->stream, bh, b,
-		offset, length, output);
-	put_cpu_ptr(stream);
+	struct squashfs_stream *stream;
+	int res;
+
+	stream = get_locked_ptr(stream_lock, percpu);
+
+	res = msblk->decompressor->decompress(msblk, stream->stream, bh, b,
+			offset, length, output);
+
+	put_locked_ptr(stream_lock, stream);
 
 	if (res < 0)
 		ERROR("%s decompression failed, data probably corrupt\n",
diff -Nur linux-5.4.5/fs/stack.c linux-5.4.5-new/fs/stack.c
--- linux-5.4.5/fs/stack.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/stack.c	2020-06-15 16:12:22.511727120 +0300
@@ -23,7 +23,7 @@
 
 	/*
 	 * But on 32-bit, we ought to make an effort to keep the two halves of
-	 * i_blocks in sync despite SMP or PREEMPT - though stat's
+	 * i_blocks in sync despite SMP or PREEMPTION - though stat's
 	 * generic_fillattr() doesn't bother, and we won't be applying quotas
 	 * (where i_blocks does become important) at the upper level.
 	 *
@@ -38,14 +38,14 @@
 		spin_unlock(&src->i_lock);
 
 	/*
-	 * If CONFIG_SMP or CONFIG_PREEMPT on 32-bit, it's vital for
+	 * If CONFIG_SMP or CONFIG_PREEMPTION on 32-bit, it's vital for
 	 * fsstack_copy_inode_size() to hold some lock around
 	 * i_size_write(), otherwise i_size_read() may spin forever (see
 	 * include/linux/fs.h).  We don't necessarily hold i_mutex when this
 	 * is called, so take i_lock for that case.
 	 *
 	 * And if on 32-bit, continue our effort to keep the two halves of
-	 * i_blocks in sync despite SMP or PREEMPT: use i_lock  for that case
+	 * i_blocks in sync despite SMP or PREEMPTION: use i_lock for that case
 	 * too, and do both at once by combining the tests.
 	 *
 	 * There is none of this locking overhead in the 64-bit case.
diff -Nur linux-5.4.5/fs/userfaultfd.c linux-5.4.5-new/fs/userfaultfd.c
--- linux-5.4.5/fs/userfaultfd.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/fs/userfaultfd.c	2020-06-15 16:12:22.511727120 +0300
@@ -61,7 +61,7 @@
 	/* waitqueue head for events */
 	wait_queue_head_t event_wqh;
 	/* a refile sequence protected by fault_pending_wqh lock */
-	struct seqcount refile_seq;
+	seqlock_t refile_seq;
 	/* pseudo fd refcounting */
 	refcount_t refcount;
 	/* userfaultfd syscall flags */
@@ -1063,7 +1063,7 @@
 			 * waitqueue could become empty if this is the
 			 * only userfault.
 			 */
-			write_seqcount_begin(&ctx->refile_seq);
+			write_seqlock(&ctx->refile_seq);
 
 			/*
 			 * The fault_pending_wqh.lock prevents the uwq
@@ -1089,7 +1089,7 @@
 			list_del(&uwq->wq.entry);
 			add_wait_queue(&ctx->fault_wqh, &uwq->wq);
 
-			write_seqcount_end(&ctx->refile_seq);
+			write_sequnlock(&ctx->refile_seq);
 
 			/* careful to always initialize msg if ret == 0 */
 			*msg = uwq->msg;
@@ -1262,11 +1262,11 @@
 	 * sure we've userfaults to wake.
 	 */
 	do {
-		seq = read_seqcount_begin(&ctx->refile_seq);
+		seq = read_seqbegin(&ctx->refile_seq);
 		need_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||
 			waitqueue_active(&ctx->fault_wqh);
 		cond_resched();
-	} while (read_seqcount_retry(&ctx->refile_seq, seq));
+	} while (read_seqretry(&ctx->refile_seq, seq));
 	if (need_wakeup)
 		__wake_userfault(ctx, range);
 }
@@ -1935,7 +1935,7 @@
 	init_waitqueue_head(&ctx->fault_wqh);
 	init_waitqueue_head(&ctx->event_wqh);
 	init_waitqueue_head(&ctx->fd_wqh);
-	seqcount_init(&ctx->refile_seq);
+	seqlock_init(&ctx->refile_seq);
 }
 
 SYSCALL_DEFINE1(userfaultfd, int, flags)
diff -Nur linux-5.4.5/include/Kbuild linux-5.4.5-new/include/Kbuild
--- linux-5.4.5/include/Kbuild	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/Kbuild	2020-06-15 16:12:25.055718170 +0300
@@ -1158,8 +1158,15 @@
 # Do not include directly
 header-test- += linux/compiler-clang.h
 header-test- += linux/compiler-gcc.h
+header-test- += linux/mutex_rt.h
 header-test- += linux/patchkey.h
 header-test- += linux/rwlock_api_smp.h
+header-test- += linux/rwlock_rt.h
+header-test- += linux/rwlock_types_rt.h
+header-test- += linux/rwsem-rt.h
+header-test- += linux/spinlock_rt.h
+header-test- += linux/spinlock_types_nort.h
+header-test- += linux/spinlock_types_rt.h
 header-test- += linux/spinlock_types_up.h
 header-test- += linux/spinlock_up.h
 header-test- += linux/wimax/debug.h
diff -Nur linux-5.4.5/include/linux/bottom_half.h linux-5.4.5-new/include/linux/bottom_half.h
--- linux-5.4.5/include/linux/bottom_half.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/bottom_half.h	2020-06-15 16:12:24.779719141 +0300
@@ -4,6 +4,10 @@
 
 #include <linux/preempt.h>
 
+#ifdef CONFIG_PREEMPT_RT
+extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
+#else
+
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
 #else
@@ -13,6 +17,7 @@
 	barrier();
 }
 #endif
+#endif
 
 static inline void local_bh_disable(void)
 {
diff -Nur linux-5.4.5/include/linux/buffer_head.h linux-5.4.5-new/include/linux/buffer_head.h
--- linux-5.4.5/include/linux/buffer_head.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/buffer_head.h	2020-06-15 16:12:24.779719141 +0300
@@ -22,9 +22,6 @@
 	BH_Dirty,	/* Is dirty */
 	BH_Lock,	/* Is locked */
 	BH_Req,		/* Has been submitted for I/O */
-	BH_Uptodate_Lock,/* Used by the first bh in a page, to serialise
-			  * IO completion of other buffers in the page
-			  */
 
 	BH_Mapped,	/* Has a disk mapping */
 	BH_New,		/* Disk mapping was newly created by get_block */
@@ -76,6 +73,9 @@
 	struct address_space *b_assoc_map;	/* mapping this buffer is
 						   associated with */
 	atomic_t b_count;		/* users using this buffer_head */
+	spinlock_t b_uptodate_lock;	/* Used by the first bh in a page, to
+					 * serialise IO completion of other
+					 * buffers in the page */
 };
 
 /*
diff -Nur linux-5.4.5/include/linux/cgroup-defs.h linux-5.4.5-new/include/linux/cgroup-defs.h
--- linux-5.4.5/include/linux/cgroup-defs.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/cgroup-defs.h	2020-06-15 16:12:24.779719141 +0300
@@ -144,9 +144,6 @@
 	struct list_head sibling;
 	struct list_head children;
 
-	/* flush target list anchored at cgrp->rstat_css_list */
-	struct list_head rstat_css_node;
-
 	/*
 	 * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
 	 * matching css can be looked up using css_from_id().
@@ -455,7 +452,6 @@
 
 	/* per-cpu recursive resource statistics */
 	struct cgroup_rstat_cpu __percpu *rstat_cpu;
-	struct list_head rstat_css_list;
 
 	/* cgroup basic resource statistics */
 	struct cgroup_base_stat pending_bstat;	/* pending from children */
@@ -633,7 +629,6 @@
 	void (*css_released)(struct cgroup_subsys_state *css);
 	void (*css_free)(struct cgroup_subsys_state *css);
 	void (*css_reset)(struct cgroup_subsys_state *css);
-	void (*css_rstat_flush)(struct cgroup_subsys_state *css, int cpu);
 	int (*css_extra_stat_show)(struct seq_file *seq,
 				   struct cgroup_subsys_state *css);
 
diff -Nur linux-5.4.5/include/linux/cgroup.h linux-5.4.5-new/include/linux/cgroup.h
--- linux-5.4.5/include/linux/cgroup.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/cgroup.h	2020-06-15 16:12:24.779719141 +0300
@@ -750,9 +750,6 @@
  */
 void cgroup_rstat_updated(struct cgroup *cgrp, int cpu);
 void cgroup_rstat_flush(struct cgroup *cgrp);
-void cgroup_rstat_flush_irqsafe(struct cgroup *cgrp);
-void cgroup_rstat_flush_hold(struct cgroup *cgrp);
-void cgroup_rstat_flush_release(void);
 
 /*
  * Basic resource stats.
diff -Nur linux-5.4.5/include/linux/completion.h linux-5.4.5-new/include/linux/completion.h
--- linux-5.4.5/include/linux/completion.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/completion.h	2020-06-15 16:12:24.779719141 +0300
@@ -9,7 +9,7 @@
  * See kernel/sched/completion.c for details.
  */
 
-#include <linux/wait.h>
+#include <linux/swait.h>
 
 /*
  * struct completion - structure used to maintain state for a "completion"
@@ -25,7 +25,7 @@
  */
 struct completion {
 	unsigned int done;
-	wait_queue_head_t wait;
+	struct swait_queue_head wait;
 };
 
 #define init_completion_map(x, m) __init_completion(x)
@@ -34,7 +34,7 @@
 static inline void complete_release(struct completion *x) {}
 
 #define COMPLETION_INITIALIZER(work) \
-	{ 0, __WAIT_QUEUE_HEAD_INITIALIZER((work).wait) }
+	{ 0, __SWAIT_QUEUE_HEAD_INITIALIZER((work).wait) }
 
 #define COMPLETION_INITIALIZER_ONSTACK_MAP(work, map) \
 	(*({ init_completion_map(&(work), &(map)); &(work); }))
@@ -85,7 +85,7 @@
 static inline void __init_completion(struct completion *x)
 {
 	x->done = 0;
-	init_waitqueue_head(&x->wait);
+	init_swait_queue_head(&x->wait);
 }
 
 /**
diff -Nur linux-5.4.5/include/linux/console.h linux-5.4.5-new/include/linux/console.h
--- linux-5.4.5/include/linux/console.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/console.h	2020-06-15 16:12:24.779719141 +0300
@@ -145,6 +145,7 @@
 struct console {
 	char	name[16];
 	void	(*write)(struct console *, const char *, unsigned);
+	void	(*write_atomic)(struct console *, const char *, unsigned);
 	int	(*read)(struct console *, char *, unsigned);
 	struct tty_driver *(*device)(struct console *, int *);
 	void	(*unblank)(void);
@@ -153,6 +154,8 @@
 	short	flags;
 	short	index;
 	int	cflag;
+	unsigned long printk_seq;
+	int	wrote_history;
 	void	*data;
 	struct	 console *next;
 };
@@ -234,4 +237,7 @@
 void dummycon_register_output_notifier(struct notifier_block *nb);
 void dummycon_unregister_output_notifier(struct notifier_block *nb);
 
+extern void console_atomic_lock(unsigned int *flags);
+extern void console_atomic_unlock(unsigned int flags);
+
 #endif /* _LINUX_CONSOLE_H */
diff -Nur linux-5.4.5/include/linux/dcache.h linux-5.4.5-new/include/linux/dcache.h
--- linux-5.4.5/include/linux/dcache.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/dcache.h	2020-06-15 16:12:24.779719141 +0300
@@ -106,7 +106,7 @@
 
 	union {
 		struct list_head d_lru;		/* LRU list */
-		wait_queue_head_t *d_wait;	/* in-lookup ones only */
+		struct swait_queue_head *d_wait;	/* in-lookup ones only */
 	};
 	struct list_head d_child;	/* child of parent list */
 	struct list_head d_subdirs;	/* our children */
@@ -236,7 +236,7 @@
 extern struct dentry * d_alloc(struct dentry *, const struct qstr *);
 extern struct dentry * d_alloc_anon(struct super_block *);
 extern struct dentry * d_alloc_parallel(struct dentry *, const struct qstr *,
-					wait_queue_head_t *);
+					struct swait_queue_head *);
 extern struct dentry * d_splice_alias(struct inode *, struct dentry *);
 extern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);
 extern struct dentry * d_exact_alias(struct dentry *, struct inode *);
diff -Nur linux-5.4.5/include/linux/delay.h linux-5.4.5-new/include/linux/delay.h
--- linux-5.4.5/include/linux/delay.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/delay.h	2020-06-15 16:12:24.779719141 +0300
@@ -65,4 +65,10 @@
 	msleep(seconds * 1000);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void cpu_chill(void);
+#else
+# define cpu_chill()	cpu_relax()
+#endif
+
 #endif /* defined(_LINUX_DELAY_H) */
diff -Nur linux-5.4.5/include/linux/dma-resv.h linux-5.4.5-new/include/linux/dma-resv.h
--- linux-5.4.5/include/linux/dma-resv.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/dma-resv.h	2020-06-15 16:12:24.779719141 +0300
@@ -65,13 +65,13 @@
 /**
  * struct dma_resv - a reservation object manages fences for a buffer
  * @lock: update side lock
- * @seq: sequence count for managing RCU read-side synchronization
+ * @seq: sequence lock for managing RCU read-side synchronization
  * @fence_excl: the exclusive fence, if there is one currently
  * @fence: list of current shared fences
  */
 struct dma_resv {
 	struct ww_mutex lock;
-	seqcount_t seq;
+	seqlock_t seq;
 
 	struct dma_fence __rcu *fence_excl;
 	struct dma_resv_list __rcu *fence;
diff -Nur linux-5.4.5/include/linux/fscache.h linux-5.4.5-new/include/linux/fscache.h
--- linux-5.4.5/include/linux/fscache.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/fscache.h	2020-06-15 16:12:24.779719141 +0300
@@ -226,6 +226,7 @@
 extern void __fscache_disable_cookie(struct fscache_cookie *, const void *, bool);
 extern void __fscache_enable_cookie(struct fscache_cookie *, const void *, loff_t,
 				    bool (*)(void *), void *);
+extern void fscache_cookie_init(void);
 
 /**
  * fscache_register_netfs - Register a filesystem as desiring caching services
diff -Nur linux-5.4.5/include/linux/fs.h linux-5.4.5-new/include/linux/fs.h
--- linux-5.4.5/include/linux/fs.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/fs.h	2020-06-15 16:12:24.779719141 +0300
@@ -716,7 +716,7 @@
 		struct block_device	*i_bdev;
 		struct cdev		*i_cdev;
 		char			*i_link;
-		unsigned		i_dir_seq;
+		unsigned		__i_dir_seq;
 	};
 
 	__u32			i_generation;
@@ -855,7 +855,7 @@
 		i_size = inode->i_size;
 	} while (read_seqcount_retry(&inode->i_size_seqcount, seq));
 	return i_size;
-#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)
+#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPTION)
 	loff_t i_size;
 
 	preempt_disable();
@@ -880,7 +880,7 @@
 	inode->i_size = i_size;
 	write_seqcount_end(&inode->i_size_seqcount);
 	preempt_enable();
-#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)
+#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPTION)
 	preempt_disable();
 	inode->i_size = i_size;
 	preempt_enable();
diff -Nur linux-5.4.5/include/linux/genhd.h linux-5.4.5-new/include/linux/genhd.h
--- linux-5.4.5/include/linux/genhd.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/genhd.h	2020-06-15 16:12:24.783719127 +0300
@@ -717,7 +717,7 @@
  * accessor function.
  *
  * Code written along the lines of i_size_read() and i_size_write().
- * CONFIG_PREEMPT case optimizes the case of UP kernel with preemption
+ * CONFIG_PREEMPTION case optimizes the case of UP kernel with preemption
  * on.
  */
 static inline sector_t part_nr_sects_read(struct hd_struct *part)
@@ -730,7 +730,7 @@
 		nr_sects = part->nr_sects;
 	} while (read_seqcount_retry(&part->nr_sects_seq, seq));
 	return nr_sects;
-#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)
+#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPTION)
 	sector_t nr_sects;
 
 	preempt_disable();
@@ -753,7 +753,7 @@
 	write_seqcount_begin(&part->nr_sects_seq);
 	part->nr_sects = size;
 	write_seqcount_end(&part->nr_sects_seq);
-#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)
+#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPTION)
 	preempt_disable();
 	part->nr_sects = size;
 	preempt_enable();
diff -Nur linux-5.4.5/include/linux/gfp.h linux-5.4.5-new/include/linux/gfp.h
--- linux-5.4.5/include/linux/gfp.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/gfp.h	2020-06-15 16:12:24.783719127 +0300
@@ -580,6 +580,7 @@
 void page_alloc_init(void);
 void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp);
 void drain_all_pages(struct zone *zone);
+void drain_cpu_pages(unsigned int cpu, struct zone *zone);
 void drain_local_pages(struct zone *zone);
 
 void page_alloc_init_late(void);
diff -Nur linux-5.4.5/include/linux/hardirq.h linux-5.4.5-new/include/linux/hardirq.h
--- linux-5.4.5/include/linux/hardirq.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/hardirq.h	2020-06-15 16:12:24.783719127 +0300
@@ -68,7 +68,6 @@
 #define nmi_enter()						\
 	do {							\
 		arch_nmi_enter();				\
-		printk_nmi_enter();				\
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi());				\
@@ -85,7 +84,6 @@
 		preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		ftrace_nmi_exit();				\
 		lockdep_on();					\
-		printk_nmi_exit();				\
 		arch_nmi_exit();				\
 	} while (0)
 
diff -Nur linux-5.4.5/include/linux/highmem.h linux-5.4.5-new/include/linux/highmem.h
--- linux-5.4.5/include/linux/highmem.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/highmem.h	2020-06-15 16:12:24.783719127 +0300
@@ -8,6 +8,7 @@
 #include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
+#include <linux/sched.h>
 
 #include <asm/cacheflush.h>
 
@@ -90,7 +91,7 @@
 
 static inline void *kmap_atomic(struct page *page)
 {
-	preempt_disable();
+	preempt_disable_nort();
 	pagefault_disable();
 	return page_address(page);
 }
@@ -99,7 +100,7 @@
 static inline void __kunmap_atomic(void *addr)
 {
 	pagefault_enable();
-	preempt_enable();
+	preempt_enable_nort();
 }
 
 #define kmap_atomic_pfn(pfn)	kmap_atomic(pfn_to_page(pfn))
@@ -111,32 +112,51 @@
 
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 
+#ifndef CONFIG_PREEMPT_RT
 DECLARE_PER_CPU(int, __kmap_atomic_idx);
+#endif
 
 static inline int kmap_atomic_idx_push(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;
 
-#ifdef CONFIG_DEBUG_HIGHMEM
+# ifdef CONFIG_DEBUG_HIGHMEM
 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
 	BUG_ON(idx >= KM_TYPE_NR);
-#endif
+# endif
 	return idx;
+#else
+	current->kmap_idx++;
+	BUG_ON(current->kmap_idx > KM_TYPE_NR);
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline int kmap_atomic_idx(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	return __this_cpu_read(__kmap_atomic_idx) - 1;
+#else
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline void kmap_atomic_idx_pop(void)
 {
-#ifdef CONFIG_DEBUG_HIGHMEM
+#ifndef CONFIG_PREEMPT_RT
+# ifdef CONFIG_DEBUG_HIGHMEM
 	int idx = __this_cpu_dec_return(__kmap_atomic_idx);
 
 	BUG_ON(idx < 0);
-#else
+# else
 	__this_cpu_dec(__kmap_atomic_idx);
+# endif
+#else
+	current->kmap_idx--;
+# ifdef CONFIG_DEBUG_HIGHMEM
+	BUG_ON(current->kmap_idx < 0);
+# endif
 #endif
 }
 
diff -Nur linux-5.4.5/include/linux/idr.h linux-5.4.5-new/include/linux/idr.h
--- linux-5.4.5/include/linux/idr.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/idr.h	2020-06-15 16:12:24.783719127 +0300
@@ -169,10 +169,7 @@
  * Each idr_preload() should be matched with an invocation of this
  * function.  See idr_preload() for details.
  */
-static inline void idr_preload_end(void)
-{
-	preempt_enable();
-}
+void idr_preload_end(void);
 
 /**
  * idr_for_each_entry() - Iterate over an IDR's elements of a given type.
diff -Nur linux-5.4.5/include/linux/interrupt.h linux-5.4.5-new/include/linux/interrupt.h
--- linux-5.4.5/include/linux/interrupt.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/interrupt.h	2020-06-15 16:12:24.783719127 +0300
@@ -546,7 +546,7 @@
 asmlinkage void do_softirq(void);
 asmlinkage void __do_softirq(void);
 
-#ifdef __ARCH_HAS_DO_SOFTIRQ
+#if defined(__ARCH_HAS_DO_SOFTIRQ) && !defined(CONFIG_PREEMPT_RT)
 void do_softirq_own_stack(void);
 #else
 static inline void do_softirq_own_stack(void)
@@ -561,6 +561,7 @@
 
 extern void raise_softirq_irqoff(unsigned int nr);
 extern void raise_softirq(unsigned int nr);
+extern void softirq_check_pending_idle(void);
 
 DECLARE_PER_CPU(struct task_struct *, ksoftirqd);
 
@@ -625,7 +626,10 @@
 
 static inline void tasklet_unlock_wait(struct tasklet_struct *t)
 {
-	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }
+	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {
+		local_bh_disable();
+		local_bh_enable();
+	}
 }
 #else
 #define tasklet_trylock(t) 1
diff -Nur linux-5.4.5/include/linux/irqdesc.h linux-5.4.5-new/include/linux/irqdesc.h
--- linux-5.4.5/include/linux/irqdesc.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/irqdesc.h	2020-06-15 16:12:24.783719127 +0300
@@ -72,6 +72,7 @@
 	unsigned int		irqs_unhandled;
 	atomic_t		threads_handled;
 	int			threads_handled_last;
+	u64			random_ip;
 	raw_spinlock_t		lock;
 	struct cpumask		*percpu_enabled;
 	const struct cpumask	*percpu_affinity;
diff -Nur linux-5.4.5/include/linux/irqflags.h linux-5.4.5-new/include/linux/irqflags.h
--- linux-5.4.5/include/linux/irqflags.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/irqflags.h	2020-06-15 16:12:24.783719127 +0300
@@ -43,14 +43,6 @@
 do {						\
 	current->hardirq_context--;		\
 } while (0)
-# define lockdep_softirq_enter()		\
-do {						\
-	current->softirq_context++;		\
-} while (0)
-# define lockdep_softirq_exit()			\
-do {						\
-	current->softirq_context--;		\
-} while (0)
 #else
 # define trace_hardirqs_on()		do { } while (0)
 # define trace_hardirqs_off()		do { } while (0)
@@ -63,6 +55,21 @@
 # define lockdep_softirq_enter()	do { } while (0)
 # define lockdep_softirq_exit()		do { } while (0)
 #endif
+
+#if defined(CONFIG_TRACE_IRQFLAGS) && !defined(CONFIG_PREEMPT_RT)
+# define lockdep_softirq_enter()		\
+do {						\
+	current->softirq_context++;		\
+} while (0)
+# define lockdep_softirq_exit()			\
+do {						\
+	current->softirq_context--;		\
+} while (0)
+
+#else
+# define lockdep_softirq_enter()	do { } while (0)
+# define lockdep_softirq_exit()		do { } while (0)
+#endif
 
 #if defined(CONFIG_IRQSOFF_TRACER) || \
 	defined(CONFIG_PREEMPT_TRACER)
diff -Nur linux-5.4.5/include/linux/irq_work.h linux-5.4.5-new/include/linux/irq_work.h
--- linux-5.4.5/include/linux/irq_work.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/irq_work.h	2020-06-15 16:12:24.783719127 +0300
@@ -18,6 +18,8 @@
 
 /* Doesn't want IPI, wait for tick: */
 #define IRQ_WORK_LAZY		BIT(2)
+/* Run hard IRQ context, even on RT */
+#define IRQ_WORK_HARD_IRQ	BIT(3)
 
 #define IRQ_WORK_CLAIMED	(IRQ_WORK_PENDING | IRQ_WORK_BUSY)
 
@@ -52,4 +54,10 @@
 static inline void irq_work_run(void) { }
 #endif
 
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void);
+#else
+static inline void irq_work_tick_soft(void) { }
+#endif
+
 #endif /* _LINUX_IRQ_WORK_H */
diff -Nur linux-5.4.5/include/linux/jbd2.h linux-5.4.5-new/include/linux/jbd2.h
--- linux-5.4.5/include/linux/jbd2.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/jbd2.h	2020-06-15 16:12:24.783719127 +0300
@@ -313,7 +313,6 @@
 	BH_Revoked,		/* Has been revoked from the log */
 	BH_RevokeValid,		/* Revoked flag is valid */
 	BH_JBDDirty,		/* Is dirty but journaled */
-	BH_State,		/* Pins most journal_head state */
 	BH_JournalHead,		/* Pins bh->b_private and jh->b_bh */
 	BH_Shadow,		/* IO on shadow buffer is running */
 	BH_Verified,		/* Metadata block has been verified ok */
@@ -342,26 +341,6 @@
 	return bh->b_private;
 }
 
-static inline void jbd_lock_bh_state(struct buffer_head *bh)
-{
-	bit_spin_lock(BH_State, &bh->b_state);
-}
-
-static inline int jbd_trylock_bh_state(struct buffer_head *bh)
-{
-	return bit_spin_trylock(BH_State, &bh->b_state);
-}
-
-static inline int jbd_is_locked_bh_state(struct buffer_head *bh)
-{
-	return bit_spin_is_locked(BH_State, &bh->b_state);
-}
-
-static inline void jbd_unlock_bh_state(struct buffer_head *bh)
-{
-	bit_spin_unlock(BH_State, &bh->b_state);
-}
-
 static inline void jbd_lock_bh_journal_head(struct buffer_head *bh)
 {
 	bit_spin_lock(BH_JournalHead, &bh->b_state);
@@ -556,9 +535,9 @@
  *      ->jbd_lock_bh_journal_head()	(This is "innermost")
  *
  *    j_state_lock
- *    ->jbd_lock_bh_state()
+ *    ->b_state_lock
  *
- *    jbd_lock_bh_state()
+ *    b_state_lock
  *    ->j_list_lock
  *
  *    j_state_lock
@@ -1257,7 +1236,7 @@
 
 /* Filing buffers */
 extern void jbd2_journal_unfile_buffer(journal_t *, struct journal_head *);
-extern void __jbd2_journal_refile_buffer(struct journal_head *);
+extern bool __jbd2_journal_refile_buffer(struct journal_head *);
 extern void jbd2_journal_refile_buffer(journal_t *, struct journal_head *);
 extern void __jbd2_journal_file_buffer(struct journal_head *, transaction_t *, int);
 extern void __journal_free_buffer(struct journal_head *bh);
diff -Nur linux-5.4.5/include/linux/journal-head.h linux-5.4.5-new/include/linux/journal-head.h
--- linux-5.4.5/include/linux/journal-head.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/journal-head.h	2020-06-15 16:12:24.783719127 +0300
@@ -11,6 +11,8 @@
 #ifndef JOURNAL_HEAD_H_INCLUDED
 #define JOURNAL_HEAD_H_INCLUDED
 
+#include <linux/spinlock.h>
+
 typedef unsigned int		tid_t;		/* Unique transaction ID */
 typedef struct transaction_s	transaction_t;	/* Compound transaction type */
 
@@ -24,13 +26,18 @@
 	struct buffer_head *b_bh;
 
 	/*
+	 * Protect the buffer head state
+	 */
+	spinlock_t b_state_lock;
+
+	/*
 	 * Reference count - see description in journal.c
 	 * [jbd_lock_bh_journal_head()]
 	 */
 	int b_jcount;
 
 	/*
-	 * Journalling list for this buffer [jbd_lock_bh_state()]
+	 * Journalling list for this buffer [b_state_lock]
 	 * NOTE: We *cannot* combine this with b_modified into a bitfield
 	 * as gcc would then (which the C standard allows but which is
 	 * very unuseful) make 64-bit accesses to the bitfield and clobber
@@ -41,20 +48,20 @@
 	/*
 	 * This flag signals the buffer has been modified by
 	 * the currently running transaction
-	 * [jbd_lock_bh_state()]
+	 * [b_state_lock]
 	 */
 	unsigned b_modified;
 
 	/*
 	 * Copy of the buffer data frozen for writing to the log.
-	 * [jbd_lock_bh_state()]
+	 * [b_state_lock]
 	 */
 	char *b_frozen_data;
 
 	/*
 	 * Pointer to a saved copy of the buffer containing no uncommitted
 	 * deallocation references, so that allocations can avoid overwriting
-	 * uncommitted deletes. [jbd_lock_bh_state()]
+	 * uncommitted deletes. [b_state_lock]
 	 */
 	char *b_committed_data;
 
@@ -63,7 +70,7 @@
 	 * metadata: either the running transaction or the committing
 	 * transaction (if there is one).  Only applies to buffers on a
 	 * transaction's data or metadata journaling list.
-	 * [j_list_lock] [jbd_lock_bh_state()]
+	 * [j_list_lock] [b_state_lock]
 	 * Either of these locks is enough for reading, both are needed for
 	 * changes.
 	 */
@@ -73,13 +80,13 @@
 	 * Pointer to the running compound transaction which is currently
 	 * modifying the buffer's metadata, if there was already a transaction
 	 * committing it when the new transaction touched it.
-	 * [t_list_lock] [jbd_lock_bh_state()]
+	 * [t_list_lock] [b_state_lock]
 	 */
 	transaction_t *b_next_transaction;
 
 	/*
 	 * Doubly-linked list of buffers on a transaction's data, metadata or
-	 * forget queue. [t_list_lock] [jbd_lock_bh_state()]
+	 * forget queue. [t_list_lock] [b_state_lock]
 	 */
 	struct journal_head *b_tnext, *b_tprev;
 
diff -Nur linux-5.4.5/include/linux/kernel.h linux-5.4.5-new/include/linux/kernel.h
--- linux-5.4.5/include/linux/kernel.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/kernel.h	2020-06-15 16:12:24.787719113 +0300
@@ -227,6 +227,10 @@
  */
 # define might_sleep() \
 	do { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
+# define might_sleep_no_state_check() \
+	do { ___might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)
+
 /**
  * cant_sleep - annotation for functions that cannot sleep
  *
@@ -258,6 +262,7 @@
   static inline void __might_sleep(const char *file, int line,
 				   int preempt_offset) { }
 # define might_sleep() do { might_resched(); } while (0)
+# define might_sleep_no_state_check() do { might_resched(); } while (0)
 # define cant_sleep() do { } while (0)
 # define sched_annotate_sleep() do { } while (0)
 # define non_block_start() do { } while (0)
diff -Nur linux-5.4.5/include/linux/kmsg_dump.h linux-5.4.5-new/include/linux/kmsg_dump.h
--- linux-5.4.5/include/linux/kmsg_dump.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/kmsg_dump.h	2020-06-15 16:12:24.787719113 +0300
@@ -46,10 +46,8 @@
 	bool registered;
 
 	/* private state of the kmsg iterator */
-	u32 cur_idx;
-	u32 next_idx;
-	u64 cur_seq;
-	u64 next_seq;
+	u64 line_seq;
+	u64 buffer_end_seq;
 };
 
 #ifdef CONFIG_PRINTK
diff -Nur linux-5.4.5/include/linux/list_bl.h linux-5.4.5-new/include/linux/list_bl.h
--- linux-5.4.5/include/linux/list_bl.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/list_bl.h	2020-06-15 16:12:24.787719113 +0300
@@ -3,6 +3,7 @@
 #define _LINUX_LIST_BL_H
 
 #include <linux/list.h>
+#include <linux/spinlock.h>
 #include <linux/bit_spinlock.h>
 
 /*
@@ -33,13 +34,24 @@
 
 struct hlist_bl_head {
 	struct hlist_bl_node *first;
+#ifdef CONFIG_PREEMPT_RT
+	raw_spinlock_t lock;
+#endif
 };
 
 struct hlist_bl_node {
 	struct hlist_bl_node *next, **pprev;
 };
-#define INIT_HLIST_BL_HEAD(ptr) \
-	((ptr)->first = NULL)
+
+#ifdef CONFIG_PREEMPT_RT
+#define INIT_HLIST_BL_HEAD(h)		\
+do {					\
+	(h)->first = NULL;		\
+	raw_spin_lock_init(&(h)->lock);	\
+} while (0)
+#else
+#define INIT_HLIST_BL_HEAD(h) (h)->first = NULL
+#endif
 
 static inline void INIT_HLIST_BL_NODE(struct hlist_bl_node *h)
 {
@@ -145,12 +157,26 @@
 
 static inline void hlist_bl_lock(struct hlist_bl_head *b)
 {
+#ifndef CONFIG_PREEMPT_RT
 	bit_spin_lock(0, (unsigned long *)b);
+#else
+	raw_spin_lock(&b->lock);
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+	__set_bit(0, (unsigned long *)b);
+#endif
+#endif
 }
 
 static inline void hlist_bl_unlock(struct hlist_bl_head *b)
 {
+#ifndef CONFIG_PREEMPT_RT
 	__bit_spin_unlock(0, (unsigned long *)b);
+#else
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+	__clear_bit(0, (unsigned long *)b);
+#endif
+	raw_spin_unlock(&b->lock);
+#endif
 }
 
 static inline bool hlist_bl_is_locked(struct hlist_bl_head *b)
diff -Nur linux-5.4.5/include/linux/locallock.h linux-5.4.5-new/include/linux/locallock.h
--- linux-5.4.5/include/linux/locallock.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/locallock.h	2020-06-15 16:12:24.787719113 +0300
@@ -0,0 +1,281 @@
+#ifndef _LINUX_LOCALLOCK_H
+#define _LINUX_LOCALLOCK_H
+
+#include <linux/percpu.h>
+#include <linux/spinlock.h>
+
+#ifdef CONFIG_PREEMPT_RT
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define LL_WARN(cond)	WARN_ON(cond)
+#else
+# define LL_WARN(cond)	do { } while (0)
+#endif
+
+/*
+ * per cpu lock based substitute for local_irq_*()
+ */
+struct local_irq_lock {
+	spinlock_t		lock;
+	struct task_struct	*owner;
+	int			nestcnt;
+	unsigned long		flags;
+};
+
+#define DEFINE_LOCAL_IRQ_LOCK(lvar)					\
+	DEFINE_PER_CPU(struct local_irq_lock, lvar) = {			\
+		.lock = __SPIN_LOCK_UNLOCKED((lvar).lock) }
+
+#define DECLARE_LOCAL_IRQ_LOCK(lvar)					\
+	DECLARE_PER_CPU(struct local_irq_lock, lvar)
+
+#define local_irq_lock_init(lvar)					\
+	do {								\
+		int __cpu;						\
+		for_each_possible_cpu(__cpu)				\
+			spin_lock_init(&per_cpu(lvar, __cpu).lock);	\
+	} while (0)
+
+static inline void __local_lock(struct local_irq_lock *lv)
+{
+	if (lv->owner != current) {
+		spin_lock(&lv->lock);
+		LL_WARN(lv->owner);
+		LL_WARN(lv->nestcnt);
+		lv->owner = current;
+	}
+	lv->nestcnt++;
+}
+
+#define local_lock(lvar)					\
+	do { __local_lock(&get_local_var(lvar)); } while (0)
+
+#define local_lock_on(lvar, cpu)				\
+	do { __local_lock(&per_cpu(lvar, cpu)); } while (0)
+
+static inline int __local_trylock(struct local_irq_lock *lv)
+{
+	if (lv->owner != current && spin_trylock(&lv->lock)) {
+		LL_WARN(lv->owner);
+		LL_WARN(lv->nestcnt);
+		lv->owner = current;
+		lv->nestcnt = 1;
+		return 1;
+	} else if (lv->owner == current) {
+		lv->nestcnt++;
+		return 1;
+	}
+	return 0;
+}
+
+#define local_trylock(lvar)						\
+	({								\
+		int __locked;						\
+		__locked = __local_trylock(&get_local_var(lvar));	\
+		if (!__locked)						\
+			put_local_var(lvar);				\
+		__locked;						\
+	})
+
+static inline void __local_unlock(struct local_irq_lock *lv)
+{
+	LL_WARN(lv->nestcnt == 0);
+	LL_WARN(lv->owner != current);
+	if (--lv->nestcnt)
+		return;
+
+	lv->owner = NULL;
+	spin_unlock(&lv->lock);
+}
+
+#define local_unlock(lvar)					\
+	do {							\
+		__local_unlock(this_cpu_ptr(&lvar));		\
+		put_local_var(lvar);				\
+	} while (0)
+
+#define local_unlock_on(lvar, cpu)                       \
+	do { __local_unlock(&per_cpu(lvar, cpu)); } while (0)
+
+static inline void __local_lock_irq(struct local_irq_lock *lv)
+{
+	spin_lock_irqsave(&lv->lock, lv->flags);
+	LL_WARN(lv->owner);
+	LL_WARN(lv->nestcnt);
+	lv->owner = current;
+	lv->nestcnt = 1;
+}
+
+#define local_lock_irq(lvar)						\
+	do { __local_lock_irq(&get_local_var(lvar)); } while (0)
+
+#define local_lock_irq_on(lvar, cpu)					\
+	do { __local_lock_irq(&per_cpu(lvar, cpu)); } while (0)
+
+static inline void __local_unlock_irq(struct local_irq_lock *lv)
+{
+	LL_WARN(!lv->nestcnt);
+	LL_WARN(lv->owner != current);
+	lv->owner = NULL;
+	lv->nestcnt = 0;
+	spin_unlock_irq(&lv->lock);
+}
+
+#define local_unlock_irq(lvar)						\
+	do {								\
+		__local_unlock_irq(this_cpu_ptr(&lvar));		\
+		put_local_var(lvar);					\
+	} while (0)
+
+#define local_unlock_irq_on(lvar, cpu)					\
+	do {								\
+		__local_unlock_irq(&per_cpu(lvar, cpu));		\
+	} while (0)
+
+static inline int __local_lock_irqsave(struct local_irq_lock *lv)
+{
+	if (lv->owner != current) {
+		__local_lock_irq(lv);
+		return 0;
+	} else {
+		lv->nestcnt++;
+		return 1;
+	}
+}
+
+#define local_lock_irqsave(lvar, _flags)				\
+	do {								\
+		if (__local_lock_irqsave(&get_local_var(lvar)))		\
+			put_local_var(lvar);				\
+		_flags = __this_cpu_read(lvar.flags);			\
+	} while (0)
+
+#define local_lock_irqsave_on(lvar, _flags, cpu)			\
+	do {								\
+		__local_lock_irqsave(&per_cpu(lvar, cpu));		\
+		_flags = per_cpu(lvar, cpu).flags;			\
+	} while (0)
+
+static inline int __local_unlock_irqrestore(struct local_irq_lock *lv,
+					    unsigned long flags)
+{
+	LL_WARN(!lv->nestcnt);
+	LL_WARN(lv->owner != current);
+	if (--lv->nestcnt)
+		return 0;
+
+	lv->owner = NULL;
+	spin_unlock_irqrestore(&lv->lock, lv->flags);
+	return 1;
+}
+
+#define local_unlock_irqrestore(lvar, flags)				\
+	do {								\
+		if (__local_unlock_irqrestore(this_cpu_ptr(&lvar), flags)) \
+			put_local_var(lvar);				\
+	} while (0)
+
+#define local_unlock_irqrestore_on(lvar, flags, cpu)			\
+	do {								\
+		__local_unlock_irqrestore(&per_cpu(lvar, cpu), flags);	\
+	} while (0)
+
+#define local_spin_trylock_irq(lvar, lock)				\
+	({								\
+		int __locked;						\
+		local_lock_irq(lvar);					\
+		__locked = spin_trylock(lock);				\
+		if (!__locked)						\
+			local_unlock_irq(lvar);				\
+		__locked;						\
+	})
+
+#define local_spin_lock_irq(lvar, lock)					\
+	do {								\
+		local_lock_irq(lvar);					\
+		spin_lock(lock);					\
+	} while (0)
+
+#define local_spin_unlock_irq(lvar, lock)				\
+	do {								\
+		spin_unlock(lock);					\
+		local_unlock_irq(lvar);					\
+	} while (0)
+
+#define local_spin_lock_irqsave(lvar, lock, flags)			\
+	do {								\
+		local_lock_irqsave(lvar, flags);			\
+		spin_lock(lock);					\
+	} while (0)
+
+#define local_spin_unlock_irqrestore(lvar, lock, flags)			\
+	do {								\
+		spin_unlock(lock);					\
+		local_unlock_irqrestore(lvar, flags);			\
+	} while (0)
+
+#define get_locked_var(lvar, var)					\
+	(*({								\
+		local_lock(lvar);					\
+		this_cpu_ptr(&var);					\
+	}))
+
+#define put_locked_var(lvar, var)	local_unlock(lvar);
+
+#define get_locked_ptr(lvar, var)					\
+	({								\
+		local_lock(lvar);					\
+		this_cpu_ptr(var);					\
+	})
+
+#define put_locked_ptr(lvar, var)	local_unlock(lvar);
+
+#define local_lock_cpu(lvar)						\
+	({								\
+		local_lock(lvar);					\
+		smp_processor_id();					\
+	})
+
+#define local_unlock_cpu(lvar)			local_unlock(lvar)
+
+#else /* PREEMPT_RT */
+
+#define DEFINE_LOCAL_IRQ_LOCK(lvar)		__typeof__(const int) lvar
+#define DECLARE_LOCAL_IRQ_LOCK(lvar)		extern __typeof__(const int) lvar
+
+static inline void local_irq_lock_init(int lvar) { }
+
+#define local_trylock(lvar)					\
+	({							\
+		preempt_disable();				\
+		1;						\
+	})
+
+#define local_lock(lvar)			preempt_disable()
+#define local_unlock(lvar)			preempt_enable()
+#define local_lock_irq(lvar)			local_irq_disable()
+#define local_lock_irq_on(lvar, cpu)		local_irq_disable()
+#define local_unlock_irq(lvar)			local_irq_enable()
+#define local_unlock_irq_on(lvar, cpu)		local_irq_enable()
+#define local_lock_irqsave(lvar, flags)		local_irq_save(flags)
+#define local_unlock_irqrestore(lvar, flags)	local_irq_restore(flags)
+
+#define local_spin_trylock_irq(lvar, lock)	spin_trylock_irq(lock)
+#define local_spin_lock_irq(lvar, lock)		spin_lock_irq(lock)
+#define local_spin_unlock_irq(lvar, lock)	spin_unlock_irq(lock)
+#define local_spin_lock_irqsave(lvar, lock, flags)	\
+	spin_lock_irqsave(lock, flags)
+#define local_spin_unlock_irqrestore(lvar, lock, flags)	\
+	spin_unlock_irqrestore(lock, flags)
+
+#define get_locked_var(lvar, var)		get_cpu_var(var)
+#define put_locked_var(lvar, var)		put_cpu_var(var)
+#define get_locked_ptr(lvar, var)		get_cpu_ptr(var)
+#define put_locked_ptr(lvar, var)		put_cpu_ptr(var)
+
+#define local_lock_cpu(lvar)			get_cpu()
+#define local_unlock_cpu(lvar)			put_cpu()
+
+#endif
+
+#endif
diff -Nur linux-5.4.5/include/linux/mm_types.h linux-5.4.5-new/include/linux/mm_types.h
--- linux-5.4.5/include/linux/mm_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/mm_types.h	2020-06-15 16:12:24.787719113 +0300
@@ -12,6 +12,7 @@
 #include <linux/completion.h>
 #include <linux/cpumask.h>
 #include <linux/uprobes.h>
+#include <linux/rcupdate.h>
 #include <linux/page-flags-layout.h>
 #include <linux/workqueue.h>
 
@@ -520,6 +521,9 @@
 		bool tlb_flush_batched;
 #endif
 		struct uprobes_state uprobes_state;
+#ifdef CONFIG_PREEMPT_RT
+		struct rcu_head delayed_drop;
+#endif
 #ifdef CONFIG_HUGETLB_PAGE
 		atomic_long_t hugetlb_usage;
 #endif
diff -Nur linux-5.4.5/include/linux/mutex.h linux-5.4.5-new/include/linux/mutex.h
--- linux-5.4.5/include/linux/mutex.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/mutex.h	2020-06-15 16:12:24.787719113 +0300
@@ -22,6 +22,17 @@
 
 struct ww_acquire_ctx;
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
+		, .dep_map = { .name = #lockname }
+#else
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/mutex_rt.h>
+#else
+
 /*
  * Simple, straightforward mutexes with strict semantics:
  *
@@ -108,13 +119,6 @@
 	__mutex_init((mutex), #mutex, &__key);				\
 } while (0)
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
-		, .dep_map = { .name = #lockname }
-#else
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
-#endif
-
 #define __MUTEX_INITIALIZER(lockname) \
 		{ .owner = ATOMIC_LONG_INIT(0) \
 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
@@ -210,4 +214,6 @@
 extern /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
 mutex_trylock_recursive(struct mutex *lock);
 
+#endif /* !PREEMPT_RT */
+
 #endif /* __LINUX_MUTEX_H */
diff -Nur linux-5.4.5/include/linux/mutex_rt.h linux-5.4.5-new/include/linux/mutex_rt.h
--- linux-5.4.5/include/linux/mutex_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/mutex_rt.h	2020-06-15 16:12:24.787719113 +0300
@@ -0,0 +1,130 @@
+#ifndef __LINUX_MUTEX_RT_H
+#define __LINUX_MUTEX_RT_H
+
+#ifndef __LINUX_MUTEX_H
+#error "Please include mutex.h"
+#endif
+
+#include <linux/rtmutex.h>
+
+/* FIXME: Just for __lockfunc */
+#include <linux/spinlock.h>
+
+struct mutex {
+	struct rt_mutex		lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __MUTEX_INITIALIZER(mutexname)					\
+	{								\
+		.lock = __RT_MUTEX_INITIALIZER(mutexname.lock)		\
+		__DEP_MAP_MUTEX_INITIALIZER(mutexname)			\
+	}
+
+#define DEFINE_MUTEX(mutexname)						\
+	struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)
+
+extern void __mutex_do_init(struct mutex *lock, const char *name, struct lock_class_key *key);
+extern void __lockfunc _mutex_lock(struct mutex *lock);
+extern void __lockfunc _mutex_lock_io(struct mutex *lock);
+extern void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_interruptible(struct mutex *lock);
+extern int __lockfunc _mutex_lock_killable(struct mutex *lock);
+extern void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass);
+extern void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);
+extern int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_trylock(struct mutex *lock);
+extern void __lockfunc _mutex_unlock(struct mutex *lock);
+
+#define mutex_is_locked(l)		rt_mutex_is_locked(&(l)->lock)
+#define mutex_lock(l)			_mutex_lock(l)
+#define mutex_lock_interruptible(l)	_mutex_lock_interruptible(l)
+#define mutex_lock_killable(l)		_mutex_lock_killable(l)
+#define mutex_trylock(l)		_mutex_trylock(l)
+#define mutex_unlock(l)			_mutex_unlock(l)
+#define mutex_lock_io(l)		_mutex_lock_io(l);
+
+#define __mutex_owner(l)		((l)->lock.owner)
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define mutex_destroy(l)		rt_mutex_destroy(&(l)->lock)
+#else
+static inline void mutex_destroy(struct mutex *lock) {}
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define mutex_lock_nested(l, s)	_mutex_lock_nested(l, s)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible_nested(l, s)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable_nested(l, s)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io_nested(l, s)
+
+# define mutex_lock_nest_lock(lock, nest_lock)				\
+do {									\
+	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);		\
+	_mutex_lock_nest_lock(lock, &(nest_lock)->dep_map);		\
+} while (0)
+
+#else
+# define mutex_lock_nested(l, s)	_mutex_lock(l)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible(l)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable(l)
+# define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
+# define mutex_lock_io_nested(l, s)	_mutex_lock_io(l)
+#endif
+
+# define mutex_init(mutex)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), #mutex, &__key);	\
+} while (0)
+
+# define __mutex_init(mutex, name, key)			\
+do {							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), name, key);		\
+} while (0)
+
+/**
+ * These values are chosen such that FAIL and SUCCESS match the
+ * values of the regular mutex_trylock().
+ */
+enum mutex_trylock_recursive_enum {
+	MUTEX_TRYLOCK_FAILED    = 0,
+	MUTEX_TRYLOCK_SUCCESS   = 1,
+	MUTEX_TRYLOCK_RECURSIVE,
+};
+/**
+ * mutex_trylock_recursive - trylock variant that allows recursive locking
+ * @lock: mutex to be locked
+ *
+ * This function should not be used, _ever_. It is purely for hysterical GEM
+ * raisins, and once those are gone this will be removed.
+ *
+ * Returns:
+ *  MUTEX_TRYLOCK_FAILED    - trylock failed,
+ *  MUTEX_TRYLOCK_SUCCESS   - lock acquired,
+ *  MUTEX_TRYLOCK_RECURSIVE - we already owned the lock.
+ */
+int __rt_mutex_owner_current(struct rt_mutex *lock);
+
+static inline /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
+mutex_trylock_recursive(struct mutex *lock)
+{
+	if (unlikely(__rt_mutex_owner_current(&lock->lock)))
+		return MUTEX_TRYLOCK_RECURSIVE;
+
+	return mutex_trylock(lock);
+}
+
+extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
+
+#endif
diff -Nur linux-5.4.5/include/linux/netdevice.h linux-5.4.5-new/include/linux/netdevice.h
--- linux-5.4.5/include/linux/netdevice.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/netdevice.h	2020-06-15 16:12:24.787719113 +0300
@@ -3016,6 +3016,7 @@
 	unsigned int		dropped;
 	struct sk_buff_head	input_pkt_queue;
 	struct napi_struct	backlog;
+	struct sk_buff_head	tofree_queue;
 
 };
 
diff -Nur linux-5.4.5/include/linux/nfs_fs.h linux-5.4.5-new/include/linux/nfs_fs.h
--- linux-5.4.5/include/linux/nfs_fs.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/nfs_fs.h	2020-06-15 16:12:24.787719113 +0300
@@ -165,7 +165,11 @@
 
 	/* Readers: in-flight sillydelete RPC calls */
 	/* Writers: rmdir */
+#ifdef CONFIG_PREEMPT_RT
+	struct semaphore        rmdir_sem;
+#else
 	struct rw_semaphore	rmdir_sem;
+#endif
 	struct mutex		commit_mutex;
 
 #if IS_ENABLED(CONFIG_NFS_V4)
diff -Nur linux-5.4.5/include/linux/nfs_xdr.h linux-5.4.5-new/include/linux/nfs_xdr.h
--- linux-5.4.5/include/linux/nfs_xdr.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/nfs_xdr.h	2020-06-15 16:12:24.787719113 +0300
@@ -1594,7 +1594,7 @@
 	struct nfs_removeargs args;
 	struct nfs_removeres res;
 	struct dentry *dentry;
-	wait_queue_head_t wq;
+	struct swait_queue_head wq;
 	const struct cred *cred;
 	struct nfs_fattr dir_attr;
 	long timeout;
diff -Nur linux-5.4.5/include/linux/percpu.h linux-5.4.5-new/include/linux/percpu.h
--- linux-5.4.5/include/linux/percpu.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/percpu.h	2020-06-15 16:12:24.791719099 +0300
@@ -19,6 +19,35 @@
 #define PERCPU_MODULE_RESERVE		0
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+
+#define get_local_var(var) (*({	\
+	migrate_disable();	\
+	this_cpu_ptr(&var);	}))
+
+#define put_local_var(var) do {	\
+	(void)&(var);		\
+	migrate_enable();	\
+} while (0)
+
+# define get_local_ptr(var) ({	\
+	migrate_disable();	\
+	this_cpu_ptr(var);	})
+
+# define put_local_ptr(var) do {	\
+	(void)(var);			\
+	migrate_enable();		\
+} while (0)
+
+#else
+
+#define get_local_var(var)	get_cpu_var(var)
+#define put_local_var(var)	put_cpu_var(var)
+#define get_local_ptr(var)	get_cpu_ptr(var)
+#define put_local_ptr(var)	put_cpu_ptr(var)
+
+#endif
+
 /* minimum unit size, also is the maximum supported allocation size */
 #define PCPU_MIN_UNIT_SIZE		PFN_ALIGN(32 << 10)
 
diff -Nur linux-5.4.5/include/linux/percpu-refcount.h linux-5.4.5-new/include/linux/percpu-refcount.h
--- linux-5.4.5/include/linux/percpu-refcount.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/percpu-refcount.h	2020-06-15 16:12:24.787719113 +0300
@@ -186,14 +186,14 @@
 {
 	unsigned long __percpu *percpu_count;
 
-	rcu_read_lock_sched();
+	rcu_read_lock();
 
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_add(*percpu_count, nr);
 	else
 		atomic_long_add(nr, &ref->count);
 
-	rcu_read_unlock_sched();
+	rcu_read_unlock();
 }
 
 /**
@@ -223,7 +223,7 @@
 	unsigned long __percpu *percpu_count;
 	bool ret;
 
-	rcu_read_lock_sched();
+	rcu_read_lock();
 
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
@@ -232,7 +232,7 @@
 		ret = atomic_long_inc_not_zero(&ref->count);
 	}
 
-	rcu_read_unlock_sched();
+	rcu_read_unlock();
 
 	return ret;
 }
@@ -257,7 +257,7 @@
 	unsigned long __percpu *percpu_count;
 	bool ret = false;
 
-	rcu_read_lock_sched();
+	rcu_read_lock();
 
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
@@ -266,7 +266,7 @@
 		ret = atomic_long_inc_not_zero(&ref->count);
 	}
 
-	rcu_read_unlock_sched();
+	rcu_read_unlock();
 
 	return ret;
 }
@@ -285,14 +285,14 @@
 {
 	unsigned long __percpu *percpu_count;
 
-	rcu_read_lock_sched();
+	rcu_read_lock();
 
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_sub(*percpu_count, nr);
 	else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
 		ref->release(ref);
 
-	rcu_read_unlock_sched();
+	rcu_read_unlock();
 }
 
 /**
diff -Nur linux-5.4.5/include/linux/percpu-rwsem.h linux-5.4.5-new/include/linux/percpu-rwsem.h
--- linux-5.4.5/include/linux/percpu-rwsem.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/percpu-rwsem.h	2020-06-15 16:12:24.791719099 +0300
@@ -119,7 +119,7 @@
 					bool read, unsigned long ip)
 {
 	lock_release(&sem->rw_sem.dep_map, 1, ip);
-#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
+#if defined(CONFIG_RWSEM_SPIN_ON_OWNER) && !defined(CONFIG_PREEMPT_RT)
 	if (!read)
 		atomic_long_set(&sem->rw_sem.owner, RWSEM_OWNER_UNKNOWN);
 #endif
@@ -129,7 +129,7 @@
 					bool read, unsigned long ip)
 {
 	lock_acquire(&sem->rw_sem.dep_map, 0, 1, read, 1, NULL, ip);
-#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
+#if defined(CONFIG_RWSEM_SPIN_ON_OWNER) && !defined(CONFIG_PREEMPT_RT)
 	if (!read)
 		atomic_long_set(&sem->rw_sem.owner, (long)current);
 #endif
diff -Nur linux-5.4.5/include/linux/pid.h linux-5.4.5-new/include/linux/pid.h
--- linux-5.4.5/include/linux/pid.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/pid.h	2020-06-15 16:12:24.791719099 +0300
@@ -3,6 +3,7 @@
 #define _LINUX_PID_H
 
 #include <linux/rculist.h>
+#include <linux/atomic.h>
 #include <linux/wait.h>
 #include <linux/refcount.h>
 
diff -Nur linux-5.4.5/include/linux/posix-timers.h linux-5.4.5-new/include/linux/posix-timers.h
--- linux-5.4.5/include/linux/posix-timers.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/posix-timers.h	2020-06-15 16:12:24.791719099 +0300
@@ -72,6 +72,7 @@
 	struct task_struct	*task;
 	struct list_head	elist;
 	int			firing;
+	int			firing_cpu;
 };
 
 static inline bool cpu_timer_enqueue(struct timerqueue_head *head,
@@ -123,6 +124,9 @@
 	struct posix_cputimer_base	bases[CPUCLOCK_MAX];
 	unsigned int			timers_active;
 	unsigned int			expiry_active;
+#ifdef CONFIG_PREEMPT_RT
+	struct task_struct		*posix_timer_list;
+#endif
 };
 
 static inline void posix_cputimers_init(struct posix_cputimers *pct)
@@ -152,9 +156,16 @@
 	INIT_CPU_TIMERBASE(b[2]),					\
 }
 
+#ifdef CONFIG_PREEMPT_RT
+# define INIT_TIMER_LIST	.posix_timer_list = NULL,
+#else
+# define INIT_TIMER_LIST
+#endif
+
 #define INIT_CPU_TIMERS(s)						\
 	.posix_cputimers = {						\
 		.bases = INIT_CPU_TIMERBASES(s.posix_cputimers.bases),	\
+		INIT_TIMER_LIST						\
 	},
 #else
 struct posix_cputimers { };
diff -Nur linux-5.4.5/include/linux/preempt.h linux-5.4.5-new/include/linux/preempt.h
--- linux-5.4.5/include/linux/preempt.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/preempt.h	2020-06-15 16:12:24.791719099 +0300
@@ -78,10 +78,8 @@
 #include <asm/preempt.h>
 
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
 				 | NMI_MASK))
-
 /*
  * Are we doing bottom half or hardware interrupt processing?
  *
@@ -96,12 +94,23 @@
  *       should not be used in new code.
  */
 #define in_irq()		(hardirq_count())
-#define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
-#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
 #define in_nmi()		(preempt_count() & NMI_MASK)
 #define in_task()		(!(preempt_count() & \
 				   (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
+#ifdef CONFIG_PREEMPT_RT
+
+#define softirq_count()		((long)current->softirq_count)
+#define in_softirq()		(softirq_count())
+#define in_serving_softirq()	(current->softirq_count & SOFTIRQ_OFFSET)
+
+#else
+
+#define softirq_count()		(preempt_count() & SOFTIRQ_MASK)
+#define in_softirq()		(softirq_count())
+#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
+
+#endif
 
 /*
  * The preempt_count offset after preempt_disable();
@@ -115,7 +124,11 @@
 /*
  * The preempt_count offset after spin_lock()
  */
+#if !defined(CONFIG_PREEMPT_RT)
 #define PREEMPT_LOCK_OFFSET	PREEMPT_DISABLE_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
 
 /*
  * The preempt_count offset needed for things like:
@@ -164,6 +177,20 @@
 #define preempt_count_inc() preempt_count_add(1)
 #define preempt_count_dec() preempt_count_sub(1)
 
+#ifdef CONFIG_PREEMPT_LAZY
+#define add_preempt_lazy_count(val)	do { preempt_lazy_count() += (val); } while (0)
+#define sub_preempt_lazy_count(val)	do { preempt_lazy_count() -= (val); } while (0)
+#define inc_preempt_lazy_count()	add_preempt_lazy_count(1)
+#define dec_preempt_lazy_count()	sub_preempt_lazy_count(1)
+#define preempt_lazy_count()		(current_thread_info()->preempt_lazy_count)
+#else
+#define add_preempt_lazy_count(val)	do { } while (0)
+#define sub_preempt_lazy_count(val)	do { } while (0)
+#define inc_preempt_lazy_count()	do { } while (0)
+#define dec_preempt_lazy_count()	do { } while (0)
+#define preempt_lazy_count()		(0)
+#endif
+
 #ifdef CONFIG_PREEMPT_COUNT
 
 #define preempt_disable() \
@@ -172,16 +199,53 @@
 	barrier(); \
 } while (0)
 
+#define preempt_lazy_disable() \
+do { \
+	inc_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define sched_preempt_enable_no_resched() \
 do { \
 	barrier(); \
 	preempt_count_dec(); \
 } while (0)
 
-#define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+# define preempt_check_resched_rt() preempt_check_resched()
+#else
+# define preempt_enable_no_resched() preempt_enable()
+# define preempt_check_resched_rt() barrier();
+#endif
 
 #define preemptible()	(preempt_count() == 0 && !irqs_disabled())
 
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+
+extern void migrate_disable(void);
+extern void migrate_enable(void);
+
+int __migrate_disabled(struct task_struct *p);
+
+#elif !defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+
+extern void migrate_disable(void);
+extern void migrate_enable(void);
+static inline int __migrate_disabled(struct task_struct *p)
+{
+	return 0;
+}
+
+#else
+#define migrate_disable()		preempt_disable()
+#define migrate_enable()		preempt_enable()
+static inline int __migrate_disabled(struct task_struct *p)
+{
+	return 0;
+}
+#endif
+
 #ifdef CONFIG_PREEMPTION
 #define preempt_enable() \
 do { \
@@ -203,6 +267,13 @@
 		__preempt_schedule(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+	preempt_check_resched(); \
+} while (0)
+
 #else /* !CONFIG_PREEMPTION */
 #define preempt_enable() \
 do { \
@@ -210,6 +281,12 @@
 	preempt_count_dec(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define preempt_enable_notrace() \
 do { \
 	barrier(); \
@@ -248,8 +325,16 @@
 #define preempt_disable_notrace()		barrier()
 #define preempt_enable_no_resched_notrace()	barrier()
 #define preempt_enable_notrace()		barrier()
+#define preempt_check_resched_rt()		barrier()
 #define preemptible()				0
 
+#define migrate_disable()			barrier()
+#define migrate_enable()			barrier()
+
+static inline int __migrate_disabled(struct task_struct *p)
+{
+	return 0;
+}
 #endif /* CONFIG_PREEMPT_COUNT */
 
 #ifdef MODULE
@@ -268,10 +353,22 @@
 } while (0)
 #define preempt_fold_need_resched() \
 do { \
-	if (tif_need_resched()) \
+	if (tif_need_resched_now()) \
 		set_preempt_need_resched(); \
 } while (0)
 
+#ifdef CONFIG_PREEMPT_RT
+# define preempt_disable_rt()		preempt_disable()
+# define preempt_enable_rt()		preempt_enable()
+# define preempt_disable_nort()		barrier()
+# define preempt_enable_nort()		barrier()
+#else
+# define preempt_disable_rt()		barrier()
+# define preempt_enable_rt()		barrier()
+# define preempt_disable_nort()		preempt_disable()
+# define preempt_enable_nort()		preempt_enable()
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
 struct preempt_notifier;
diff -Nur linux-5.4.5/include/linux/printk.h linux-5.4.5-new/include/linux/printk.h
--- linux-5.4.5/include/linux/printk.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/printk.h	2020-06-15 16:12:24.791719099 +0300
@@ -58,6 +58,7 @@
  */
 #define CONSOLE_LOGLEVEL_DEFAULT CONFIG_CONSOLE_LOGLEVEL_DEFAULT
 #define CONSOLE_LOGLEVEL_QUIET	 CONFIG_CONSOLE_LOGLEVEL_QUIET
+#define CONSOLE_LOGLEVEL_EMERGENCY CONFIG_CONSOLE_LOGLEVEL_EMERGENCY
 
 extern int console_printk[];
 
@@ -65,6 +66,7 @@
 #define default_message_loglevel (console_printk[1])
 #define minimum_console_loglevel (console_printk[2])
 #define default_console_loglevel (console_printk[3])
+#define emergency_console_loglevel (console_printk[4])
 
 static inline void console_silent(void)
 {
@@ -146,18 +148,6 @@
 void early_printk(const char *s, ...) { }
 #endif
 
-#ifdef CONFIG_PRINTK_NMI
-extern void printk_nmi_enter(void);
-extern void printk_nmi_exit(void);
-extern void printk_nmi_direct_enter(void);
-extern void printk_nmi_direct_exit(void);
-#else
-static inline void printk_nmi_enter(void) { }
-static inline void printk_nmi_exit(void) { }
-static inline void printk_nmi_direct_enter(void) { }
-static inline void printk_nmi_direct_exit(void) { }
-#endif /* PRINTK_NMI */
-
 #ifdef CONFIG_PRINTK
 asmlinkage __printf(5, 0)
 int vprintk_emit(int facility, int level,
@@ -202,9 +192,7 @@
 void dump_stack_print_info(const char *log_lvl);
 void show_regs_print_info(const char *log_lvl);
 extern asmlinkage void dump_stack(void) __cold;
-extern void printk_safe_init(void);
-extern void printk_safe_flush(void);
-extern void printk_safe_flush_on_panic(void);
+struct wait_queue_head *printk_wait_queue(void);
 #else
 static inline __printf(1, 0)
 int vprintk(const char *s, va_list args)
@@ -268,18 +256,6 @@
 static inline void dump_stack(void)
 {
 }
-
-static inline void printk_safe_init(void)
-{
-}
-
-static inline void printk_safe_flush(void)
-{
-}
-
-static inline void printk_safe_flush_on_panic(void)
-{
-}
 #endif
 
 extern int kptr_restrict;
diff -Nur linux-5.4.5/include/linux/printk_ringbuffer.h linux-5.4.5-new/include/linux/printk_ringbuffer.h
--- linux-5.4.5/include/linux/printk_ringbuffer.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/printk_ringbuffer.h	2020-06-15 16:12:24.791719099 +0300
@@ -0,0 +1,114 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_PRINTK_RINGBUFFER_H
+#define _LINUX_PRINTK_RINGBUFFER_H
+
+#include <linux/irq_work.h>
+#include <linux/atomic.h>
+#include <linux/percpu.h>
+#include <linux/wait.h>
+
+struct prb_cpulock {
+	atomic_t owner;
+	unsigned long __percpu *irqflags;
+};
+
+struct printk_ringbuffer {
+	void *buffer;
+	unsigned int size_bits;
+
+	u64 seq;
+	atomic_long_t lost;
+
+	atomic_long_t tail;
+	atomic_long_t head;
+	atomic_long_t reserve;
+
+	struct prb_cpulock *cpulock;
+	atomic_t ctx;
+
+	struct wait_queue_head *wq;
+	atomic_long_t wq_counter;
+	struct irq_work *wq_work;
+};
+
+struct prb_entry {
+	unsigned int size;
+	u64 seq;
+	char data[0];
+};
+
+struct prb_handle {
+	struct printk_ringbuffer *rb;
+	unsigned int cpu;
+	struct prb_entry *entry;
+};
+
+#define DECLARE_STATIC_PRINTKRB_CPULOCK(name)				\
+static DEFINE_PER_CPU(unsigned long, _##name##_percpu_irqflags);	\
+static struct prb_cpulock name = {					\
+	.owner = ATOMIC_INIT(-1),					\
+	.irqflags = &_##name##_percpu_irqflags,				\
+}
+
+#define PRB_INIT ((unsigned long)-1)
+
+#define DECLARE_STATIC_PRINTKRB_ITER(name, rbaddr)			\
+static struct prb_iterator name = {					\
+	.rb = rbaddr,							\
+	.lpos = PRB_INIT,						\
+}
+
+struct prb_iterator {
+	struct printk_ringbuffer *rb;
+	unsigned long lpos;
+};
+
+#define DECLARE_STATIC_PRINTKRB(name, szbits, cpulockptr)		\
+static char _##name##_buffer[1 << (szbits)]				\
+	__aligned(__alignof__(long));					\
+static DECLARE_WAIT_QUEUE_HEAD(_##name##_wait);				\
+static void _##name##_wake_work_func(struct irq_work *irq_work)		\
+{									\
+	wake_up_interruptible_all(&_##name##_wait);			\
+}									\
+static struct irq_work _##name##_wake_work = {				\
+	.func = _##name##_wake_work_func,				\
+	.flags = IRQ_WORK_LAZY,						\
+};									\
+static struct printk_ringbuffer name = {				\
+	.buffer = &_##name##_buffer[0],					\
+	.size_bits = szbits,						\
+	.seq = 0,							\
+	.lost = ATOMIC_LONG_INIT(0),					\
+	.tail = ATOMIC_LONG_INIT(-111 * sizeof(long)),			\
+	.head = ATOMIC_LONG_INIT(-111 * sizeof(long)),			\
+	.reserve = ATOMIC_LONG_INIT(-111 * sizeof(long)),		\
+	.cpulock = cpulockptr,						\
+	.ctx = ATOMIC_INIT(0),						\
+	.wq = &_##name##_wait,						\
+	.wq_counter = ATOMIC_LONG_INIT(0),				\
+	.wq_work = &_##name##_wake_work,				\
+}
+
+/* writer interface */
+char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+		  unsigned int size);
+void prb_commit(struct prb_handle *h);
+
+/* reader interface */
+void prb_iter_init(struct prb_iterator *iter, struct printk_ringbuffer *rb,
+		   u64 *seq);
+void prb_iter_copy(struct prb_iterator *dest, struct prb_iterator *src);
+int prb_iter_next(struct prb_iterator *iter, char *buf, int size, u64 *seq);
+int prb_iter_wait_next(struct prb_iterator *iter, char *buf, int size,
+		       u64 *seq);
+int prb_iter_seek(struct prb_iterator *iter, u64 seq);
+int prb_iter_data(struct prb_iterator *iter, char *buf, int size, u64 *seq);
+
+/* utility functions */
+int prb_buffer_size(struct printk_ringbuffer *rb);
+void prb_inc_lost(struct printk_ringbuffer *rb);
+void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store);
+void prb_unlock(struct prb_cpulock *cpu_lock, unsigned int cpu_store);
+
+#endif /*_LINUX_PRINTK_RINGBUFFER_H */
diff -Nur linux-5.4.5/include/linux/radix-tree.h linux-5.4.5-new/include/linux/radix-tree.h
--- linux-5.4.5/include/linux/radix-tree.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/radix-tree.h	2020-06-15 16:12:24.791719099 +0300
@@ -226,6 +226,7 @@
 			unsigned int max_items);
 int radix_tree_preload(gfp_t gfp_mask);
 int radix_tree_maybe_preload(gfp_t gfp_mask);
+void radix_tree_preload_end(void);
 void radix_tree_init(void);
 void *radix_tree_tag_set(struct radix_tree_root *,
 			unsigned long index, unsigned int tag);
@@ -243,11 +244,6 @@
 		unsigned int max_items, unsigned int tag);
 int radix_tree_tagged(const struct radix_tree_root *, unsigned int tag);
 
-static inline void radix_tree_preload_end(void)
-{
-	preempt_enable();
-}
-
 void __rcu **idr_get_free(struct radix_tree_root *root,
 			      struct radix_tree_iter *iter, gfp_t gfp,
 			      unsigned long max);
diff -Nur linux-5.4.5/include/linux/random.h linux-5.4.5-new/include/linux/random.h
--- linux-5.4.5/include/linux/random.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/random.h	2020-06-15 16:12:24.791719099 +0300
@@ -33,7 +33,7 @@
 
 extern void add_input_randomness(unsigned int type, unsigned int code,
 				 unsigned int value) __latent_entropy;
-extern void add_interrupt_randomness(int irq, int irq_flags) __latent_entropy;
+extern void add_interrupt_randomness(int irq, int irq_flags, __u64 ip) __latent_entropy;
 
 extern void get_random_bytes(void *buf, int nbytes);
 extern int wait_for_random_bytes(void);
diff -Nur linux-5.4.5/include/linux/ratelimit.h linux-5.4.5-new/include/linux/ratelimit.h
--- linux-5.4.5/include/linux/ratelimit.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/ratelimit.h	2020-06-15 16:12:24.791719099 +0300
@@ -59,7 +59,7 @@
 		return;
 
 	if (rs->missed) {
-		pr_warn("%s: %d output lines suppressed due to ratelimiting\n",
+		pr_info("%s: %d output lines suppressed due to ratelimiting\n",
 			current->comm, rs->missed);
 		rs->missed = 0;
 	}
diff -Nur linux-5.4.5/include/linux/rbtree.h linux-5.4.5-new/include/linux/rbtree.h
--- linux-5.4.5/include/linux/rbtree.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/rbtree.h	2020-06-15 16:12:24.791719099 +0300
@@ -19,7 +19,7 @@
 
 #include <linux/kernel.h>
 #include <linux/stddef.h>
-#include <linux/rcupdate.h>
+#include <linux/rcu_assign_pointer.h>
 
 struct rb_node {
 	unsigned long  __rb_parent_color;
diff -Nur linux-5.4.5/include/linux/rcu_assign_pointer.h linux-5.4.5-new/include/linux/rcu_assign_pointer.h
--- linux-5.4.5/include/linux/rcu_assign_pointer.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/rcu_assign_pointer.h	2020-06-15 16:12:24.791719099 +0300
@@ -0,0 +1,62 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+#ifndef __LINUX_RCU_ASSIGN_POINTER_H__
+#define __LINUX_RCU_ASSIGN_POINTER_H__
+#include <linux/compiler.h>
+#include <asm/barrier.h>
+
+#ifdef __CHECKER__
+#define rcu_check_sparse(p, space) \
+	((void)(((typeof(*p) space *)p) == p))
+#else /* #ifdef __CHECKER__ */
+#define rcu_check_sparse(p, space)
+#endif /* #else #ifdef __CHECKER__ */
+
+/**
+ * RCU_INITIALIZER() - statically initialize an RCU-protected global variable
+ * @v: The value to statically initialize with.
+ */
+#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)
+
+/**
+ * rcu_assign_pointer() - assign to RCU-protected pointer
+ * @p: pointer to assign to
+ * @v: value to assign (publish)
+ *
+ * Assigns the specified value to the specified RCU-protected
+ * pointer, ensuring that any concurrent RCU readers will see
+ * any prior initialization.
+ *
+ * Inserts memory barriers on architectures that require them
+ * (which is most of them), and also prevents the compiler from
+ * reordering the code that initializes the structure after the pointer
+ * assignment.  More importantly, this call documents which pointers
+ * will be dereferenced by RCU read-side code.
+ *
+ * In some special cases, you may use RCU_INIT_POINTER() instead
+ * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
+ * to the fact that it does not constrain either the CPU or the compiler.
+ * That said, using RCU_INIT_POINTER() when you should have used
+ * rcu_assign_pointer() is a very bad thing that results in
+ * impossible-to-diagnose memory corruption.  So please be careful.
+ * See the RCU_INIT_POINTER() comment header for details.
+ *
+ * Note that rcu_assign_pointer() evaluates each of its arguments only
+ * once, appearances notwithstanding.  One of the "extra" evaluations
+ * is in typeof() and the other visible only to sparse (__CHECKER__),
+ * neither of which actually execute the argument.  As with most cpp
+ * macros, this execute-arguments-only-once property is important, so
+ * please be careful when making changes to rcu_assign_pointer() and the
+ * other macros that it invokes.
+ */
+#define rcu_assign_pointer(p, v)					\
+do {									\
+	uintptr_t _r_a_p__v = (uintptr_t)(v);				\
+	rcu_check_sparse(p, __rcu);					\
+									\
+	if (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)	\
+		WRITE_ONCE((p), (typeof(p))(_r_a_p__v));		\
+	else								\
+		smp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v));	\
+} while (0)
+
+#endif
diff -Nur linux-5.4.5/include/linux/rcupdate.h linux-5.4.5-new/include/linux/rcupdate.h
--- linux-5.4.5/include/linux/rcupdate.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/rcupdate.h	2020-06-15 16:12:24.791719099 +0300
@@ -29,6 +29,7 @@
 #include <linux/lockdep.h>
 #include <asm/processor.h>
 #include <linux/cpumask.h>
+#include <linux/rcu_assign_pointer.h>
 
 #define ULONG_CMP_GE(a, b)	(ULONG_MAX / 2 >= (a) - (b))
 #define ULONG_CMP_LT(a, b)	(ULONG_MAX / 2 < (a) - (b))
@@ -51,6 +52,11 @@
  * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
  */
 #define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+#ifndef CONFIG_PREEMPT_RT
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+#else
+static inline int sched_rcu_preempt_depth(void) { return 0; }
+#endif
 
 #else /* #ifdef CONFIG_PREEMPT_RCU */
 
@@ -69,6 +75,8 @@
 	return 0;
 }
 
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+
 #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
 
 /* Internal to kernel */
@@ -154,7 +162,7 @@
  *
  * This macro resembles cond_resched(), except that it is defined to
  * report potential quiescent states to RCU-tasks even if the cond_resched()
- * machinery were to be shut off, as some advocate for PREEMPT kernels.
+ * machinery were to be shut off, as some advocate for PREEMPTION kernels.
  */
 #define cond_resched_tasks_rcu_qs() \
 do { \
@@ -279,7 +287,8 @@
 #define rcu_sleep_check()						\
 	do {								\
 		rcu_preempt_sleep_check();				\
-		RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
+		if (!IS_ENABLED(CONFIG_PREEMPT_RT))		\
+		    RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),	\
 				 "Illegal context switch in RCU-bh read-side critical section"); \
 		RCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),	\
 				 "Illegal context switch in RCU-sched read-side critical section"); \
@@ -300,13 +309,6 @@
  * (e.g., __srcu), should this make sense in the future.
  */
 
-#ifdef __CHECKER__
-#define rcu_check_sparse(p, space) \
-	((void)(((typeof(*p) space *)p) == p))
-#else /* #ifdef __CHECKER__ */
-#define rcu_check_sparse(p, space)
-#endif /* #else #ifdef __CHECKER__ */
-
 #define __rcu_access_pointer(p, space) \
 ({ \
 	typeof(*p) *_________p1 = (typeof(*p) *__force)READ_ONCE(p); \
@@ -335,54 +337,6 @@
 })
 
 /**
- * RCU_INITIALIZER() - statically initialize an RCU-protected global variable
- * @v: The value to statically initialize with.
- */
-#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)
-
-/**
- * rcu_assign_pointer() - assign to RCU-protected pointer
- * @p: pointer to assign to
- * @v: value to assign (publish)
- *
- * Assigns the specified value to the specified RCU-protected
- * pointer, ensuring that any concurrent RCU readers will see
- * any prior initialization.
- *
- * Inserts memory barriers on architectures that require them
- * (which is most of them), and also prevents the compiler from
- * reordering the code that initializes the structure after the pointer
- * assignment.  More importantly, this call documents which pointers
- * will be dereferenced by RCU read-side code.
- *
- * In some special cases, you may use RCU_INIT_POINTER() instead
- * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
- * to the fact that it does not constrain either the CPU or the compiler.
- * That said, using RCU_INIT_POINTER() when you should have used
- * rcu_assign_pointer() is a very bad thing that results in
- * impossible-to-diagnose memory corruption.  So please be careful.
- * See the RCU_INIT_POINTER() comment header for details.
- *
- * Note that rcu_assign_pointer() evaluates each of its arguments only
- * once, appearances notwithstanding.  One of the "extra" evaluations
- * is in typeof() and the other visible only to sparse (__CHECKER__),
- * neither of which actually execute the argument.  As with most cpp
- * macros, this execute-arguments-only-once property is important, so
- * please be careful when making changes to rcu_assign_pointer() and the
- * other macros that it invokes.
- */
-#define rcu_assign_pointer(p, v)					      \
-do {									      \
-	uintptr_t _r_a_p__v = (uintptr_t)(v);				      \
-	rcu_check_sparse(p, __rcu);					      \
-									      \
-	if (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)	      \
-		WRITE_ONCE((p), (typeof(p))(_r_a_p__v));		      \
-	else								      \
-		smp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \
-} while (0)
-
-/**
  * rcu_swap_protected() - swap an RCU and a regular pointer
  * @rcu_ptr: RCU pointer
  * @ptr: regular pointer
@@ -580,7 +534,7 @@
  *
  * You can avoid reading and understanding the next paragraph by
  * following this rule: don't put anything in an rcu_read_lock() RCU
- * read-side critical section that would block in a !PREEMPT kernel.
+ * read-side critical section that would block in a !PREEMPTION kernel.
  * But if you want the full story, read on!
  *
  * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
diff -Nur linux-5.4.5/include/linux/rtmutex.h linux-5.4.5-new/include/linux/rtmutex.h
--- linux-5.4.5/include/linux/rtmutex.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/rtmutex.h	2020-06-15 16:12:24.795719085 +0300
@@ -14,11 +14,15 @@
 #define __LINUX_RT_MUTEX_H
 
 #include <linux/linkage.h>
+#include <linux/spinlock_types_raw.h>
 #include <linux/rbtree.h>
-#include <linux/spinlock_types.h>
 
 extern int max_lock_depth; /* for sysctl */
 
+#ifdef CONFIG_DEBUG_MUTEXES
+#include <linux/debug_locks.h>
+#endif
+
 /**
  * The rt_mutex structure
  *
@@ -31,8 +35,8 @@
 	raw_spinlock_t		wait_lock;
 	struct rb_root_cached   waiters;
 	struct task_struct	*owner;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
 	int			save_state;
+#ifdef CONFIG_DEBUG_RT_MUTEXES
 	const char		*name, *file;
 	int			line;
 	void			*magic;
@@ -82,16 +86,23 @@
 #define __DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
 #endif
 
-#define __RT_MUTEX_INITIALIZER(mutexname) \
-	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
+#define __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
 	, .waiters = RB_ROOT_CACHED \
 	, .owner = NULL \
 	__DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
-	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)}
+	__DEP_MAP_RT_MUTEX_INITIALIZER(mutexname)
+
+#define __RT_MUTEX_INITIALIZER(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname) }
 
 #define DEFINE_RT_MUTEX(mutexname) \
 	struct rt_mutex mutexname = __RT_MUTEX_INITIALIZER(mutexname)
 
+#define __RT_MUTEX_INITIALIZER_SAVE_STATE(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname)    \
+		, .save_state = 1 }
+
 /**
  * rt_mutex_is_locked - is the mutex locked
  * @lock: the mutex to be queried
@@ -115,6 +126,7 @@
 #endif
 
 extern int rt_mutex_lock_interruptible(struct rt_mutex *lock);
+extern int rt_mutex_lock_killable(struct rt_mutex *lock);
 extern int rt_mutex_timed_lock(struct rt_mutex *lock,
 			       struct hrtimer_sleeper *timeout);
 
diff -Nur linux-5.4.5/include/linux/rwlock_rt.h linux-5.4.5-new/include/linux/rwlock_rt.h
--- linux-5.4.5/include/linux/rwlock_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/rwlock_rt.h	2020-06-15 16:12:24.795719085 +0300
@@ -0,0 +1,119 @@
+#ifndef __LINUX_RWLOCK_RT_H
+#define __LINUX_RWLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+extern void __lockfunc rt_write_lock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_trylock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_trylock(rwlock_t *rwlock);
+extern void __lockfunc rt_write_unlock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_unlock(rwlock_t *rwlock);
+extern int __lockfunc rt_read_can_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_can_lock(rwlock_t *rwlock);
+extern void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key);
+
+#define read_can_lock(rwlock)		rt_read_can_lock(rwlock)
+#define write_can_lock(rwlock)		rt_write_can_lock(rwlock)
+
+#define read_trylock(lock)	__cond_lock(lock, rt_read_trylock(lock))
+#define write_trylock(lock)	__cond_lock(lock, rt_write_trylock(lock))
+
+static inline int __write_trylock_rt_irqsave(rwlock_t *lock, unsigned long *flags)
+{
+	/* XXX ARCH_IRQ_ENABLED */
+	*flags = 0;
+	return rt_write_trylock(lock);
+}
+
+#define write_trylock_irqsave(lock, flags)		\
+	__cond_lock(lock, __write_trylock_rt_irqsave(lock, &(flags)))
+
+#define read_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_read_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define write_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		rt_write_lock(lock);			\
+		flags = 0;				\
+	} while (0)
+
+#define read_lock(lock)		rt_read_lock(lock)
+
+#define read_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_read_lock(lock);			\
+	} while (0)
+
+#define read_lock_irq(lock)	read_lock(lock)
+
+#define write_lock(lock)	rt_write_lock(lock)
+
+#define write_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_write_lock(lock);			\
+	} while (0)
+
+#define write_lock_irq(lock)	write_lock(lock)
+
+#define read_unlock(lock)	rt_read_unlock(lock)
+
+#define read_unlock_bh(lock)				\
+	do {						\
+		rt_read_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define read_unlock_irq(lock)	read_unlock(lock)
+
+#define write_unlock(lock)	rt_write_unlock(lock)
+
+#define write_unlock_bh(lock)				\
+	do {						\
+		rt_write_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define write_unlock_irq(lock)	write_unlock(lock)
+
+#define read_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_read_unlock(lock);			\
+	} while (0)
+
+#define write_unlock_irqrestore(lock, flags) \
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_write_unlock(lock);			\
+	} while (0)
+
+#define rwlock_init(rwl)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__rt_rwlock_init(rwl, #rwl, &__key);		\
+} while (0)
+
+/*
+ * Internal functions made global for CPU pinning
+ */
+void __read_rt_lock(struct rt_rw_lock *lock);
+int __read_rt_trylock(struct rt_rw_lock *lock);
+void __write_rt_lock(struct rt_rw_lock *lock);
+int __write_rt_trylock(struct rt_rw_lock *lock);
+void __read_rt_unlock(struct rt_rw_lock *lock);
+void __write_rt_unlock(struct rt_rw_lock *lock);
+
+#endif
diff -Nur linux-5.4.5/include/linux/rwlock_types.h linux-5.4.5-new/include/linux/rwlock_types.h
--- linux-5.4.5/include/linux/rwlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/rwlock_types.h	2020-06-15 16:12:24.795719085 +0300
@@ -1,6 +1,10 @@
 #ifndef __LINUX_RWLOCK_TYPES_H
 #define __LINUX_RWLOCK_TYPES_H
 
+#if !defined(__LINUX_SPINLOCK_TYPES_H)
+# error "Do not include directly, include spinlock_types.h"
+#endif
+
 /*
  * include/linux/rwlock_types.h - generic rwlock type definitions
  *				  and initializers
diff -Nur linux-5.4.5/include/linux/rwlock_types_rt.h linux-5.4.5-new/include/linux/rwlock_types_rt.h
--- linux-5.4.5/include/linux/rwlock_types_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/rwlock_types_rt.h	2020-06-15 16:12:24.795719085 +0300
@@ -0,0 +1,55 @@
+#ifndef __LINUX_RWLOCK_TYPES_RT_H
+#define __LINUX_RWLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RW_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define RW_DEP_MAP_INIT(lockname)
+#endif
+
+typedef struct rt_rw_lock rwlock_t;
+
+#define __RW_LOCK_UNLOCKED(name) __RWLOCK_RT_INITIALIZER(name)
+
+#define DEFINE_RWLOCK(name) \
+	rwlock_t name = __RW_LOCK_UNLOCKED(name)
+
+/*
+ * A reader biased implementation primarily for CPU pinning.
+ *
+ * Can be selected as general replacement for the single reader RT rwlock
+ * variant
+ */
+struct rt_rw_lock {
+	struct rt_mutex		rtmutex;
+	atomic_t		readers;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define READER_BIAS	(1U << 31)
+#define WRITER_BIAS	(1U << 30)
+
+#define __RWLOCK_RT_INITIALIZER(name)					\
+{									\
+	.readers = ATOMIC_INIT(READER_BIAS),				\
+	.rtmutex = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)						\
+}
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key);
+
+#define rwlock_biased_rt_init(rwlock)					\
+	do {								\
+		static struct lock_class_key __key;			\
+									\
+		__rwlock_biased_rt_init((rwlock), #rwlock, &__key);	\
+	} while (0)
+
+#endif
diff -Nur linux-5.4.5/include/linux/rwsem.h linux-5.4.5-new/include/linux/rwsem.h
--- linux-5.4.5/include/linux/rwsem.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/rwsem.h	2020-06-15 16:12:24.795719085 +0300
@@ -16,6 +16,11 @@
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
 #include <linux/err.h>
+
+#ifdef CONFIG_PREEMPT_RT
+#include <linux/rwsem-rt.h>
+#else /* PREEMPT_RT */
+
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #include <linux/osq_lock.h>
 #endif
@@ -121,6 +126,13 @@
 	return !list_empty(&sem->wait_list);
 }
 
+#endif /* !PREEMPT_RT */
+
+/*
+ * The functions below are the same for all rwsem implementations including
+ * the RT specific variant.
+ */
+
 /*
  * lock for reading
  */
diff -Nur linux-5.4.5/include/linux/rwsem-rt.h linux-5.4.5-new/include/linux/rwsem-rt.h
--- linux-5.4.5/include/linux/rwsem-rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/rwsem-rt.h	2020-06-15 16:12:24.795719085 +0300
@@ -0,0 +1,68 @@
+#ifndef _LINUX_RWSEM_RT_H
+#define _LINUX_RWSEM_RT_H
+
+#ifndef _LINUX_RWSEM_H
+#error "Include rwsem.h"
+#endif
+
+#include <linux/rtmutex.h>
+#include <linux/swait.h>
+
+#define READER_BIAS		(1U << 31)
+#define WRITER_BIAS		(1U << 30)
+
+struct rw_semaphore {
+	atomic_t		readers;
+	struct rt_mutex		rtmutex;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __RWSEM_INITIALIZER(name)				\
+{								\
+	.readers = ATOMIC_INIT(READER_BIAS),			\
+	.rtmutex = __RT_MUTEX_INITIALIZER(name.rtmutex),	\
+	RW_DEP_MAP_INIT(name)					\
+}
+
+#define DECLARE_RWSEM(lockname) \
+	struct rw_semaphore lockname = __RWSEM_INITIALIZER(lockname)
+
+extern void  __rwsem_init(struct rw_semaphore *rwsem, const char *name,
+			  struct lock_class_key *key);
+
+#define __init_rwsem(sem, name, key)			\
+do {							\
+		rt_mutex_init(&(sem)->rtmutex);		\
+		__rwsem_init((sem), (name), (key));	\
+} while (0)
+
+#define init_rwsem(sem)					\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__init_rwsem((sem), #sem, &__key);		\
+} while (0)
+
+static inline int rwsem_is_locked(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) != READER_BIAS;
+}
+
+static inline int rwsem_is_contended(struct rw_semaphore *sem)
+{
+	return atomic_read(&sem->readers) > 0;
+}
+
+extern void __down_read(struct rw_semaphore *sem);
+extern int __down_read_killable(struct rw_semaphore *sem);
+extern int __down_read_trylock(struct rw_semaphore *sem);
+extern void __down_write(struct rw_semaphore *sem);
+extern int __must_check __down_write_killable(struct rw_semaphore *sem);
+extern int __down_write_trylock(struct rw_semaphore *sem);
+extern void __up_read(struct rw_semaphore *sem);
+extern void __up_write(struct rw_semaphore *sem);
+extern void __downgrade_write(struct rw_semaphore *sem);
+
+#endif
diff -Nur linux-5.4.5/include/linux/sched/mm.h linux-5.4.5-new/include/linux/sched/mm.h
--- linux-5.4.5/include/linux/sched/mm.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/sched/mm.h	2020-06-15 16:12:25.007718339 +0300
@@ -49,6 +49,17 @@
 		__mmdrop(mm);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+extern void __mmdrop_delayed(struct rcu_head *rhp);
+static inline void mmdrop_delayed(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_count))
+		call_rcu(&mm->delayed_drop, __mmdrop_delayed);
+}
+#else
+# define mmdrop_delayed(mm)	mmdrop(mm)
+#endif
+
 /*
  * This has to be called after a get_task_mm()/mmget_not_zero()
  * followed by taking the mmap_sem for writing before modifying the
diff -Nur linux-5.4.5/include/linux/sched/wake_q.h linux-5.4.5-new/include/linux/sched/wake_q.h
--- linux-5.4.5/include/linux/sched/wake_q.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/sched/wake_q.h	2020-06-15 16:12:25.007718339 +0300
@@ -58,6 +58,17 @@
 
 extern void wake_q_add(struct wake_q_head *head, struct task_struct *task);
 extern void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task);
-extern void wake_up_q(struct wake_q_head *head);
+extern void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task);
+extern void __wake_up_q(struct wake_q_head *head, bool sleeper);
+
+static inline void wake_up_q(struct wake_q_head *head)
+{
+	__wake_up_q(head, false);
+}
+
+static inline void wake_up_q_sleeper(struct wake_q_head *head)
+{
+	__wake_up_q(head, true);
+}
 
 #endif /* _LINUX_SCHED_WAKE_Q_H */
diff -Nur linux-5.4.5/include/linux/sched.h linux-5.4.5-new/include/linux/sched.h
--- linux-5.4.5/include/linux/sched.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/sched.h	2020-06-15 16:12:24.795719085 +0300
@@ -31,6 +31,7 @@
 #include <linux/task_io_accounting.h>
 #include <linux/posix-timers.h>
 #include <linux/rseq.h>
+#include <asm/kmap_types.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -107,12 +108,8 @@
 					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
 					 TASK_PARKED)
 
-#define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
-
 #define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
 
-#define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
-
 #define task_contributes_to_load(task)	((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
 					 (task->flags & PF_FROZEN) == 0 && \
 					 (task->state & TASK_NOLOAD) == 0)
@@ -140,6 +137,9 @@
 		smp_store_mb(current->state, (state_value));	\
 	} while (0)
 
+#define __set_current_state_no_track(state_value)		\
+	current->state = (state_value);
+
 #define set_special_state(state_value)					\
 	do {								\
 		unsigned long flags; /* may shadow */			\
@@ -149,6 +149,7 @@
 		current->state = (state_value);				\
 		raw_spin_unlock_irqrestore(&current->pi_lock, flags);	\
 	} while (0)
+
 #else
 /*
  * set_current_state() includes a barrier so that the write of current->state
@@ -193,6 +194,9 @@
 #define set_current_state(state_value)					\
 	smp_store_mb(current->state, (state_value))
 
+#define __set_current_state_no_track(state_value)	\
+	__set_current_state(state_value)
+
 /*
  * set_special_state() should be used for those states when the blocking task
  * can not use the regular condition based wait-loop. In that case we must
@@ -230,6 +234,8 @@
 extern long io_schedule_timeout(long timeout);
 extern void io_schedule(void);
 
+int cpu_nr_pinned(int cpu);
+
 /**
  * struct prev_cputime - snapshot of system and user cputime
  * @utime: time spent in user mode
@@ -631,6 +637,8 @@
 #endif
 	/* -1 unrunnable, 0 runnable, >0 stopped: */
 	volatile long			state;
+	/* saved state for "spinlock sleepers" */
+	volatile long			saved_state;
 
 	/*
 	 * This begins the randomizable portion of task_struct. Only
@@ -700,6 +708,20 @@
 	int				nr_cpus_allowed;
 	const cpumask_t			*cpus_ptr;
 	cpumask_t			cpus_mask;
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	int				migrate_disable;
+	bool				migrate_disable_scheduled;
+# ifdef CONFIG_SCHED_DEBUG
+	int				pinned_on_cpu;
+# endif
+#elif !defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+# ifdef CONFIG_SCHED_DEBUG
+	int				migrate_disable;
+# endif
+#endif
+#ifdef CONFIG_PREEMPT_RT
+	int				sleeping_lock;
+#endif
 
 #ifdef CONFIG_PREEMPT_RCU
 	int				rcu_read_lock_nesting;
@@ -913,11 +935,17 @@
 	/* Signal handlers: */
 	struct signal_struct		*signal;
 	struct sighand_struct		*sighand;
+	struct sigqueue			*sigqueue_cache;
+
 	sigset_t			blocked;
 	sigset_t			real_blocked;
 	/* Restored if set_restore_sigmask() was used: */
 	sigset_t			saved_sigmask;
 	struct sigpending		pending;
+#ifdef CONFIG_PREEMPT_RT
+	/* TODO: move me into ->restart_block ? */
+	struct				kernel_siginfo forced_info;
+#endif
 	unsigned long			sas_ss_sp;
 	size_t				sas_ss_size;
 	unsigned int			sas_ss_flags;
@@ -944,6 +972,7 @@
 	raw_spinlock_t			pi_lock;
 
 	struct wake_q_node		wake_q;
+	struct wake_q_node		wake_q_sleeper;
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task: */
@@ -978,6 +1007,9 @@
 	int				softirqs_enabled;
 	int				softirq_context;
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	int				softirq_count;
+#endif
 
 #ifdef CONFIG_LOCKDEP
 # define MAX_LOCK_DEPTH			48UL
@@ -1241,6 +1273,12 @@
 	unsigned int			sequential_io;
 	unsigned int			sequential_io_avg;
 #endif
+#ifdef CONFIG_PREEMPT_RT
+# if defined CONFIG_HIGHMEM || defined CONFIG_X86_32
+	int				kmap_idx;
+	pte_t				kmap_pte[KM_TYPE_NR];
+# endif
+#endif
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	unsigned long			task_state_change;
 #endif
@@ -1672,6 +1710,7 @@
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
+extern int wake_up_lock_sleeper(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
@@ -1754,6 +1793,89 @@
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+static inline void set_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int test_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY));
+}
+
+static inline int need_resched_lazy(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#else
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk) { }
+static inline int need_resched_lazy(void) { return 0; }
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#endif
+
+
+static inline bool __task_is_stopped_or_traced(struct task_struct *task)
+{
+	if (task->state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	if (task->saved_state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#endif
+	return false;
+}
+
+static inline bool task_is_stopped_or_traced(struct task_struct *task)
+{
+	bool traced_stopped;
+
+#ifdef CONFIG_PREEMPT_RT
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&task->pi_lock, flags);
+	traced_stopped = __task_is_stopped_or_traced(task);
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+#else
+	traced_stopped = __task_is_stopped_or_traced(task);
+#endif
+	return traced_stopped;
+}
+
+static inline bool task_is_traced(struct task_struct *task)
+{
+	bool traced = false;
+
+	if (task->state & __TASK_TRACED)
+		return true;
+#ifdef CONFIG_PREEMPT_RT
+	/* in case the task is sleeping on tasklist_lock */
+	raw_spin_lock_irq(&task->pi_lock);
+	if (task->state & __TASK_TRACED)
+		traced = true;
+	else if (task->saved_state & __TASK_TRACED)
+		traced = true;
+	raw_spin_unlock_irq(&task->pi_lock);
+#endif
+	return traced;
+}
+
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
@@ -1806,6 +1928,23 @@
 	return unlikely(tif_need_resched());
 }
 
+#ifdef CONFIG_PREEMPT_RT
+static inline void sleeping_lock_inc(void)
+{
+	current->sleeping_lock++;
+}
+
+static inline void sleeping_lock_dec(void)
+{
+	current->sleeping_lock--;
+}
+
+#else
+
+static inline void sleeping_lock_inc(void) { }
+static inline void sleeping_lock_dec(void) { }
+#endif
+
 /*
  * Wrappers for p->thread_info->cpu access. No-op on UP.
  */
@@ -1997,4 +2136,6 @@
 
 const struct cpumask *sched_trace_rd_span(struct root_domain *rd);
 
+extern struct task_struct *takedown_cpu_task;
+
 #endif
diff -Nur linux-5.4.5/include/linux/semaphore.h linux-5.4.5-new/include/linux/semaphore.h
--- linux-5.4.5/include/linux/semaphore.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/semaphore.h	2020-06-15 14:49:31.251855645 +0300
@@ -1,5 +1,10 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *
  * Copyright (c) 2008 Intel Corporation
  * Author: Matthew Wilcox <willy@linux.intel.com>
  *
@@ -10,6 +15,7 @@
 
 #include <linux/list.h>
 #include <linux/spinlock.h>
+#include <linux/hrtimer.h>
 
 /* Please don't access any members of this structure directly */
 struct semaphore {
@@ -40,6 +46,7 @@
 extern int __must_check down_killable(struct semaphore *sem);
 extern int __must_check down_trylock(struct semaphore *sem);
 extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
+extern int __must_check down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout);
 extern void up(struct semaphore *sem);
 
 #endif /* __LINUX_SEMAPHORE_H */
diff -Nur linux-5.4.5/include/linux/seqlock.h linux-5.4.5-new/include/linux/seqlock.h
--- linux-5.4.5/include/linux/seqlock.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/seqlock.h	2020-06-15 16:12:24.795719085 +0300
@@ -221,20 +221,30 @@
 	return __read_seqcount_retry(s, start);
 }
 
-
-
-static inline void raw_write_seqcount_begin(seqcount_t *s)
+static inline void __raw_write_seqcount_begin(seqcount_t *s)
 {
 	s->sequence++;
 	smp_wmb();
 }
 
-static inline void raw_write_seqcount_end(seqcount_t *s)
+static inline void raw_write_seqcount_begin(seqcount_t *s)
+{
+	preempt_disable_rt();
+	__raw_write_seqcount_begin(s);
+}
+
+static inline void __raw_write_seqcount_end(seqcount_t *s)
 {
 	smp_wmb();
 	s->sequence++;
 }
 
+static inline void raw_write_seqcount_end(seqcount_t *s)
+{
+	__raw_write_seqcount_end(s);
+	preempt_enable_rt();
+}
+
 /**
  * raw_write_seqcount_barrier - do a seq write barrier
  * @s: pointer to seqcount_t
@@ -428,10 +438,33 @@
 /*
  * Read side functions for starting and finalizing a read side section.
  */
+#ifndef CONFIG_PREEMPT_RT
 static inline unsigned read_seqbegin(const seqlock_t *sl)
 {
 	return read_seqcount_begin(&sl->seqcount);
 }
+#else
+/*
+ * Starvation safe read side for RT
+ */
+static inline unsigned read_seqbegin(seqlock_t *sl)
+{
+	unsigned ret;
+
+repeat:
+	ret = READ_ONCE(sl->seqcount.sequence);
+	if (unlikely(ret & 1)) {
+		/*
+		 * Take the lock and let the writer proceed (i.e. evtl
+		 * boost it), otherwise we could loop here forever.
+		 */
+		spin_unlock_wait(&sl->lock);
+		goto repeat;
+	}
+	smp_rmb();
+	return ret;
+}
+#endif
 
 static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
 {
@@ -446,36 +479,45 @@
 static inline void write_seqlock(seqlock_t *sl)
 {
 	spin_lock(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
+}
+
+static inline int try_write_seqlock(seqlock_t *sl)
+{
+	if (spin_trylock(&sl->lock)) {
+		__raw_write_seqcount_begin(&sl->seqcount);
+		return 1;
+	}
+	return 0;
 }
 
 static inline void write_sequnlock(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock(&sl->lock);
 }
 
 static inline void write_seqlock_bh(seqlock_t *sl)
 {
 	spin_lock_bh(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 }
 
 static inline void write_sequnlock_bh(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_bh(&sl->lock);
 }
 
 static inline void write_seqlock_irq(seqlock_t *sl)
 {
 	spin_lock_irq(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 }
 
 static inline void write_sequnlock_irq(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_irq(&sl->lock);
 }
 
@@ -484,7 +526,7 @@
 	unsigned long flags;
 
 	spin_lock_irqsave(&sl->lock, flags);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 	return flags;
 }
 
@@ -494,7 +536,7 @@
 static inline void
 write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_irqrestore(&sl->lock, flags);
 }
 
diff -Nur linux-5.4.5/include/linux/serial_8250.h linux-5.4.5-new/include/linux/serial_8250.h
--- linux-5.4.5/include/linux/serial_8250.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/serial_8250.h	2020-06-15 16:12:24.795719085 +0300
@@ -7,6 +7,7 @@
 #ifndef _LINUX_SERIAL_8250_H
 #define _LINUX_SERIAL_8250_H
 
+#include <linux/atomic.h>
 #include <linux/serial_core.h>
 #include <linux/serial_reg.h>
 #include <linux/platform_device.h>
@@ -123,6 +124,8 @@
 #define MSR_SAVE_FLAGS UART_MSR_ANY_DELTA
 	unsigned char		msr_saved_flags;
 
+	atomic_t		console_printing;
+
 	struct uart_8250_dma	*dma;
 	const struct uart_8250_ops *ops;
 
@@ -174,6 +177,8 @@
 void serial8250_set_defaults(struct uart_8250_port *up);
 void serial8250_console_write(struct uart_8250_port *up, const char *s,
 			      unsigned int count);
+void serial8250_console_write_atomic(struct uart_8250_port *up, const char *s,
+				     unsigned int count);
 int serial8250_console_setup(struct uart_port *port, char *options, bool probe);
 
 extern void serial8250_set_isa_configurator(void (*v)
diff -Nur linux-5.4.5/include/linux/signal.h linux-5.4.5-new/include/linux/signal.h
--- linux-5.4.5/include/linux/signal.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/signal.h	2020-06-15 16:12:24.799719071 +0300
@@ -255,6 +255,7 @@
 }
 
 extern void flush_sigqueue(struct sigpending *queue);
+extern void flush_task_sigqueue(struct task_struct *tsk);
 
 /* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
 static inline int valid_signal(unsigned long sig)
diff -Nur linux-5.4.5/include/linux/skbuff.h linux-5.4.5-new/include/linux/skbuff.h
--- linux-5.4.5/include/linux/skbuff.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/skbuff.h	2020-06-15 16:12:24.799719071 +0300
@@ -293,6 +293,7 @@
 
 	__u32		qlen;
 	spinlock_t	lock;
+	raw_spinlock_t	raw_lock;
 };
 
 struct sk_buff;
@@ -1844,6 +1845,12 @@
 	__skb_queue_head_init(list);
 }
 
+static inline void skb_queue_head_init_raw(struct sk_buff_head *list)
+{
+	raw_spin_lock_init(&list->raw_lock);
+	__skb_queue_head_init(list);
+}
+
 static inline void skb_queue_head_init_class(struct sk_buff_head *list,
 		struct lock_class_key *class)
 {
diff -Nur linux-5.4.5/include/linux/smp.h linux-5.4.5-new/include/linux/smp.h
--- linux-5.4.5/include/linux/smp.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/smp.h	2020-06-15 16:12:24.799719071 +0300
@@ -222,6 +222,9 @@
 #define get_cpu()		({ preempt_disable(); __smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
+#define get_cpu_light()		({ migrate_disable(); __smp_processor_id(); })
+#define put_cpu_light()		migrate_enable()
+
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the
  * boot command line:
diff -Nur linux-5.4.5/include/linux/spinlock_api_smp.h linux-5.4.5-new/include/linux/spinlock_api_smp.h
--- linux-5.4.5/include/linux/spinlock_api_smp.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_api_smp.h	2020-06-15 16:12:24.799719071 +0300
@@ -187,6 +187,8 @@
 	return 0;
 }
 
-#include <linux/rwlock_api_smp.h>
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/rwlock_api_smp.h>
+#endif
 
 #endif /* __LINUX_SPINLOCK_API_SMP_H */
diff -Nur linux-5.4.5/include/linux/spinlock.h linux-5.4.5-new/include/linux/spinlock.h
--- linux-5.4.5/include/linux/spinlock.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock.h	2020-06-15 16:12:24.799719071 +0300
@@ -307,7 +307,11 @@
 })
 
 /* Include rwlock functions */
-#include <linux/rwlock.h>
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/rwlock_rt.h>
+#else
+# include <linux/rwlock.h>
+#endif
 
 /*
  * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
@@ -318,6 +322,10 @@
 # include <linux/spinlock_api_up.h>
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+# include <linux/spinlock_rt.h>
+#else /* PREEMPT_RT */
+
 /*
  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
  */
@@ -438,6 +446,8 @@
 
 #define assert_spin_locked(lock)	assert_raw_spin_locked(&(lock)->rlock)
 
+#endif /* !PREEMPT_RT */
+
 /*
  * Pull the atomic_t declaration:
  * (asm-mips/atomic.h needs above definitions)
diff -Nur linux-5.4.5/include/linux/spinlock_rt.h linux-5.4.5-new/include/linux/spinlock_rt.h
--- linux-5.4.5/include/linux/spinlock_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_rt.h	2020-06-15 16:12:24.799719071 +0300
@@ -0,0 +1,156 @@
+#ifndef __LINUX_SPINLOCK_RT_H
+#define __LINUX_SPINLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+#include <linux/bug.h>
+
+extern void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key);
+
+#define spin_lock_init(slock)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(slock)->lock);			\
+	__rt_spin_lock_init(slock, #slock, &__key);	\
+} while (0)
+
+extern void __lockfunc rt_spin_lock(spinlock_t *lock);
+extern unsigned long __lockfunc rt_spin_lock_trace_flags(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass);
+extern void __lockfunc rt_spin_unlock(spinlock_t *lock);
+extern void __lockfunc rt_spin_unlock_wait(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags);
+extern int __lockfunc rt_spin_trylock_bh(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock(spinlock_t *lock);
+extern int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock);
+
+/*
+ * lockdep-less calls, for derived types like rwlock:
+ * (for trylock they can use rt_mutex_trylock() directly.
+ * Migrate disable handling must be done at the call site.
+ */
+extern void __lockfunc __rt_spin_lock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_trylock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_unlock(struct rt_mutex *lock);
+
+#define spin_lock(lock)			rt_spin_lock(lock)
+
+#define spin_lock_bh(lock)			\
+	do {					\
+		local_bh_disable();		\
+		rt_spin_lock(lock);		\
+	} while (0)
+
+#define spin_lock_irq(lock)		spin_lock(lock)
+
+#define spin_do_trylock(lock)		__cond_lock(lock, rt_spin_trylock(lock))
+
+#define spin_trylock(lock)			\
+({						\
+	int __locked;				\
+	__locked = spin_do_trylock(lock);	\
+	__locked;				\
+})
+
+#ifdef CONFIG_LOCKDEP
+# define spin_lock_nested(lock, subclass)		\
+	do {						\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+#define spin_lock_bh_nested(lock, subclass)		\
+	do {						\
+		local_bh_disable();			\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		rt_spin_lock_nested(lock, subclass);	 \
+	} while (0)
+#else
+# define spin_lock_nested(lock, subclass)	spin_lock(lock)
+# define spin_lock_bh_nested(lock, subclass)	spin_lock_bh(lock)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+#endif
+
+#define spin_lock_irqsave(lock, flags)			 \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+
+static inline unsigned long spin_lock_trace_flags(spinlock_t *lock)
+{
+	unsigned long flags = 0;
+#ifdef CONFIG_TRACE_IRQFLAGS
+	flags = rt_spin_lock_trace_flags(lock);
+#else
+	spin_lock(lock); /* lock_local */
+#endif
+	return flags;
+}
+
+/* FIXME: we need rt_spin_lock_nest_lock */
+#define spin_lock_nest_lock(lock, nest_lock) spin_lock_nested(lock, 0)
+
+#define spin_unlock(lock)			rt_spin_unlock(lock)
+
+#define spin_unlock_bh(lock)				\
+	do {						\
+		rt_spin_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define spin_unlock_irq(lock)		spin_unlock(lock)
+
+#define spin_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		spin_unlock(lock);			\
+	} while (0)
+
+#define spin_trylock_bh(lock)	__cond_lock(lock, rt_spin_trylock_bh(lock))
+#define spin_trylock_irq(lock)	spin_trylock(lock)
+
+#define spin_trylock_irqsave(lock, flags)	\
+	rt_spin_trylock_irqsave(lock, &(flags))
+
+#define spin_unlock_wait(lock)		rt_spin_unlock_wait(lock)
+
+#ifdef CONFIG_GENERIC_LOCKBREAK
+# define spin_is_contended(lock)	((lock)->break_lock)
+#else
+# define spin_is_contended(lock)	(((void)(lock), 0))
+#endif
+
+static inline int spin_can_lock(spinlock_t *lock)
+{
+	return !rt_mutex_is_locked(&lock->lock);
+}
+
+static inline int spin_is_locked(spinlock_t *lock)
+{
+	return rt_mutex_is_locked(&lock->lock);
+}
+
+static inline void assert_spin_locked(spinlock_t *lock)
+{
+	BUG_ON(!spin_is_locked(lock));
+}
+
+#endif
diff -Nur linux-5.4.5/include/linux/spinlock_types.h linux-5.4.5-new/include/linux/spinlock_types.h
--- linux-5.4.5/include/linux/spinlock_types.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_types.h	2020-06-15 16:12:24.799719071 +0300
@@ -9,77 +9,15 @@
  * Released under the General Public License (GPL).
  */
 
-#if defined(CONFIG_SMP)
-# include <asm/spinlock_types.h>
-#else
-# include <linux/spinlock_types_up.h>
-#endif
-
-#include <linux/lockdep.h>
-
-typedef struct raw_spinlock {
-	arch_spinlock_t raw_lock;
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned int magic, owner_cpu;
-	void *owner;
-#endif
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	struct lockdep_map dep_map;
-#endif
-} raw_spinlock_t;
-
-#define SPINLOCK_MAGIC		0xdead4ead
-
-#define SPINLOCK_OWNER_INIT	((void *)-1L)
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
-#else
-# define SPIN_DEP_MAP_INIT(lockname)
-#endif
+#include <linux/spinlock_types_raw.h>
 
-#ifdef CONFIG_DEBUG_SPINLOCK
-# define SPIN_DEBUG_INIT(lockname)		\
-	.magic = SPINLOCK_MAGIC,		\
-	.owner_cpu = -1,			\
-	.owner = SPINLOCK_OWNER_INIT,
+#ifndef CONFIG_PREEMPT_RT
+# include <linux/spinlock_types_nort.h>
+# include <linux/rwlock_types.h>
 #else
-# define SPIN_DEBUG_INIT(lockname)
+# include <linux/rtmutex.h>
+# include <linux/spinlock_types_rt.h>
+# include <linux/rwlock_types_rt.h>
 #endif
 
-#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	SPIN_DEP_MAP_INIT(lockname) }
-
-#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
-	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
-
-typedef struct spinlock {
-	union {
-		struct raw_spinlock rlock;
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
-		struct {
-			u8 __padding[LOCK_PADSIZE];
-			struct lockdep_map dep_map;
-		};
-#endif
-	};
-} spinlock_t;
-
-#define __SPIN_LOCK_INITIALIZER(lockname) \
-	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
-
-#define __SPIN_LOCK_UNLOCKED(lockname) \
-	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
-
-#include <linux/rwlock_types.h>
-
 #endif /* __LINUX_SPINLOCK_TYPES_H */
diff -Nur linux-5.4.5/include/linux/spinlock_types_nort.h linux-5.4.5-new/include/linux/spinlock_types_nort.h
--- linux-5.4.5/include/linux/spinlock_types_nort.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_types_nort.h	2020-06-15 16:12:24.799719071 +0300
@@ -0,0 +1,33 @@
+#ifndef __LINUX_SPINLOCK_TYPES_NORT_H
+#define __LINUX_SPINLOCK_TYPES_NORT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+/*
+ * The non RT version maps spinlocks to raw_spinlocks
+ */
+typedef struct spinlock {
+	union {
+		struct raw_spinlock rlock;
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
+		struct {
+			u8 __padding[LOCK_PADSIZE];
+			struct lockdep_map dep_map;
+		};
+#endif
+	};
+} spinlock_t;
+
+#define __SPIN_LOCK_INITIALIZER(lockname) \
+	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
+
+#define __SPIN_LOCK_UNLOCKED(lockname) \
+	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff -Nur linux-5.4.5/include/linux/spinlock_types_raw.h linux-5.4.5-new/include/linux/spinlock_types_raw.h
--- linux-5.4.5/include/linux/spinlock_types_raw.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_types_raw.h	2020-06-15 16:12:24.799719071 +0300
@@ -0,0 +1,55 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RAW_H
+#define __LINUX_SPINLOCK_TYPES_RAW_H
+
+#include <linux/types.h>
+
+#if defined(CONFIG_SMP)
+# include <asm/spinlock_types.h>
+#else
+# include <linux/spinlock_types_up.h>
+#endif
+
+#include <linux/lockdep.h>
+
+typedef struct raw_spinlock {
+	arch_spinlock_t raw_lock;
+#ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned int magic, owner_cpu;
+	void *owner;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} raw_spinlock_t;
+
+#define SPINLOCK_MAGIC		0xdead4ead
+
+#define SPINLOCK_OWNER_INIT	((void *)-1L)
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define SPIN_DEP_MAP_INIT(lockname)
+#endif
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define SPIN_DEBUG_INIT(lockname)		\
+	.magic = SPINLOCK_MAGIC,		\
+	.owner_cpu = -1,			\
+	.owner = SPINLOCK_OWNER_INIT,
+#else
+# define SPIN_DEBUG_INIT(lockname)
+#endif
+
+#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
+	{					\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	SPIN_DEP_MAP_INIT(lockname) }
+
+#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
+	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff -Nur linux-5.4.5/include/linux/spinlock_types_rt.h linux-5.4.5-new/include/linux/spinlock_types_rt.h
--- linux-5.4.5/include/linux/spinlock_types_rt.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_types_rt.h	2020-06-15 16:12:24.799719071 +0300
@@ -0,0 +1,48 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RT_H
+#define __LINUX_SPINLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#include <linux/cache.h>
+
+/*
+ * PREEMPT_RT: spinlocks - an RT mutex plus lock-break field:
+ */
+typedef struct spinlock {
+	struct rt_mutex		lock;
+	unsigned int		break_lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+} spinlock_t;
+
+#ifdef CONFIG_DEBUG_RT_MUTEXES
+# define __RT_SPIN_INITIALIZER(name) \
+	{ \
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock), \
+	.save_state = 1, \
+	.file = __FILE__, \
+	.line = __LINE__ , \
+	}
+#else
+# define __RT_SPIN_INITIALIZER(name) \
+	{								\
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),		\
+	.save_state = 1, \
+	}
+#endif
+
+/*
+.wait_list = PLIST_HEAD_INIT_RAW((name).lock.wait_list, (name).lock.wait_lock)
+*/
+
+#define __SPIN_LOCK_UNLOCKED(name)			\
+	{ .lock = __RT_SPIN_INITIALIZER(name.lock),		\
+	  SPIN_DEP_MAP_INIT(name) }
+
+#define DEFINE_SPINLOCK(name) \
+	spinlock_t name = __SPIN_LOCK_UNLOCKED(name)
+
+#endif
diff -Nur linux-5.4.5/include/linux/spinlock_types_up.h linux-5.4.5-new/include/linux/spinlock_types_up.h
--- linux-5.4.5/include/linux/spinlock_types_up.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/spinlock_types_up.h	2020-06-15 16:12:24.799719071 +0300
@@ -1,10 +1,6 @@
 #ifndef __LINUX_SPINLOCK_TYPES_UP_H
 #define __LINUX_SPINLOCK_TYPES_UP_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 /*
  * include/linux/spinlock_types_up.h - spinlock type definitions for UP
  *
diff -Nur linux-5.4.5/include/linux/stop_machine.h linux-5.4.5-new/include/linux/stop_machine.h
--- linux-5.4.5/include/linux/stop_machine.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/stop_machine.h	2020-06-15 16:12:24.799719071 +0300
@@ -26,6 +26,8 @@
 	cpu_stop_fn_t		fn;
 	void			*arg;
 	struct cpu_stop_done	*done;
+	/* Did not run due to disabled stopper; for nowait debug checks */
+	bool			disabled;
 };
 
 int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg);
diff -Nur linux-5.4.5/include/linux/suspend.h linux-5.4.5-new/include/linux/suspend.h
--- linux-5.4.5/include/linux/suspend.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/suspend.h	2020-06-15 16:12:24.799719071 +0300
@@ -197,6 +197,12 @@
 	void (*end)(void);
 };
 
+#if defined(CONFIG_SUSPEND) || defined(CONFIG_HIBERNATION)
+extern bool pm_in_action;
+#else
+# define pm_in_action false
+#endif
+
 #ifdef CONFIG_SUSPEND
 extern suspend_state_t mem_sleep_current;
 extern suspend_state_t mem_sleep_default;
diff -Nur linux-5.4.5/include/linux/swait.h linux-5.4.5-new/include/linux/swait.h
--- linux-5.4.5/include/linux/swait.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/swait.h	2020-06-15 16:12:24.803719056 +0300
@@ -160,7 +160,9 @@
 extern void swake_up_one(struct swait_queue_head *q);
 extern void swake_up_all(struct swait_queue_head *q);
 extern void swake_up_locked(struct swait_queue_head *q);
+extern void swake_up_all_locked(struct swait_queue_head *q);
 
+extern void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);
 extern void prepare_to_swait_exclusive(struct swait_queue_head *q, struct swait_queue *wait, int state);
 extern long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait, int state);
 
@@ -297,4 +299,18 @@
 	__ret;								\
 })
 
+#define __swait_event_lock_irq(wq, condition, lock, cmd)		\
+	___swait_event(wq, condition, TASK_UNINTERRUPTIBLE, 0,		\
+		       raw_spin_unlock_irq(&lock);			\
+		       cmd;						\
+		       schedule();					\
+		       raw_spin_lock_irq(&lock))
+
+#define swait_event_lock_irq(wq_head, condition, lock)			\
+	do {								\
+		if (condition)						\
+			break;						\
+		__swait_event_lock_irq(wq_head, condition, lock, );	\
+	} while (0)
+
 #endif /* _LINUX_SWAIT_H */
diff -Nur linux-5.4.5/include/linux/thread_info.h linux-5.4.5-new/include/linux/thread_info.h
--- linux-5.4.5/include/linux/thread_info.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/thread_info.h	2020-06-15 16:12:24.803719056 +0300
@@ -97,7 +97,17 @@
 #define test_thread_flag(flag) \
 	test_ti_thread_flag(current_thread_info(), flag)
 
-#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)
+#ifdef CONFIG_PREEMPT_LAZY
+#define tif_need_resched()	(test_thread_flag(TIF_NEED_RESCHED) || \
+				 test_thread_flag(TIF_NEED_RESCHED_LAZY))
+#define tif_need_resched_now()	(test_thread_flag(TIF_NEED_RESCHED))
+#define tif_need_resched_lazy()	test_thread_flag(TIF_NEED_RESCHED_LAZY))
+
+#else
+#define tif_need_resched()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_now()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_lazy()	0
+#endif
 
 #ifndef CONFIG_HAVE_ARCH_WITHIN_STACK_FRAMES
 static inline int arch_within_stack_frames(const void * const stack,
diff -Nur linux-5.4.5/include/linux/trace_events.h linux-5.4.5-new/include/linux/trace_events.h
--- linux-5.4.5/include/linux/trace_events.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/trace_events.h	2020-06-15 16:12:24.803719056 +0300
@@ -62,6 +62,9 @@
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;
+	unsigned short		migrate_disable;
+	unsigned short		padding;
+	unsigned char		preempt_lazy_count;
 };
 
 #define TRACE_EVENT_TYPE_MAX						\
diff -Nur linux-5.4.5/include/linux/uaccess.h linux-5.4.5-new/include/linux/uaccess.h
--- linux-5.4.5/include/linux/uaccess.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/uaccess.h	2020-06-15 16:12:24.803719056 +0300
@@ -182,6 +182,7 @@
  */
 static inline void pagefault_disable(void)
 {
+	migrate_disable();
 	pagefault_disabled_inc();
 	/*
 	 * make sure to have issued the store before a pagefault
@@ -198,6 +199,7 @@
 	 */
 	barrier();
 	pagefault_disabled_dec();
+	migrate_enable();
 }
 
 /*
diff -Nur linux-5.4.5/include/linux/vmstat.h linux-5.4.5-new/include/linux/vmstat.h
--- linux-5.4.5/include/linux/vmstat.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/vmstat.h	2020-06-15 16:12:24.803719056 +0300
@@ -54,7 +54,9 @@
  */
 static inline void __count_vm_event(enum vm_event_item item)
 {
+	preempt_disable_rt();
 	raw_cpu_inc(vm_event_states.event[item]);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_event(enum vm_event_item item)
@@ -64,7 +66,9 @@
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
+	preempt_disable_rt();
 	raw_cpu_add(vm_event_states.event[item], delta);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
diff -Nur linux-5.4.5/include/linux/wait.h linux-5.4.5-new/include/linux/wait.h
--- linux-5.4.5/include/linux/wait.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/linux/wait.h	2020-06-15 16:12:24.803719056 +0300
@@ -10,6 +10,7 @@
 
 #include <asm/current.h>
 #include <uapi/linux/wait.h>
+#include <linux/atomic.h>
 
 typedef struct wait_queue_entry wait_queue_entry_t;
 
diff -Nur linux-5.4.5/include/net/gen_stats.h linux-5.4.5-new/include/net/gen_stats.h
--- linux-5.4.5/include/net/gen_stats.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/net/gen_stats.h	2020-06-15 16:12:25.175717748 +0300
@@ -6,6 +6,7 @@
 #include <linux/socket.h>
 #include <linux/rtnetlink.h>
 #include <linux/pkt_sched.h>
+#include <net/net_seq_lock.h>
 
 struct gnet_stats_basic_cpu {
 	struct gnet_stats_basic_packed bstats;
@@ -36,15 +37,15 @@
 				 spinlock_t *lock, struct gnet_dump *d,
 				 int padattr);
 
-int gnet_stats_copy_basic(const seqcount_t *running,
+int gnet_stats_copy_basic(net_seqlock_t *running,
 			  struct gnet_dump *d,
 			  struct gnet_stats_basic_cpu __percpu *cpu,
 			  struct gnet_stats_basic_packed *b);
-void __gnet_stats_copy_basic(const seqcount_t *running,
+void __gnet_stats_copy_basic(net_seqlock_t *running,
 			     struct gnet_stats_basic_packed *bstats,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
-int gnet_stats_copy_basic_hw(const seqcount_t *running,
+int gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			     struct gnet_dump *d,
 			     struct gnet_stats_basic_cpu __percpu *cpu,
 			     struct gnet_stats_basic_packed *b);
@@ -64,13 +65,13 @@
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running, struct nlattr *opt);
+		      net_seqlock_t *running, struct nlattr *opt);
 void gen_kill_estimator(struct net_rate_estimator __rcu **ptr);
 int gen_replace_estimator(struct gnet_stats_basic_packed *bstats,
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **ptr,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt);
+			  net_seqlock_t *running, struct nlattr *opt);
 bool gen_estimator_active(struct net_rate_estimator __rcu **ptr);
 bool gen_estimator_read(struct net_rate_estimator __rcu **ptr,
 			struct gnet_stats_rate_est64 *sample);
diff -Nur linux-5.4.5/include/net/neighbour.h linux-5.4.5-new/include/net/neighbour.h
--- linux-5.4.5/include/net/neighbour.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/net/neighbour.h	2020-06-15 16:12:25.175717748 +0300
@@ -460,7 +460,7 @@
 }
 #endif
 
-static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)
+static inline int neigh_hh_output(struct hh_cache *hh, struct sk_buff *skb)
 {
 	unsigned int hh_alen = 0;
 	unsigned int seq;
@@ -503,7 +503,7 @@
 static inline int neigh_output(struct neighbour *n, struct sk_buff *skb,
 			       bool skip_cache)
 {
-	const struct hh_cache *hh = &n->hh;
+	struct hh_cache *hh = &n->hh;
 
 	if ((n->nud_state & NUD_CONNECTED) && hh->hh_len && !skip_cache)
 		return neigh_hh_output(hh, skb);
@@ -544,7 +544,7 @@
 
 #define NEIGH_CB(skb)	((struct neighbour_cb *)(skb)->cb)
 
-static inline void neigh_ha_snapshot(char *dst, const struct neighbour *n,
+static inline void neigh_ha_snapshot(char *dst, struct neighbour *n,
 				     const struct net_device *dev)
 {
 	unsigned int seq;
diff -Nur linux-5.4.5/include/net/net_seq_lock.h linux-5.4.5-new/include/net/net_seq_lock.h
--- linux-5.4.5/include/net/net_seq_lock.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/net/net_seq_lock.h	2020-06-15 16:12:25.179717734 +0300
@@ -0,0 +1,15 @@
+#ifndef __NET_NET_SEQ_LOCK_H__
+#define __NET_NET_SEQ_LOCK_H__
+
+#ifdef CONFIG_PREEMPT_RT
+# define net_seqlock_t			seqlock_t
+# define net_seq_begin(__r)		read_seqbegin(__r)
+# define net_seq_retry(__r, __s)	read_seqretry(__r, __s)
+
+#else
+# define net_seqlock_t			seqcount_t
+# define net_seq_begin(__r)		read_seqcount_begin(__r)
+# define net_seq_retry(__r, __s)	read_seqcount_retry(__r, __s)
+#endif
+
+#endif
diff -Nur linux-5.4.5/include/net/sch_generic.h linux-5.4.5-new/include/net/sch_generic.h
--- linux-5.4.5/include/net/sch_generic.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/net/sch_generic.h	2020-06-15 16:12:25.179717734 +0300
@@ -10,6 +10,7 @@
 #include <linux/percpu.h>
 #include <linux/dynamic_queue_limits.h>
 #include <linux/list.h>
+#include <net/net_seq_lock.h>
 #include <linux/refcount.h>
 #include <linux/workqueue.h>
 #include <linux/mutex.h>
@@ -100,7 +101,7 @@
 	struct sk_buff_head	gso_skb ____cacheline_aligned_in_smp;
 	struct qdisc_skb_head	q;
 	struct gnet_stats_basic_packed bstats;
-	seqcount_t		running;
+	net_seqlock_t		running;
 	struct gnet_stats_queue	qstats;
 	unsigned long		state;
 	struct Qdisc            *next_sched;
@@ -138,7 +139,11 @@
 {
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		return spin_is_locked(&qdisc->seqlock);
+#ifdef CONFIG_PREEMPT_RT
+	return spin_is_locked(&qdisc->running.lock) ? true : false;
+#else
 	return (raw_read_seqcount(&qdisc->running) & 1) ? true : false;
+#endif
 }
 
 static inline bool qdisc_is_percpu_stats(const struct Qdisc *q)
@@ -162,17 +167,27 @@
 	} else if (qdisc_is_running(qdisc)) {
 		return false;
 	}
+#ifdef CONFIG_PREEMPT_RT
+	if (try_write_seqlock(&qdisc->running))
+		return true;
+	return false;
+#else
 	/* Variant of write_seqcount_begin() telling lockdep a trylock
 	 * was attempted.
 	 */
 	raw_write_seqcount_begin(&qdisc->running);
 	seqcount_acquire(&qdisc->running.dep_map, 0, 1, _RET_IP_);
 	return true;
+#endif
 }
 
 static inline void qdisc_run_end(struct Qdisc *qdisc)
 {
+#ifdef CONFIG_PREEMPT_RT
+	write_sequnlock(&qdisc->running);
+#else
 	write_seqcount_end(&qdisc->running);
+#endif
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		spin_unlock(&qdisc->seqlock);
 }
@@ -535,7 +550,7 @@
 	return qdisc_lock(root);
 }
 
-static inline seqcount_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
+static inline net_seqlock_t *qdisc_root_sleeping_running(const struct Qdisc *qdisc)
 {
 	struct Qdisc *root = qdisc_root_sleeping(qdisc);
 
diff -Nur linux-5.4.5/include/rtdm/uapi/net.h linux-5.4.5-new/include/rtdm/uapi/net.h
--- linux-5.4.5/include/rtdm/uapi/net.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/rtdm/uapi/net.h	2020-06-15 16:12:25.547716438 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2005-2011 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *  As a special exception to the GNU General Public license, the RTnet
+ *  project allows you to use this header file in unmodified form to produce
+ *  application programs executing in user-space which use RTnet services by
+ *  normal system calls. The resulting executable will not be covered by the
+ *  GNU General Public License merely as a result of this header file use.
+ *  Instead, this header file use will be considered normal use of RTnet and
+ *  not a "derived work" in the sense of the GNU General Public License.
+ *
+ *  This exception does not apply when the application code is built as a
+ *  static or dynamically loadable portion of the Linux kernel nor does the
+ *  exception override other reasons justifying application of the GNU General
+ *  Public License.
+ *
+ *  This exception applies only to the code released by the RTnet project
+ *  under the name RTnet and bearing this exception notice. If you copy code
+ *  from other sources into a copy of RTnet, the exception does not apply to
+ *  the code that you add in this way.
+ *
+ */
+
+#include <rtdm/uapi/rtdm.h>
+
+#ifndef _RTDM_UAPI_NET_H
+#define _RTDM_UAPI_NET_H
+
+/* sub-classes: RTDM_CLASS_NETWORK */
+#define RTDM_SUBCLASS_RTNET     0
+
+#define RTIOC_TYPE_NETWORK      RTDM_CLASS_NETWORK
+
+/* RTnet-specific IOCTLs */
+#define RTNET_RTIOC_XMITPARAMS  _IOW(RTIOC_TYPE_NETWORK, 0x10, unsigned int)
+#define RTNET_RTIOC_PRIORITY    RTNET_RTIOC_XMITPARAMS  /* legacy */
+#define RTNET_RTIOC_TIMEOUT     _IOW(RTIOC_TYPE_NETWORK, 0x11, int64_t)
+/* RTNET_RTIOC_CALLBACK         _IOW(RTIOC_TYPE_NETWORK, 0x12, ...
+ * IOCTL only usable inside the kernel. */
+/* RTNET_RTIOC_NONBLOCK         _IOW(RTIOC_TYPE_NETWORK, 0x13, unsigned int)
+ * This IOCTL is no longer supported (and it was buggy anyway).
+ * Use RTNET_RTIOC_TIMEOUT with any negative timeout value instead. */
+#define RTNET_RTIOC_EXTPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x14, unsigned int)
+#define RTNET_RTIOC_SHRPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x15, unsigned int)
+
+/* socket transmission priorities */
+#define SOCK_MAX_PRIO           0
+#define SOCK_DEF_PRIO           SOCK_MAX_PRIO + \
+				    (SOCK_MIN_PRIO-SOCK_MAX_PRIO+1)/2
+#define SOCK_MIN_PRIO           SOCK_NRT_PRIO - 1
+#define SOCK_NRT_PRIO           31
+
+/* socket transmission channels */
+#define SOCK_DEF_RT_CHANNEL     0           /* default rt xmit channel     */
+#define SOCK_DEF_NRT_CHANNEL    1           /* default non-rt xmit channel */
+#define SOCK_USER_CHANNEL       2           /* first user-defined channel  */
+
+/* argument construction for RTNET_RTIOC_XMITPARAMS */
+#define SOCK_XMIT_PARAMS(priority, channel) ((priority) | ((channel) << 16))
+
+#endif  /* !_RTDM_UAPI_NET_H */
diff -Nur linux-5.4.5/include/rtdm/uapi/rtdm.h linux-5.4.5-new/include/rtdm/uapi/rtdm.h
--- linux-5.4.5/include/rtdm/uapi/rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/rtdm/uapi/rtdm.h	2020-06-15 16:12:25.547716438 +0300
@@ -0,0 +1,202 @@
+/**
+ * @file
+ * Real-Time Driver Model for Xenomai, user API header.
+ *
+ * @note Copyright (C) 2005, 2006 Jan Kiszka <jan.kiszka@web.de>
+ * @note Copyright (C) 2005 Joerg Langenberg <joerg.langenberg@gmx.net>
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
+ * @ingroup rtdm_user_api
+ */
+#ifndef _RTDM_UAPI_RTDM_H
+#define _RTDM_UAPI_RTDM_H
+
+/*!
+ * @addtogroup rtdm
+ * @{
+ */
+
+/*!
+ * @anchor rtdm_api_versioning @name API Versioning
+ * @{ */
+/** Common user and driver API version */
+#define RTDM_API_VER			9
+
+/** Minimum API revision compatible with the current release */
+#define RTDM_API_MIN_COMPAT_VER		9
+/** @} API Versioning */
+
+/** RTDM type for representing absolute dates. Its base type is a 64 bit
+ *  unsigned integer. The unit is 1 nanosecond. */
+typedef uint64_t nanosecs_abs_t;
+
+/** RTDM type for representing relative intervals. Its base type is a 64 bit
+ *  signed integer. The unit is 1 nanosecond. Relative intervals can also
+ *  encode the special timeouts "infinite" and "non-blocking", see
+ *  @ref RTDM_TIMEOUT_xxx. */
+typedef int64_t nanosecs_rel_t;
+
+/*!
+ * @anchor RTDM_TIMEOUT_xxx @name RTDM_TIMEOUT_xxx
+ * Special timeout values
+ * @{ */
+/** Block forever. */
+#define RTDM_TIMEOUT_INFINITE		0
+
+/** Any negative timeout means non-blocking. */
+#define RTDM_TIMEOUT_NONE		(-1)
+/** @} RTDM_TIMEOUT_xxx */
+/** @} rtdm */
+
+/*!
+ * @addtogroup rtdm_profiles
+ * @{
+ */
+
+/*!
+ * @anchor RTDM_CLASS_xxx   @name RTDM_CLASS_xxx
+ * Device classes
+ * @{ */
+#define RTDM_CLASS_PARPORT		1
+#define RTDM_CLASS_SERIAL		2
+#define RTDM_CLASS_CAN			3
+#define RTDM_CLASS_NETWORK		4
+#define RTDM_CLASS_RTMAC		5
+#define RTDM_CLASS_TESTING		6
+#define RTDM_CLASS_RTIPC		7
+#define RTDM_CLASS_COBALT		8
+#define RTDM_CLASS_UDD			9
+#define RTDM_CLASS_MEMORY		10
+#define RTDM_CLASS_GPIO			11
+#define RTDM_CLASS_SPI			12
+
+#define RTDM_CLASS_MISC			223
+#define RTDM_CLASS_EXPERIMENTAL		224
+#define RTDM_CLASS_MAX			255
+/** @} RTDM_CLASS_xxx */
+
+#define RTDM_SUBCLASS_GENERIC		(-1)
+
+#define RTIOC_TYPE_COMMON		0
+
+/*!
+ * @anchor device_naming    @name Device Naming
+ * Maximum length of device names (excluding the final null character)
+ * @{
+ */
+#define RTDM_MAX_DEVNAME_LEN		31
+/** @} Device Naming */
+
+/**
+ * Device information
+ */
+typedef struct rtdm_device_info {
+	/** Device flags, see @ref dev_flags "Device Flags" for details */
+	int device_flags;
+
+	/** Device class ID, see @ref RTDM_CLASS_xxx */
+	int device_class;
+
+	/** Device sub-class, either RTDM_SUBCLASS_GENERIC or a
+	 *  RTDM_SUBCLASS_xxx definition of the related @ref rtdm_profiles
+	 *  "Device Profile" */
+	int device_sub_class;
+
+	/** Supported device profile version */
+	int profile_version;
+} rtdm_device_info_t;
+
+/*!
+ * @anchor RTDM_PURGE_xxx_BUFFER    @name RTDM_PURGE_xxx_BUFFER
+ * Flags selecting buffers to be purged
+ * @{ */
+#define RTDM_PURGE_RX_BUFFER		0x0001
+#define RTDM_PURGE_TX_BUFFER		0x0002
+/** @} RTDM_PURGE_xxx_BUFFER*/
+
+/*!
+ * @anchor common_IOCTLs    @name Common IOCTLs
+ * The following IOCTLs are common to all device rtdm_profiles.
+ * @{
+ */
+
+/**
+ * Retrieve information about a device or socket.
+ * @param[out] arg Pointer to information buffer (struct rtdm_device_info)
+ */
+#define RTIOC_DEVICE_INFO \
+	_IOR(RTIOC_TYPE_COMMON, 0x00, struct rtdm_device_info)
+
+/**
+ * Purge internal device or socket buffers.
+ * @param[in] arg Purge mask, see @ref RTDM_PURGE_xxx_BUFFER
+ */
+#define RTIOC_PURGE		_IOW(RTIOC_TYPE_COMMON, 0x10, int)
+/** @} Common IOCTLs */
+/** @} rtdm */
+
+/* Internally used for mapping socket functions on IOCTLs */
+struct _rtdm_getsockopt_args {
+	int level;
+	int optname;
+	void *optval;
+	socklen_t *optlen;
+};
+
+struct _rtdm_setsockopt_args {
+	int level;
+	int optname;
+	const void *optval;
+	socklen_t optlen;
+};
+
+struct _rtdm_getsockaddr_args {
+	struct sockaddr *addr;
+	socklen_t *addrlen;
+};
+
+struct _rtdm_setsockaddr_args {
+	const struct sockaddr *addr;
+	socklen_t addrlen;
+};
+
+#define _RTIOC_GETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x20,		\
+				     struct _rtdm_getsockopt_args)
+#define _RTIOC_SETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x21,		\
+				     struct _rtdm_setsockopt_args)
+#define _RTIOC_BIND		_IOW(RTIOC_TYPE_COMMON, 0x22,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_CONNECT		_IOW(RTIOC_TYPE_COMMON, 0x23,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_LISTEN		_IOW(RTIOC_TYPE_COMMON, 0x24,		\
+				     int)
+#define _RTIOC_ACCEPT		_IOW(RTIOC_TYPE_COMMON, 0x25,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETSOCKNAME	_IOW(RTIOC_TYPE_COMMON, 0x26,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETPEERNAME	_IOW(RTIOC_TYPE_COMMON, 0x27,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_SHUTDOWN		_IOW(RTIOC_TYPE_COMMON, 0x28,		\
+				     int)
+
+/* Internally used for mmap() */
+struct _rtdm_mmap_request {
+	__u64 offset;
+	size_t length;
+	int prot;
+	int flags;
+};
+
+#endif /* !_RTDM_UAPI_RTDM_H */
diff -Nur linux-5.4.5/include/uapi/rtdm.h linux-5.4.5-new/include/uapi/rtdm.h
--- linux-5.4.5/include/uapi/rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/include/uapi/rtdm.h	2020-06-15 16:12:25.311717269 +0300
@@ -0,0 +1,202 @@
+/**
+ * @file
+ * Real-Time Driver Model for Xenomai, user API header.
+ *
+ * @note Copyright (C) 2005, 2006 Jan Kiszka <jan.kiszka@web.de>
+ * @note Copyright (C) 2005 Joerg Langenberg <joerg.langenberg@gmx.net>
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
+ * @ingroup rtdm_user_api
+ */
+#ifndef _RTDM_UAPI_RTDM_H
+#define _RTDM_UAPI_RTDM_H
+
+/*!
+ * @addtogroup rtdm
+ * @{
+ */
+
+/*!
+ * @anchor rtdm_api_versioning @name API Versioning
+ * @{ */
+/** Common user and driver API version */
+#define RTDM_API_VER			9
+
+/** Minimum API revision compatible with the current release */
+#define RTDM_API_MIN_COMPAT_VER		9
+/** @} API Versioning */
+
+/** RTDM type for representing absolute dates. Its base type is a 64 bit
+ *  unsigned integer. The unit is 1 nanosecond. */
+typedef uint64_t nanosecs_abs_t;
+
+/** RTDM type for representing relative intervals. Its base type is a 64 bit
+ *  signed integer. The unit is 1 nanosecond. Relative intervals can also
+ *  encode the special timeouts "infinite" and "non-blocking", see
+ *  @ref RTDM_TIMEOUT_xxx. */
+typedef int64_t nanosecs_rel_t;
+
+/*!
+ * @anchor RTDM_TIMEOUT_xxx @name RTDM_TIMEOUT_xxx
+ * Special timeout values
+ * @{ */
+/** Block forever. */
+#define RTDM_TIMEOUT_INFINITE		0
+
+/** Any negative timeout means non-blocking. */
+#define RTDM_TIMEOUT_NONE		(-1)
+/** @} RTDM_TIMEOUT_xxx */
+/** @} rtdm */
+
+/*!
+ * @addtogroup rtdm_profiles
+ * @{
+ */
+
+/*!
+ * @anchor RTDM_CLASS_xxx   @name RTDM_CLASS_xxx
+ * Device classes
+ * @{ */
+#define RTDM_CLASS_PARPORT		1
+#define RTDM_CLASS_SERIAL		2
+#define RTDM_CLASS_CAN			3
+#define RTDM_CLASS_NETWORK		4
+#define RTDM_CLASS_RTMAC		5
+#define RTDM_CLASS_TESTING		6
+#define RTDM_CLASS_RTIPC		7
+#define RTDM_CLASS_COBALT		8
+#define RTDM_CLASS_UDD			9
+#define RTDM_CLASS_MEMORY		10
+#define RTDM_CLASS_GPIO			11
+#define RTDM_CLASS_SPI			12
+
+#define RTDM_CLASS_MISC			223
+#define RTDM_CLASS_EXPERIMENTAL		224
+#define RTDM_CLASS_MAX			255
+/** @} RTDM_CLASS_xxx */
+
+#define RTDM_SUBCLASS_GENERIC		(-1)
+
+#define RTIOC_TYPE_COMMON		0
+
+/*!
+ * @anchor device_naming    @name Device Naming
+ * Maximum length of device names (excluding the final null character)
+ * @{
+ */
+#define RTDM_MAX_DEVNAME_LEN		31
+/** @} Device Naming */
+
+/**
+ * Device information
+ */
+typedef struct rtdm_device_info {
+	/** Device flags, see @ref dev_flags "Device Flags" for details */
+	int device_flags;
+
+	/** Device class ID, see @ref RTDM_CLASS_xxx */
+	int device_class;
+
+	/** Device sub-class, either RTDM_SUBCLASS_GENERIC or a
+	 *  RTDM_SUBCLASS_xxx definition of the related @ref rtdm_profiles
+	 *  "Device Profile" */
+	int device_sub_class;
+
+	/** Supported device profile version */
+	int profile_version;
+} rtdm_device_info_t;
+
+/*!
+ * @anchor RTDM_PURGE_xxx_BUFFER    @name RTDM_PURGE_xxx_BUFFER
+ * Flags selecting buffers to be purged
+ * @{ */
+#define RTDM_PURGE_RX_BUFFER		0x0001
+#define RTDM_PURGE_TX_BUFFER		0x0002
+/** @} RTDM_PURGE_xxx_BUFFER*/
+
+/*!
+ * @anchor common_IOCTLs    @name Common IOCTLs
+ * The following IOCTLs are common to all device rtdm_profiles.
+ * @{
+ */
+
+/**
+ * Retrieve information about a device or socket.
+ * @param[out] arg Pointer to information buffer (struct rtdm_device_info)
+ */
+#define RTIOC_DEVICE_INFO \
+	_IOR(RTIOC_TYPE_COMMON, 0x00, struct rtdm_device_info)
+
+/**
+ * Purge internal device or socket buffers.
+ * @param[in] arg Purge mask, see @ref RTDM_PURGE_xxx_BUFFER
+ */
+#define RTIOC_PURGE		_IOW(RTIOC_TYPE_COMMON, 0x10, int)
+/** @} Common IOCTLs */
+/** @} rtdm */
+
+/* Internally used for mapping socket functions on IOCTLs */
+struct _rtdm_getsockopt_args {
+	int level;
+	int optname;
+	void *optval;
+	socklen_t *optlen;
+};
+
+struct _rtdm_setsockopt_args {
+	int level;
+	int optname;
+	const void *optval;
+	socklen_t optlen;
+};
+
+struct _rtdm_getsockaddr_args {
+	struct sockaddr *addr;
+	socklen_t *addrlen;
+};
+
+struct _rtdm_setsockaddr_args {
+	const struct sockaddr *addr;
+	socklen_t addrlen;
+};
+
+#define _RTIOC_GETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x20,		\
+				     struct _rtdm_getsockopt_args)
+#define _RTIOC_SETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x21,		\
+				     struct _rtdm_setsockopt_args)
+#define _RTIOC_BIND		_IOW(RTIOC_TYPE_COMMON, 0x22,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_CONNECT		_IOW(RTIOC_TYPE_COMMON, 0x23,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_LISTEN		_IOW(RTIOC_TYPE_COMMON, 0x24,		\
+				     int)
+#define _RTIOC_ACCEPT		_IOW(RTIOC_TYPE_COMMON, 0x25,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETSOCKNAME	_IOW(RTIOC_TYPE_COMMON, 0x26,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETPEERNAME	_IOW(RTIOC_TYPE_COMMON, 0x27,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_SHUTDOWN		_IOW(RTIOC_TYPE_COMMON, 0x28,		\
+				     int)
+
+/* Internally used for mmap() */
+struct _rtdm_mmap_request {
+	__u64 offset;
+	size_t length;
+	int prot;
+	int flags;
+};
+
+#endif /* !_RTDM_UAPI_RTDM_H */
diff -Nur linux-5.4.5/include/xen/xen-ops.h linux-5.4.5-new/include/xen/xen-ops.h
--- linux-5.4.5/include/xen/xen-ops.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/include/xen/xen-ops.h	2020-06-15 16:12:25.543716452 +0300
@@ -215,7 +215,7 @@
 void xen_efi_runtime_setup(void);
 
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 
 static inline void xen_preemptible_hcall_begin(void)
 {
@@ -239,6 +239,6 @@
 	__this_cpu_write(xen_in_preemptible_hcall, false);
 }
 
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 #endif /* INCLUDE_XEN_OPS_H */
diff -Nur linux-5.4.5/init/init_task.c linux-5.4.5-new/init/init_task.c
--- linux-5.4.5/init/init_task.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/init/init_task.c	2020-06-15 16:12:25.563716382 +0300
@@ -73,6 +73,10 @@
 	.cpus_ptr	= &init_task.cpus_mask,
 	.cpus_mask	= CPU_MASK_ALL,
 	.nr_cpus_allowed= NR_CPUS,
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT) && \
+    defined(CONFIG_SCHED_DEBUG)
+	.pinned_on_cpu	= -1,
+#endif
 	.mm		= NULL,
 	.active_mm	= &init_mm,
 	.restart_block	= {
diff -Nur linux-5.4.5/init/Kconfig linux-5.4.5-new/init/Kconfig
--- linux-5.4.5/init/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/init/Kconfig	2020-06-15 16:12:25.563716382 +0300
@@ -923,6 +923,7 @@
 config RT_GROUP_SCHED
 	bool "Group scheduling for SCHED_RR/FIFO"
 	depends on CGROUP_SCHED
+	depends on !PREEMPT_RT
 	default n
 	help
 	  This feature lets you explicitly allocate real CPU bandwidth
@@ -1629,6 +1630,7 @@
 # syscall, maps, verifier
 config BPF_SYSCALL
 	bool "Enable bpf() system call"
+	depends on !PREEMPT_RT
 	select BPF
 	select IRQ_WORK
 	default n
@@ -1804,6 +1806,7 @@
 
 config SLAB
 	bool "SLAB"
+	depends on !PREEMPT_RT
 	select HAVE_HARDENED_USERCOPY_ALLOCATOR
 	help
 	  The regular slab allocator that is established and known to work
@@ -1824,6 +1827,7 @@
 config SLOB
 	depends on EXPERT
 	bool "SLOB (Simple Allocator)"
+	depends on !PREEMPT_RT
 	help
 	   SLOB replaces the stock allocator with a drastically simpler
 	   allocator. SLOB is generally more space efficient but
@@ -1889,7 +1893,7 @@
 
 config SLUB_CPU_PARTIAL
 	default y
-	depends on SLUB && SMP
+	depends on SLUB && SMP && !PREEMPT_RT
 	bool "SLUB per cpu partial cache"
 	help
 	  Per cpu partial caches accelerate objects allocation and freeing
diff -Nur linux-5.4.5/init/main.c linux-5.4.5-new/init/main.c
--- linux-5.4.5/init/main.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/init/main.c	2020-06-15 16:12:25.563716382 +0300
@@ -693,7 +693,6 @@
 	boot_init_stack_canary();
 
 	time_init();
-	printk_safe_init();
 	perf_event_init();
 	profile_init();
 	call_function_init();
diff -Nur linux-5.4.5/kernel/cgroup/cgroup.c linux-5.4.5-new/kernel/cgroup/cgroup.c
--- linux-5.4.5/kernel/cgroup/cgroup.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/cgroup/cgroup.c	2020-06-15 16:12:25.639716115 +0300
@@ -1957,7 +1957,6 @@
 	cgrp->dom_cgrp = cgrp;
 	cgrp->max_descendants = INT_MAX;
 	cgrp->max_depth = INT_MAX;
-	INIT_LIST_HEAD(&cgrp->rstat_css_list);
 	prev_cputime_init(&cgrp->prev_cputime);
 
 	for_each_subsys(ss, ssid)
@@ -5013,12 +5012,6 @@
 	list_del_rcu(&css->sibling);
 
 	if (ss) {
-		/* css release path */
-		if (!list_empty(&css->rstat_css_node)) {
-			cgroup_rstat_flush(cgrp);
-			list_del_rcu(&css->rstat_css_node);
-		}
-
 		cgroup_idr_replace(&ss->css_idr, NULL, css->id);
 		if (ss->css_released)
 			ss->css_released(css);
@@ -5080,7 +5073,6 @@
 	css->id = -1;
 	INIT_LIST_HEAD(&css->sibling);
 	INIT_LIST_HEAD(&css->children);
-	INIT_LIST_HEAD(&css->rstat_css_node);
 	css->serial_nr = css_serial_nr_next++;
 	atomic_set(&css->online_cnt, 0);
 
@@ -5089,9 +5081,6 @@
 		css_get(css->parent);
 	}
 
-	if (cgroup_on_dfl(cgrp) && ss->css_rstat_flush)
-		list_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);
-
 	BUG_ON(cgroup_css(cgrp, ss));
 }
 
@@ -5193,7 +5182,6 @@
 err_list_del:
 	list_del_rcu(&css->sibling);
 err_free_css:
-	list_del_rcu(&css->rstat_css_node);
 	INIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);
 	queue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);
 	return ERR_PTR(err);
diff -Nur linux-5.4.5/kernel/cgroup/cpuset.c linux-5.4.5-new/kernel/cgroup/cpuset.c
--- linux-5.4.5/kernel/cgroup/cpuset.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/cgroup/cpuset.c	2020-06-15 16:12:25.639716115 +0300
@@ -345,7 +345,7 @@
 	percpu_up_read(&cpuset_rwsem);
 }
 
-static DEFINE_SPINLOCK(callback_lock);
+static DEFINE_RAW_SPINLOCK(callback_lock);
 
 static struct workqueue_struct *cpuset_migrate_mm_wq;
 
@@ -1255,7 +1255,7 @@
 	 * Newly added CPUs will be removed from effective_cpus and
 	 * newly deleted ones will be added back to effective_cpus.
 	 */
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (adding) {
 		cpumask_or(parent->subparts_cpus,
 			   parent->subparts_cpus, tmp->addmask);
@@ -1274,7 +1274,7 @@
 	}
 
 	parent->nr_subparts_cpus = cpumask_weight(parent->subparts_cpus);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	return cmd == partcmd_update;
 }
@@ -1379,7 +1379,7 @@
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 
 		cpumask_copy(cp->effective_cpus, tmp->new_cpus);
 		if (cp->nr_subparts_cpus &&
@@ -1410,7 +1410,7 @@
 					= cpumask_weight(cp->subparts_cpus);
 			}
 		}
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));
@@ -1528,7 +1528,7 @@
 			return -EINVAL;
 	}
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);
 
 	/*
@@ -1539,7 +1539,7 @@
 			       cs->cpus_allowed);
 		cs->nr_subparts_cpus = cpumask_weight(cs->subparts_cpus);
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	update_cpumasks_hier(cs, &tmp);
 
@@ -1733,9 +1733,9 @@
 			continue;
 		rcu_read_unlock();
 
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		cp->effective_mems = *new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 
 		WARN_ON(!is_in_v2_mode() &&
 			!nodes_equal(cp->mems_allowed, cp->effective_mems));
@@ -1803,9 +1803,9 @@
 	if (retval < 0)
 		goto done;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = trialcs->mems_allowed;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/* use trialcs->mems_allowed as a temp variable */
 	update_nodemasks_hier(cs, &trialcs->mems_allowed);
@@ -1896,9 +1896,9 @@
 	spread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))
 			|| (is_spread_page(cs) != is_spread_page(trialcs)));
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->flags = trialcs->flags;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)
 		rebuild_sched_domains_locked();
@@ -2407,7 +2407,7 @@
 	cpuset_filetype_t type = seq_cft(sf)->private;
 	int ret = 0;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	switch (type) {
 	case FILE_CPULIST:
@@ -2429,7 +2429,7 @@
 		ret = -EINVAL;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	return ret;
 }
 
@@ -2742,14 +2742,14 @@
 
 	cpuset_inc();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	if (is_in_v2_mode()) {
 		cpumask_copy(cs->effective_cpus, parent->effective_cpus);
 		cs->effective_mems = parent->effective_mems;
 		cs->use_parent_ecpus = true;
 		parent->child_ecpus_count++;
 	}
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
 		goto out_unlock;
@@ -2776,12 +2776,12 @@
 	}
 	rcu_read_unlock();
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cs->mems_allowed = parent->mems_allowed;
 	cs->effective_mems = parent->mems_allowed;
 	cpumask_copy(cs->cpus_allowed, parent->cpus_allowed);
 	cpumask_copy(cs->effective_cpus, parent->cpus_allowed);
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 out_unlock:
 	percpu_up_write(&cpuset_rwsem);
 	put_online_cpus();
@@ -2837,7 +2837,7 @@
 static void cpuset_bind(struct cgroup_subsys_state *root_css)
 {
 	percpu_down_write(&cpuset_rwsem);
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 
 	if (is_in_v2_mode()) {
 		cpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);
@@ -2848,7 +2848,7 @@
 		top_cpuset.mems_allowed = top_cpuset.effective_mems;
 	}
 
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 	percpu_up_write(&cpuset_rwsem);
 }
 
@@ -2945,12 +2945,12 @@
 {
 	bool is_empty;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->cpus_allowed, new_cpus);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->mems_allowed = *new_mems;
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	/*
 	 * Don't call update_tasks_cpumask() if the cpuset becomes empty,
@@ -2987,10 +2987,10 @@
 	if (nodes_empty(*new_mems))
 		*new_mems = parent_cs(cs)->effective_mems;
 
-	spin_lock_irq(&callback_lock);
+	raw_spin_lock_irq(&callback_lock);
 	cpumask_copy(cs->effective_cpus, new_cpus);
 	cs->effective_mems = *new_mems;
-	spin_unlock_irq(&callback_lock);
+	raw_spin_unlock_irq(&callback_lock);
 
 	if (cpus_updated)
 		update_tasks_cpumask(cs);
@@ -3145,7 +3145,7 @@
 
 	/* synchronize cpus_allowed to cpu_active_mask */
 	if (cpus_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			cpumask_copy(top_cpuset.cpus_allowed, &new_cpus);
 		/*
@@ -3165,17 +3165,17 @@
 			}
 		}
 		cpumask_copy(top_cpuset.effective_cpus, &new_cpus);
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		/* we don't mess with cpumasks of tasks in top_cpuset */
 	}
 
 	/* synchronize mems_allowed to N_MEMORY */
 	if (mems_updated) {
-		spin_lock_irq(&callback_lock);
+		raw_spin_lock_irq(&callback_lock);
 		if (!on_dfl)
 			top_cpuset.mems_allowed = new_mems;
 		top_cpuset.effective_mems = new_mems;
-		spin_unlock_irq(&callback_lock);
+		raw_spin_unlock_irq(&callback_lock);
 		update_tasks_nodemask(&top_cpuset);
 	}
 
@@ -3276,11 +3276,11 @@
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_cpus(task_cs(tsk), pmask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 }
 
 /**
@@ -3341,11 +3341,11 @@
 	nodemask_t mask;
 	unsigned long flags;
 
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 	rcu_read_lock();
 	guarantee_online_mems(task_cs(tsk), &mask);
 	rcu_read_unlock();
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 
 	return mask;
 }
@@ -3437,14 +3437,14 @@
 		return true;
 
 	/* Not hardwall and node outside mems_allowed: scan up cpusets */
-	spin_lock_irqsave(&callback_lock, flags);
+	raw_spin_lock_irqsave(&callback_lock, flags);
 
 	rcu_read_lock();
 	cs = nearest_hardwall_ancestor(task_cs(current));
 	allowed = node_isset(node, cs->mems_allowed);
 	rcu_read_unlock();
 
-	spin_unlock_irqrestore(&callback_lock, flags);
+	raw_spin_unlock_irqrestore(&callback_lock, flags);
 	return allowed;
 }
 
diff -Nur linux-5.4.5/kernel/cgroup/rstat.c linux-5.4.5-new/kernel/cgroup/rstat.c
--- linux-5.4.5/kernel/cgroup/rstat.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/cgroup/rstat.c	2020-06-15 16:12:25.643716101 +0300
@@ -149,7 +149,7 @@
 }
 
 /* see cgroup_rstat_flush() */
-static void cgroup_rstat_flush_locked(struct cgroup *cgrp, bool may_sleep)
+static void cgroup_rstat_flush_locked(struct cgroup *cgrp)
 	__releases(&cgroup_rstat_lock) __acquires(&cgroup_rstat_lock)
 {
 	int cpu;
@@ -161,27 +161,17 @@
 						       cpu);
 		struct cgroup *pos = NULL;
 
-		raw_spin_lock(cpu_lock);
-		while ((pos = cgroup_rstat_cpu_pop_updated(pos, cgrp, cpu))) {
-			struct cgroup_subsys_state *css;
-
+		raw_spin_lock_irq(cpu_lock);
+		while ((pos = cgroup_rstat_cpu_pop_updated(pos, cgrp, cpu)))
 			cgroup_base_stat_flush(pos, cpu);
 
-			rcu_read_lock();
-			list_for_each_entry_rcu(css, &pos->rstat_css_list,
-						rstat_css_node)
-				css->ss->css_rstat_flush(css, cpu);
-			rcu_read_unlock();
-		}
-		raw_spin_unlock(cpu_lock);
+		raw_spin_unlock_irq(cpu_lock);
 
-		/* if @may_sleep, play nice and yield if necessary */
-		if (may_sleep && (need_resched() ||
-				  spin_needbreak(&cgroup_rstat_lock))) {
-			spin_unlock_irq(&cgroup_rstat_lock);
+		if (need_resched() || spin_needbreak(&cgroup_rstat_lock)) {
+			spin_unlock(&cgroup_rstat_lock);
 			if (!cond_resched())
 				cpu_relax();
-			spin_lock_irq(&cgroup_rstat_lock);
+			spin_lock(&cgroup_rstat_lock);
 		}
 	}
 }
@@ -203,24 +193,9 @@
 {
 	might_sleep();
 
-	spin_lock_irq(&cgroup_rstat_lock);
-	cgroup_rstat_flush_locked(cgrp, true);
-	spin_unlock_irq(&cgroup_rstat_lock);
-}
-
-/**
- * cgroup_rstat_flush_irqsafe - irqsafe version of cgroup_rstat_flush()
- * @cgrp: target cgroup
- *
- * This function can be called from any context.
- */
-void cgroup_rstat_flush_irqsafe(struct cgroup *cgrp)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&cgroup_rstat_lock, flags);
-	cgroup_rstat_flush_locked(cgrp, false);
-	spin_unlock_irqrestore(&cgroup_rstat_lock, flags);
+	spin_lock(&cgroup_rstat_lock);
+	cgroup_rstat_flush_locked(cgrp);
+	spin_unlock(&cgroup_rstat_lock);
 }
 
 /**
@@ -232,21 +207,21 @@
  *
  * This function may block.
  */
-void cgroup_rstat_flush_hold(struct cgroup *cgrp)
+static void cgroup_rstat_flush_hold(struct cgroup *cgrp)
 	__acquires(&cgroup_rstat_lock)
 {
 	might_sleep();
-	spin_lock_irq(&cgroup_rstat_lock);
-	cgroup_rstat_flush_locked(cgrp, true);
+	spin_lock(&cgroup_rstat_lock);
+	cgroup_rstat_flush_locked(cgrp);
 }
 
 /**
  * cgroup_rstat_flush_release - release cgroup_rstat_flush_hold()
  */
-void cgroup_rstat_flush_release(void)
+static void cgroup_rstat_flush_release(void)
 	__releases(&cgroup_rstat_lock)
 {
-	spin_unlock_irq(&cgroup_rstat_lock);
+	spin_unlock(&cgroup_rstat_lock);
 }
 
 int cgroup_rstat_init(struct cgroup *cgrp)
diff -Nur linux-5.4.5/kernel/cpu.c linux-5.4.5-new/kernel/cpu.c
--- linux-5.4.5/kernel/cpu.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/cpu.c	2020-06-15 16:12:26.135714369 +0300
@@ -849,6 +849,15 @@
 	int err, cpu = smp_processor_id();
 	int ret;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * If any tasks disabled migration before we got here,
+	 * go back and sleep again.
+	 */
+	if (cpu_nr_pinned(cpu))
+		return -EAGAIN;
+#endif
+
 	/* Ensure this CPU doesn't handle any more interrupts. */
 	err = __cpu_disable();
 	if (err < 0)
@@ -878,6 +887,10 @@
 	return 0;
 }
 
+#ifdef CONFIG_PREEMPT_RT
+struct task_struct *takedown_cpu_task;
+#endif
+
 static int takedown_cpu(unsigned int cpu)
 {
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
@@ -892,11 +905,39 @@
 	 */
 	irq_lock_sparse();
 
+#ifdef CONFIG_PREEMPT_RT
+	WARN_ON_ONCE(takedown_cpu_task);
+	takedown_cpu_task = current;
+
+again:
+	/*
+	 * If a task pins this CPU after we pass this check, take_cpu_down
+	 * will return -EAGAIN.
+	 */
+	for (;;) {
+		int nr_pinned;
+
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		nr_pinned = cpu_nr_pinned(cpu);
+		if (nr_pinned == 0)
+			break;
+		schedule();
+	}
+	set_current_state(TASK_RUNNING);
+#endif
+
 	/*
 	 * So now all preempt/rcu users must observe !cpu_active().
 	 */
 	err = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));
+#ifdef CONFIG_PREEMPT_RT
+	if (err == -EAGAIN)
+		goto again;
+#endif
 	if (err) {
+#ifdef CONFIG_PREEMPT_RT
+		takedown_cpu_task = NULL;
+#endif
 		/* CPU refused to die */
 		irq_unlock_sparse();
 		/* Unpark the hotplug thread so we can rollback there */
@@ -915,6 +956,9 @@
 	wait_for_ap_thread(st, false);
 	BUG_ON(st->state != CPUHP_AP_IDLE_DEAD);
 
+#ifdef CONFIG_PREEMPT_RT
+	takedown_cpu_task = NULL;
+#endif
 	/* Interrupts are moved away from the dying cpu, reenable alloc/free */
 	irq_unlock_sparse();
 
diff -Nur linux-5.4.5/kernel/events/core.c linux-5.4.5-new/kernel/events/core.c
--- linux-5.4.5/kernel/events/core.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/events/core.c	2020-06-15 16:12:25.683715959 +0300
@@ -10258,7 +10258,7 @@
 		goto unlock;
 	}
 
-	list_for_each_entry_rcu(pmu, &pmus, entry) {
+	list_for_each_entry_rcu(pmu, &pmus, entry, lockdep_is_held(&pmus_srcu)) {
 		ret = perf_try_init_event(pmu, event);
 		if (!ret)
 			goto unlock;
diff -Nur linux-5.4.5/kernel/exit.c linux-5.4.5-new/kernel/exit.c
--- linux-5.4.5/kernel/exit.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/exit.c	2020-06-15 16:12:26.135714369 +0300
@@ -161,7 +161,7 @@
 	 * Do this under ->siglock, we can race with another thread
 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
 	 */
-	flush_sigqueue(&tsk->pending);
+	flush_task_sigqueue(tsk);
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
diff -Nur linux-5.4.5/kernel/fork.c linux-5.4.5-new/kernel/fork.c
--- linux-5.4.5/kernel/fork.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/fork.c	2020-06-15 16:12:26.135714369 +0300
@@ -43,6 +43,7 @@
 #include <linux/hmm.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
+#include <linux/kprobes.h>
 #include <linux/vmacache.h>
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
@@ -289,7 +290,7 @@
 			return;
 		}
 
-		vfree_atomic(tsk->stack);
+		vfree(tsk->stack);
 		return;
 	}
 #endif
@@ -696,6 +697,19 @@
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * RCU callback for delayed mm drop. Not strictly rcu, but we don't
+ * want another facility to make this work.
+ */
+void __mmdrop_delayed(struct rcu_head *rhp)
+{
+	struct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);
+
+	__mmdrop(mm);
+}
+#endif
+
 static void mmdrop_async_fn(struct work_struct *work)
 {
 	struct mm_struct *mm;
@@ -737,6 +751,15 @@
 	WARN_ON(refcount_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	/*
+	 * Remove function-return probe instances associated with this
+	 * task and put them back on the free list.
+	 */
+	kprobe_flush_task(tsk);
+
+	/* Task is done with its stack. */
+	put_task_stack(tsk);
+
 	cgroup_free(tsk);
 	task_numa_free(tsk, true);
 	security_task_free(tsk);
@@ -927,6 +950,7 @@
 	tsk->splice_pipe = NULL;
 	tsk->task_frag.page = NULL;
 	tsk->wake_q.next = NULL;
+	tsk->wake_q_sleeper.next = NULL;
 
 	account_kernel_stack(tsk, 1);
 
@@ -1900,6 +1924,7 @@
 	spin_lock_init(&p->alloc_lock);
 
 	init_sigpending(&p->pending);
+	p->sigqueue_cache = NULL;
 
 	p->utime = p->stime = p->gtime = 0;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
diff -Nur linux-5.4.5/kernel/futex.c linux-5.4.5-new/kernel/futex.c
--- linux-5.4.5/kernel/futex.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/futex.c	2020-06-15 16:12:26.135714369 +0300
@@ -945,7 +945,9 @@
 		if (head->next != next) {
 			/* retain curr->pi_lock for the loop invariant */
 			raw_spin_unlock(&pi_state->pi_mutex.wait_lock);
+			raw_spin_unlock_irq(&curr->pi_lock);
 			spin_unlock(&hb->lock);
+			raw_spin_lock_irq(&curr->pi_lock);
 			put_pi_state(pi_state);
 			continue;
 		}
@@ -1554,6 +1556,7 @@
 	struct task_struct *new_owner;
 	bool postunlock = false;
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	int ret = 0;
 
 	new_owner = rt_mutex_next_owner(&pi_state->pi_mutex);
@@ -1613,13 +1616,13 @@
 	pi_state->owner = new_owner;
 	raw_spin_unlock(&new_owner->pi_lock);
 
-	postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);
-
+	postunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q,
+					     &wake_sleeper_q);
 out_unlock:
 	raw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 
 	return ret;
 }
@@ -2243,6 +2246,16 @@
 				requeue_pi_wake_futex(this, &key2, hb2);
 				drop_count++;
 				continue;
+			} else if (ret == -EAGAIN) {
+				/*
+				 * Waiter was woken by timeout or
+				 * signal and has set pi_blocked_on to
+				 * PI_WAKEUP_INPROGRESS before we
+				 * tried to enqueue it on the rtmutex.
+				 */
+				this->pi_state = NULL;
+				put_pi_state(pi_state);
+				continue;
 			} else if (ret) {
 				/*
 				 * rt_mutex_start_proxy_lock() detected a
@@ -2951,7 +2964,7 @@
 		goto no_block;
 	}
 
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	/*
 	 * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not
@@ -2967,6 +2980,14 @@
 	 * before __rt_mutex_start_proxy_lock() is done.
 	 */
 	raw_spin_lock_irq(&q.pi_state->pi_mutex.wait_lock);
+	/*
+	 * the migrate_disable() here disables migration in the in_atomic() fast
+	 * path which is enabled again in the following spin_unlock(). We have
+	 * one migrate_disable() pending in the slow-path which is reversed
+	 * after the raw_spin_unlock_irq() where we leave the atomic context.
+	 */
+	migrate_disable();
+
 	spin_unlock(q.lock_ptr);
 	/*
 	 * __rt_mutex_start_proxy_lock() unconditionally enqueues the @rt_waiter
@@ -2975,6 +2996,7 @@
 	 */
 	ret = __rt_mutex_start_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter, current);
 	raw_spin_unlock_irq(&q.pi_state->pi_mutex.wait_lock);
+	migrate_enable();
 
 	if (ret) {
 		if (ret == 1)
@@ -3123,10 +3145,19 @@
 		 * rt_waiter. Also see the WARN in wake_futex_pi().
 		 */
 		raw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);
+		/*
+		 * Magic trickery for now to make the RT migrate disable
+		 * logic happy. The following spin_unlock() happens with
+		 * interrupts disabled so the internal migrate_enable()
+		 * won't undo the migrate_disable() which was issued when
+		 * locking hb->lock.
+		 */
+		migrate_disable();
 		spin_unlock(&hb->lock);
 
 		/* drops pi_state->pi_mutex.wait_lock */
 		ret = wake_futex_pi(uaddr, uval, pi_state);
+		migrate_enable();
 
 		put_pi_state(pi_state);
 
@@ -3298,7 +3329,7 @@
 	struct hrtimer_sleeper timeout, *to;
 	struct futex_pi_state *pi_state = NULL;
 	struct rt_mutex_waiter rt_waiter;
-	struct futex_hash_bucket *hb;
+	struct futex_hash_bucket *hb, *hb2;
 	union futex_key key2 = FUTEX_KEY_INIT;
 	struct futex_q q = futex_q_init;
 	int res, ret;
@@ -3319,7 +3350,7 @@
 	 * The waiter is allocated on our stack, manipulated by the requeue
 	 * code while we sleep on uaddr.
 	 */
-	rt_mutex_init_waiter(&rt_waiter);
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);
 	if (unlikely(ret != 0))
@@ -3350,20 +3381,55 @@
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
 	futex_wait_queue_me(hb, &q, to);
 
-	spin_lock(&hb->lock);
-	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
-	spin_unlock(&hb->lock);
-	if (ret)
-		goto out_put_keys;
+	/*
+	 * On RT we must avoid races with requeue and trying to block
+	 * on two mutexes (hb->lock and uaddr2's rtmutex) by
+	 * serializing access to pi_blocked_on with pi_lock.
+	 */
+	raw_spin_lock_irq(&current->pi_lock);
+	if (current->pi_blocked_on) {
+		/*
+		 * We have been requeued or are in the process of
+		 * being requeued.
+		 */
+		raw_spin_unlock_irq(&current->pi_lock);
+	} else {
+		/*
+		 * Setting pi_blocked_on to PI_WAKEUP_INPROGRESS
+		 * prevents a concurrent requeue from moving us to the
+		 * uaddr2 rtmutex. After that we can safely acquire
+		 * (and possibly block on) hb->lock.
+		 */
+		current->pi_blocked_on = PI_WAKEUP_INPROGRESS;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		spin_lock(&hb->lock);
+
+		/*
+		 * Clean up pi_blocked_on. We might leak it otherwise
+		 * when we succeeded with the hb->lock in the fast
+		 * path.
+		 */
+		raw_spin_lock_irq(&current->pi_lock);
+		current->pi_blocked_on = NULL;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
+		spin_unlock(&hb->lock);
+		if (ret)
+			goto out_put_keys;
+	}
 
 	/*
-	 * In order for us to be here, we know our q.key == key2, and since
-	 * we took the hb->lock above, we also know that futex_requeue() has
-	 * completed and we no longer have to concern ourselves with a wakeup
-	 * race with the atomic proxy lock acquisition by the requeue code. The
-	 * futex_requeue dropped our key1 reference and incremented our key2
-	 * reference count.
+	 * In order to be here, we have either been requeued, are in
+	 * the process of being requeued, or requeue successfully
+	 * acquired uaddr2 on our behalf.  If pi_blocked_on was
+	 * non-null above, we may be racing with a requeue.  Do not
+	 * rely on q->lock_ptr to be hb2->lock until after blocking on
+	 * hb->lock or hb2->lock. The futex_requeue dropped our key1
+	 * reference and incremented our key2 reference count.
 	 */
+	hb2 = hash_futex(&key2);
 
 	/* Check if the requeue code acquired the second futex for us. */
 	if (!q.rt_waiter) {
@@ -3372,7 +3438,8 @@
 		 * did a lock-steal - fix up the PI-state in that case.
 		 */
 		if (q.pi_state && (q.pi_state->owner != current)) {
-			spin_lock(q.lock_ptr);
+			spin_lock(&hb2->lock);
+			BUG_ON(&hb2->lock != q.lock_ptr);
 			ret = fixup_pi_state_owner(uaddr2, &q, current);
 			if (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {
 				pi_state = q.pi_state;
@@ -3383,7 +3450,7 @@
 			 * the requeue_pi() code acquired for us.
 			 */
 			put_pi_state(q.pi_state);
-			spin_unlock(q.lock_ptr);
+			spin_unlock(&hb2->lock);
 		}
 	} else {
 		struct rt_mutex *pi_mutex;
@@ -3397,7 +3464,8 @@
 		pi_mutex = &q.pi_state->pi_mutex;
 		ret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);
 
-		spin_lock(q.lock_ptr);
+		spin_lock(&hb2->lock);
+		BUG_ON(&hb2->lock != q.lock_ptr);
 		if (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))
 			ret = 0;
 
diff -Nur linux-5.4.5/kernel/irq/handle.c linux-5.4.5-new/kernel/irq/handle.c
--- linux-5.4.5/kernel/irq/handle.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/irq/handle.c	2020-06-15 16:12:25.715715847 +0300
@@ -185,10 +185,16 @@
 {
 	irqreturn_t retval;
 	unsigned int flags = 0;
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 
 	retval = __handle_irq_event_percpu(desc, &flags);
 
-	add_interrupt_randomness(desc->irq_data.irq, flags);
+#ifdef CONFIG_PREEMPT_RT
+	desc->random_ip = ip;
+#else
+	add_interrupt_randomness(desc->irq_data.irq, flags, ip);
+#endif
 
 	if (!noirqdebug)
 		note_interrupt(desc, retval);
diff -Nur linux-5.4.5/kernel/irq/manage.c linux-5.4.5-new/kernel/irq/manage.c
--- linux-5.4.5/kernel/irq/manage.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/irq/manage.c	2020-06-15 16:12:25.715715847 +0300
@@ -1099,6 +1099,12 @@
 		if (action_ret == IRQ_WAKE_THREAD)
 			irq_wake_secondary(desc, action);
 
+#ifdef CONFIG_PREEMPT_RT
+		migrate_disable();
+		add_interrupt_randomness(action->irq, 0,
+				 desc->random_ip ^ (unsigned long) action);
+		migrate_enable();
+#endif
 		wake_threads_waitq(desc);
 	}
 
@@ -2681,7 +2687,7 @@
  *	This call sets the internal irqchip state of an interrupt,
  *	depending on the value of @which.
  *
- *	This function should be called with preemption disabled if the
+ *	This function should be called with migration disabled if the
  *	interrupt controller has per-cpu registers.
  */
 int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
diff -Nur linux-5.4.5/kernel/irq/spurious.c linux-5.4.5-new/kernel/irq/spurious.c
--- linux-5.4.5/kernel/irq/spurious.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/irq/spurious.c	2020-06-15 16:12:25.719715833 +0300
@@ -442,6 +442,10 @@
 
 static int __init irqfixup_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqfixup boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 1;
 	printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
 	printk(KERN_WARNING "This may impact system performance.\n");
@@ -454,6 +458,10 @@
 
 static int __init irqpoll_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT
+	pr_warn("irqpoll boot option not supported w/ CONFIG_PREEMPT_RT\n");
+	return 1;
+#endif
 	irqfixup = 2;
 	printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
 				"enabled\n");
diff -Nur linux-5.4.5/kernel/irq_work.c linux-5.4.5-new/kernel/irq_work.c
--- linux-5.4.5/kernel/irq_work.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/irq_work.c	2020-06-15 16:12:26.135714369 +0300
@@ -18,6 +18,7 @@
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/interrupt.h>
 #include <asm/processor.h>
 
 
@@ -60,13 +61,19 @@
 /* Enqueue on current CPU, work must already be claimed and preempt disabled */
 static void __irq_work_queue_local(struct irq_work *work)
 {
+	struct llist_head *list;
+	bool lazy_work, realtime = IS_ENABLED(CONFIG_PREEMPT_RT);
+
+	lazy_work = work->flags & IRQ_WORK_LAZY;
+
 	/* If the work is "lazy", handle it from next tick if any */
-	if (work->flags & IRQ_WORK_LAZY) {
-		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
-		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
-	} else {
-		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
+	if (lazy_work || (realtime && !(work->flags & IRQ_WORK_HARD_IRQ)))
+		list = this_cpu_ptr(&lazy_list);
+	else
+		list = this_cpu_ptr(&raised_list);
+
+	if (llist_add(&work->llnode, list)) {
+		if (!lazy_work || tick_nohz_tick_stopped())
 			arch_irq_work_raise();
 	}
 }
@@ -108,9 +115,16 @@
 
 	preempt_disable();
 	if (cpu != smp_processor_id()) {
+		struct llist_head *list;
+
 		/* Arch remote IPI send/receive backend aren't NMI safe */
 		WARN_ON_ONCE(in_nmi());
-		if (llist_add(&work->llnode, &per_cpu(raised_list, cpu)))
+		if (IS_ENABLED(CONFIG_PREEMPT_RT) && !(work->flags & IRQ_WORK_HARD_IRQ))
+			list = &per_cpu(lazy_list, cpu);
+		else
+			list = &per_cpu(raised_list, cpu);
+
+		if (llist_add(&work->llnode, list))
 			arch_send_call_function_single_ipi(cpu);
 	} else {
 		__irq_work_queue_local(work);
@@ -129,9 +143,8 @@
 	raised = this_cpu_ptr(&raised_list);
 	lazy = this_cpu_ptr(&lazy_list);
 
-	if (llist_empty(raised) || arch_irq_work_has_interrupt())
-		if (llist_empty(lazy))
-			return false;
+	if (llist_empty(raised) && llist_empty(lazy))
+		return false;
 
 	/* All work should have been flushed before going offline */
 	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
@@ -145,8 +158,12 @@
 	struct llist_node *llnode;
 	unsigned long flags;
 
+#ifndef CONFIG_PREEMPT_RT
+	/*
+	 * nort: On RT IRQ-work may run in SOFTIRQ context.
+	 */
 	BUG_ON(!irqs_disabled());
-
+#endif
 	if (llist_empty(list))
 		return;
 
@@ -178,7 +195,16 @@
 void irq_work_run(void)
 {
 	irq_work_run_list(this_cpu_ptr(&raised_list));
-	irq_work_run_list(this_cpu_ptr(&lazy_list));
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * NOTE: we raise softirq via IPI for safety,
+		 * and execute in irq_work_tick() to move the
+		 * overhead from hard to soft irq context.
+		 */
+		if (!llist_empty(this_cpu_ptr(&lazy_list)))
+			raise_softirq(TIMER_SOFTIRQ);
+	} else
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
 EXPORT_SYMBOL_GPL(irq_work_run);
 
@@ -188,8 +214,17 @@
 
 	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
 		irq_work_run_list(raised);
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
+}
+
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT)
+void irq_work_tick_soft(void)
+{
 	irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
+#endif
 
 /*
  * Synchronize against the irq_work @entry, ensures the entry is not
diff -Nur linux-5.4.5/kernel/Kconfig.locks linux-5.4.5-new/kernel/Kconfig.locks
--- linux-5.4.5/kernel/Kconfig.locks	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/Kconfig.locks	2020-06-15 16:12:26.135714369 +0300
@@ -101,7 +101,7 @@
 # unlock and unlock_irq functions are inlined when:
 #   - DEBUG_SPINLOCK=n and ARCH_INLINE_*LOCK=y
 #  or
-#   - DEBUG_SPINLOCK=n and PREEMPT=n
+#   - DEBUG_SPINLOCK=n and PREEMPTION=n
 #
 # unlock_bh and unlock_irqrestore functions are inlined when:
 #   - DEBUG_SPINLOCK=n and ARCH_INLINE_*LOCK=y
@@ -139,7 +139,7 @@
 
 config INLINE_SPIN_UNLOCK_IRQ
 	def_bool y
-	depends on !PREEMPT || ARCH_INLINE_SPIN_UNLOCK_IRQ
+	depends on !PREEMPTION || ARCH_INLINE_SPIN_UNLOCK_IRQ
 
 config INLINE_SPIN_UNLOCK_IRQRESTORE
 	def_bool y
@@ -168,7 +168,7 @@
 
 config INLINE_READ_UNLOCK
 	def_bool y
-	depends on !PREEMPT || ARCH_INLINE_READ_UNLOCK
+	depends on !PREEMPTION || ARCH_INLINE_READ_UNLOCK
 
 config INLINE_READ_UNLOCK_BH
 	def_bool y
@@ -176,7 +176,7 @@
 
 config INLINE_READ_UNLOCK_IRQ
 	def_bool y
-	depends on !PREEMPT || ARCH_INLINE_READ_UNLOCK_IRQ
+	depends on !PREEMPTION || ARCH_INLINE_READ_UNLOCK_IRQ
 
 config INLINE_READ_UNLOCK_IRQRESTORE
 	def_bool y
@@ -205,7 +205,7 @@
 
 config INLINE_WRITE_UNLOCK
 	def_bool y
-	depends on !PREEMPT || ARCH_INLINE_WRITE_UNLOCK
+	depends on !PREEMPTION || ARCH_INLINE_WRITE_UNLOCK
 
 config INLINE_WRITE_UNLOCK_BH
 	def_bool y
@@ -213,7 +213,7 @@
 
 config INLINE_WRITE_UNLOCK_IRQ
 	def_bool y
-	depends on !PREEMPT || ARCH_INLINE_WRITE_UNLOCK_IRQ
+	depends on !PREEMPTION || ARCH_INLINE_WRITE_UNLOCK_IRQ
 
 config INLINE_WRITE_UNLOCK_IRQRESTORE
 	def_bool y
diff -Nur linux-5.4.5/kernel/Kconfig.preempt linux-5.4.5-new/kernel/Kconfig.preempt
--- linux-5.4.5/kernel/Kconfig.preempt	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/Kconfig.preempt	2020-06-15 16:12:26.135714369 +0300
@@ -1,5 +1,11 @@
 # SPDX-License-Identifier: GPL-2.0-only
 
+config HAVE_PREEMPT_LAZY
+	bool
+
+config PREEMPT_LAZY
+	def_bool y if HAVE_PREEMPT_LAZY && PREEMPT_RT
+
 choice
 	prompt "Preemption Model"
 	default PREEMPT_NONE
diff -Nur linux-5.4.5/kernel/kexec_core.c linux-5.4.5-new/kernel/kexec_core.c
--- linux-5.4.5/kernel/kexec_core.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/kexec_core.c	2020-06-15 16:12:26.139714355 +0300
@@ -972,7 +972,6 @@
 	old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);
 	if (old_cpu == PANIC_CPU_INVALID) {
 		/* This is the 1st CPU which comes here, so go ahead. */
-		printk_safe_flush_on_panic();
 		__crash_kexec(regs);
 
 		/*
diff -Nur linux-5.4.5/kernel/ksysfs.c linux-5.4.5-new/kernel/ksysfs.c
--- linux-5.4.5/kernel/ksysfs.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/ksysfs.c	2020-06-15 16:12:26.139714355 +0300
@@ -138,6 +138,15 @@
 
 #endif /* CONFIG_CRASH_CORE */
 
+#if defined(CONFIG_PREEMPT_RT)
+static ssize_t realtime_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", 1);
+}
+KERNEL_ATTR_RO(realtime);
+#endif
+
 /* whether file capabilities are enabled */
 static ssize_t fscaps_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
@@ -229,6 +238,9 @@
 	&rcu_expedited_attr.attr,
 	&rcu_normal_attr.attr,
 #endif
+#ifdef CONFIG_PREEMPT_RT
+	&realtime_attr.attr,
+#endif
 	NULL
 };
 
diff -Nur linux-5.4.5/kernel/locking/lockdep.c linux-5.4.5-new/kernel/locking/lockdep.c
--- linux-5.4.5/kernel/locking/lockdep.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/lockdep.c	2020-06-15 16:12:25.743715749 +0300
@@ -4410,6 +4410,7 @@
 		}
 	}
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * We dont accurately track softirq state in e.g.
 	 * hardirq contexts (such as on 4KSTACKS), so only
@@ -4424,6 +4425,7 @@
 			DEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);
 		}
 	}
+#endif
 
 	if (!debug_locks)
 		print_irqtrace_events(current);
diff -Nur linux-5.4.5/kernel/locking/locktorture.c linux-5.4.5-new/kernel/locking/locktorture.c
--- linux-5.4.5/kernel/locking/locktorture.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/locktorture.c	2020-06-15 16:12:25.743715749 +0300
@@ -16,7 +16,6 @@
 #include <linux/kthread.h>
 #include <linux/sched/rt.h>
 #include <linux/spinlock.h>
-#include <linux/rwlock.h>
 #include <linux/mutex.h>
 #include <linux/rwsem.h>
 #include <linux/smp.h>
diff -Nur linux-5.4.5/kernel/locking/Makefile linux-5.4.5-new/kernel/locking/Makefile
--- linux-5.4.5/kernel/locking/Makefile	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/Makefile	2020-06-15 16:12:25.739715763 +0300
@@ -3,7 +3,7 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
-obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o
+obj-y += semaphore.o rwsem.o percpu-rwsem.o
 
 ifdef CONFIG_FUNCTION_TRACER
 CFLAGS_REMOVE_lockdep.o = $(CC_FLAGS_FTRACE)
@@ -12,19 +12,23 @@
 CFLAGS_REMOVE_rtmutex-debug.o = $(CC_FLAGS_FTRACE)
 endif
 
-obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
 obj-$(CONFIG_LOCKDEP) += lockdep.o
 ifeq ($(CONFIG_PROC_FS),y)
 obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
 endif
 obj-$(CONFIG_SMP) += spinlock.o
-obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
 obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
 obj-$(CONFIG_QUEUED_SPINLOCKS) += qspinlock.o
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
+ifneq ($(CONFIG_PREEMPT_RT),y)
+obj-y += mutex.o
+obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
+obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+endif
+obj-$(CONFIG_PREEMPT_RT) += mutex-rt.o rwsem-rt.o rwlock-rt.o
 obj-$(CONFIG_QUEUED_RWLOCKS) += qrwlock.o
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
 obj-$(CONFIG_WW_MUTEX_SELFTEST) += test-ww_mutex.o
diff -Nur linux-5.4.5/kernel/locking/mutex-rt.c linux-5.4.5-new/kernel/locking/mutex-rt.c
--- linux-5.4.5/kernel/locking/mutex-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/mutex-rt.c	2020-06-15 16:12:25.743715749 +0300
@@ -0,0 +1,223 @@
+/*
+ * kernel/rt.c
+ *
+ * Real-Time Preemption Support
+ *
+ * started by Ingo Molnar:
+ *
+ *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *
+ * historic credit for proving that Linux spinlocks can be implemented via
+ * RT-aware mutexes goes to many people: The Pmutex project (Dirk Grambow
+ * and others) who prototyped it on 2.4 and did lots of comparative
+ * research and analysis; TimeSys, for proving that you can implement a
+ * fully preemptible kernel via the use of IRQ threading and mutexes;
+ * Bill Huey for persuasively arguing on lkml that the mutex model is the
+ * right one; and to MontaVista, who ported pmutexes to 2.6.
+ *
+ * This code is a from-scratch implementation and is not based on pmutexes,
+ * but the idea of converting spinlocks to mutexes is used here too.
+ *
+ * lock debugging, locking tree, deadlock detection:
+ *
+ *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey
+ *  Released under the General Public License (GPL).
+ *
+ * Includes portions of the generic R/W semaphore implementation from:
+ *
+ *  Copyright (c) 2001   David Howells (dhowells@redhat.com).
+ *  - Derived partially from idea by Andrea Arcangeli <andrea@suse.de>
+ *  - Derived also from comments by Linus
+ *
+ * Pending ownership of locks and ownership stealing:
+ *
+ *  Copyright (C) 2005, Kihon Technologies Inc., Steven Rostedt
+ *
+ *   (also by Steven Rostedt)
+ *    - Converted single pi_lock to individual task locks.
+ *
+ * By Esben Nielsen:
+ *    Doing priority inheritance with help of the scheduler.
+ *
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *  - major rework based on Esben Nielsens initial patch
+ *  - replaced thread_info references by task_struct refs
+ *  - removed task->pending_owner dependency
+ *  - BKL drop/reacquire for semaphore style locks to avoid deadlocks
+ *    in the scheduler return path as discussed with Steven Rostedt
+ *
+ *  Copyright (C) 2006, Kihon Technologies Inc.
+ *    Steven Rostedt <rostedt@goodmis.org>
+ *  - debugged and patched Thomas Gleixner's rework.
+ *  - added back the cmpxchg to the rework.
+ *  - turned atomic require back on for SMP.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/rtmutex.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/syscalls.h>
+#include <linux/interrupt.h>
+#include <linux/plist.h>
+#include <linux/fs.h>
+#include <linux/futex.h>
+#include <linux/hrtimer.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * struct mutex functions
+ */
+void __mutex_do_init(struct mutex *mutex, const char *name,
+		     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)mutex, sizeof(*mutex));
+	lockdep_init_map(&mutex->dep_map, name, key, 0);
+#endif
+	mutex->lock.save_state = 0;
+}
+EXPORT_SYMBOL(__mutex_do_init);
+
+void __lockfunc _mutex_lock(struct mutex *lock)
+{
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock);
+
+void __lockfunc _mutex_lock_io(struct mutex *lock)
+{
+	int token;
+
+	token = io_schedule_prepare();
+	_mutex_lock(lock);
+	io_schedule_finish(token);
+}
+EXPORT_SYMBOL_GPL(_mutex_lock_io);
+
+int __lockfunc _mutex_lock_interruptible(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible);
+
+int __lockfunc _mutex_lock_killable(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass)
+{
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nested);
+
+void __lockfunc _mutex_lock_io_nested(struct mutex *lock, int subclass)
+{
+	int token;
+
+	token = io_schedule_prepare();
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+
+	io_schedule_finish(token);
+}
+EXPORT_SYMBOL_GPL(_mutex_lock_io_nested);
+
+void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)
+{
+	mutex_acquire_nest(&lock->dep_map, 0, 0, nest, _RET_IP_);
+	__rt_mutex_lock_state(&lock->lock, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(_mutex_lock_nest_lock);
+
+int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_INTERRUPTIBLE);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible_nested);
+
+int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	ret = __rt_mutex_lock_state(&lock->lock, TASK_KILLABLE);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable_nested);
+#endif
+
+int __lockfunc _mutex_trylock(struct mutex *lock)
+{
+	int ret = __rt_mutex_trylock(&lock->lock);
+
+	if (ret)
+		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_trylock);
+
+void __lockfunc _mutex_unlock(struct mutex *lock)
+{
+	mutex_release(&lock->dep_map, 1, _RET_IP_);
+	__rt_mutex_unlock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_unlock);
+
+/**
+ * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0
+ * @cnt: the atomic which we are to dec
+ * @lock: the mutex to return holding if we dec to 0
+ *
+ * return true and hold lock if we dec to 0, return false otherwise
+ */
+int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)
+{
+	/* dec if we can't possibly hit 0 */
+	if (atomic_add_unless(cnt, -1, 1))
+		return 0;
+	/* we might hit 0, so take the lock */
+	mutex_lock(lock);
+	if (!atomic_dec_and_test(cnt)) {
+		/* when we actually did the dec, we didn't hit 0 */
+		mutex_unlock(lock);
+		return 0;
+	}
+	/* we hit 0, and we hold the lock */
+	return 1;
+}
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
diff -Nur linux-5.4.5/kernel/locking/rtmutex.c linux-5.4.5-new/kernel/locking/rtmutex.c
--- linux-5.4.5/kernel/locking/rtmutex.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rtmutex.c	2020-06-15 16:12:25.743715749 +0300
@@ -8,6 +8,11 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ * Adaptive Spinlocks:
+ *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,
+ *				     and Peter Morreale,
+ * Adaptive Spinlocks simplification:
+ *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>
  *
  *  See Documentation/locking/rt-mutex-design.rst for details.
  */
@@ -19,6 +24,8 @@
 #include <linux/sched/wake_q.h>
 #include <linux/sched/debug.h>
 #include <linux/timer.h>
+#include <linux/ww_mutex.h>
+#include <linux/blkdev.h>
 
 #include "rtmutex_common.h"
 
@@ -136,6 +143,12 @@
 		WRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);
 }
 
+static int rt_mutex_real_waiter(struct rt_mutex_waiter *waiter)
+{
+	return waiter && waiter != PI_WAKEUP_INPROGRESS &&
+		waiter != PI_REQUEUE_INPROGRESS;
+}
+
 /*
  * We can speed up the acquire/release, if there's no debugging state to be
  * set up.
@@ -229,7 +242,7 @@
  * Only use with rt_mutex_waiter_{less,equal}()
  */
 #define task_to_waiter(p)	\
-	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline }
+	&(struct rt_mutex_waiter){ .prio = (p)->prio, .deadline = (p)->dl.deadline, .task = (p) }
 
 static inline int
 rt_mutex_waiter_less(struct rt_mutex_waiter *left,
@@ -269,6 +282,27 @@
 	return 1;
 }
 
+#define STEAL_NORMAL  0
+#define STEAL_LATERAL 1
+
+static inline int
+rt_mutex_steal(struct rt_mutex *lock, struct rt_mutex_waiter *waiter, int mode)
+{
+	struct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);
+
+	if (waiter == top_waiter || rt_mutex_waiter_less(waiter, top_waiter))
+		return 1;
+
+	/*
+	 * Note that RT tasks are excluded from lateral-steals
+	 * to prevent the introduction of an unbounded latency.
+	 */
+	if (mode == STEAL_NORMAL || rt_task(waiter->task))
+		return 0;
+
+	return rt_mutex_waiter_equal(waiter, top_waiter);
+}
+
 static void
 rt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)
 {
@@ -373,6 +407,14 @@
 	return debug_rt_mutex_detect_deadlock(waiter, chwalk);
 }
 
+static void rt_mutex_wake_waiter(struct rt_mutex_waiter *waiter)
+{
+	if (waiter->savestate)
+		wake_up_lock_sleeper(waiter->task);
+	else
+		wake_up_process(waiter->task);
+}
+
 /*
  * Max number of times we'll walk the boosting chain:
  */
@@ -380,7 +422,8 @@
 
 static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)
 {
-	return p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;
+	return rt_mutex_real_waiter(p->pi_blocked_on) ?
+		p->pi_blocked_on->lock : NULL;
 }
 
 /*
@@ -516,7 +559,7 @@
 	 * reached or the state of the chain has changed while we
 	 * dropped the locks.
 	 */
-	if (!waiter)
+	if (!rt_mutex_real_waiter(waiter))
 		goto out_unlock_pi;
 
 	/*
@@ -696,13 +739,16 @@
 	 * follow here. This is the end of the chain we are walking.
 	 */
 	if (!rt_mutex_owner(lock)) {
+		struct rt_mutex_waiter *lock_top_waiter;
+
 		/*
 		 * If the requeue [7] above changed the top waiter,
 		 * then we need to wake the new top waiter up to try
 		 * to get the lock.
 		 */
-		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
-			wake_up_process(rt_mutex_top_waiter(lock)->task);
+		lock_top_waiter = rt_mutex_top_waiter(lock);
+		if (prerequeue_top_waiter != lock_top_waiter)
+			rt_mutex_wake_waiter(lock_top_waiter);
 		raw_spin_unlock_irq(&lock->wait_lock);
 		return 0;
 	}
@@ -803,9 +849,11 @@
  * @task:   The task which wants to acquire the lock
  * @waiter: The waiter that is queued to the lock's wait tree if the
  *	    callsite called task_blocked_on_lock(), otherwise NULL
+ * @mode:   Lock steal mode (STEAL_NORMAL, STEAL_LATERAL)
  */
-static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
-				struct rt_mutex_waiter *waiter)
+static int __try_to_take_rt_mutex(struct rt_mutex *lock,
+				  struct task_struct *task,
+				  struct rt_mutex_waiter *waiter, int mode)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -841,12 +889,11 @@
 	 */
 	if (waiter) {
 		/*
-		 * If waiter is not the highest priority waiter of
-		 * @lock, give up.
+		 * If waiter is not the highest priority waiter of @lock,
+		 * or its peer when lateral steal is allowed, give up.
 		 */
-		if (waiter != rt_mutex_top_waiter(lock))
+		if (!rt_mutex_steal(lock, waiter, mode))
 			return 0;
-
 		/*
 		 * We can acquire the lock. Remove the waiter from the
 		 * lock waiters tree.
@@ -864,14 +911,12 @@
 		 */
 		if (rt_mutex_has_waiters(lock)) {
 			/*
-			 * If @task->prio is greater than or equal to
-			 * the top waiter priority (kernel view),
-			 * @task lost.
+			 * If @task->prio is greater than the top waiter
+			 * priority (kernel view), or equal to it when a
+			 * lateral steal is forbidden, @task lost.
 			 */
-			if (!rt_mutex_waiter_less(task_to_waiter(task),
-						  rt_mutex_top_waiter(lock)))
+			if (!rt_mutex_steal(lock, task_to_waiter(task), mode))
 				return 0;
-
 			/*
 			 * The current top waiter stays enqueued. We
 			 * don't have to change anything in the lock
@@ -918,6 +963,344 @@
 	return 1;
 }
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * preemptible spin_lock functions:
+ */
+static inline void rt_spin_lock_fastlock(struct rt_mutex *lock,
+					 void  (*slowfn)(struct rt_mutex *lock))
+{
+	might_sleep_no_state_check();
+
+	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
+		return;
+	else
+		slowfn(lock);
+}
+
+static inline void rt_spin_lock_fastunlock(struct rt_mutex *lock,
+					   void  (*slowfn)(struct rt_mutex *lock))
+{
+	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
+		return;
+	else
+		slowfn(lock);
+}
+#ifdef CONFIG_SMP
+/*
+ * Note that owner is a speculative pointer and dereferencing relies
+ * on rcu_read_lock() and the check against the lock owner.
+ */
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *owner)
+{
+	int res = 0;
+
+	rcu_read_lock();
+	for (;;) {
+		if (owner != rt_mutex_owner(lock))
+			break;
+		/*
+		 * Ensure that owner->on_cpu is dereferenced _after_
+		 * checking the above to be valid.
+		 */
+		barrier();
+		if (!owner->on_cpu) {
+			res = 1;
+			break;
+		}
+		cpu_relax();
+	}
+	rcu_read_unlock();
+	return res;
+}
+#else
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *orig_owner)
+{
+	return 1;
+}
+#endif
+
+static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+				   struct rt_mutex_waiter *waiter,
+				   struct task_struct *task,
+				   enum rtmutex_chainwalk chwalk);
+/*
+ * Slow path lock function spin_lock style: this variant is very
+ * careful not to miss any non-lock wakeups.
+ *
+ * We store the current state under p->pi_lock in p->saved_state and
+ * the try_to_wake_up() code handles this accordingly.
+ */
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags)
+{
+	struct task_struct *lock_owner, *self = current;
+	struct rt_mutex_waiter *top_waiter;
+	int ret;
+
+	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL))
+		return;
+
+	BUG_ON(rt_mutex_owner(lock) == self);
+
+	/*
+	 * We save whatever state the task is in and we'll restore it
+	 * after acquiring the lock taking real wakeups into account
+	 * as well. We are serialized via pi_lock against wakeups. See
+	 * try_to_wake_up().
+	 */
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	ret = task_blocks_on_rt_mutex(lock, waiter, self, RT_MUTEX_MIN_CHAINWALK);
+	BUG_ON(ret);
+
+	for (;;) {
+		/* Try to acquire the lock again. */
+		if (__try_to_take_rt_mutex(lock, self, waiter, STEAL_LATERAL))
+			break;
+
+		top_waiter = rt_mutex_top_waiter(lock);
+		lock_owner = rt_mutex_owner(lock);
+
+		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+		debug_rt_mutex_print_deadlock(waiter);
+
+		if (top_waiter != waiter || adaptive_wait(lock, lock_owner))
+			schedule();
+
+		raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+
+	/*
+	 * Restore the task state to current->saved_state. We set it
+	 * to the original state above and the try_to_wake_up() code
+	 * has possibly updated it when a real (non-rtmutex) wakeup
+	 * happened while we were blocked. Clear saved_state so
+	 * try_to_wakeup() does not get confused.
+	 */
+	raw_spin_lock(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock(&self->pi_lock);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up:
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	BUG_ON(rt_mutex_has_waiters(lock) && waiter == rt_mutex_top_waiter(lock));
+	BUG_ON(!RB_EMPTY_NODE(&waiter->tree_entry));
+}
+
+static void noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	rt_mutex_init_waiter(&waiter, true);
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	rt_spin_lock_slowlock_locked(lock, &waiter, flags);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper);
+/*
+ * Slow path to release a rt_mutex spin_lock style
+ */
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock)
+{
+	unsigned long flags;
+	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
+	bool postunlock;
+
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	postunlock = __rt_mutex_unlock_common(lock, &wake_q, &wake_sleeper_q);
+	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+
+	if (postunlock)
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
+}
+
+void __lockfunc rt_spin_lock(spinlock_t *lock)
+{
+	sleeping_lock_inc();
+	rcu_read_lock();
+	migrate_disable();
+	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+}
+EXPORT_SYMBOL(rt_spin_lock);
+
+void __lockfunc __rt_spin_lock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastlock(lock, rt_spin_lock_slowlock);
+}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass)
+{
+	sleeping_lock_inc();
+	rcu_read_lock();
+	migrate_disable();
+	spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+}
+EXPORT_SYMBOL(rt_spin_lock_nested);
+#endif
+
+void __lockfunc rt_spin_unlock(spinlock_t *lock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	spin_release(&lock->dep_map, 1, _RET_IP_);
+	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock);
+	migrate_enable();
+	rcu_read_unlock();
+	sleeping_lock_dec();
+}
+EXPORT_SYMBOL(rt_spin_unlock);
+
+void __lockfunc __rt_spin_unlock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastunlock(lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(__rt_spin_unlock);
+
+/*
+ * Wait for the lock to get unlocked: instead of polling for an unlock
+ * (like raw spinlocks do), we lock and unlock, to force the kernel to
+ * schedule if there's contention:
+ */
+void __lockfunc rt_spin_unlock_wait(spinlock_t *lock)
+{
+	spin_lock(lock);
+	spin_unlock(lock);
+}
+EXPORT_SYMBOL(rt_spin_unlock_wait);
+
+int __lockfunc rt_spin_trylock(spinlock_t *lock)
+{
+	int ret;
+
+	sleeping_lock_inc();
+	migrate_disable();
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+	} else {
+		migrate_enable();
+		sleeping_lock_dec();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock);
+
+int __lockfunc rt_spin_trylock_bh(spinlock_t *lock)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		sleeping_lock_inc();
+		rcu_read_lock();
+		migrate_disable();
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	} else
+		local_bh_enable();
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_bh);
+
+int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags)
+{
+	int ret;
+
+	*flags = 0;
+	ret = __rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		sleeping_lock_inc();
+		rcu_read_lock();
+		migrate_disable();
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_irqsave);
+
+void
+__rt_spin_lock_init(spinlock_t *lock, const char *name, struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+}
+EXPORT_SYMBOL(__rt_spin_lock_init);
+
+#endif /* PREEMPT_RT */
+
+#ifdef CONFIG_PREEMPT_RT
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);
+
+	if (!hold_ctx)
+		return 0;
+
+	if (unlikely(ctx == hold_ctx))
+		return -EALREADY;
+
+	if (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
+	    (ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(ctx->contending_lock);
+		ctx->contending_lock = ww;
+#endif
+		return -EDEADLK;
+	}
+
+	return 0;
+}
+#else
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	BUG();
+	return 0;
+}
+
+#endif
+
+static inline int
+try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+		     struct rt_mutex_waiter *waiter)
+{
+	return __try_to_take_rt_mutex(lock, task, waiter, STEAL_NORMAL);
+}
+
 /*
  * Task blocks on lock.
  *
@@ -950,6 +1333,22 @@
 		return -EDEADLK;
 
 	raw_spin_lock(&task->pi_lock);
+	/*
+	 * In the case of futex requeue PI, this will be a proxy
+	 * lock. The task will wake unaware that it is enqueueed on
+	 * this lock. Avoid blocking on two locks and corrupting
+	 * pi_blocked_on via the PI_WAKEUP_INPROGRESS
+	 * flag. futex_wait_requeue_pi() sets this when it wakes up
+	 * before requeue (due to a signal or timeout). Do not enqueue
+	 * the task if PI_WAKEUP_INPROGRESS is set.
+	 */
+	if (task != current && task->pi_blocked_on == PI_WAKEUP_INPROGRESS) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+
+       BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on));
+
 	waiter->task = task;
 	waiter->lock = lock;
 	waiter->prio = task->prio;
@@ -973,7 +1372,7 @@
 		rt_mutex_enqueue_pi(owner, waiter);
 
 		rt_mutex_adjust_prio(owner);
-		if (owner->pi_blocked_on)
+		if (rt_mutex_real_waiter(owner->pi_blocked_on))
 			chain_walk = 1;
 	} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {
 		chain_walk = 1;
@@ -1015,6 +1414,7 @@
  * Called with lock->wait_lock held and interrupts disabled.
  */
 static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,
+				    struct wake_q_head *wake_sleeper_q,
 				    struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter *waiter;
@@ -1054,7 +1454,10 @@
 	 * Pairs with preempt_enable() in rt_mutex_postunlock();
 	 */
 	preempt_disable();
-	wake_q_add(wake_q, waiter->task);
+	if (waiter->savestate)
+		wake_q_add_sleeper(wake_sleeper_q, waiter->task);
+	else
+		wake_q_add(wake_q, waiter->task);
 	raw_spin_unlock(&current->pi_lock);
 }
 
@@ -1069,7 +1472,7 @@
 {
 	bool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));
 	struct task_struct *owner = rt_mutex_owner(lock);
-	struct rt_mutex *next_lock;
+	struct rt_mutex *next_lock = NULL;
 
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1095,7 +1498,8 @@
 	rt_mutex_adjust_prio(owner);
 
 	/* Store the lock on which owner is blocked or NULL */
-	next_lock = task_blocked_on_lock(owner);
+	if (rt_mutex_real_waiter(owner->pi_blocked_on))
+		next_lock = task_blocked_on_lock(owner);
 
 	raw_spin_unlock(&owner->pi_lock);
 
@@ -1131,26 +1535,28 @@
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
 
 	waiter = task->pi_blocked_on;
-	if (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
+	if (!rt_mutex_real_waiter(waiter) ||
+	    rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		return;
 	}
 	next_lock = waiter->lock;
-	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 
 	/* gets dropped in rt_mutex_adjust_prio_chain()! */
 	get_task_struct(task);
 
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 	rt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,
 				   next_lock, NULL, task);
 }
 
-void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
+void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savestate)
 {
 	debug_rt_mutex_init_waiter(waiter);
 	RB_CLEAR_NODE(&waiter->pi_tree_entry);
 	RB_CLEAR_NODE(&waiter->tree_entry);
 	waiter->task = NULL;
+	waiter->savestate = savestate;
 }
 
 /**
@@ -1166,7 +1572,8 @@
 static int __sched
 __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		    struct hrtimer_sleeper *timeout,
-		    struct rt_mutex_waiter *waiter)
+		    struct rt_mutex_waiter *waiter,
+		    struct ww_acquire_ctx *ww_ctx)
 {
 	int ret = 0;
 
@@ -1175,16 +1582,17 @@
 		if (try_to_take_rt_mutex(lock, current, waiter))
 			break;
 
-		/*
-		 * TASK_INTERRUPTIBLE checks for signals and
-		 * timeout. Ignored otherwise.
-		 */
-		if (likely(state == TASK_INTERRUPTIBLE)) {
-			/* Signal pending? */
-			if (signal_pending(current))
-				ret = -EINTR;
-			if (timeout && !timeout->task)
-				ret = -ETIMEDOUT;
+		if (timeout && !timeout->task) {
+			ret = -ETIMEDOUT;
+			break;
+		}
+		if (signal_pending_state(state, current)) {
+			ret = -EINTR;
+			break;
+		}
+
+		if (ww_ctx && ww_ctx->acquired > 0) {
+			ret = __mutex_lock_check_stamp(lock, ww_ctx);
 			if (ret)
 				break;
 		}
@@ -1223,33 +1631,104 @@
 	}
 }
 
-/*
- * Slow path lock function:
- */
-static int __sched
-rt_mutex_slowlock(struct rt_mutex *lock, int state,
-		  struct hrtimer_sleeper *timeout,
-		  enum rtmutex_chainwalk chwalk)
+static __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,
+						   struct ww_acquire_ctx *ww_ctx)
 {
-	struct rt_mutex_waiter waiter;
-	unsigned long flags;
-	int ret = 0;
+#ifdef CONFIG_DEBUG_MUTEXES
+	/*
+	 * If this WARN_ON triggers, you used ww_mutex_lock to acquire,
+	 * but released with a normal mutex_unlock in this call.
+	 *
+	 * This should never happen, always use ww_mutex_unlock.
+	 */
+	DEBUG_LOCKS_WARN_ON(ww->ctx);
 
-	rt_mutex_init_waiter(&waiter);
+	/*
+	 * Not quite done after calling ww_acquire_done() ?
+	 */
+	DEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);
+
+	if (ww_ctx->contending_lock) {
+		/*
+		 * After -EDEADLK you tried to
+		 * acquire a different ww_mutex? Bad!
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);
+
+		/*
+		 * You called ww_mutex_lock after receiving -EDEADLK,
+		 * but 'forgot' to unlock everything else first?
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);
+		ww_ctx->contending_lock = NULL;
+	}
 
 	/*
-	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
-	 * be called in early boot if the cmpxchg() fast path is disabled
-	 * (debug, no architecture support). In this case we will acquire the
-	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
-	 * enable interrupts in that early boot case. So we need to use the
-	 * irqsave/restore variants.
+	 * Naughty, using a different class will lead to undefined behavior!
 	 */
-	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+	DEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);
+#endif
+	ww_ctx->acquired++;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct rt_mutex_waiter *waiter, *n;
+
+	/*
+	 * This branch gets optimized out for the common case,
+	 * and is only important for ww_mutex_lock.
+	 */
+	ww_mutex_lock_acquired(ww, ww_ctx);
+	ww->ctx = ww_ctx;
+
+	/*
+	 * Give any possible sleeping processes the chance to wake up,
+	 * so they can recheck if they have to back off.
+	 */
+	rbtree_postorder_for_each_entry_safe(waiter, n, &lock->waiters.rb_root,
+					     tree_entry) {
+		/* XXX debug rt mutex waiter wakeup */
+
+		BUG_ON(waiter->lock != lock);
+		rt_mutex_wake_waiter(waiter);
+	}
+}
+
+#else
+
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	BUG();
+}
+#endif
+
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter)
+{
+	int ret;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (ww_ctx) {
+		struct ww_mutex *ww;
+
+		ww = container_of(lock, struct ww_mutex, base.lock);
+		if (unlikely(ww_ctx == READ_ONCE(ww->ctx)))
+			return -EALREADY;
+	}
+#endif
 
 	/* Try to acquire the lock again: */
 	if (try_to_take_rt_mutex(lock, current, NULL)) {
-		raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+		if (ww_ctx)
+			ww_mutex_account_lock(lock, ww_ctx);
 		return 0;
 	}
 
@@ -1259,16 +1738,26 @@
 	if (unlikely(timeout))
 		hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);
 
-	ret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);
+	ret = task_blocks_on_rt_mutex(lock, waiter, current, chwalk);
 
-	if (likely(!ret))
+	if (likely(!ret)) {
 		/* sleep on the mutex */
-		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter);
+		ret = __rt_mutex_slowlock(lock, state, timeout, waiter,
+					  ww_ctx);
+	} else if (ww_ctx) {
+		/* ww_mutex received EDEADLK, let it become EALREADY */
+		ret = __mutex_lock_check_stamp(lock, ww_ctx);
+		BUG_ON(!ret);
+	}
 
 	if (unlikely(ret)) {
 		__set_current_state(TASK_RUNNING);
-		remove_waiter(lock, &waiter);
-		rt_mutex_handle_deadlock(ret, chwalk, &waiter);
+		remove_waiter(lock, waiter);
+		/* ww_mutex wants to report EDEADLK/EALREADY, let it */
+		if (!ww_ctx)
+			rt_mutex_handle_deadlock(ret, chwalk, waiter);
+	} else if (ww_ctx) {
+		ww_mutex_account_lock(lock, ww_ctx);
 	}
 
 	/*
@@ -1276,6 +1765,36 @@
 	 * unconditionally. We might have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	return ret;
+}
+
+/*
+ * Slow path lock function:
+ */
+static int __sched
+rt_mutex_slowlock(struct rt_mutex *lock, int state,
+		  struct hrtimer_sleeper *timeout,
+		  enum rtmutex_chainwalk chwalk,
+		  struct ww_acquire_ctx *ww_ctx)
+{
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+	int ret = 0;
+
+	rt_mutex_init_waiter(&waiter, false);
+
+	/*
+	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
+	 * be called in early boot if the cmpxchg() fast path is disabled
+	 * (debug, no architecture support). In this case we will acquire the
+	 * rtmutex with lock->wait_lock held. But we cannot unconditionally
+	 * enable interrupts in that early boot case. So we need to use the
+	 * irqsave/restore variants.
+	 */
+	raw_spin_lock_irqsave(&lock->wait_lock, flags);
+
+	ret = rt_mutex_slowlock_locked(lock, state, timeout, chwalk, ww_ctx,
+				       &waiter);
 
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
@@ -1336,7 +1855,8 @@
  * Return whether the current task needs to call rt_mutex_postunlock().
  */
 static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,
-					struct wake_q_head *wake_q)
+					struct wake_q_head *wake_q,
+					struct wake_q_head *wake_sleeper_q)
 {
 	unsigned long flags;
 
@@ -1390,7 +1910,7 @@
 	 *
 	 * Queue the next waiter for wakeup once we release the wait_lock.
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wake_sleeper_q, lock);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	return true; /* call rt_mutex_postunlock() */
@@ -1404,29 +1924,45 @@
  */
 static inline int
 rt_mutex_fastlock(struct rt_mutex *lock, int state,
+		  struct ww_acquire_ctx *ww_ctx,
 		  int (*slowfn)(struct rt_mutex *lock, int state,
 				struct hrtimer_sleeper *timeout,
-				enum rtmutex_chainwalk chwalk))
+				enum rtmutex_chainwalk chwalk,
+				struct ww_acquire_ctx *ww_ctx))
 {
 	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
 		return 0;
 
-	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);
+	/*
+	 * If rt_mutex blocks, the function sched_submit_work will not call
+	 * blk_schedule_flush_plug (because tsk_is_pi_blocked would be true).
+	 * We must call blk_schedule_flush_plug here, if we don't call it,
+	 * a deadlock in I/O may happen.
+	 */
+	if (unlikely(blk_needs_flush_plug(current)))
+		blk_schedule_flush_plug(current);
+
+	return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK, ww_ctx);
 }
 
 static inline int
 rt_mutex_timed_fastlock(struct rt_mutex *lock, int state,
 			struct hrtimer_sleeper *timeout,
 			enum rtmutex_chainwalk chwalk,
+			struct ww_acquire_ctx *ww_ctx,
 			int (*slowfn)(struct rt_mutex *lock, int state,
 				      struct hrtimer_sleeper *timeout,
-				      enum rtmutex_chainwalk chwalk))
+				      enum rtmutex_chainwalk chwalk,
+				      struct ww_acquire_ctx *ww_ctx))
 {
 	if (chwalk == RT_MUTEX_MIN_CHAINWALK &&
 	    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
 		return 0;
 
-	return slowfn(lock, state, timeout, chwalk);
+	if (unlikely(blk_needs_flush_plug(current)))
+		blk_schedule_flush_plug(current);
+
+	return slowfn(lock, state, timeout, chwalk, ww_ctx);
 }
 
 static inline int
@@ -1442,9 +1978,11 @@
 /*
  * Performs the wakeup of the the top-waiter and re-enables preemption.
  */
-void rt_mutex_postunlock(struct wake_q_head *wake_q)
+void rt_mutex_postunlock(struct wake_q_head *wake_q,
+			 struct wake_q_head *wake_sleeper_q)
 {
 	wake_up_q(wake_q);
+	wake_up_q_sleeper(wake_sleeper_q);
 
 	/* Pairs with preempt_disable() in rt_mutex_slowunlock() */
 	preempt_enable();
@@ -1453,23 +1991,46 @@
 static inline void
 rt_mutex_fastunlock(struct rt_mutex *lock,
 		    bool (*slowfn)(struct rt_mutex *lock,
-				   struct wake_q_head *wqh))
+				   struct wake_q_head *wqh,
+				   struct wake_q_head *wq_sleeper))
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 
 	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
 		return;
 
-	if (slowfn(lock, &wake_q))
-		rt_mutex_postunlock(&wake_q);
+	if (slowfn(lock, &wake_q, &wake_sleeper_q))
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
-static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+int __sched __rt_mutex_lock_state(struct rt_mutex *lock, int state)
 {
 	might_sleep();
+	return rt_mutex_fastlock(lock, state, NULL, rt_mutex_slowlock);
+}
+
+/**
+ * rt_mutex_lock_state - lock a rt_mutex with a given state
+ *
+ * @lock:      The rt_mutex to be locked
+ * @state:     The state to set when blocking on the rt_mutex
+ */
+static inline int __sched rt_mutex_lock_state(struct rt_mutex *lock,
+					      unsigned int subclass, int state)
+{
+	int ret;
 
 	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
-	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);
+	ret = __rt_mutex_lock_state(lock, state);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+
+static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)
+{
+	rt_mutex_lock_state(lock, subclass, TASK_UNINTERRUPTIBLE);
 }
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -1510,16 +2071,7 @@
  */
 int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)
 {
-	int ret;
-
-	might_sleep();
-
-	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
-	ret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);
-	if (ret)
-		mutex_release(&lock->dep_map, 1, _RET_IP_);
-
-	return ret;
+	return rt_mutex_lock_state(lock, 0, TASK_INTERRUPTIBLE);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_lock_interruptible);
 
@@ -1537,6 +2089,22 @@
 }
 
 /**
+ * rt_mutex_lock_killable - lock a rt_mutex killable
+ *
+ * @lock:              the rt_mutex to be locked
+ * @detect_deadlock:   deadlock detection on/off
+ *
+ * Returns:
+ *  0          on success
+ * -EINTR      when interrupted by a signal
+ */
+int __sched rt_mutex_lock_killable(struct rt_mutex *lock)
+{
+	return rt_mutex_lock_state(lock, 0, TASK_KILLABLE);
+}
+EXPORT_SYMBOL_GPL(rt_mutex_lock_killable);
+
+/**
  * rt_mutex_timed_lock - lock a rt_mutex interruptible
  *			the timeout structure is provided
  *			by the caller
@@ -1559,6 +2127,7 @@
 	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	ret = rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
 				       RT_MUTEX_MIN_CHAINWALK,
+				       NULL,
 				       rt_mutex_slowlock);
 	if (ret)
 		mutex_release(&lock->dep_map, 1, _RET_IP_);
@@ -1567,6 +2136,18 @@
 }
 EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);
 
+int __sched __rt_mutex_trylock(struct rt_mutex *lock)
+{
+#ifdef CONFIG_PREEMPT_RT
+	if (WARN_ON_ONCE(in_irq() || in_nmi()))
+#else
+	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
+#endif
+		return 0;
+
+	return rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
+}
+
 /**
  * rt_mutex_trylock - try to lock a rt_mutex
  *
@@ -1582,10 +2163,7 @@
 {
 	int ret;
 
-	if (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))
-		return 0;
-
-	ret = rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
+	ret = __rt_mutex_trylock(lock);
 	if (ret)
 		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
 
@@ -1593,6 +2171,11 @@
 }
 EXPORT_SYMBOL_GPL(rt_mutex_trylock);
 
+void __sched __rt_mutex_unlock(struct rt_mutex *lock)
+{
+	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+}
+
 /**
  * rt_mutex_unlock - unlock a rt_mutex
  *
@@ -1601,16 +2184,13 @@
 void __sched rt_mutex_unlock(struct rt_mutex *lock)
 {
 	mutex_release(&lock->dep_map, 1, _RET_IP_);
-	rt_mutex_fastunlock(lock, rt_mutex_slowunlock);
+	__rt_mutex_unlock(lock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_unlock);
 
-/**
- * Futex variant, that since futex variants do not use the fast-path, can be
- * simple and will not need to retry.
- */
-bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				    struct wake_q_head *wake_q)
+static bool __sched __rt_mutex_unlock_common(struct rt_mutex *lock,
+					     struct wake_q_head *wake_q,
+					     struct wake_q_head *wq_sleeper)
 {
 	lockdep_assert_held(&lock->wait_lock);
 
@@ -1627,23 +2207,35 @@
 	 * avoid inversion prior to the wakeup.  preempt_disable()
 	 * therein pairs with rt_mutex_postunlock().
 	 */
-	mark_wakeup_next_waiter(wake_q, lock);
+	mark_wakeup_next_waiter(wake_q, wq_sleeper, lock);
 
 	return true; /* call postunlock() */
 }
 
+/**
+ * Futex variant, that since futex variants do not use the fast-path, can be
+ * simple and will not need to retry.
+ */
+bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,
+				     struct wake_q_head *wake_q,
+				     struct wake_q_head *wq_sleeper)
+{
+	return __rt_mutex_unlock_common(lock, wake_q, wq_sleeper);
+}
+
 void __sched rt_mutex_futex_unlock(struct rt_mutex *lock)
 {
 	DEFINE_WAKE_Q(wake_q);
+	DEFINE_WAKE_Q(wake_sleeper_q);
 	unsigned long flags;
 	bool postunlock;
 
 	raw_spin_lock_irqsave(&lock->wait_lock, flags);
-	postunlock = __rt_mutex_futex_unlock(lock, &wake_q);
+	postunlock = __rt_mutex_futex_unlock(lock, &wake_q, &wake_sleeper_q);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	if (postunlock)
-		rt_mutex_postunlock(&wake_q);
+		rt_mutex_postunlock(&wake_q, &wake_sleeper_q);
 }
 
 /**
@@ -1682,7 +2274,7 @@
 	if (name && key)
 		debug_rt_mutex_init(lock, name, key);
 }
-EXPORT_SYMBOL_GPL(__rt_mutex_init);
+EXPORT_SYMBOL(__rt_mutex_init);
 
 /**
  * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
@@ -1702,6 +2294,14 @@
 				struct task_struct *proxy_owner)
 {
 	__rt_mutex_init(lock, NULL, NULL);
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/*
+	 * get another key class for the wait_lock. LOCK_PI and UNLOCK_PI is
+	 * holding the ->wait_lock of the proxy_lock while unlocking a sleeping
+	 * lock.
+	 */
+	raw_spin_lock_init(&lock->wait_lock);
+#endif
 	debug_rt_mutex_proxy_lock(lock, proxy_owner);
 	rt_mutex_set_owner(lock, proxy_owner);
 }
@@ -1725,6 +2325,26 @@
 	rt_mutex_set_owner(lock, NULL);
 }
 
+static void fixup_rt_mutex_blocked(struct rt_mutex *lock)
+{
+	struct task_struct *tsk = current;
+	/*
+	 * RT has a problem here when the wait got interrupted by a timeout
+	 * or a signal. task->pi_blocked_on is still set. The task must
+	 * acquire the hash bucket lock when returning from this function.
+	 *
+	 * If the hash bucket lock is contended then the
+	 * BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on)) in
+	 * task_blocks_on_rt_mutex() will trigger. This can be avoided by
+	 * clearing task->pi_blocked_on which removes the task from the
+	 * boosting chain of the rtmutex. That's correct because the task
+	 * is not longer blocked on it.
+	 */
+	raw_spin_lock(&tsk->pi_lock);
+	tsk->pi_blocked_on = NULL;
+	raw_spin_unlock(&tsk->pi_lock);
+}
+
 /**
  * __rt_mutex_start_proxy_lock() - Start lock acquisition for another task
  * @lock:		the rt_mutex to take
@@ -1755,6 +2375,34 @@
 	if (try_to_take_rt_mutex(lock, task, NULL))
 		return 1;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * In PREEMPT_RT there's an added race.
+	 * If the task, that we are about to requeue, times out,
+	 * it can set the PI_WAKEUP_INPROGRESS. This tells the requeue
+	 * to skip this task. But right after the task sets
+	 * its pi_blocked_on to PI_WAKEUP_INPROGRESS it can then
+	 * block on the spin_lock(&hb->lock), which in RT is an rtmutex.
+	 * This will replace the PI_WAKEUP_INPROGRESS with the actual
+	 * lock that it blocks on. We *must not* place this task
+	 * on this proxy lock in that case.
+	 *
+	 * To prevent this race, we first take the task's pi_lock
+	 * and check if it has updated its pi_blocked_on. If it has,
+	 * we assume that it woke up and we return -EAGAIN.
+	 * Otherwise, we set the task's pi_blocked_on to
+	 * PI_REQUEUE_INPROGRESS, so that if the task is waking up
+	 * it will know that we are in the process of requeuing it.
+	 */
+	raw_spin_lock(&task->pi_lock);
+	if (task->pi_blocked_on) {
+		raw_spin_unlock(&task->pi_lock);
+		return -EAGAIN;
+	}
+	task->pi_blocked_on = PI_REQUEUE_INPROGRESS;
+	raw_spin_unlock(&task->pi_lock);
+#endif
+
 	/* We enforce deadlock detection for futexes */
 	ret = task_blocks_on_rt_mutex(lock, waiter, task,
 				      RT_MUTEX_FULL_CHAINWALK);
@@ -1769,6 +2417,9 @@
 		ret = 0;
 	}
 
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
+
 	debug_rt_mutex_print_deadlock(waiter);
 
 	return ret;
@@ -1854,12 +2505,15 @@
 	raw_spin_lock_irq(&lock->wait_lock);
 	/* sleep on the mutex */
 	set_current_state(TASK_INTERRUPTIBLE);
-	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);
+	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter, NULL);
 	/*
 	 * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might
 	 * have to fix that up.
 	 */
 	fixup_rt_mutex_waiters(lock);
+	if (ret)
+		fixup_rt_mutex_blocked(lock);
+
 	raw_spin_unlock_irq(&lock->wait_lock);
 
 	return ret;
@@ -1921,3 +2575,99 @@
 
 	return cleanup;
 }
+
+static inline int
+ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH
+	unsigned tmp;
+
+	if (ctx->deadlock_inject_countdown-- == 0) {
+		tmp = ctx->deadlock_inject_interval;
+		if (tmp > UINT_MAX/4)
+			tmp = UINT_MAX;
+		else
+			tmp = tmp*2 + tmp + tmp/2;
+
+		ctx->deadlock_inject_interval = tmp;
+		ctx->deadlock_inject_countdown = tmp;
+		ctx->contending_lock = lock;
+
+		ww_mutex_unlock(lock);
+
+		return -EDEADLK;
+	}
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_PREEMPT_RT
+int __sched
+ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_INTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock_interruptible);
+
+int __sched
+ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0,
+			   ctx ? &ctx->dep_map : NULL, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_UNINTERRUPTIBLE, NULL, 0,
+				ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
+	else if (!ret && ctx && ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ww_mutex_lock);
+
+void __sched ww_mutex_unlock(struct ww_mutex *lock)
+{
+	int nest = !!lock->ctx;
+
+	/*
+	 * The unlocking fastpath is the 0->1 transition from 'locked'
+	 * into 'unlocked' state:
+	 */
+	if (nest) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);
+#endif
+		if (lock->ctx->acquired > 0)
+			lock->ctx->acquired--;
+		lock->ctx = NULL;
+	}
+
+	mutex_release(&lock->base.dep_map, nest, _RET_IP_);
+	__rt_mutex_unlock(&lock->base.lock);
+}
+EXPORT_SYMBOL(ww_mutex_unlock);
+
+int __rt_mutex_owner_current(struct rt_mutex *lock)
+{
+	return rt_mutex_owner(lock) == current;
+}
+EXPORT_SYMBOL(__rt_mutex_owner_current);
+#endif
diff -Nur linux-5.4.5/kernel/locking/rtmutex_common.h linux-5.4.5-new/kernel/locking/rtmutex_common.h
--- linux-5.4.5/kernel/locking/rtmutex_common.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rtmutex_common.h	2020-06-15 16:12:25.743715749 +0300
@@ -15,6 +15,7 @@
 
 #include <linux/rtmutex.h>
 #include <linux/sched/wake_q.h>
+#include <linux/sched/debug.h>
 
 /*
  * This is the control structure for tasks blocked on a rt_mutex,
@@ -29,6 +30,7 @@
 	struct rb_node          pi_tree_entry;
 	struct task_struct	*task;
 	struct rt_mutex		*lock;
+	bool			savestate;
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 	unsigned long		ip;
 	struct pid		*deadlock_task_pid;
@@ -130,12 +132,15 @@
 /*
  * PI-futex support (proxy locking functions, etc.):
  */
+#define PI_WAKEUP_INPROGRESS	((struct rt_mutex_waiter *) 1)
+#define PI_REQUEUE_INPROGRESS	((struct rt_mutex_waiter *) 2)
+
 extern struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock);
 extern void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				       struct task_struct *proxy_owner);
 extern void rt_mutex_proxy_unlock(struct rt_mutex *lock,
 				  struct task_struct *proxy_owner);
-extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter);
+extern void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savetate);
 extern int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 				     struct rt_mutex_waiter *waiter,
 				     struct task_struct *task);
@@ -153,9 +158,27 @@
 
 extern void rt_mutex_futex_unlock(struct rt_mutex *lock);
 extern bool __rt_mutex_futex_unlock(struct rt_mutex *lock,
-				 struct wake_q_head *wqh);
+				 struct wake_q_head *wqh,
+				 struct wake_q_head *wq_sleeper);
+
+extern void rt_mutex_postunlock(struct wake_q_head *wake_q,
+				struct wake_q_head *wake_sleeper_q);
+
+/* RW semaphore special interface */
+struct ww_acquire_ctx;
 
-extern void rt_mutex_postunlock(struct wake_q_head *wake_q);
+extern int __rt_mutex_lock_state(struct rt_mutex *lock, int state);
+extern int __rt_mutex_trylock(struct rt_mutex *lock);
+extern void __rt_mutex_unlock(struct rt_mutex *lock);
+int __sched rt_mutex_slowlock_locked(struct rt_mutex *lock, int state,
+				     struct hrtimer_sleeper *timeout,
+				     enum rtmutex_chainwalk chwalk,
+				     struct ww_acquire_ctx *ww_ctx,
+				     struct rt_mutex_waiter *waiter);
+void __sched rt_spin_lock_slowlock_locked(struct rt_mutex *lock,
+					  struct rt_mutex_waiter *waiter,
+					  unsigned long flags);
+void __sched rt_spin_lock_slowunlock(struct rt_mutex *lock);
 
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 # include "rtmutex-debug.h"
diff -Nur linux-5.4.5/kernel/locking/rwlock-rt.c linux-5.4.5-new/kernel/locking/rwlock-rt.c
--- linux-5.4.5/kernel/locking/rwlock-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rwlock-rt.c	2020-06-15 16:12:25.743715749 +0300
@@ -0,0 +1,384 @@
+/*
+ */
+#include <linux/sched/debug.h>
+#include <linux/export.h>
+
+#include "rtmutex_common.h"
+#include <linux/rwlock_types_rt.h>
+
+/*
+ * RT-specific reader/writer locks
+ *
+ * write_lock()
+ *  1) Lock lock->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * write_unlock()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock lock->rtmutex to release blocked readers
+ *
+ * read_lock()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take lock->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on lock->rtmutex
+ *  5) unlock lock->rtmutex, goto 1)
+ *
+ * read_unlock()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in write_lock()#3
+ *
+ * read_lock()#3 has the consequence, that rw locks on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think tasklist
+ * lock), are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw locks writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers
+ * to block on the rtmutex, but the rtmutex would have to be proxy locked
+ * for one reader after the other. We can't use multi-reader inheritance
+ * because there is no way to support that with
+ * SCHED_DEADLINE. Implementing the one by one reader boosting/handover
+ * mechanism is a major surgery for a very dubious value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwlock_biased_rt_init(struct rt_rw_lock *lock, const char *name,
+			     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+	atomic_set(&lock->readers, READER_BIAS);
+	rt_mutex_init(&lock->rtmutex);
+	lock->rtmutex.save_state = 1;
+}
+
+int __read_rt_trylock(struct rt_rw_lock *lock)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if lock->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&lock->readers); r < 0;) {
+		old = atomic_cmpxchg(&lock->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+void __sched __read_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct rt_mutex_waiter waiter;
+	unsigned long flags;
+
+	if (__read_rt_trylock(lock))
+		return;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&lock->readers) != WRITER_BIAS) {
+		atomic_inc(&lock->readers);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * read_lock()
+	 * unlock(m->wait_lock)
+	 *			read_unlock()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					lock->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					write_unlock()
+	 *					lock->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			read_lock()
+	 *					write_lock()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call read_unlock() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, true);
+	rt_spin_lock_slowlock_locked(m, &waiter, flags);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex is
+	 * now held, so there can't be a writer active. Increment the reader
+	 * count and immediately drop the rtmutex again.
+	 */
+	atomic_inc(&lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+void __read_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&lock->readers))
+		return;
+
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path, but to clean up the rw
+	 * lock it needs to acquire m->wait_lock. The worst case which can
+	 * happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __write_unlock_common(struct rt_rw_lock *lock, int bias,
+				  unsigned long flags)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &lock->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	rt_spin_lock_slowunlock(m);
+}
+
+void __sched __write_rt_lock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	struct task_struct *self = current;
+	unsigned long flags;
+
+	/* Take the rtmutex as a first step */
+	__rt_spin_lock(m);
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+	raw_spin_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock(&self->pi_lock);
+
+	for (;;) {
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&lock->readers)) {
+			atomic_set(&lock->readers, WRITER_BIAS);
+			raw_spin_lock(&self->pi_lock);
+			__set_current_state_no_track(self->saved_state);
+			self->saved_state = TASK_RUNNING;
+			raw_spin_unlock(&self->pi_lock);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return;
+		}
+
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&lock->readers) != 0)
+			schedule();
+
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+
+		raw_spin_lock(&self->pi_lock);
+		__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+		raw_spin_unlock(&self->pi_lock);
+	}
+}
+
+int __write_rt_trylock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &lock->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&lock->readers)) {
+		atomic_set(&lock->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__write_unlock_common(lock, 0, flags);
+	return 0;
+}
+
+void __write_rt_unlock(struct rt_rw_lock *lock)
+{
+	struct rt_mutex *m = &lock->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__write_unlock_common(lock, WRITER_BIAS, flags);
+}
+
+/* Map the reader biased implementation */
+static inline int do_read_rt_trylock(rwlock_t *rwlock)
+{
+	return __read_rt_trylock(rwlock);
+}
+
+static inline int do_write_rt_trylock(rwlock_t *rwlock)
+{
+	return __write_rt_trylock(rwlock);
+}
+
+static inline void do_read_rt_lock(rwlock_t *rwlock)
+{
+	__read_rt_lock(rwlock);
+}
+
+static inline void do_write_rt_lock(rwlock_t *rwlock)
+{
+	__write_rt_lock(rwlock);
+}
+
+static inline void do_read_rt_unlock(rwlock_t *rwlock)
+{
+	__read_rt_unlock(rwlock);
+}
+
+static inline void do_write_rt_unlock(rwlock_t *rwlock)
+{
+	__write_rt_unlock(rwlock);
+}
+
+static inline void do_rwlock_rt_init(rwlock_t *rwlock, const char *name,
+				     struct lock_class_key *key)
+{
+	__rwlock_biased_rt_init(rwlock, name, key);
+}
+
+int __lockfunc rt_read_can_lock(rwlock_t *rwlock)
+{
+	return  atomic_read(&rwlock->readers) < 0;
+}
+
+int __lockfunc rt_write_can_lock(rwlock_t *rwlock)
+{
+	return atomic_read(&rwlock->readers) == READER_BIAS;
+}
+
+/*
+ * The common functions which get wrapped into the rwlock API.
+ */
+int __lockfunc rt_read_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	sleeping_lock_inc();
+	migrate_disable();
+	ret = do_read_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire_read(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+	} else {
+		migrate_enable();
+		sleeping_lock_dec();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_read_trylock);
+
+int __lockfunc rt_write_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	sleeping_lock_inc();
+	migrate_disable();
+	ret = do_write_rt_trylock(rwlock);
+	if (ret) {
+		rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
+		rcu_read_lock();
+	} else {
+		migrate_enable();
+		sleeping_lock_dec();
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_write_trylock);
+
+void __lockfunc rt_read_lock(rwlock_t *rwlock)
+{
+	sleeping_lock_inc();
+	rcu_read_lock();
+	migrate_disable();
+	rwlock_acquire_read(&rwlock->dep_map, 0, 0, _RET_IP_);
+	do_read_rt_lock(rwlock);
+}
+EXPORT_SYMBOL(rt_read_lock);
+
+void __lockfunc rt_write_lock(rwlock_t *rwlock)
+{
+	sleeping_lock_inc();
+	rcu_read_lock();
+	migrate_disable();
+	rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
+	do_write_rt_lock(rwlock);
+}
+EXPORT_SYMBOL(rt_write_lock);
+
+void __lockfunc rt_read_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
+	do_read_rt_unlock(rwlock);
+	migrate_enable();
+	rcu_read_unlock();
+	sleeping_lock_dec();
+}
+EXPORT_SYMBOL(rt_read_unlock);
+
+void __lockfunc rt_write_unlock(rwlock_t *rwlock)
+{
+	rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
+	do_write_rt_unlock(rwlock);
+	migrate_enable();
+	rcu_read_unlock();
+	sleeping_lock_dec();
+}
+EXPORT_SYMBOL(rt_write_unlock);
+
+void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key)
+{
+	do_rwlock_rt_init(rwlock, name, key);
+}
+EXPORT_SYMBOL(__rt_rwlock_init);
diff -Nur linux-5.4.5/kernel/locking/rwsem.c linux-5.4.5-new/kernel/locking/rwsem.c
--- linux-5.4.5/kernel/locking/rwsem.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rwsem.c	2020-06-15 16:12:25.743715749 +0300
@@ -29,6 +29,8 @@
 #include <linux/atomic.h>
 
 #include "rwsem.h"
+
+#ifndef CONFIG_PREEMPT_RT
 #include "lock_events.h"
 
 /*
@@ -1335,6 +1337,7 @@
 	return sem;
 }
 
+
 /*
  * lock for reading
  */
@@ -1485,6 +1488,7 @@
 	if (tmp & RWSEM_FLAG_WAITERS)
 		rwsem_downgrade_wake(sem);
 }
+#endif
 
 /*
  * lock for reading
@@ -1616,6 +1620,7 @@
 }
 EXPORT_SYMBOL(_down_write_nest_lock);
 
+#ifndef CONFIG_PREEMPT_RT
 void down_read_non_owner(struct rw_semaphore *sem)
 {
 	might_sleep();
@@ -1623,6 +1628,7 @@
 	__rwsem_set_reader_owned(sem, NULL);
 }
 EXPORT_SYMBOL(down_read_non_owner);
+#endif
 
 void down_write_nested(struct rw_semaphore *sem, int subclass)
 {
@@ -1647,11 +1653,13 @@
 }
 EXPORT_SYMBOL(down_write_killable_nested);
 
+#ifndef CONFIG_PREEMPT_RT
 void up_read_non_owner(struct rw_semaphore *sem)
 {
 	DEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);
 	__up_read(sem);
 }
 EXPORT_SYMBOL(up_read_non_owner);
+#endif
 
 #endif
diff -Nur linux-5.4.5/kernel/locking/rwsem.h linux-5.4.5-new/kernel/locking/rwsem.h
--- linux-5.4.5/kernel/locking/rwsem.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rwsem.h	2020-06-15 16:12:25.743715749 +0300
@@ -4,7 +4,9 @@
 #define __INTERNAL_RWSEM_H
 #include <linux/rwsem.h>
 
+#ifndef CONFIG_PREEMPT_RT
 extern void __down_read(struct rw_semaphore *sem);
 extern void __up_read(struct rw_semaphore *sem);
+#endif
 
 #endif /* __INTERNAL_RWSEM_H */
diff -Nur linux-5.4.5/kernel/locking/rwsem-rt.c linux-5.4.5-new/kernel/locking/rwsem-rt.c
--- linux-5.4.5/kernel/locking/rwsem-rt.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/rwsem-rt.c	2020-06-15 16:12:25.743715749 +0300
@@ -0,0 +1,302 @@
+/*
+ */
+#include <linux/blkdev.h>
+#include <linux/rwsem.h>
+#include <linux/sched/debug.h>
+#include <linux/sched/signal.h>
+#include <linux/export.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * RT-specific reader/writer semaphores
+ *
+ * down_write()
+ *  1) Lock sem->rtmutex
+ *  2) Remove the reader BIAS to force readers into the slow path
+ *  3) Wait until all readers have left the critical region
+ *  4) Mark it write locked
+ *
+ * up_write()
+ *  1) Remove the write locked marker
+ *  2) Set the reader BIAS so readers can use the fast path again
+ *  3) Unlock sem->rtmutex to release blocked readers
+ *
+ * down_read()
+ *  1) Try fast path acquisition (reader BIAS is set)
+ *  2) Take sem->rtmutex.wait_lock which protects the writelocked flag
+ *  3) If !writelocked, acquire it for read
+ *  4) If writelocked, block on sem->rtmutex
+ *  5) unlock sem->rtmutex, goto 1)
+ *
+ * up_read()
+ *  1) Try fast path release (reader count != 1)
+ *  2) Wake the writer waiting in down_write()#3
+ *
+ * down_read()#3 has the consequence, that rw semaphores on RT are not writer
+ * fair, but writers, which should be avoided in RT tasks (think mmap_sem),
+ * are subject to the rtmutex priority/DL inheritance mechanism.
+ *
+ * It's possible to make the rw semaphores writer fair by keeping a list of
+ * active readers. A blocked writer would force all newly incoming readers to
+ * block on the rtmutex, but the rtmutex would have to be proxy locked for one
+ * reader after the other. We can't use multi-reader inheritance because there
+ * is no way to support that with SCHED_DEADLINE. Implementing the one by one
+ * reader boosting/handover mechanism is a major surgery for a very dubious
+ * value.
+ *
+ * The risk of writer starvation is there, but the pathological use cases
+ * which trigger it are not necessarily the typical RT workloads.
+ */
+
+void __rwsem_init(struct rw_semaphore *sem, const char *name,
+		  struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held semaphore:
+	 */
+	debug_check_no_locks_freed((void *)sem, sizeof(*sem));
+	lockdep_init_map(&sem->dep_map, name, key, 0);
+#endif
+	atomic_set(&sem->readers, READER_BIAS);
+}
+EXPORT_SYMBOL(__rwsem_init);
+
+int __down_read_trylock(struct rw_semaphore *sem)
+{
+	int r, old;
+
+	/*
+	 * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is
+	 * set.
+	 */
+	for (r = atomic_read(&sem->readers); r < 0;) {
+		old = atomic_cmpxchg(&sem->readers, r, r + 1);
+		if (likely(old == r))
+			return 1;
+		r = old;
+	}
+	return 0;
+}
+
+static int __sched __down_read_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct rt_mutex_waiter waiter;
+	int ret;
+
+	if (__down_read_trylock(sem))
+		return 0;
+	/*
+	 * If rt_mutex blocks, the function sched_submit_work will not call
+	 * blk_schedule_flush_plug (because tsk_is_pi_blocked would be true).
+	 * We must call blk_schedule_flush_plug here, if we don't call it,
+	 * a deadlock in I/O may happen.
+	 */
+	if (unlikely(blk_needs_flush_plug(current)))
+		blk_schedule_flush_plug(current);
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Allow readers as long as the writer has not completely
+	 * acquired the semaphore for write.
+	 */
+	if (atomic_read(&sem->readers) != WRITER_BIAS) {
+		atomic_inc(&sem->readers);
+		raw_spin_unlock_irq(&m->wait_lock);
+		return 0;
+	}
+
+	/*
+	 * Call into the slow lock path with the rtmutex->wait_lock
+	 * held, so this can't result in the following race:
+	 *
+	 * Reader1		Reader2		Writer
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * down_read()
+	 * unlock(m->wait_lock)
+	 *			up_read()
+	 *			swake()
+	 *					lock(m->wait_lock)
+	 *					sem->writelocked=true
+	 *					unlock(m->wait_lock)
+	 *
+	 *					up_write()
+	 *					sem->writelocked=false
+	 *					rtmutex_unlock(m)
+	 *			down_read()
+	 *					down_write()
+	 *					rtmutex_lock(m)
+	 *					swait()
+	 * rtmutex_lock(m)
+	 *
+	 * That would put Reader1 behind the writer waiting on
+	 * Reader2 to call up_read() which might be unbound.
+	 */
+	rt_mutex_init_waiter(&waiter, false);
+	ret = rt_mutex_slowlock_locked(m, state, NULL, RT_MUTEX_MIN_CHAINWALK,
+				       NULL, &waiter);
+	/*
+	 * The slowlock() above is guaranteed to return with the rtmutex (for
+	 * ret = 0) is now held, so there can't be a writer active. Increment
+	 * the reader count and immediately drop the rtmutex again.
+	 * For ret != 0 we don't hold the rtmutex and need unlock the wait_lock.
+	 * We don't own the lock then.
+	 */
+	if (!ret)
+		atomic_inc(&sem->readers);
+	raw_spin_unlock_irq(&m->wait_lock);
+	if (!ret)
+		__rt_mutex_unlock(m);
+
+	debug_rt_mutex_free_waiter(&waiter);
+	return ret;
+}
+
+void __down_read(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_UNINTERRUPTIBLE);
+	WARN_ON_ONCE(ret);
+}
+
+int __down_read_killable(struct rw_semaphore *sem)
+{
+	int ret;
+
+	ret = __down_read_common(sem, TASK_KILLABLE);
+	if (likely(!ret))
+		return ret;
+	WARN_ONCE(ret != -EINTR, "Unexpected state: %d\n", ret);
+	return -EINTR;
+}
+
+void __up_read(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	struct task_struct *tsk;
+
+	/*
+	 * sem->readers can only hit 0 when a writer is waiting for the
+	 * active readers to leave the critical region.
+	 */
+	if (!atomic_dec_and_test(&sem->readers))
+		return;
+
+	might_sleep();
+	raw_spin_lock_irq(&m->wait_lock);
+	/*
+	 * Wake the writer, i.e. the rtmutex owner. It might release the
+	 * rtmutex concurrently in the fast path (due to a signal), but to
+	 * clean up the rwsem it needs to acquire m->wait_lock. The worst
+	 * case which can happen is a spurious wakeup.
+	 */
+	tsk = rt_mutex_owner(m);
+	if (tsk)
+		wake_up_process(tsk);
+
+	raw_spin_unlock_irq(&m->wait_lock);
+}
+
+static void __up_write_unlock(struct rw_semaphore *sem, int bias,
+			      unsigned long flags)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+
+	atomic_add(READER_BIAS - bias, &sem->readers);
+	raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+	__rt_mutex_unlock(m);
+}
+
+static int __sched __down_write_common(struct rw_semaphore *sem, int state)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	/* Take the rtmutex as a first step */
+	if (__rt_mutex_lock_state(m, state))
+		return -EINTR;
+
+	/* Force readers into slow path */
+	atomic_sub(READER_BIAS, &sem->readers);
+	might_sleep();
+
+	set_current_state(state);
+	for (;;) {
+		raw_spin_lock_irqsave(&m->wait_lock, flags);
+		/* Have all readers left the critical region? */
+		if (!atomic_read(&sem->readers)) {
+			atomic_set(&sem->readers, WRITER_BIAS);
+			__set_current_state(TASK_RUNNING);
+			raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+			return 0;
+		}
+
+		if (signal_pending_state(state, current)) {
+			__set_current_state(TASK_RUNNING);
+			__up_write_unlock(sem, 0, flags);
+			return -EINTR;
+		}
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+
+		if (atomic_read(&sem->readers) != 0) {
+			schedule();
+			set_current_state(state);
+		}
+	}
+}
+
+void __sched __down_write(struct rw_semaphore *sem)
+{
+	__down_write_common(sem, TASK_UNINTERRUPTIBLE);
+}
+
+int __sched __down_write_killable(struct rw_semaphore *sem)
+{
+	return __down_write_common(sem, TASK_KILLABLE);
+}
+
+int __down_write_trylock(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	if (!__rt_mutex_trylock(m))
+		return 0;
+
+	atomic_sub(READER_BIAS, &sem->readers);
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	if (!atomic_read(&sem->readers)) {
+		atomic_set(&sem->readers, WRITER_BIAS);
+		raw_spin_unlock_irqrestore(&m->wait_lock, flags);
+		return 1;
+	}
+	__up_write_unlock(sem, 0, flags);
+	return 0;
+}
+
+void __up_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	__up_write_unlock(sem, WRITER_BIAS, flags);
+}
+
+void __downgrade_write(struct rw_semaphore *sem)
+{
+	struct rt_mutex *m = &sem->rtmutex;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&m->wait_lock, flags);
+	/* Release it and account current as reader */
+	__up_write_unlock(sem, WRITER_BIAS - 1, flags);
+}
diff -Nur linux-5.4.5/kernel/locking/semaphore.c linux-5.4.5-new/kernel/locking/semaphore.c
--- linux-5.4.5/kernel/locking/semaphore.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/semaphore.c	2020-06-15 14:49:23.643895584 +0300
@@ -1,5 +1,10 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *
  * Copyright (c) 2008 Intel Corporation
  * Author: Matthew Wilcox <willy@linux.intel.com>
  *
@@ -37,6 +42,7 @@
 static noinline int __down_interruptible(struct semaphore *sem);
 static noinline int __down_killable(struct semaphore *sem);
 static noinline int __down_timeout(struct semaphore *sem, long timeout);
+static noinline int __down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout);
 static noinline void __up(struct semaphore *sem);
 
 /**
@@ -168,6 +174,22 @@
 }
 EXPORT_SYMBOL(down_timeout);
 
+int down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout)
+{
+	unsigned long flags;
+	int result = 0;
+
+	raw_spin_lock_irqsave(&sem->lock, flags);
+	if (likely(sem->count > 0))
+		sem->count--;
+	else
+		result = __down_hrtimeout(sem, timeout);
+	raw_spin_unlock_irqrestore(&sem->lock, flags);
+
+	return result;
+}
+EXPORT_SYMBOL(down_hrtimeout);
+
 /**
  * up - release the semaphore
  * @sem: the semaphore to release
@@ -232,6 +254,42 @@
 	return -EINTR;
 }
 
+/*
+ * Because this function is inlined, the 'state' parameter will be
+ * constant, and thus optimised away by the compiler.  Likewise the
+ * 'timeout' parameter for the cases without timeouts.
+ */
+static inline int __sched __down_common_hrtimeout(struct semaphore *sem, long state,
+								struct hrtimer_sleeper *timeout)
+{
+	struct semaphore_waiter waiter;
+
+	list_add_tail(&waiter.list, &sem->wait_list);
+	waiter.task = current;
+	waiter.up = false;
+
+	for (;;) {
+		if (signal_pending_state(state, current))
+			goto interrupted;
+		if (timeout && !timeout->task)
+			goto timed_out;
+		__set_current_state(state);
+		raw_spin_unlock_irq(&sem->lock);
+		schedule();
+		raw_spin_lock_irq(&sem->lock);
+		if (waiter.up)
+			return 0;
+	}
+
+ timed_out:
+	list_del(&waiter.list);
+	return -ETIME;
+
+ interrupted:
+	list_del(&waiter.list);
+	return -EINTR;
+}
+
 static noinline void __sched __down(struct semaphore *sem)
 {
 	__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
@@ -252,6 +310,11 @@
 	return __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);
 }
 
+static noinline int __sched __down_hrtimeout(struct semaphore *sem, struct hrtimer_sleeper *timeout)
+{
+	return __down_common_hrtimeout(sem, TASK_INTERRUPTIBLE, timeout);
+}
+
 static noinline void __sched __up(struct semaphore *sem)
 {
 	struct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,
diff -Nur linux-5.4.5/kernel/locking/spinlock.c linux-5.4.5-new/kernel/locking/spinlock.c
--- linux-5.4.5/kernel/locking/spinlock.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/spinlock.c	2020-06-15 16:12:25.743715749 +0300
@@ -124,8 +124,11 @@
  *         __[spin|read|write]_lock_bh()
  */
 BUILD_LOCK_OPS(spin, raw_spinlock);
+
+#ifndef CONFIG_PREEMPT_RT
 BUILD_LOCK_OPS(read, rwlock);
 BUILD_LOCK_OPS(write, rwlock);
+#endif
 
 #endif
 
@@ -209,6 +212,8 @@
 EXPORT_SYMBOL(_raw_spin_unlock_bh);
 #endif
 
+#ifndef CONFIG_PREEMPT_RT
+
 #ifndef CONFIG_INLINE_READ_TRYLOCK
 int __lockfunc _raw_read_trylock(rwlock_t *lock)
 {
@@ -353,6 +358,8 @@
 EXPORT_SYMBOL(_raw_write_unlock_bh);
 #endif
 
+#endif /* !PREEMPT_RT */
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
diff -Nur linux-5.4.5/kernel/locking/spinlock_debug.c linux-5.4.5-new/kernel/locking/spinlock_debug.c
--- linux-5.4.5/kernel/locking/spinlock_debug.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/locking/spinlock_debug.c	2020-06-15 16:12:25.743715749 +0300
@@ -31,6 +31,7 @@
 
 EXPORT_SYMBOL(__raw_spin_lock_init);
 
+#ifndef CONFIG_PREEMPT_RT
 void __rwlock_init(rwlock_t *lock, const char *name,
 		   struct lock_class_key *key)
 {
@@ -48,6 +49,7 @@
 }
 
 EXPORT_SYMBOL(__rwlock_init);
+#endif
 
 static void spin_dump(raw_spinlock_t *lock, const char *msg)
 {
@@ -139,6 +141,7 @@
 	arch_spin_unlock(&lock->raw_lock);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 static void rwlock_bug(rwlock_t *lock, const char *msg)
 {
 	if (!debug_locks_off())
@@ -228,3 +231,5 @@
 	debug_write_unlock(lock);
 	arch_write_unlock(&lock->raw_lock);
 }
+
+#endif
diff -Nur linux-5.4.5/kernel/panic.c linux-5.4.5-new/kernel/panic.c
--- linux-5.4.5/kernel/panic.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/panic.c	2020-06-15 16:12:26.139714355 +0300
@@ -237,7 +237,6 @@
 	 * Bypass the panic_cpu check and call __crash_kexec directly.
 	 */
 	if (!_crash_kexec_post_notifiers) {
-		printk_safe_flush_on_panic();
 		__crash_kexec(NULL);
 
 		/*
@@ -261,8 +260,6 @@
 	 */
 	atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
 
-	/* Call flush even twice. It tries harder with a single online CPU */
-	printk_safe_flush_on_panic();
 	kmsg_dump(KMSG_DUMP_PANIC);
 
 	/*
@@ -524,9 +521,11 @@
 
 static int init_oops_id(void)
 {
+#ifndef CONFIG_PREEMPT_RT
 	if (!oops_id)
 		get_random_bytes(&oops_id, sizeof(oops_id));
 	else
+#endif
 		oops_id++;
 
 	return 0;
diff -Nur linux-5.4.5/kernel/power/hibernate.c linux-5.4.5-new/kernel/power/hibernate.c
--- linux-5.4.5/kernel/power/hibernate.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/power/hibernate.c	2020-06-15 16:12:25.795715566 +0300
@@ -689,6 +689,10 @@
 	return error;
 }
 
+#ifndef CONFIG_SUSPEND
+bool pm_in_action;
+#endif
+
 /**
  * hibernate - Carry out system hibernation, including saving the image.
  */
@@ -702,6 +706,8 @@
 		return -EPERM;
 	}
 
+	pm_in_action = true;
+
 	lock_system_sleep();
 	/* The snapshot device should not be opened while we're running */
 	if (!atomic_add_unless(&snapshot_device_available, -1, 0)) {
@@ -778,6 +784,7 @@
 	atomic_inc(&snapshot_device_available);
  Unlock:
 	unlock_system_sleep();
+	pm_in_action = false;
 	pr_info("hibernation exit\n");
 
 	return error;
diff -Nur linux-5.4.5/kernel/power/suspend.c linux-5.4.5-new/kernel/power/suspend.c
--- linux-5.4.5/kernel/power/suspend.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/power/suspend.c	2020-06-15 16:12:25.795715566 +0300
@@ -594,6 +594,8 @@
 	return error;
 }
 
+bool pm_in_action;
+
 /**
  * pm_suspend - Externally visible function for suspending the system.
  * @state: System sleep state to enter.
@@ -608,6 +610,7 @@
 	if (state <= PM_SUSPEND_ON || state >= PM_SUSPEND_MAX)
 		return -EINVAL;
 
+	pm_in_action = true;
 	pr_info("suspend entry (%s)\n", mem_sleep_labels[state]);
 	error = enter_state(state);
 	if (error) {
@@ -617,6 +620,7 @@
 		suspend_stats.success++;
 	}
 	pr_info("suspend exit\n");
+	pm_in_action = false;
 	return error;
 }
 EXPORT_SYMBOL(pm_suspend);
diff -Nur linux-5.4.5/kernel/printk/internal.h linux-5.4.5-new/kernel/printk/internal.h
--- linux-5.4.5/kernel/printk/internal.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/printk/internal.h	1970-01-01 02:00:00.000000000 +0200
@@ -1,67 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * internal.h - printk internal definitions
- */
-#include <linux/percpu.h>
-
-#ifdef CONFIG_PRINTK
-
-#define PRINTK_SAFE_CONTEXT_MASK	 0x3fffffff
-#define PRINTK_NMI_DIRECT_CONTEXT_MASK	 0x40000000
-#define PRINTK_NMI_CONTEXT_MASK		 0x80000000
-
-extern raw_spinlock_t logbuf_lock;
-
-__printf(5, 0)
-int vprintk_store(int facility, int level,
-		  const char *dict, size_t dictlen,
-		  const char *fmt, va_list args);
-
-__printf(1, 0) int vprintk_default(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_deferred(const char *fmt, va_list args);
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args);
-void __printk_safe_enter(void);
-void __printk_safe_exit(void);
-
-#define printk_safe_enter_irqsave(flags)	\
-	do {					\
-		local_irq_save(flags);		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irqrestore(flags)	\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_restore(flags);	\
-	} while (0)
-
-#define printk_safe_enter_irq()		\
-	do {					\
-		local_irq_disable();		\
-		__printk_safe_enter();		\
-	} while (0)
-
-#define printk_safe_exit_irq()			\
-	do {					\
-		__printk_safe_exit();		\
-		local_irq_enable();		\
-	} while (0)
-
-void defer_console_output(void);
-
-#else
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args) { return 0; }
-
-/*
- * In !PRINTK builds we still export logbuf_lock spin_lock, console_sem
- * semaphore and some of console functions (console_unlock()/etc.), so
- * printk-safe must preserve the existing local IRQ guarantees.
- */
-#define printk_safe_enter_irqsave(flags) local_irq_save(flags)
-#define printk_safe_exit_irqrestore(flags) local_irq_restore(flags)
-
-#define printk_safe_enter_irq() local_irq_disable()
-#define printk_safe_exit_irq() local_irq_enable()
-
-#endif /* CONFIG_PRINTK */
diff -Nur linux-5.4.5/kernel/printk/Makefile linux-5.4.5-new/kernel/printk/Makefile
--- linux-5.4.5/kernel/printk/Makefile	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/printk/Makefile	2020-06-15 16:12:25.823715467 +0300
@@ -1,4 +1,3 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-y	= printk.o
-obj-$(CONFIG_PRINTK)	+= printk_safe.o
 obj-$(CONFIG_A11Y_BRAILLE_CONSOLE)	+= braille.o
diff -Nur linux-5.4.5/kernel/printk/printk.c linux-5.4.5-new/kernel/printk/printk.c
--- linux-5.4.5/kernel/printk/printk.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/printk/printk.c	2020-06-15 16:12:25.823715467 +0300
@@ -45,6 +45,9 @@
 #include <linux/irq_work.h>
 #include <linux/ctype.h>
 #include <linux/uio.h>
+#include <linux/kthread.h>
+#include <linux/clocksource.h>
+#include <linux/printk_ringbuffer.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
@@ -58,13 +61,13 @@
 
 #include "console_cmdline.h"
 #include "braille.h"
-#include "internal.h"
 
-int console_printk[4] = {
+int console_printk[5] = {
 	CONSOLE_LOGLEVEL_DEFAULT,	/* console_loglevel */
 	MESSAGE_LOGLEVEL_DEFAULT,	/* default_message_loglevel */
 	CONSOLE_LOGLEVEL_MIN,		/* minimum_console_loglevel */
 	CONSOLE_LOGLEVEL_DEFAULT,	/* default_console_loglevel */
+	CONSOLE_LOGLEVEL_EMERGENCY,	/* emergency_console_loglevel */
 };
 EXPORT_SYMBOL_GPL(console_printk);
 
@@ -225,19 +228,7 @@
 
 static int __down_trylock_console_sem(unsigned long ip)
 {
-	int lock_failed;
-	unsigned long flags;
-
-	/*
-	 * Here and in __up_console_sem() we need to be in safe mode,
-	 * because spindump/WARN/etc from under console ->lock will
-	 * deadlock in printk()->down_trylock_console_sem() otherwise.
-	 */
-	printk_safe_enter_irqsave(flags);
-	lock_failed = down_trylock(&console_sem);
-	printk_safe_exit_irqrestore(flags);
-
-	if (lock_failed)
+	if (down_trylock(&console_sem))
 		return 1;
 	mutex_acquire(&console_lock_dep_map, 0, 1, ip);
 	return 0;
@@ -246,13 +237,9 @@
 
 static void __up_console_sem(unsigned long ip)
 {
-	unsigned long flags;
-
 	mutex_release(&console_lock_dep_map, 1, ip);
 
-	printk_safe_enter_irqsave(flags);
 	up(&console_sem);
-	printk_safe_exit_irqrestore(flags);
 }
 #define up_console_sem() __up_console_sem(_RET_IP_)
 
@@ -267,11 +254,6 @@
 static int console_locked, console_suspended;
 
 /*
- * If exclusive_console is non-NULL then only this console is to be printed to.
- */
-static struct console *exclusive_console;
-
-/*
  *	Array of consoles built from command line options (console=)
  */
 
@@ -367,6 +349,7 @@
 
 struct printk_log {
 	u64 ts_nsec;		/* timestamp in nanoseconds */
+	u16 cpu;		/* cpu that generated record */
 	u16 len;		/* length of entire record */
 	u16 text_len;		/* length of text buffer */
 	u16 dict_len;		/* length of dictionary buffer */
@@ -382,65 +365,22 @@
 #endif
 ;
 
-/*
- * The logbuf_lock protects kmsg buffer, indices, counters.  This can be taken
- * within the scheduler's rq lock. It must be released before calling
- * console_unlock() or anything else that might wake up a process.
- */
-DEFINE_RAW_SPINLOCK(logbuf_lock);
-
-/*
- * Helper macros to lock/unlock logbuf_lock and switch between
- * printk-safe/unsafe modes.
- */
-#define logbuf_lock_irq()				\
-	do {						\
-		printk_safe_enter_irq();		\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irq()				\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irq();			\
-	} while (0)
-
-#define logbuf_lock_irqsave(flags)			\
-	do {						\
-		printk_safe_enter_irqsave(flags);	\
-		raw_spin_lock(&logbuf_lock);		\
-	} while (0)
-
-#define logbuf_unlock_irqrestore(flags)		\
-	do {						\
-		raw_spin_unlock(&logbuf_lock);		\
-		printk_safe_exit_irqrestore(flags);	\
-	} while (0)
+DECLARE_STATIC_PRINTKRB_CPULOCK(printk_cpulock);
 
 #ifdef CONFIG_PRINTK
-DECLARE_WAIT_QUEUE_HEAD(log_wait);
-/* the next printk record to read by syslog(READ) or /proc/kmsg */
+/* record buffer */
+DECLARE_STATIC_PRINTKRB(printk_rb, CONFIG_LOG_BUF_SHIFT, &printk_cpulock);
+
+static DEFINE_MUTEX(syslog_lock);
+DECLARE_STATIC_PRINTKRB_ITER(syslog_iter, &printk_rb);
+
+/* the last printk record to read by syslog(READ) or /proc/kmsg */
 static u64 syslog_seq;
-static u32 syslog_idx;
 static size_t syslog_partial;
 static bool syslog_time;
 
-/* index and sequence number of the first record stored in the buffer */
-static u64 log_first_seq;
-static u32 log_first_idx;
-
-/* index and sequence number of the next record to store in the buffer */
-static u64 log_next_seq;
-static u32 log_next_idx;
-
-/* the next printk record to write to the console */
-static u64 console_seq;
-static u32 console_idx;
-static u64 exclusive_console_stop_seq;
-
 /* the next printk record to read after the last 'clear' command */
 static u64 clear_seq;
-static u32 clear_idx;
 
 #ifdef CONFIG_PRINTK_CALLER
 #define PREFIX_MAX		48
@@ -452,24 +392,16 @@
 #define LOG_LEVEL(v)		((v) & 0x07)
 #define LOG_FACILITY(v)		((v) >> 3 & 0xff)
 
-/* record buffer */
-#define LOG_ALIGN __alignof__(struct printk_log)
-#define __LOG_BUF_LEN (1 << CONFIG_LOG_BUF_SHIFT)
-#define LOG_BUF_LEN_MAX (u32)(1 << 31)
-static char __log_buf[__LOG_BUF_LEN] __aligned(LOG_ALIGN);
-static char *log_buf = __log_buf;
-static u32 log_buf_len = __LOG_BUF_LEN;
-
 /* Return log buffer address */
 char *log_buf_addr_get(void)
 {
-	return log_buf;
+	return printk_rb.buffer;
 }
 
 /* Return log buffer size */
 u32 log_buf_len_get(void)
 {
-	return log_buf_len;
+	return (1 << printk_rb.size_bits);
 }
 
 /* human readable text of the record */
@@ -484,180 +416,50 @@
 	return (char *)msg + sizeof(struct printk_log) + msg->text_len;
 }
 
-/* get record by index; idx must point to valid msg */
-static struct printk_log *log_from_idx(u32 idx)
-{
-	struct printk_log *msg = (struct printk_log *)(log_buf + idx);
-
-	/*
-	 * A length == 0 record is the end of buffer marker. Wrap around and
-	 * read the message at the start of the buffer.
-	 */
-	if (!msg->len)
-		return (struct printk_log *)log_buf;
-	return msg;
-}
-
-/* get next record; idx must point to valid msg */
-static u32 log_next(u32 idx)
-{
-	struct printk_log *msg = (struct printk_log *)(log_buf + idx);
-
-	/* length == 0 indicates the end of the buffer; wrap */
-	/*
-	 * A length == 0 record is the end of buffer marker. Wrap around and
-	 * read the message at the start of the buffer as *this* one, and
-	 * return the one after that.
-	 */
-	if (!msg->len) {
-		msg = (struct printk_log *)log_buf;
-		return msg->len;
-	}
-	return idx + msg->len;
-}
-
-/*
- * Check whether there is enough free space for the given message.
- *
- * The same values of first_idx and next_idx mean that the buffer
- * is either empty or full.
- *
- * If the buffer is empty, we must respect the position of the indexes.
- * They cannot be reset to the beginning of the buffer.
- */
-static int logbuf_has_space(u32 msg_size, bool empty)
-{
-	u32 free;
-
-	if (log_next_idx > log_first_idx || empty)
-		free = max(log_buf_len - log_next_idx, log_first_idx);
-	else
-		free = log_first_idx - log_next_idx;
-
-	/*
-	 * We need space also for an empty header that signalizes wrapping
-	 * of the buffer.
-	 */
-	return free >= msg_size + sizeof(struct printk_log);
-}
-
-static int log_make_free_space(u32 msg_size)
-{
-	while (log_first_seq < log_next_seq &&
-	       !logbuf_has_space(msg_size, false)) {
-		/* drop old messages until we have enough contiguous space */
-		log_first_idx = log_next(log_first_idx);
-		log_first_seq++;
-	}
-
-	if (clear_seq < log_first_seq) {
-		clear_seq = log_first_seq;
-		clear_idx = log_first_idx;
-	}
-
-	/* sequence numbers are equal, so the log buffer is empty */
-	if (logbuf_has_space(msg_size, log_first_seq == log_next_seq))
-		return 0;
-
-	return -ENOMEM;
-}
-
-/* compute the message size including the padding bytes */
-static u32 msg_used_size(u16 text_len, u16 dict_len, u32 *pad_len)
-{
-	u32 size;
-
-	size = sizeof(struct printk_log) + text_len + dict_len;
-	*pad_len = (-size) & (LOG_ALIGN - 1);
-	size += *pad_len;
-
-	return size;
-}
-
-/*
- * Define how much of the log buffer we could take at maximum. The value
- * must be greater than two. Note that only half of the buffer is available
- * when the index points to the middle.
- */
-#define MAX_LOG_TAKE_PART 4
-static const char trunc_msg[] = "<truncated>";
-
-static u32 truncate_msg(u16 *text_len, u16 *trunc_msg_len,
-			u16 *dict_len, u32 *pad_len)
-{
-	/*
-	 * The message should not take the whole buffer. Otherwise, it might
-	 * get removed too soon.
-	 */
-	u32 max_text_len = log_buf_len / MAX_LOG_TAKE_PART;
-	if (*text_len > max_text_len)
-		*text_len = max_text_len;
-	/* enable the warning message */
-	*trunc_msg_len = strlen(trunc_msg);
-	/* disable the "dict" completely */
-	*dict_len = 0;
-	/* compute the size again, count also the warning message */
-	return msg_used_size(*text_len + *trunc_msg_len, 0, pad_len);
-}
+static void printk_emergency(char *buffer, int level, u64 ts_nsec, u16 cpu,
+			     char *text, u16 text_len);
 
 /* insert record into the buffer, discard old ones, update heads */
 static int log_store(u32 caller_id, int facility, int level,
-		     enum log_flags flags, u64 ts_nsec,
+		     enum log_flags flags, u64 ts_nsec, u16 cpu,
 		     const char *dict, u16 dict_len,
 		     const char *text, u16 text_len)
 {
 	struct printk_log *msg;
-	u32 size, pad_len;
-	u16 trunc_msg_len = 0;
-
-	/* number of '\0' padding bytes to next message */
-	size = msg_used_size(text_len, dict_len, &pad_len);
+	struct prb_handle h;
+	char *rbuf;
+	u32 size;
 
-	if (log_make_free_space(size)) {
-		/* truncate the message if it is too long for empty buffer */
-		size = truncate_msg(&text_len, &trunc_msg_len,
-				    &dict_len, &pad_len);
-		/* survive when the log buffer is too small for trunc_msg */
-		if (log_make_free_space(size))
-			return 0;
-	}
+	size = sizeof(*msg) + text_len + dict_len;
 
-	if (log_next_idx + size + sizeof(struct printk_log) > log_buf_len) {
+	rbuf = prb_reserve(&h, &printk_rb, size);
+	if (!rbuf) {
 		/*
-		 * This message + an additional empty header does not fit
-		 * at the end of the buffer. Add an empty header with len == 0
-		 * to signify a wrap around.
+		 * An emergency message would have been printed, but
+		 * it cannot be stored in the log.
 		 */
-		memset(log_buf + log_next_idx, 0, sizeof(struct printk_log));
-		log_next_idx = 0;
+		prb_inc_lost(&printk_rb);
+		return 0;
 	}
 
 	/* fill message */
-	msg = (struct printk_log *)(log_buf + log_next_idx);
+	msg = (struct printk_log *)rbuf;
 	memcpy(log_text(msg), text, text_len);
 	msg->text_len = text_len;
-	if (trunc_msg_len) {
-		memcpy(log_text(msg) + text_len, trunc_msg, trunc_msg_len);
-		msg->text_len += trunc_msg_len;
-	}
 	memcpy(log_dict(msg), dict, dict_len);
 	msg->dict_len = dict_len;
 	msg->facility = facility;
 	msg->level = level & 7;
 	msg->flags = flags & 0x1f;
-	if (ts_nsec > 0)
-		msg->ts_nsec = ts_nsec;
-	else
-		msg->ts_nsec = local_clock();
+	msg->ts_nsec = ts_nsec;
 #ifdef CONFIG_PRINTK_CALLER
 	msg->caller_id = caller_id;
 #endif
-	memset(log_dict(msg) + dict_len, 0, pad_len);
+	msg->cpu = cpu;
 	msg->len = size;
 
 	/* insert message */
-	log_next_idx += msg->len;
-	log_next_seq++;
+	prb_commit(&h);
 
 	return msg->text_len;
 }
@@ -727,9 +529,9 @@
 
 	do_div(ts_usec, 1000);
 
-	return scnprintf(buf, size, "%u,%llu,%llu,%c%s;",
+	return scnprintf(buf, size, "%u,%llu,%llu,%c%s,%hu;",
 			 (msg->facility << 3) | msg->level, seq, ts_usec,
-			 msg->flags & LOG_CONT ? 'c' : '-', caller);
+			 msg->flags & LOG_CONT ? 'c' : '-', caller, msg->cpu);
 }
 
 static ssize_t msg_print_ext_body(char *buf, size_t size,
@@ -780,13 +582,18 @@
 	return p - buf;
 }
 
+#define PRINTK_SPRINT_MAX (LOG_LINE_MAX + PREFIX_MAX)
+#define PRINTK_RECORD_MAX (sizeof(struct printk_log) + \
+				CONSOLE_EXT_LOG_MAX + PRINTK_SPRINT_MAX)
+
 /* /dev/kmsg - userspace message inject/listen interface */
 struct devkmsg_user {
 	u64 seq;
-	u32 idx;
+	struct prb_iterator iter;
 	struct ratelimit_state rs;
 	struct mutex lock;
 	char buf[CONSOLE_EXT_LOG_MAX];
+	char msgbuf[PRINTK_RECORD_MAX];
 };
 
 static __printf(3, 4) __cold
@@ -869,9 +676,11 @@
 			    size_t count, loff_t *ppos)
 {
 	struct devkmsg_user *user = file->private_data;
+	struct prb_iterator backup_iter;
 	struct printk_log *msg;
-	size_t len;
 	ssize_t ret;
+	size_t len;
+	u64 seq;
 
 	if (!user)
 		return -EBADF;
@@ -880,52 +689,63 @@
 	if (ret)
 		return ret;
 
-	logbuf_lock_irq();
-	while (user->seq == log_next_seq) {
-		if (file->f_flags & O_NONBLOCK) {
-			ret = -EAGAIN;
-			logbuf_unlock_irq();
-			goto out;
-		}
+	/* make a backup copy in case there is a problem */
+	prb_iter_copy(&backup_iter, &user->iter);
 
-		logbuf_unlock_irq();
-		ret = wait_event_interruptible(log_wait,
-					       user->seq != log_next_seq);
-		if (ret)
-			goto out;
-		logbuf_lock_irq();
+	if (file->f_flags & O_NONBLOCK) {
+		ret = prb_iter_next(&user->iter, &user->msgbuf[0],
+				      sizeof(user->msgbuf), &seq);
+	} else {
+		ret = prb_iter_wait_next(&user->iter, &user->msgbuf[0],
+					   sizeof(user->msgbuf), &seq);
 	}
-
-	if (user->seq < log_first_seq) {
-		/* our last seen message is gone, return error and reset */
-		user->idx = log_first_idx;
-		user->seq = log_first_seq;
+	if (ret == 0) {
+		/* end of list */
+		ret = -EAGAIN;
+		goto out;
+	} else if (ret == -EINVAL) {
+		/* iterator invalid, return error and reset */
 		ret = -EPIPE;
-		logbuf_unlock_irq();
+		prb_iter_init(&user->iter, &printk_rb, &user->seq);
 		goto out;
+	} else if (ret < 0) {
+		/* interrupted by signal */
+		goto out;
+	}
+
+	user->seq++;
+	if (user->seq < seq) {
+		ret = -EPIPE;
+		goto restore_out;
 	}
 
-	msg = log_from_idx(user->idx);
+	msg = (struct printk_log *)&user->msgbuf[0];
 	len = msg_print_ext_header(user->buf, sizeof(user->buf),
 				   msg, user->seq);
 	len += msg_print_ext_body(user->buf + len, sizeof(user->buf) - len,
 				  log_dict(msg), msg->dict_len,
 				  log_text(msg), msg->text_len);
 
-	user->idx = log_next(user->idx);
-	user->seq++;
-	logbuf_unlock_irq();
-
 	if (len > count) {
 		ret = -EINVAL;
-		goto out;
+		goto restore_out;
 	}
 
 	if (copy_to_user(buf, user->buf, len)) {
 		ret = -EFAULT;
-		goto out;
+		goto restore_out;
 	}
+
 	ret = len;
+	goto out;
+restore_out:
+	/*
+	 * There was an error, but this message should not be
+	 * lost because of it. Restore the backup and setup
+	 * seq so that it will work with the next read.
+	 */
+	prb_iter_copy(&user->iter, &backup_iter);
+	user->seq = seq - 1;
 out:
 	mutex_unlock(&user->lock);
 	return ret;
@@ -934,19 +754,22 @@
 static loff_t devkmsg_llseek(struct file *file, loff_t offset, int whence)
 {
 	struct devkmsg_user *user = file->private_data;
-	loff_t ret = 0;
+	loff_t ret;
+	u64 seq;
 
 	if (!user)
 		return -EBADF;
 	if (offset)
 		return -ESPIPE;
 
-	logbuf_lock_irq();
+	ret = mutex_lock_interruptible(&user->lock);
+	if (ret)
+		return ret;
+
 	switch (whence) {
 	case SEEK_SET:
 		/* the first record */
-		user->idx = log_first_idx;
-		user->seq = log_first_seq;
+		prb_iter_init(&user->iter, &printk_rb, &user->seq);
 		break;
 	case SEEK_DATA:
 		/*
@@ -954,40 +777,87 @@
 		 * like issued by 'dmesg -c'. Reading /dev/kmsg itself
 		 * changes no global state, and does not clear anything.
 		 */
-		user->idx = clear_idx;
-		user->seq = clear_seq;
+		for (;;) {
+			prb_iter_init(&user->iter, &printk_rb, &seq);
+			ret = prb_iter_seek(&user->iter, clear_seq);
+			if (ret > 0) {
+				/* seeked to clear seq */
+				user->seq = clear_seq;
+				break;
+			} else if (ret == 0) {
+				/*
+				 * The end of the list was hit without
+				 * ever seeing the clear seq. Just
+				 * seek to the beginning of the list.
+				 */
+				prb_iter_init(&user->iter, &printk_rb,
+						&user->seq);
+				break;
+			}
+			/* iterator invalid, start over */
+
+			/* reset clear_seq if it is no longer available */
+			if (seq > clear_seq)
+				clear_seq = 0;
+		}
+		ret = 0;
 		break;
 	case SEEK_END:
 		/* after the last record */
-		user->idx = log_next_idx;
-		user->seq = log_next_seq;
+		for (;;) {
+			ret = prb_iter_next(&user->iter, NULL, 0, &user->seq);
+			if (ret == 0)
+				break;
+			else if (ret > 0)
+				continue;
+			/* iterator invalid, start over */
+			prb_iter_init(&user->iter, &printk_rb, &user->seq);
+		}
+		ret = 0;
 		break;
 	default:
 		ret = -EINVAL;
 	}
-	logbuf_unlock_irq();
+
+	mutex_unlock(&user->lock);
 	return ret;
 }
 
+struct wait_queue_head *printk_wait_queue(void)
+{
+	/* FIXME: using prb internals! */
+	return printk_rb.wq;
+}
+
 static __poll_t devkmsg_poll(struct file *file, poll_table *wait)
 {
 	struct devkmsg_user *user = file->private_data;
+	struct prb_iterator iter;
 	__poll_t ret = 0;
+	int rbret;
+	u64 seq;
 
 	if (!user)
 		return EPOLLERR|EPOLLNVAL;
 
-	poll_wait(file, &log_wait, wait);
+	poll_wait(file, printk_wait_queue(), wait);
 
-	logbuf_lock_irq();
-	if (user->seq < log_next_seq) {
-		/* return error when data has vanished underneath us */
-		if (user->seq < log_first_seq)
-			ret = EPOLLIN|EPOLLRDNORM|EPOLLERR|EPOLLPRI;
-		else
-			ret = EPOLLIN|EPOLLRDNORM;
-	}
-	logbuf_unlock_irq();
+	mutex_lock(&user->lock);
+
+	/* use copy so no actual iteration takes place */
+	prb_iter_copy(&iter, &user->iter);
+
+	rbret = prb_iter_next(&iter, &user->msgbuf[0],
+				sizeof(user->msgbuf), &seq);
+	if (rbret == 0)
+		goto out;
+
+	ret = EPOLLIN|EPOLLRDNORM;
+
+	if (rbret < 0 || (seq - user->seq) != 1)
+		ret |= EPOLLERR|EPOLLPRI;
+out:
+	mutex_unlock(&user->lock);
 
 	return ret;
 }
@@ -1017,10 +887,7 @@
 
 	mutex_init(&user->lock);
 
-	logbuf_lock_irq();
-	user->idx = log_first_idx;
-	user->seq = log_first_seq;
-	logbuf_unlock_irq();
+	prb_iter_init(&user->iter, &printk_rb, &user->seq);
 
 	file->private_data = user;
 	return 0;
@@ -1060,11 +927,6 @@
  */
 void log_buf_vmcoreinfo_setup(void)
 {
-	VMCOREINFO_SYMBOL(log_buf);
-	VMCOREINFO_SYMBOL(log_buf_len);
-	VMCOREINFO_SYMBOL(log_first_idx);
-	VMCOREINFO_SYMBOL(clear_idx);
-	VMCOREINFO_SYMBOL(log_next_idx);
 	/*
 	 * Export struct printk_log size and field offsets. User space tools can
 	 * parse it and detect any changes to structure down the line.
@@ -1080,6 +942,8 @@
 }
 #endif
 
+/* FIXME: no support for buffer resizing */
+#if 0
 /* requested log_buf_len from kernel cmdline */
 static unsigned long __initdata new_log_buf_len;
 
@@ -1145,9 +1009,12 @@
 #else /* !CONFIG_SMP */
 static inline void log_buf_add_cpu(void) {}
 #endif /* CONFIG_SMP */
+#endif /* 0 */
 
 void __init setup_log_buf(int early)
 {
+/* FIXME: no support for buffer resizing */
+#if 0
 	unsigned long flags;
 	char *new_log_buf;
 	unsigned int free;
@@ -1179,6 +1046,7 @@
 	pr_info("log_buf_len: %u bytes\n", log_buf_len);
 	pr_info("early log buf free: %u(%u%%)\n",
 		free, (free * 100) / __LOG_BUF_LEN);
+#endif
 }
 
 static bool __read_mostly ignore_loglevel;
@@ -1259,6 +1127,11 @@
 static bool printk_time = IS_ENABLED(CONFIG_PRINTK_TIME);
 module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);
 
+static size_t print_cpu(u16 cpu, char *buf)
+{
+	return sprintf(buf, "%03hu: ", cpu);
+}
+
 static size_t print_syslog(unsigned int level, char *buf)
 {
 	return sprintf(buf, "<%u>", level);
@@ -1302,6 +1175,7 @@
 		buf[len++] = ' ';
 		buf[len] = '\0';
 	}
+	len += print_cpu(msg->cpu, buf + len);
 
 	return len;
 }
@@ -1347,30 +1221,42 @@
 	return len;
 }
 
-static int syslog_print(char __user *buf, int size)
+static int syslog_print(char __user *buf, int size, char *text,
+			char *msgbuf, int *locked)
 {
-	char *text;
+	struct prb_iterator iter;
 	struct printk_log *msg;
 	int len = 0;
-
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
-	if (!text)
-		return -ENOMEM;
+	u64 seq;
+	int ret;
 
 	while (size > 0) {
 		size_t n;
 		size_t skip;
 
-		logbuf_lock_irq();
-		if (syslog_seq < log_first_seq) {
-			/* messages are gone, move to first one */
-			syslog_seq = log_first_seq;
-			syslog_idx = log_first_idx;
-			syslog_partial = 0;
+		for (;;) {
+			prb_iter_copy(&iter, &syslog_iter);
+			ret = prb_iter_next(&iter, msgbuf,
+					    PRINTK_RECORD_MAX, &seq);
+			if (ret < 0) {
+				/* messages are gone, move to first one */
+				prb_iter_init(&syslog_iter, &printk_rb,
+					      &syslog_seq);
+				syslog_partial = 0;
+				continue;
+			}
+			break;
 		}
-		if (syslog_seq == log_next_seq) {
-			logbuf_unlock_irq();
+		if (ret == 0)
 			break;
+
+		/*
+		 * If messages have been missed, the partial tracker
+		 * is no longer valid and must be reset.
+		 */
+		if (syslog_seq > 0 && seq - 1 != syslog_seq) {
+			syslog_seq = seq - 1;
+			syslog_partial = 0;
 		}
 
 		/*
@@ -1380,131 +1266,215 @@
 		if (!syslog_partial)
 			syslog_time = printk_time;
 
+		msg = (struct printk_log *)msgbuf;
+
 		skip = syslog_partial;
-		msg = log_from_idx(syslog_idx);
 		n = msg_print_text(msg, true, syslog_time, text,
-				   LOG_LINE_MAX + PREFIX_MAX);
+				   PRINTK_SPRINT_MAX);
 		if (n - syslog_partial <= size) {
 			/* message fits into buffer, move forward */
-			syslog_idx = log_next(syslog_idx);
-			syslog_seq++;
+			prb_iter_next(&syslog_iter, NULL, 0, &syslog_seq);
 			n -= syslog_partial;
 			syslog_partial = 0;
-		} else if (!len){
+		} else if (!len) {
 			/* partial read(), remember position */
 			n = size;
 			syslog_partial += n;
 		} else
 			n = 0;
-		logbuf_unlock_irq();
 
 		if (!n)
 			break;
 
+		mutex_unlock(&syslog_lock);
 		if (copy_to_user(buf, text + skip, n)) {
 			if (!len)
 				len = -EFAULT;
+			*locked = 0;
 			break;
 		}
+		ret = mutex_lock_interruptible(&syslog_lock);
 
 		len += n;
 		size -= n;
 		buf += n;
+
+		if (ret) {
+			if (!len)
+				len = ret;
+			*locked = 0;
+			break;
+		}
 	}
 
-	kfree(text);
 	return len;
 }
 
-static int syslog_print_all(char __user *buf, int size, bool clear)
+static int count_remaining(struct prb_iterator *iter, u64 until_seq,
+			   char *msgbuf, int size, bool records, bool time)
 {
-	char *text;
+	struct prb_iterator local_iter;
+	struct printk_log *msg;
 	int len = 0;
-	u64 next_seq;
 	u64 seq;
-	u32 idx;
+	int ret;
+
+	prb_iter_copy(&local_iter, iter);
+	for (;;) {
+		ret = prb_iter_next(&local_iter, msgbuf, size, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/* the iter is invalid, restart from head */
+			prb_iter_init(&local_iter, &printk_rb, NULL);
+			len = 0;
+			continue;
+		}
+
+		if (until_seq && seq >= until_seq)
+			break;
+
+		if (records) {
+			len++;
+		} else {
+			msg = (struct printk_log *)msgbuf;
+			len += msg_print_text(msg, true, time, NULL, 0);
+		}
+	}
+
+	return len;
+}
+
+static void syslog_clear(void)
+{
+	struct prb_iterator iter;
+	int ret;
+
+	prb_iter_init(&iter, &printk_rb, &clear_seq);
+	for (;;) {
+		ret = prb_iter_next(&iter, NULL, 0, &clear_seq);
+		if (ret == 0)
+			break;
+		else if (ret < 0)
+			prb_iter_init(&iter, &printk_rb, &clear_seq);
+	}
+}
+
+static int syslog_print_all(char __user *buf, int size, bool clear)
+{
+	struct prb_iterator iter;
+	struct printk_log *msg;
+	char *msgbuf = NULL;
+	char *text = NULL;
+	int textlen;
+	u64 seq = 0;
+	int len = 0;
 	bool time;
+	int ret;
 
-	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
 	if (!text)
 		return -ENOMEM;
+	msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!msgbuf) {
+		kfree(text);
+		return -ENOMEM;
+	}
 
 	time = printk_time;
-	logbuf_lock_irq();
+
 	/*
-	 * Find first record that fits, including all following records,
-	 * into the user-provided buffer for this dump.
+	 * Setup iter to last event before clear. Clear may
+	 * be lost, but keep going with a best effort.
 	 */
-	seq = clear_seq;
-	idx = clear_idx;
-	while (seq < log_next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
-
-		len += msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
-	}
-
-	/* move first record forward until length fits into the buffer */
-	seq = clear_seq;
-	idx = clear_idx;
-	while (len > size && seq < log_next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	prb_iter_init(&iter, &printk_rb, NULL);
+	prb_iter_seek(&iter, clear_seq);
+
+	/* count the total bytes after clear */
+	len = count_remaining(&iter, 0, msgbuf, PRINTK_RECORD_MAX,
+			      false, time);
+
+	/* move iter forward until length fits into the buffer */
+	while (len > size) {
+		ret = prb_iter_next(&iter, msgbuf,
+				    PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * The iter is now invalid so clear will
+			 * also be invalid. Restart from the head.
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			len = count_remaining(&iter, 0, msgbuf,
+					      PRINTK_RECORD_MAX, false, time);
+			continue;
+		}
 
+		msg = (struct printk_log *)msgbuf;
 		len -= msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
-	}
 
-	/* last message fitting into this dump */
-	next_seq = log_next_seq;
+		if (clear)
+			clear_seq = seq;
+	}
 
+	/* copy messages to buffer */
 	len = 0;
-	while (len >= 0 && seq < next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
-		int textlen = msg_print_text(msg, true, time, text,
-					     LOG_LINE_MAX + PREFIX_MAX);
+	while (len >= 0 && len < size) {
+		if (clear)
+			clear_seq = seq;
+
+		ret = prb_iter_next(&iter, msgbuf,
+				    PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * The iter is now invalid. Make a best
+			 * effort to grab the rest of the log
+			 * from the new head.
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
+
+		msg = (struct printk_log *)msgbuf;
+		textlen = msg_print_text(msg, true, time, text,
+					 PRINTK_SPRINT_MAX);
+		if (textlen < 0) {
+			len = textlen;
+			break;
+		}
 
-		idx = log_next(idx);
-		seq++;
+		if (len + textlen > size)
+			break;
 
-		logbuf_unlock_irq();
 		if (copy_to_user(buf + len, text, textlen))
 			len = -EFAULT;
 		else
 			len += textlen;
-		logbuf_lock_irq();
-
-		if (seq < log_first_seq) {
-			/* messages are gone, move to next one */
-			seq = log_first_seq;
-			idx = log_first_idx;
-		}
 	}
 
-	if (clear) {
-		clear_seq = log_next_seq;
-		clear_idx = log_next_idx;
-	}
-	logbuf_unlock_irq();
+	if (clear && !seq)
+		syslog_clear();
 
-	kfree(text);
+	if (text)
+		kfree(text);
+	if (msgbuf)
+		kfree(msgbuf);
 	return len;
 }
 
-static void syslog_clear(void)
-{
-	logbuf_lock_irq();
-	clear_seq = log_next_seq;
-	clear_idx = log_next_idx;
-	logbuf_unlock_irq();
-}
-
 int do_syslog(int type, char __user *buf, int len, int source)
 {
 	bool clear = false;
 	static int saved_console_loglevel = LOGLEVEL_DEFAULT;
+	struct prb_iterator iter;
+	char *msgbuf = NULL;
+	char *text = NULL;
+	int locked;
 	int error;
+	int ret;
 
 	error = check_syslog_permissions(type, source);
 	if (error)
@@ -1522,11 +1492,49 @@
 			return 0;
 		if (!access_ok(buf, len))
 			return -EFAULT;
-		error = wait_event_interruptible(log_wait,
-						 syslog_seq != log_next_seq);
+
+		text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+		msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+		if (!text || !msgbuf) {
+			error = -ENOMEM;
+			goto out;
+		}
+
+		error = mutex_lock_interruptible(&syslog_lock);
 		if (error)
-			return error;
-		error = syslog_print(buf, len);
+			goto out;
+
+		/*
+		 * Wait until a first message is available. Use a copy
+		 * because no iteration should occur for syslog now.
+		 */
+		for (;;) {
+			prb_iter_copy(&iter, &syslog_iter);
+
+			mutex_unlock(&syslog_lock);
+			ret = prb_iter_wait_next(&iter, NULL, 0, NULL);
+			if (ret == -ERESTARTSYS) {
+				error = ret;
+				goto out;
+			}
+			error = mutex_lock_interruptible(&syslog_lock);
+			if (error)
+				goto out;
+
+			if (ret == -EINVAL) {
+				prb_iter_init(&syslog_iter, &printk_rb,
+					      &syslog_seq);
+				syslog_partial = 0;
+				continue;
+			}
+			break;
+		}
+
+		/* print as much as will fit in the user buffer */
+		locked = 1;
+		error = syslog_print(buf, len, text, msgbuf, &locked);
+		if (locked)
+			mutex_unlock(&syslog_lock);
 		break;
 	/* Read/clear last kernel messages */
 	case SYSLOG_ACTION_READ_CLEAR:
@@ -1571,47 +1579,45 @@
 		break;
 	/* Number of chars in the log buffer */
 	case SYSLOG_ACTION_SIZE_UNREAD:
-		logbuf_lock_irq();
-		if (syslog_seq < log_first_seq) {
-			/* messages are gone, move to first one */
-			syslog_seq = log_first_seq;
-			syslog_idx = log_first_idx;
-			syslog_partial = 0;
-		}
+		msgbuf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+		if (!msgbuf)
+			return -ENOMEM;
+
+		error = mutex_lock_interruptible(&syslog_lock);
+		if (error)
+			goto out;
+
 		if (source == SYSLOG_FROM_PROC) {
 			/*
 			 * Short-cut for poll(/"proc/kmsg") which simply checks
 			 * for pending data, not the size; return the count of
 			 * records, not the length.
 			 */
-			error = log_next_seq - syslog_seq;
+			error = count_remaining(&syslog_iter, 0, msgbuf,
+						PRINTK_RECORD_MAX, true,
+						printk_time);
 		} else {
-			u64 seq = syslog_seq;
-			u32 idx = syslog_idx;
-			bool time = syslog_partial ? syslog_time : printk_time;
-
-			while (seq < log_next_seq) {
-				struct printk_log *msg = log_from_idx(idx);
-
-				error += msg_print_text(msg, true, time, NULL,
-							0);
-				time = printk_time;
-				idx = log_next(idx);
-				seq++;
-			}
+			error = count_remaining(&syslog_iter, 0, msgbuf,
+						PRINTK_RECORD_MAX, false,
+						printk_time);
 			error -= syslog_partial;
 		}
-		logbuf_unlock_irq();
+
+		mutex_unlock(&syslog_lock);
 		break;
 	/* Size of the log buffer */
 	case SYSLOG_ACTION_SIZE_BUFFER:
-		error = log_buf_len;
+		error = prb_buffer_size(&printk_rb);
 		break;
 	default:
 		error = -EINVAL;
 		break;
 	}
-
+out:
+	if (msgbuf)
+		kfree(msgbuf);
+	if (text)
+		kfree(text);
 	return error;
 }
 
@@ -1620,144 +1626,128 @@
 	return do_syslog(type, buf, len, SYSLOG_FROM_READER);
 }
 
-/*
- * Special console_lock variants that help to reduce the risk of soft-lockups.
- * They allow to pass console_lock to another printk() call using a busy wait.
- */
+int printk_delay_msec __read_mostly;
 
-#ifdef CONFIG_LOCKDEP
-static struct lockdep_map console_owner_dep_map = {
-	.name = "console_owner"
-};
-#endif
+static inline void printk_delay(int level)
+{
+	boot_delay_msec(level);
+	if (unlikely(printk_delay_msec)) {
+		int m = printk_delay_msec;
 
-static DEFINE_RAW_SPINLOCK(console_owner_lock);
-static struct task_struct *console_owner;
-static bool console_waiter;
+		while (m--) {
+			mdelay(1);
+			touch_nmi_watchdog();
+		}
+	}
+}
 
-/**
- * console_lock_spinning_enable - mark beginning of code where another
- *	thread might safely busy wait
- *
- * This basically converts console_lock into a spinlock. This marks
- * the section where the console_lock owner can not sleep, because
- * there may be a waiter spinning (like a spinlock). Also it must be
- * ready to hand over the lock at the end of the section.
- */
-static void console_lock_spinning_enable(void)
-{
-	raw_spin_lock(&console_owner_lock);
-	console_owner = current;
-	raw_spin_unlock(&console_owner_lock);
+static void print_console_dropped(struct console *con, u64 count)
+{
+	char text[64];
+	int len;
 
-	/* The waiter may spin on us after setting console_owner */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
+	len = sprintf(text, "** %llu printk message%s dropped **\n",
+		      count, count > 1 ? "s" : "");
+	con->write(con, text, len);
 }
 
-/**
- * console_lock_spinning_disable_and_check - mark end of code where another
- *	thread was able to busy wait and check if there is a waiter
- *
- * This is called at the end of the section where spinning is allowed.
- * It has two functions. First, it is a signal that it is no longer
- * safe to start busy waiting for the lock. Second, it checks if
- * there is a busy waiter and passes the lock rights to her.
- *
- * Important: Callers lose the lock if there was a busy waiter.
- *	They must not touch items synchronized by console_lock
- *	in this case.
- *
- * Return: 1 if the lock rights were passed, 0 otherwise.
- */
-static int console_lock_spinning_disable_and_check(void)
+static void format_text(struct printk_log *msg, u64 seq,
+			char *ext_text, size_t *ext_len,
+			char *text, size_t *len, bool time)
 {
-	int waiter;
-
-	raw_spin_lock(&console_owner_lock);
-	waiter = READ_ONCE(console_waiter);
-	console_owner = NULL;
-	raw_spin_unlock(&console_owner_lock);
+	if (suppress_message_printing(msg->level)) {
+		/*
+		 * Skip record that has level above the console
+		 * loglevel and update each console's local seq.
+		 */
+		*len = 0;
+		*ext_len = 0;
+		return;
+	}
 
-	if (!waiter) {
-		spin_release(&console_owner_dep_map, 1, _THIS_IP_);
-		return 0;
+	*len = msg_print_text(msg, console_msg_format & MSG_FORMAT_SYSLOG,
+			      time, text, PRINTK_SPRINT_MAX);
+	if (nr_ext_console_drivers) {
+		*ext_len = msg_print_ext_header(ext_text, CONSOLE_EXT_LOG_MAX,
+						msg, seq);
+		*ext_len += msg_print_ext_body(ext_text + *ext_len,
+					       CONSOLE_EXT_LOG_MAX - *ext_len,
+					       log_dict(msg), msg->dict_len,
+					       log_text(msg), msg->text_len);
+	} else {
+		*ext_len = 0;
 	}
+}
 
-	/* The waiter is now free to continue */
-	WRITE_ONCE(console_waiter, false);
+static void printk_write_history(struct console *con, u64 master_seq)
+{
+	struct prb_iterator iter;
+	bool time = printk_time;
+	static char *ext_text;
+	static char *text;
+	static char *buf;
+	u64 seq;
 
-	spin_release(&console_owner_dep_map, 1, _THIS_IP_);
+	ext_text = kmalloc(CONSOLE_EXT_LOG_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+	buf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!ext_text || !text || !buf)
+		return;
 
-	/*
-	 * Hand off console_lock to waiter. The waiter will perform
-	 * the up(). After this, the waiter is the console_lock owner.
-	 */
-	mutex_release(&console_lock_dep_map, 1, _THIS_IP_);
-	return 1;
-}
+	if (!(con->flags & CON_ENABLED))
+		goto out;
 
-/**
- * console_trylock_spinning - try to get console_lock by busy waiting
- *
- * This allows to busy wait for the console_lock when the current
- * owner is running in specially marked sections. It means that
- * the current owner is running and cannot reschedule until it
- * is ready to lose the lock.
- *
- * Return: 1 if we got the lock, 0 othrewise
- */
-static int console_trylock_spinning(void)
-{
-	struct task_struct *owner = NULL;
-	bool waiter;
-	bool spin = false;
-	unsigned long flags;
+	if (!con->write)
+		goto out;
 
-	if (console_trylock())
-		return 1;
+	if (!cpu_online(raw_smp_processor_id()) &&
+	    !(con->flags & CON_ANYTIME))
+		goto out;
 
-	printk_safe_enter_irqsave(flags);
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-	raw_spin_lock(&console_owner_lock);
-	owner = READ_ONCE(console_owner);
-	waiter = READ_ONCE(console_waiter);
-	if (!waiter && owner && owner != current) {
-		WRITE_ONCE(console_waiter, true);
-		spin = true;
-	}
-	raw_spin_unlock(&console_owner_lock);
-
-	/*
-	 * If there is an active printk() writing to the
-	 * consoles, instead of having it write our data too,
-	 * see if we can offload that load from the active
-	 * printer, and do some printing ourselves.
-	 * Go into a spin only if there isn't already a waiter
-	 * spinning, and there is an active printer, and
-	 * that active printer isn't us (recursive printk?).
-	 */
-	if (!spin) {
-		printk_safe_exit_irqrestore(flags);
-		return 0;
-	}
+	for (;;) {
+		struct printk_log *msg;
+		size_t ext_len;
+		size_t len;
+		int ret;
 
-	/* We spin waiting for the owner to release us */
-	spin_acquire(&console_owner_dep_map, 0, 0, _THIS_IP_);
-	/* Owner will clear console_waiter on hand off */
-	while (READ_ONCE(console_waiter))
-		cpu_relax();
-	spin_release(&console_owner_dep_map, 1, _THIS_IP_);
+		ret = prb_iter_next(&iter, buf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
 
-	printk_safe_exit_irqrestore(flags);
-	/*
-	 * The owner passed the console lock to us.
-	 * Since we did not spin on console lock, annotate
-	 * this as a trylock. Otherwise lockdep will
-	 * complain.
-	 */
-	mutex_acquire(&console_lock_dep_map, 0, 1, _THIS_IP_);
+		if (seq > master_seq)
+			break;
 
-	return 1;
+		con->printk_seq++;
+		if (con->printk_seq < seq) {
+			print_console_dropped(con, seq - con->printk_seq);
+			con->printk_seq = seq;
+		}
+
+		msg = (struct printk_log *)buf;
+		format_text(msg, master_seq, ext_text, &ext_len, text,
+			    &len, time);
+
+		if (len == 0 && ext_len == 0)
+			continue;
+
+		if (con->flags & CON_EXTENDED)
+			con->write(con, ext_text, ext_len);
+		else
+			con->write(con, text, len);
+
+		printk_delay(msg->level);
+	}
+out:
+	con->wrote_history = 1;
+	kfree(ext_text);
+	kfree(text);
+	kfree(buf);
 }
 
 /*
@@ -1765,8 +1755,9 @@
  * log_buf[start] to log_buf[end - 1].
  * The console_lock must be held.
  */
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len)
+static void call_console_drivers(u64 seq, const char *ext_text, size_t ext_len,
+				 const char *text, size_t len, int level,
+				 int facility)
 {
 	struct console *con;
 
@@ -1776,15 +1767,40 @@
 		return;
 
 	for_each_console(con) {
-		if (exclusive_console && con != exclusive_console)
-			continue;
 		if (!(con->flags & CON_ENABLED))
 			continue;
+		if (!con->wrote_history) {
+			if (con->flags & CON_PRINTBUFFER) {
+				printk_write_history(con, seq);
+				continue;
+			}
+			con->wrote_history = 1;
+			con->printk_seq = seq - 1;
+		}
+		if (con->flags & CON_BOOT && facility == 0) {
+			/* skip boot messages, already printed */
+			if (con->printk_seq < seq)
+				con->printk_seq = seq;
+			continue;
+		}
 		if (!con->write)
 			continue;
-		if (!cpu_online(smp_processor_id()) &&
+		if (!cpu_online(raw_smp_processor_id()) &&
 		    !(con->flags & CON_ANYTIME))
 			continue;
+		if (con->printk_seq >= seq)
+			continue;
+
+		con->printk_seq++;
+		if (con->printk_seq < seq) {
+			print_console_dropped(con, seq - con->printk_seq);
+			con->printk_seq = seq;
+		}
+
+		/* for supressed messages, only seq is updated */
+		if (len == 0 && ext_len == 0)
+			continue;
+
 		if (con->flags & CON_EXTENDED)
 			con->write(con, ext_text, ext_len);
 		else
@@ -1792,20 +1808,6 @@
 	}
 }
 
-int printk_delay_msec __read_mostly;
-
-static inline void printk_delay(void)
-{
-	if (unlikely(printk_delay_msec)) {
-		int m = printk_delay_msec;
-
-		while (m--) {
-			mdelay(1);
-			touch_nmi_watchdog();
-		}
-	}
-}
-
 static inline u32 printk_caller_id(void)
 {
 	return in_task() ? task_pid_nr(current) :
@@ -1822,101 +1824,94 @@
 	char buf[LOG_LINE_MAX];
 	size_t len;			/* length == 0 means unused buffer */
 	u32 caller_id;			/* printk_caller_id() of first print */
+	int cpu_owner;			/* cpu of first print */
 	u64 ts_nsec;			/* time of first print */
 	u8 level;			/* log level of first message */
 	u8 facility;			/* log facility of first message */
 	enum log_flags flags;		/* prefix, newline flags */
-} cont;
+} cont[2];
 
-static void cont_flush(void)
+static void cont_flush(int ctx)
 {
-	if (cont.len == 0)
+	struct cont *c = &cont[ctx];
+
+	if (c->len == 0)
 		return;
 
-	log_store(cont.caller_id, cont.facility, cont.level, cont.flags,
-		  cont.ts_nsec, NULL, 0, cont.buf, cont.len);
-	cont.len = 0;
+	log_store(c->caller_id, c->facility, c->level, c->flags,
+		  c->ts_nsec, c->cpu_owner, NULL, 0, c->buf, c->len);
+	c->len = 0;
 }
 
-static bool cont_add(u32 caller_id, int facility, int level,
+static void cont_add(int ctx, int cpu, u32 caller_id, int facility, int level,
 		     enum log_flags flags, const char *text, size_t len)
 {
+	struct cont *c = &cont[ctx];
+
+	if (cpu != c->cpu_owner || !(flags & LOG_CONT))
+		cont_flush(ctx);
+
 	/* If the line gets too long, split it up in separate records. */
-	if (cont.len + len > sizeof(cont.buf)) {
-		cont_flush();
-		return false;
-	}
+	while (c->len + len > sizeof(c->buf))
+		cont_flush(ctx);
 
-	if (!cont.len) {
-		cont.facility = facility;
-		cont.level = level;
-		cont.caller_id = caller_id;
-		cont.ts_nsec = local_clock();
-		cont.flags = flags;
+	if (!c->len) {
+		c->facility = facility;
+		c->level = level;
+		c->caller_id = caller_id;
+		c->ts_nsec = local_clock();
+		c->flags = flags;
+		c->cpu_owner = cpu;
 	}
 
-	memcpy(cont.buf + cont.len, text, len);
-	cont.len += len;
+	memcpy(c->buf + c->len, text, len);
+	c->len += len;
 
 	// The original flags come from the first line,
 	// but later continuations can add a newline.
 	if (flags & LOG_NEWLINE) {
-		cont.flags |= LOG_NEWLINE;
-		cont_flush();
+		c->flags |= LOG_NEWLINE;
 	}
-
-	return true;
 }
 
-static size_t log_output(int facility, int level, enum log_flags lflags, const char *dict, size_t dictlen, char *text, size_t text_len)
+/* ring buffer used as memory allocator for temporary sprint buffers */
+DECLARE_STATIC_PRINTKRB(sprint_rb,
+			ilog2(PRINTK_RECORD_MAX + sizeof(struct prb_entry) +
+			      sizeof(long)) + 2, &printk_cpulock);
+
+asmlinkage int vprintk_emit(int facility, int level,
+			    const char *dict, size_t dictlen,
+			    const char *fmt, va_list args)
 {
 	const u32 caller_id = printk_caller_id();
+	int ctx = !!in_nmi();
+	enum log_flags lflags = 0;
+	int printed_len = 0;
+	struct prb_handle h;
+	size_t text_len;
+	u64 ts_nsec;
+	char *text;
+	char *rbuf;
+	int cpu;
 
-	/*
-	 * If an earlier line was buffered, and we're a continuation
-	 * write from the same context, try to add it to the buffer.
-	 */
-	if (cont.len) {
-		if (cont.caller_id == caller_id && (lflags & LOG_CONT)) {
-			if (cont_add(caller_id, facility, level, lflags, text, text_len))
-				return text_len;
-		}
-		/* Otherwise, make sure it's flushed */
-		cont_flush();
-	}
-
-	/* Skip empty continuation lines that couldn't be added - they just flush */
-	if (!text_len && (lflags & LOG_CONT))
-		return 0;
+	ts_nsec = local_clock();
 
-	/* If it doesn't end in a newline, try to buffer the current line */
-	if (!(lflags & LOG_NEWLINE)) {
-		if (cont_add(caller_id, facility, level, lflags, text, text_len))
-			return text_len;
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_SPRINT_MAX);
+	if (!rbuf) {
+		prb_inc_lost(&printk_rb);
+		return printed_len;
 	}
 
-	/* Store it in the record log */
-	return log_store(caller_id, facility, level, lflags, 0,
-			 dict, dictlen, text, text_len);
-}
-
-/* Must be called under logbuf_lock. */
-int vprintk_store(int facility, int level,
-		  const char *dict, size_t dictlen,
-		  const char *fmt, va_list args)
-{
-	static char textbuf[LOG_LINE_MAX];
-	char *text = textbuf;
-	size_t text_len;
-	enum log_flags lflags = 0;
+	cpu = raw_smp_processor_id();
 
 	/*
-	 * The printf needs to come first; we need the syslog
-	 * prefix which might be passed-in as a parameter.
+	 * If this turns out to be an emergency message, there
+	 * may need to be a prefix added. Leave room for it.
 	 */
-	text_len = vscnprintf(text, sizeof(textbuf), fmt, args);
+	text = rbuf + PREFIX_MAX;
+	text_len = vscnprintf(text, PRINTK_SPRINT_MAX - PREFIX_MAX, fmt, args);
 
-	/* mark and strip a trailing newline */
+	/* strip and flag a trailing newline */
 	if (text_len && text[text_len-1] == '\n') {
 		text_len--;
 		lflags |= LOG_NEWLINE;
@@ -1947,62 +1942,37 @@
 	if (dict)
 		lflags |= LOG_NEWLINE;
 
-	return log_output(facility, level, lflags,
-			  dict, dictlen, text, text_len);
-}
-
-asmlinkage int vprintk_emit(int facility, int level,
-			    const char *dict, size_t dictlen,
-			    const char *fmt, va_list args)
-{
-	int printed_len;
-	bool in_sched = false, pending_output;
-	unsigned long flags;
-	u64 curr_log_seq;
-
-	/* Suppress unimportant messages after panic happens */
-	if (unlikely(suppress_printk))
-		return 0;
-
-	if (level == LOGLEVEL_SCHED) {
-		level = LOGLEVEL_DEFAULT;
-		in_sched = true;
+	/*
+	 * NOTE:
+	 * - rbuf points to beginning of allocated buffer
+	 * - text points to beginning of text
+	 * - there is room before text for prefix
+	 */
+	if (facility == 0) {
+		/* only the kernel can create emergency messages */
+		printk_emergency(rbuf, level & 7, ts_nsec, cpu, text, text_len);
 	}
 
-	boot_delay_msec(level);
-	printk_delay();
-
-	/* This stops the holder of console_sem just where we want him */
-	logbuf_lock_irqsave(flags);
-	curr_log_seq = log_next_seq;
-	printed_len = vprintk_store(facility, level, dict, dictlen, fmt, args);
-	pending_output = (curr_log_seq != log_next_seq);
-	logbuf_unlock_irqrestore(flags);
-
-	/* If called from the scheduler, we can not call up(). */
-	if (!in_sched && pending_output) {
-		/*
-		 * Disable preemption to avoid being preempted while holding
-		 * console_sem which would prevent anyone from printing to
-		 * console
-		 */
-		preempt_disable();
-		/*
-		 * Try to acquire and then immediately release the console
-		 * semaphore.  The release will print out buffers and wake up
-		 * /dev/kmsg and syslog() users.
-		 */
-		if (console_trylock_spinning())
-			console_unlock();
-		preempt_enable();
+	if ((lflags & LOG_CONT) || !(lflags & LOG_NEWLINE)) {
+		 cont_add(ctx, cpu, caller_id, facility, level, lflags, text, text_len);
+		 printed_len = text_len;
+	} else {
+		if (cpu == cont[ctx].cpu_owner)
+			cont_flush(ctx);
+		printed_len = log_store(caller_id, facility, level, lflags, ts_nsec, cpu,
+					dict, dictlen, text, text_len);
 	}
 
-	if (pending_output)
-		wake_up_klogd();
+	prb_commit(&h);
 	return printed_len;
 }
 EXPORT_SYMBOL(vprintk_emit);
 
+static __printf(1, 0) int vprintk_func(const char *fmt, va_list args)
+{
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
+}
+
 asmlinkage int vprintk(const char *fmt, va_list args)
 {
 	return vprintk_func(fmt, args);
@@ -2059,39 +2029,6 @@
 	return r;
 }
 EXPORT_SYMBOL(printk);
-
-#else /* CONFIG_PRINTK */
-
-#define LOG_LINE_MAX		0
-#define PREFIX_MAX		0
-#define printk_time		false
-
-static u64 syslog_seq;
-static u32 syslog_idx;
-static u64 console_seq;
-static u32 console_idx;
-static u64 exclusive_console_stop_seq;
-static u64 log_first_seq;
-static u32 log_first_idx;
-static u64 log_next_seq;
-static char *log_text(const struct printk_log *msg) { return NULL; }
-static char *log_dict(const struct printk_log *msg) { return NULL; }
-static struct printk_log *log_from_idx(u32 idx) { return NULL; }
-static u32 log_next(u32 idx) { return 0; }
-static ssize_t msg_print_ext_header(char *buf, size_t size,
-				    struct printk_log *msg,
-				    u64 seq) { return 0; }
-static ssize_t msg_print_ext_body(char *buf, size_t size,
-				  char *dict, size_t dict_len,
-				  char *text, size_t text_len) { return 0; }
-static void console_lock_spinning_enable(void) { }
-static int console_lock_spinning_disable_and_check(void) { return 0; }
-static void call_console_drivers(const char *ext_text, size_t ext_len,
-				 const char *text, size_t len) {}
-static size_t msg_print_text(const struct printk_log *msg, bool syslog,
-			     bool time, char *buf, size_t size) { return 0; }
-static bool suppress_message_printing(int level) { return false; }
-
 #endif /* CONFIG_PRINTK */
 
 #ifdef CONFIG_EARLY_PRINTK
@@ -2322,187 +2259,23 @@
 }
 EXPORT_SYMBOL(is_console_locked);
 
-/*
- * Check if we have any console that is capable of printing while cpu is
- * booting or shutting down. Requires console_sem.
- */
-static int have_callable_console(void)
-{
-	struct console *con;
-
-	for_each_console(con)
-		if ((con->flags & CON_ENABLED) &&
-				(con->flags & CON_ANYTIME))
-			return 1;
-
-	return 0;
-}
-
-/*
- * Can we actually use the console at this time on this cpu?
- *
- * Console drivers may assume that per-cpu resources have been allocated. So
- * unless they're explicitly marked as being able to cope (CON_ANYTIME) don't
- * call them until this CPU is officially up.
- */
-static inline int can_use_console(void)
-{
-	return cpu_online(raw_smp_processor_id()) || have_callable_console();
-}
-
 /**
  * console_unlock - unlock the console system
  *
  * Releases the console_lock which the caller holds on the console system
  * and the console driver list.
  *
- * While the console_lock was held, console output may have been buffered
- * by printk().  If this is the case, console_unlock(); emits
- * the output prior to releasing the lock.
- *
- * If there is output waiting, we wake /dev/kmsg and syslog() users.
- *
  * console_unlock(); may be called from any context.
  */
 void console_unlock(void)
 {
-	static char ext_text[CONSOLE_EXT_LOG_MAX];
-	static char text[LOG_LINE_MAX + PREFIX_MAX];
-	unsigned long flags;
-	bool do_cond_resched, retry;
-
 	if (console_suspended) {
 		up_console_sem();
 		return;
 	}
 
-	/*
-	 * Console drivers are called with interrupts disabled, so
-	 * @console_may_schedule should be cleared before; however, we may
-	 * end up dumping a lot of lines, for example, if called from
-	 * console registration path, and should invoke cond_resched()
-	 * between lines if allowable.  Not doing so can cause a very long
-	 * scheduling stall on a slow console leading to RCU stall and
-	 * softlockup warnings which exacerbate the issue with more
-	 * messages practically incapacitating the system.
-	 *
-	 * console_trylock() is not able to detect the preemptive
-	 * context reliably. Therefore the value must be stored before
-	 * and cleared after the the "again" goto label.
-	 */
-	do_cond_resched = console_may_schedule;
-again:
-	console_may_schedule = 0;
-
-	/*
-	 * We released the console_sem lock, so we need to recheck if
-	 * cpu is online and (if not) is there at least one CON_ANYTIME
-	 * console.
-	 */
-	if (!can_use_console()) {
-		console_locked = 0;
-		up_console_sem();
-		return;
-	}
-
-	for (;;) {
-		struct printk_log *msg;
-		size_t ext_len = 0;
-		size_t len;
-
-		printk_safe_enter_irqsave(flags);
-		raw_spin_lock(&logbuf_lock);
-		if (console_seq < log_first_seq) {
-			len = sprintf(text,
-				      "** %llu printk messages dropped **\n",
-				      log_first_seq - console_seq);
-
-			/* messages are gone, move to first one */
-			console_seq = log_first_seq;
-			console_idx = log_first_idx;
-		} else {
-			len = 0;
-		}
-skip:
-		if (console_seq == log_next_seq)
-			break;
-
-		msg = log_from_idx(console_idx);
-		if (suppress_message_printing(msg->level)) {
-			/*
-			 * Skip record we have buffered and already printed
-			 * directly to the console when we received it, and
-			 * record that has level above the console loglevel.
-			 */
-			console_idx = log_next(console_idx);
-			console_seq++;
-			goto skip;
-		}
-
-		/* Output to all consoles once old messages replayed. */
-		if (unlikely(exclusive_console &&
-			     console_seq >= exclusive_console_stop_seq)) {
-			exclusive_console = NULL;
-		}
-
-		len += msg_print_text(msg,
-				console_msg_format & MSG_FORMAT_SYSLOG,
-				printk_time, text + len, sizeof(text) - len);
-		if (nr_ext_console_drivers) {
-			ext_len = msg_print_ext_header(ext_text,
-						sizeof(ext_text),
-						msg, console_seq);
-			ext_len += msg_print_ext_body(ext_text + ext_len,
-						sizeof(ext_text) - ext_len,
-						log_dict(msg), msg->dict_len,
-						log_text(msg), msg->text_len);
-		}
-		console_idx = log_next(console_idx);
-		console_seq++;
-		raw_spin_unlock(&logbuf_lock);
-
-		/*
-		 * While actively printing out messages, if another printk()
-		 * were to occur on another CPU, it may wait for this one to
-		 * finish. This task can not be preempted if there is a
-		 * waiter waiting to take over.
-		 */
-		console_lock_spinning_enable();
-
-		stop_critical_timings();	/* don't trace print latency */
-		call_console_drivers(ext_text, ext_len, text, len);
-		start_critical_timings();
-
-		if (console_lock_spinning_disable_and_check()) {
-			printk_safe_exit_irqrestore(flags);
-			return;
-		}
-
-		printk_safe_exit_irqrestore(flags);
-
-		if (do_cond_resched)
-			cond_resched();
-	}
-
 	console_locked = 0;
-
-	raw_spin_unlock(&logbuf_lock);
-
 	up_console_sem();
-
-	/*
-	 * Someone could have filled up the buffer again, so re-check if there's
-	 * something to flush. In case we cannot trylock the console_sem again,
-	 * there's a new owner and the console_unlock() from them will do the
-	 * flush, no worries.
-	 */
-	raw_spin_lock(&logbuf_lock);
-	retry = console_seq != log_next_seq;
-	raw_spin_unlock(&logbuf_lock);
-	printk_safe_exit_irqrestore(flags);
-
-	if (retry && console_trylock())
-		goto again;
 }
 EXPORT_SYMBOL(console_unlock);
 
@@ -2553,24 +2326,10 @@
 void console_flush_on_panic(enum con_flush_mode mode)
 {
 	/*
-	 * If someone else is holding the console lock, trylock will fail
-	 * and may_schedule may be set.  Ignore and proceed to unlock so
-	 * that messages are flushed out.  As this can be called from any
-	 * context and we don't want to get preempted while flushing,
-	 * ensure may_schedule is cleared.
+	 * FIXME: This is currently a NOP. Emergency messages will have been
+	 * printed, but what about if write_atomic is not available on the
+	 * console? What if the printk kthread is still alive?
 	 */
-	console_trylock();
-	console_may_schedule = 0;
-
-	if (mode == CONSOLE_REPLAY_ALL) {
-		unsigned long flags;
-
-		logbuf_lock_irqsave(flags);
-		console_seq = log_first_seq;
-		console_idx = log_first_idx;
-		logbuf_unlock_irqrestore(flags);
-	}
-	console_unlock();
 }
 
 /*
@@ -2648,7 +2407,6 @@
 void register_console(struct console *newcon)
 {
 	int i;
-	unsigned long flags;
 	struct console *bcon = NULL;
 	struct console_cmdline *c;
 	static bool has_preferred;
@@ -2764,27 +2522,6 @@
 	if (newcon->flags & CON_EXTENDED)
 		nr_ext_console_drivers++;
 
-	if (newcon->flags & CON_PRINTBUFFER) {
-		/*
-		 * console_unlock(); will print out the buffered messages
-		 * for us.
-		 */
-		logbuf_lock_irqsave(flags);
-		console_seq = syslog_seq;
-		console_idx = syslog_idx;
-		/*
-		 * We're about to replay the log buffer.  Only do this to the
-		 * just-registered console to avoid excessive message spam to
-		 * the already-registered consoles.
-		 *
-		 * Set exclusive_console with disabled interrupts to reduce
-		 * race window with eventual console_flush_on_panic() that
-		 * ignores console_lock.
-		 */
-		exclusive_console = newcon;
-		exclusive_console_stop_seq = console_seq;
-		logbuf_unlock_irqrestore(flags);
-	}
 	console_unlock();
 	console_sysfs_notify();
 
@@ -2794,6 +2531,10 @@
 	 * boot consoles, real consoles, etc - this is to ensure that end
 	 * users know there might be something in the kernel's log buffer that
 	 * went to the bootconsole (that they do not see on the real console)
+	 *
+	 * This message is also important because it will trigger the
+	 * printk kthread to begin dumping the log buffer to the newly
+	 * registered console.
 	 */
 	pr_info("%sconsole [%s%d] enabled\n",
 		(newcon->flags & CON_BOOT) ? "boot" : "" ,
@@ -2937,59 +2678,74 @@
 late_initcall(printk_late_init);
 
 #if defined CONFIG_PRINTK
-/*
- * Delayed printk version, for scheduler-internal messages:
- */
-#define PRINTK_PENDING_WAKEUP	0x01
-#define PRINTK_PENDING_OUTPUT	0x02
+static int printk_kthread_func(void *data)
+{
+	struct prb_iterator iter;
+	struct printk_log *msg;
+	size_t ext_len;
+	char *ext_text;
+	u64 master_seq;
+	size_t len;
+	char *text;
+	char *buf;
+	int ret;
 
-static DEFINE_PER_CPU(int, printk_pending);
+	ext_text = kmalloc(CONSOLE_EXT_LOG_MAX, GFP_KERNEL);
+	text = kmalloc(PRINTK_SPRINT_MAX, GFP_KERNEL);
+	buf = kmalloc(PRINTK_RECORD_MAX, GFP_KERNEL);
+	if (!ext_text || !text || !buf)
+		return -1;
 
-static void wake_up_klogd_work_func(struct irq_work *irq_work)
-{
-	int pending = __this_cpu_xchg(printk_pending, 0);
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-	if (pending & PRINTK_PENDING_OUTPUT) {
-		/* If trylock fails, someone else is doing the printing */
-		if (console_trylock())
-			console_unlock();
+	/* the printk kthread never exits */
+	for (;;) {
+		ret = prb_iter_wait_next(&iter, buf,
+					 PRINTK_RECORD_MAX, &master_seq);
+		if (ret == -ERESTARTSYS) {
+			continue;
+		} else if (ret < 0) {
+			/* iterator invalid, start over */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
+
+		msg = (struct printk_log *)buf;
+		format_text(msg, master_seq, ext_text, &ext_len, text,
+			    &len, printk_time);
+
+		console_lock();
+		call_console_drivers(master_seq, ext_text, ext_len, text, len,
+				     msg->level, msg->facility);
+		if (len > 0 || ext_len > 0)
+			printk_delay(msg->level);
+		console_unlock();
 	}
 
-	if (pending & PRINTK_PENDING_WAKEUP)
-		wake_up_interruptible(&log_wait);
-}
+	kfree(ext_text);
+	kfree(text);
+	kfree(buf);
 
-static DEFINE_PER_CPU(struct irq_work, wake_up_klogd_work) = {
-	.func = wake_up_klogd_work_func,
-	.flags = IRQ_WORK_LAZY,
-};
+	return 0;
+}
 
-void wake_up_klogd(void)
+static int __init init_printk_kthread(void)
 {
-	preempt_disable();
-	if (waitqueue_active(&log_wait)) {
-		this_cpu_or(printk_pending, PRINTK_PENDING_WAKEUP);
-		irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
+	struct task_struct *thread;
+
+	thread = kthread_run(printk_kthread_func, NULL, "printk");
+	if (IS_ERR(thread)) {
+		pr_err("printk: unable to create printing thread\n");
+		return PTR_ERR(thread);
 	}
-	preempt_enable();
-}
 
-void defer_console_output(void)
-{
-	preempt_disable();
-	__this_cpu_or(printk_pending, PRINTK_PENDING_OUTPUT);
-	irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
-	preempt_enable();
+	return 0;
 }
+late_initcall(init_printk_kthread);
 
-int vprintk_deferred(const char *fmt, va_list args)
+static int vprintk_deferred(const char *fmt, va_list args)
 {
-	int r;
-
-	r = vprintk_emit(0, LOGLEVEL_SCHED, NULL, 0, fmt, args);
-	defer_console_output();
-
-	return r;
+	return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
 }
 
 int printk_deferred(const char *fmt, ...)
@@ -3111,8 +2867,8 @@
  */
 void kmsg_dump(enum kmsg_dump_reason reason)
 {
+	struct kmsg_dumper dumper_local;
 	struct kmsg_dumper *dumper;
-	unsigned long flags;
 
 	if ((reason > KMSG_DUMP_OOPS) && !always_kmsg_dump)
 		return;
@@ -3122,21 +2878,18 @@
 		if (dumper->max_reason && reason > dumper->max_reason)
 			continue;
 
-		/* initialize iterator with data about the stored records */
-		dumper->active = true;
+		/*
+		 * use a local copy to avoid modifying the
+		 * iterator used by any other cpus/contexts
+		 */
+		memcpy(&dumper_local, dumper, sizeof(dumper_local));
 
-		logbuf_lock_irqsave(flags);
-		dumper->cur_seq = clear_seq;
-		dumper->cur_idx = clear_idx;
-		dumper->next_seq = log_next_seq;
-		dumper->next_idx = log_next_idx;
-		logbuf_unlock_irqrestore(flags);
+		/* initialize iterator with data about the stored records */
+		dumper_local.active = true;
+		kmsg_dump_rewind(&dumper_local);
 
 		/* invoke dumper which will iterate over records */
-		dumper->dump(dumper, reason);
-
-		/* reset iterator */
-		dumper->active = false;
+		dumper_local.dump(&dumper_local, reason);
 	}
 	rcu_read_unlock();
 }
@@ -3163,33 +2916,67 @@
 bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
 			       char *line, size_t size, size_t *len)
 {
+	struct prb_iterator iter;
 	struct printk_log *msg;
-	size_t l = 0;
-	bool ret = false;
+	struct prb_handle h;
+	bool cont = false;
+	char *msgbuf;
+	char *rbuf;
+	size_t l;
+	u64 seq;
+	int ret;
 
 	if (!dumper->active)
-		goto out;
+		return cont;
+
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_RECORD_MAX);
+	if (!rbuf)
+		return cont;
+	msgbuf = rbuf;
+retry:
+	for (;;) {
+		prb_iter_init(&iter, &printk_rb, &seq);
 
-	if (dumper->cur_seq < log_first_seq) {
-		/* messages are gone, move to first available one */
-		dumper->cur_seq = log_first_seq;
-		dumper->cur_idx = log_first_idx;
+		if (dumper->line_seq == seq) {
+			/* already where we want to be */
+			break;
+		} else if (dumper->line_seq < seq) {
+			/* messages are gone, move to first available one */
+			dumper->line_seq = seq;
+			break;
+		}
+
+		ret = prb_iter_seek(&iter, dumper->line_seq);
+		if (ret > 0) {
+			/* seeked to line_seq */
+			break;
+		} else if (ret == 0) {
+			/*
+			 * The end of the list was hit without ever seeing
+			 * line_seq. Reset it to the beginning of the list.
+			 */
+			prb_iter_init(&iter, &printk_rb, &dumper->line_seq);
+			break;
+		}
+		/* iterator invalid, start over */
 	}
 
-	/* last entry */
-	if (dumper->cur_seq >= log_next_seq)
+	ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX,
+			    &dumper->line_seq);
+	if (ret == 0)
 		goto out;
+	else if (ret < 0)
+		goto retry;
 
-	msg = log_from_idx(dumper->cur_idx);
+	msg = (struct printk_log *)msgbuf;
 	l = msg_print_text(msg, syslog, printk_time, line, size);
 
-	dumper->cur_idx = log_next(dumper->cur_idx);
-	dumper->cur_seq++;
-	ret = true;
-out:
 	if (len)
 		*len = l;
-	return ret;
+	cont = true;
+out:
+	prb_commit(&h);
+	return cont;
 }
 
 /**
@@ -3212,12 +2999,9 @@
 bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
 			char *line, size_t size, size_t *len)
 {
-	unsigned long flags;
 	bool ret;
 
-	logbuf_lock_irqsave(flags);
 	ret = kmsg_dump_get_line_nolock(dumper, syslog, line, size, len);
-	logbuf_unlock_irqrestore(flags);
 
 	return ret;
 }
@@ -3245,74 +3029,101 @@
 bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
 			  char *buf, size_t size, size_t *len)
 {
-	unsigned long flags;
-	u64 seq;
-	u32 idx;
-	u64 next_seq;
-	u32 next_idx;
-	size_t l = 0;
-	bool ret = false;
+	struct prb_iterator iter;
 	bool time = printk_time;
+	struct printk_log *msg;
+	u64 new_end_seq = 0;
+	struct prb_handle h;
+	bool cont = false;
+	char *msgbuf;
+	u64 end_seq;
+	int textlen;
+	u64 seq = 0;
+	char *rbuf;
+	int l = 0;
+	int ret;
 
 	if (!dumper->active)
-		goto out;
+		return cont;
 
-	logbuf_lock_irqsave(flags);
-	if (dumper->cur_seq < log_first_seq) {
-		/* messages are gone, move to first available one */
-		dumper->cur_seq = log_first_seq;
-		dumper->cur_idx = log_first_idx;
-	}
+	rbuf = prb_reserve(&h, &sprint_rb, PRINTK_RECORD_MAX);
+	if (!rbuf)
+		return cont;
+	msgbuf = rbuf;
 
-	/* last entry */
-	if (dumper->cur_seq >= dumper->next_seq) {
-		logbuf_unlock_irqrestore(flags);
-		goto out;
-	}
+	prb_iter_init(&iter, &printk_rb, NULL);
 
-	/* calculate length of entire buffer */
-	seq = dumper->cur_seq;
-	idx = dumper->cur_idx;
-	while (seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	/*
+	 * seek to the start record, which is set/modified
+	 * by kmsg_dump_get_line_nolock()
+	 */
+	ret = prb_iter_seek(&iter, dumper->line_seq);
+	if (ret <= 0)
+		prb_iter_init(&iter, &printk_rb, &seq);
 
-		l += msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
+	/* work with a local end seq to have a constant value */
+	end_seq = dumper->buffer_end_seq;
+	if (!end_seq) {
+		/* initialize end seq to "infinity" */
+		end_seq = -1;
+		dumper->buffer_end_seq = end_seq;
 	}
+retry:
+	if (seq >= end_seq)
+		goto out;
 
-	/* move first record forward until length fits into the buffer */
-	seq = dumper->cur_seq;
-	idx = dumper->cur_idx;
-	while (l >= size && seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+	/* count the total bytes after seq */
+	textlen = count_remaining(&iter, end_seq, msgbuf,
+				  PRINTK_RECORD_MAX, 0, time);
+
+	/* move iter forward until length fits into the buffer */
+	while (textlen > size) {
+		ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0 || seq >= end_seq) {
+			prb_iter_init(&iter, &printk_rb, &seq);
+			goto retry;
+		}
 
-		l -= msg_print_text(msg, true, time, NULL, 0);
-		idx = log_next(idx);
-		seq++;
+		msg = (struct printk_log *)msgbuf;
+		textlen -= msg_print_text(msg, true, time, NULL, 0);
 	}
 
-	/* last message in next interation */
-	next_seq = seq;
-	next_idx = idx;
+	/* save end seq for the next interation */
+	new_end_seq = seq + 1;
+
+	/* copy messages to buffer */
+	while (l < size) {
+		ret = prb_iter_next(&iter, msgbuf, PRINTK_RECORD_MAX, &seq);
+		if (ret == 0) {
+			break;
+		} else if (ret < 0) {
+			/*
+			 * iterator (and thus also the start position)
+			 * invalid, start over from beginning of list
+			 */
+			prb_iter_init(&iter, &printk_rb, NULL);
+			continue;
+		}
 
-	l = 0;
-	while (seq < dumper->next_seq) {
-		struct printk_log *msg = log_from_idx(idx);
+		if (seq >= end_seq)
+			break;
 
-		l += msg_print_text(msg, syslog, time, buf + l, size - l);
-		idx = log_next(idx);
-		seq++;
+		msg = (struct printk_log *)msgbuf;
+		textlen = msg_print_text(msg, syslog, time, buf + l, size - l);
+		if (textlen > 0)
+			l += textlen;
+		cont = true;
 	}
 
-	dumper->next_seq = next_seq;
-	dumper->next_idx = next_idx;
-	ret = true;
-	logbuf_unlock_irqrestore(flags);
-out:
-	if (len)
+	if (cont && len)
 		*len = l;
-	return ret;
+out:
+	prb_commit(&h);
+	if (new_end_seq)
+		dumper->buffer_end_seq = new_end_seq;
+	return cont;
 }
 EXPORT_SYMBOL_GPL(kmsg_dump_get_buffer);
 
@@ -3328,10 +3139,8 @@
  */
 void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper)
 {
-	dumper->cur_seq = clear_seq;
-	dumper->cur_idx = clear_idx;
-	dumper->next_seq = log_next_seq;
-	dumper->next_idx = log_next_idx;
+	dumper->line_seq = 0;
+	dumper->buffer_end_seq = 0;
 }
 
 /**
@@ -3344,12 +3153,89 @@
  */
 void kmsg_dump_rewind(struct kmsg_dumper *dumper)
 {
-	unsigned long flags;
-
-	logbuf_lock_irqsave(flags);
 	kmsg_dump_rewind_nolock(dumper);
-	logbuf_unlock_irqrestore(flags);
 }
 EXPORT_SYMBOL_GPL(kmsg_dump_rewind);
 
+static bool console_can_emergency(int level)
+{
+	struct console *con;
+
+	for_each_console(con) {
+		if (!(con->flags & CON_ENABLED))
+			continue;
+		if (con->write_atomic && oops_in_progress)
+			return true;
+		if (con->write && (con->flags & CON_BOOT))
+			return true;
+	}
+	return false;
+}
+
+static void call_emergency_console_drivers(int level, const char *text,
+					   size_t text_len)
+{
+	struct console *con;
+
+	for_each_console(con) {
+		if (!(con->flags & CON_ENABLED))
+			continue;
+		if (con->write_atomic && oops_in_progress) {
+			con->write_atomic(con, text, text_len);
+			continue;
+		}
+		if (con->write && (con->flags & CON_BOOT)) {
+			con->write(con, text, text_len);
+			continue;
+		}
+	}
+}
+
+static void printk_emergency(char *buffer, int level, u64 ts_nsec, u16 cpu,
+			     char *text, u16 text_len)
+{
+	struct printk_log msg;
+	size_t prefix_len;
+
+	if (!console_can_emergency(level))
+		return;
+
+	msg.level = level;
+	msg.ts_nsec = ts_nsec;
+	msg.cpu = cpu;
+	msg.facility = 0;
+
+	/* "text" must have PREFIX_MAX preceding bytes available */
+
+	prefix_len = print_prefix(&msg,
+				  console_msg_format & MSG_FORMAT_SYSLOG,
+				  printk_time, buffer);
+	/* move the prefix forward to the beginning of the message text */
+	text -= prefix_len;
+	memmove(text, buffer, prefix_len);
+	text_len += prefix_len;
+
+	text[text_len++] = '\n';
+
+	call_emergency_console_drivers(level, text, text_len);
+
+	touch_softlockup_watchdog_sync();
+	clocksource_touch_watchdog();
+	rcu_cpu_stall_reset();
+	touch_nmi_watchdog();
+
+	printk_delay(level);
+}
 #endif
+
+void console_atomic_lock(unsigned int *flags)
+{
+	prb_lock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_lock);
+
+void console_atomic_unlock(unsigned int flags)
+{
+	prb_unlock(&printk_cpulock, flags);
+}
+EXPORT_SYMBOL(console_atomic_unlock);
diff -Nur linux-5.4.5/kernel/printk/printk_safe.c linux-5.4.5-new/kernel/printk/printk_safe.c
--- linux-5.4.5/kernel/printk/printk_safe.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/printk/printk_safe.c	1970-01-01 02:00:00.000000000 +0200
@@ -1,415 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-or-later
-/*
- * printk_safe.c - Safe printk for printk-deadlock-prone contexts
- */
-
-#include <linux/preempt.h>
-#include <linux/spinlock.h>
-#include <linux/debug_locks.h>
-#include <linux/smp.h>
-#include <linux/cpumask.h>
-#include <linux/irq_work.h>
-#include <linux/printk.h>
-
-#include "internal.h"
-
-/*
- * printk() could not take logbuf_lock in NMI context. Instead,
- * it uses an alternative implementation that temporary stores
- * the strings into a per-CPU buffer. The content of the buffer
- * is later flushed into the main ring buffer via IRQ work.
- *
- * The alternative implementation is chosen transparently
- * by examinig current printk() context mask stored in @printk_context
- * per-CPU variable.
- *
- * The implementation allows to flush the strings also from another CPU.
- * There are situations when we want to make sure that all buffers
- * were handled or when IRQs are blocked.
- */
-static int printk_safe_irq_ready __read_mostly;
-
-#define SAFE_LOG_BUF_LEN ((1 << CONFIG_PRINTK_SAFE_LOG_BUF_SHIFT) -	\
-				sizeof(atomic_t) -			\
-				sizeof(atomic_t) -			\
-				sizeof(struct irq_work))
-
-struct printk_safe_seq_buf {
-	atomic_t		len;	/* length of written data */
-	atomic_t		message_lost;
-	struct irq_work		work;	/* IRQ work that flushes the buffer */
-	unsigned char		buffer[SAFE_LOG_BUF_LEN];
-};
-
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, safe_print_seq);
-static DEFINE_PER_CPU(int, printk_context);
-
-#ifdef CONFIG_PRINTK_NMI
-static DEFINE_PER_CPU(struct printk_safe_seq_buf, nmi_print_seq);
-#endif
-
-/* Get flushed in a more safe context. */
-static void queue_flush_work(struct printk_safe_seq_buf *s)
-{
-	if (printk_safe_irq_ready)
-		irq_work_queue(&s->work);
-}
-
-/*
- * Add a message to per-CPU context-dependent buffer. NMI and printk-safe
- * have dedicated buffers, because otherwise printk-safe preempted by
- * NMI-printk would have overwritten the NMI messages.
- *
- * The messages are flushed from irq work (or from panic()), possibly,
- * from other CPU, concurrently with printk_safe_log_store(). Should this
- * happen, printk_safe_log_store() will notice the buffer->len mismatch
- * and repeat the write.
- */
-static __printf(2, 0) int printk_safe_log_store(struct printk_safe_seq_buf *s,
-						const char *fmt, va_list args)
-{
-	int add;
-	size_t len;
-	va_list ap;
-
-again:
-	len = atomic_read(&s->len);
-
-	/* The trailing '\0' is not counted into len. */
-	if (len >= sizeof(s->buffer) - 1) {
-		atomic_inc(&s->message_lost);
-		queue_flush_work(s);
-		return 0;
-	}
-
-	/*
-	 * Make sure that all old data have been read before the buffer
-	 * was reset. This is not needed when we just append data.
-	 */
-	if (!len)
-		smp_rmb();
-
-	va_copy(ap, args);
-	add = vscnprintf(s->buffer + len, sizeof(s->buffer) - len, fmt, ap);
-	va_end(ap);
-	if (!add)
-		return 0;
-
-	/*
-	 * Do it once again if the buffer has been flushed in the meantime.
-	 * Note that atomic_cmpxchg() is an implicit memory barrier that
-	 * makes sure that the data were written before updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, len + add) != len)
-		goto again;
-
-	queue_flush_work(s);
-	return add;
-}
-
-static inline void printk_safe_flush_line(const char *text, int len)
-{
-	/*
-	 * Avoid any console drivers calls from here, because we may be
-	 * in NMI or printk_safe context (when in panic). The messages
-	 * must go only into the ring buffer at this stage.  Consoles will
-	 * get explicitly called later when a crashdump is not generated.
-	 */
-	printk_deferred("%.*s", len, text);
-}
-
-/* printk part of the temporary buffer line by line */
-static int printk_safe_flush_buffer(const char *start, size_t len)
-{
-	const char *c, *end;
-	bool header;
-
-	c = start;
-	end = start + len;
-	header = true;
-
-	/* Print line by line. */
-	while (c < end) {
-		if (*c == '\n') {
-			printk_safe_flush_line(start, c - start + 1);
-			start = ++c;
-			header = true;
-			continue;
-		}
-
-		/* Handle continuous lines or missing new line. */
-		if ((c + 1 < end) && printk_get_level(c)) {
-			if (header) {
-				c = printk_skip_level(c);
-				continue;
-			}
-
-			printk_safe_flush_line(start, c - start);
-			start = c++;
-			header = true;
-			continue;
-		}
-
-		header = false;
-		c++;
-	}
-
-	/* Check if there was a partial line. Ignore pure header. */
-	if (start < end && !header) {
-		static const char newline[] = KERN_CONT "\n";
-
-		printk_safe_flush_line(start, end - start);
-		printk_safe_flush_line(newline, strlen(newline));
-	}
-
-	return len;
-}
-
-static void report_message_lost(struct printk_safe_seq_buf *s)
-{
-	int lost = atomic_xchg(&s->message_lost, 0);
-
-	if (lost)
-		printk_deferred("Lost %d message(s)!\n", lost);
-}
-
-/*
- * Flush data from the associated per-CPU buffer. The function
- * can be called either via IRQ work or independently.
- */
-static void __printk_safe_flush(struct irq_work *work)
-{
-	static raw_spinlock_t read_lock =
-		__RAW_SPIN_LOCK_INITIALIZER(read_lock);
-	struct printk_safe_seq_buf *s =
-		container_of(work, struct printk_safe_seq_buf, work);
-	unsigned long flags;
-	size_t len;
-	int i;
-
-	/*
-	 * The lock has two functions. First, one reader has to flush all
-	 * available message to make the lockless synchronization with
-	 * writers easier. Second, we do not want to mix messages from
-	 * different CPUs. This is especially important when printing
-	 * a backtrace.
-	 */
-	raw_spin_lock_irqsave(&read_lock, flags);
-
-	i = 0;
-more:
-	len = atomic_read(&s->len);
-
-	/*
-	 * This is just a paranoid check that nobody has manipulated
-	 * the buffer an unexpected way. If we printed something then
-	 * @len must only increase. Also it should never overflow the
-	 * buffer size.
-	 */
-	if ((i && i >= len) || len > sizeof(s->buffer)) {
-		const char *msg = "printk_safe_flush: internal error\n";
-
-		printk_safe_flush_line(msg, strlen(msg));
-		len = 0;
-	}
-
-	if (!len)
-		goto out; /* Someone else has already flushed the buffer. */
-
-	/* Make sure that data has been written up to the @len */
-	smp_rmb();
-	i += printk_safe_flush_buffer(s->buffer + i, len - i);
-
-	/*
-	 * Check that nothing has got added in the meantime and truncate
-	 * the buffer. Note that atomic_cmpxchg() is an implicit memory
-	 * barrier that makes sure that the data were copied before
-	 * updating s->len.
-	 */
-	if (atomic_cmpxchg(&s->len, len, 0) != len)
-		goto more;
-
-out:
-	report_message_lost(s);
-	raw_spin_unlock_irqrestore(&read_lock, flags);
-}
-
-/**
- * printk_safe_flush - flush all per-cpu nmi buffers.
- *
- * The buffers are flushed automatically via IRQ work. This function
- * is useful only when someone wants to be sure that all buffers have
- * been flushed at some point.
- */
-void printk_safe_flush(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-#ifdef CONFIG_PRINTK_NMI
-		__printk_safe_flush(&per_cpu(nmi_print_seq, cpu).work);
-#endif
-		__printk_safe_flush(&per_cpu(safe_print_seq, cpu).work);
-	}
-}
-
-/**
- * printk_safe_flush_on_panic - flush all per-cpu nmi buffers when the system
- *	goes down.
- *
- * Similar to printk_safe_flush() but it can be called even in NMI context when
- * the system goes down. It does the best effort to get NMI messages into
- * the main ring buffer.
- *
- * Note that it could try harder when there is only one CPU online.
- */
-void printk_safe_flush_on_panic(void)
-{
-	/*
-	 * Make sure that we could access the main ring buffer.
-	 * Do not risk a double release when more CPUs are up.
-	 */
-	if (raw_spin_is_locked(&logbuf_lock)) {
-		if (num_online_cpus() > 1)
-			return;
-
-		debug_locks_off();
-		raw_spin_lock_init(&logbuf_lock);
-	}
-
-	printk_safe_flush();
-}
-
-#ifdef CONFIG_PRINTK_NMI
-/*
- * Safe printk() for NMI context. It uses a per-CPU buffer to
- * store the message. NMIs are not nested, so there is always only
- * one writer running. But the buffer might get flushed from another
- * CPU, so we need to be careful.
- */
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&nmi_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-void notrace printk_nmi_enter(void)
-{
-	this_cpu_or(printk_context, PRINTK_NMI_CONTEXT_MASK);
-}
-
-void notrace printk_nmi_exit(void)
-{
-	this_cpu_and(printk_context, ~PRINTK_NMI_CONTEXT_MASK);
-}
-
-/*
- * Marks a code that might produce many messages in NMI context
- * and the risk of losing them is more critical than eventual
- * reordering.
- *
- * It has effect only when called in NMI context. Then printk()
- * will try to store the messages into the main logbuf directly
- * and use the per-CPU buffers only as a fallback when the lock
- * is not available.
- */
-void printk_nmi_direct_enter(void)
-{
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		this_cpu_or(printk_context, PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-void printk_nmi_direct_exit(void)
-{
-	this_cpu_and(printk_context, ~PRINTK_NMI_DIRECT_CONTEXT_MASK);
-}
-
-#else
-
-static __printf(1, 0) int vprintk_nmi(const char *fmt, va_list args)
-{
-	return 0;
-}
-
-#endif /* CONFIG_PRINTK_NMI */
-
-/*
- * Lock-less printk(), to avoid deadlocks should the printk() recurse
- * into itself. It uses a per-CPU buffer to store the message, just like
- * NMI.
- */
-static __printf(1, 0) int vprintk_safe(const char *fmt, va_list args)
-{
-	struct printk_safe_seq_buf *s = this_cpu_ptr(&safe_print_seq);
-
-	return printk_safe_log_store(s, fmt, args);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_enter(void)
-{
-	this_cpu_inc(printk_context);
-}
-
-/* Can be preempted by NMI. */
-void __printk_safe_exit(void)
-{
-	this_cpu_dec(printk_context);
-}
-
-__printf(1, 0) int vprintk_func(const char *fmt, va_list args)
-{
-	/*
-	 * Try to use the main logbuf even in NMI. But avoid calling console
-	 * drivers that might have their own locks.
-	 */
-	if ((this_cpu_read(printk_context) & PRINTK_NMI_DIRECT_CONTEXT_MASK) &&
-	    raw_spin_trylock(&logbuf_lock)) {
-		int len;
-
-		len = vprintk_store(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
-		raw_spin_unlock(&logbuf_lock);
-		defer_console_output();
-		return len;
-	}
-
-	/* Use extra buffer in NMI when logbuf_lock is taken or in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)
-		return vprintk_nmi(fmt, args);
-
-	/* Use extra buffer to prevent a recursion deadlock in safe mode. */
-	if (this_cpu_read(printk_context) & PRINTK_SAFE_CONTEXT_MASK)
-		return vprintk_safe(fmt, args);
-
-	/* No obstacles. */
-	return vprintk_default(fmt, args);
-}
-
-void __init printk_safe_init(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-		struct printk_safe_seq_buf *s;
-
-		s = &per_cpu(safe_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-
-#ifdef CONFIG_PRINTK_NMI
-		s = &per_cpu(nmi_print_seq, cpu);
-		init_irq_work(&s->work, __printk_safe_flush);
-#endif
-	}
-
-	/*
-	 * In the highly unlikely event that a NMI were to trigger at
-	 * this moment. Make sure IRQ work is set up before this
-	 * variable is set.
-	 */
-	barrier();
-	printk_safe_irq_ready = 1;
-
-	/* Flush pending messages that did not have scheduled IRQ works. */
-	printk_safe_flush();
-}
diff -Nur linux-5.4.5/kernel/ptrace.c linux-5.4.5-new/kernel/ptrace.c
--- linux-5.4.5/kernel/ptrace.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/ptrace.c	2020-06-15 16:12:26.139714355 +0300
@@ -180,7 +180,14 @@
 
 	spin_lock_irq(&task->sighand->siglock);
 	if (task_is_traced(task) && !__fatal_signal_pending(task)) {
-		task->state = __TASK_TRACED;
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&task->pi_lock, flags);
+		if (task->state & __TASK_TRACED)
+			task->state = __TASK_TRACED;
+		else
+			task->saved_state = __TASK_TRACED;
+		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		ret = true;
 	}
 	spin_unlock_irq(&task->sighand->siglock);
diff -Nur linux-5.4.5/kernel/rcu/Kconfig linux-5.4.5-new/kernel/rcu/Kconfig
--- linux-5.4.5/kernel/rcu/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/Kconfig	2020-06-15 16:12:25.835715425 +0300
@@ -161,8 +161,8 @@
 
 config RCU_BOOST
 	bool "Enable RCU priority boosting"
-	depends on RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT
-	default n
+	depends on (RT_MUTEXES && PREEMPT_RCU && RCU_EXPERT) || PREEMPT_RT
+	default y if PREEMPT_RT
 	help
 	  This option boosts the priority of preempted RCU readers that
 	  block the current preemptible RCU grace period for too long.
@@ -200,8 +200,8 @@
 	  specified at boot time by the rcu_nocbs parameter.  For each
 	  such CPU, a kthread ("rcuox/N") will be created to invoke
 	  callbacks, where the "N" is the CPU being offloaded, and where
-	  the "p" for RCU-preempt (PREEMPT kernels) and "s" for RCU-sched
-	  (!PREEMPT kernels).  Nothing prevents this kthread from running
+	  the "p" for RCU-preempt (PREEMPTION kernels) and "s" for RCU-sched
+	  (!PREEMPTION kernels).  Nothing prevents this kthread from running
 	  on the specified CPUs, but (1) the kthreads may be preempted
 	  between each callback, and (2) affinity or cgroups can be used
 	  to force the kthreads to run on whatever set of CPUs is desired.
diff -Nur linux-5.4.5/kernel/rcu/rcutorture.c linux-5.4.5-new/kernel/rcu/rcutorture.c
--- linux-5.4.5/kernel/rcu/rcutorture.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/rcutorture.c	2020-06-15 16:12:25.835715425 +0300
@@ -60,10 +60,13 @@
 #define RCUTORTURE_RDR_RBH	 0x08	/*  ... rcu_read_lock_bh(). */
 #define RCUTORTURE_RDR_SCHED	 0x10	/*  ... rcu_read_lock_sched(). */
 #define RCUTORTURE_RDR_RCU	 0x20	/*  ... entering another RCU reader. */
-#define RCUTORTURE_RDR_NBITS	 6	/* Number of bits defined above. */
+#define RCUTORTURE_RDR_ATOM_BH	 0x40	/*  ... disabling bh while atomic */
+#define RCUTORTURE_RDR_ATOM_RBH	 0x80	/*  ... RBH while atomic */
+#define RCUTORTURE_RDR_NBITS	 8	/* Number of bits defined above. */
 #define RCUTORTURE_MAX_EXTEND	 \
 	(RCUTORTURE_RDR_BH | RCUTORTURE_RDR_IRQ | RCUTORTURE_RDR_PREEMPT | \
-	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED)
+	 RCUTORTURE_RDR_RBH | RCUTORTURE_RDR_SCHED | \
+	 RCUTORTURE_RDR_ATOM_BH | RCUTORTURE_RDR_ATOM_RBH)
 #define RCUTORTURE_RDR_MAX_LOOPS 0x7	/* Maximum reader extensions. */
 					/* Must be power of two minus one. */
 #define RCUTORTURE_RDR_MAX_SEGS (RCUTORTURE_RDR_MAX_LOOPS + 3)
@@ -1152,31 +1155,52 @@
 	WARN_ON_ONCE((idxold >> RCUTORTURE_RDR_SHIFT) > 1);
 	rtrsp->rt_readstate = newstate;
 
-	/* First, put new protection in place to avoid critical-section gap. */
+	/*
+	 * First, put new protection in place to avoid critical-section gap.
+	 * Disable preemption around the ATOM disables to ensure that
+	 * in_atomic() is true.
+	 */
 	if (statesnew & RCUTORTURE_RDR_BH)
 		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_RBH)
+		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_IRQ)
 		local_irq_disable();
 	if (statesnew & RCUTORTURE_RDR_PREEMPT)
 		preempt_disable();
-	if (statesnew & RCUTORTURE_RDR_RBH)
-		rcu_read_lock_bh();
 	if (statesnew & RCUTORTURE_RDR_SCHED)
 		rcu_read_lock_sched();
+	preempt_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_BH)
+		local_bh_disable();
+	if (statesnew & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_lock_bh();
+	preempt_enable();
 	if (statesnew & RCUTORTURE_RDR_RCU)
 		idxnew = cur_ops->readlock() << RCUTORTURE_RDR_SHIFT;
 
-	/* Next, remove old protection, irq first due to bh conflict. */
+	/*
+	 * Next, remove old protection, in decreasing order of strength
+	 * to avoid unlock paths that aren't safe in the stronger
+	 * context.  Disable preemption around the ATOM enables in
+	 * case the context was only atomic due to IRQ disabling.
+	 */
+	preempt_disable();
 	if (statesold & RCUTORTURE_RDR_IRQ)
 		local_irq_enable();
-	if (statesold & RCUTORTURE_RDR_BH)
+	if (statesold & RCUTORTURE_RDR_ATOM_BH)
 		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_ATOM_RBH)
+		rcu_read_unlock_bh();
+	preempt_enable();
 	if (statesold & RCUTORTURE_RDR_PREEMPT)
 		preempt_enable();
-	if (statesold & RCUTORTURE_RDR_RBH)
-		rcu_read_unlock_bh();
 	if (statesold & RCUTORTURE_RDR_SCHED)
 		rcu_read_unlock_sched();
+	if (statesold & RCUTORTURE_RDR_BH)
+		local_bh_enable();
+	if (statesold & RCUTORTURE_RDR_RBH)
+		rcu_read_unlock_bh();
 	if (statesold & RCUTORTURE_RDR_RCU)
 		cur_ops->readunlock(idxold >> RCUTORTURE_RDR_SHIFT);
 
@@ -1212,6 +1236,12 @@
 	int mask = rcutorture_extend_mask_max();
 	unsigned long randmask1 = torture_random(trsp) >> 8;
 	unsigned long randmask2 = randmask1 >> 3;
+	unsigned long preempts = RCUTORTURE_RDR_PREEMPT | RCUTORTURE_RDR_SCHED;
+	unsigned long preempts_irq = preempts | RCUTORTURE_RDR_IRQ;
+	unsigned long nonatomic_bhs = RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+	unsigned long atomic_bhs = RCUTORTURE_RDR_ATOM_BH |
+				   RCUTORTURE_RDR_ATOM_RBH;
+	unsigned long tmp;
 
 	WARN_ON_ONCE(mask >> RCUTORTURE_RDR_SHIFT);
 	/* Mostly only one bit (need preemption!), sometimes lots of bits. */
@@ -1219,11 +1249,49 @@
 		mask = mask & randmask2;
 	else
 		mask = mask & (1 << (randmask2 % RCUTORTURE_RDR_NBITS));
-	/* Can't enable bh w/irq disabled. */
-	if ((mask & RCUTORTURE_RDR_IRQ) &&
-	    ((!(mask & RCUTORTURE_RDR_BH) && (oldmask & RCUTORTURE_RDR_BH)) ||
-	     (!(mask & RCUTORTURE_RDR_RBH) && (oldmask & RCUTORTURE_RDR_RBH))))
-		mask |= RCUTORTURE_RDR_BH | RCUTORTURE_RDR_RBH;
+
+	/*
+	 * Can't enable bh w/irq disabled.
+	 */
+	tmp = atomic_bhs | nonatomic_bhs;
+	if (mask & RCUTORTURE_RDR_IRQ)
+		mask |= oldmask & tmp;
+
+	/*
+	 * Ideally these sequences would be detected in debug builds
+	 * (regardless of RT), but until then don't stop testing
+	 * them on non-RT.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * Can't release the outermost rcu lock in an irq disabled
+		 * section without preemption also being disabled, if irqs
+		 * had ever been enabled during this RCU critical section
+		 * (could leak a special flag and delay reporting the qs).
+		 */
+		if ((oldmask & RCUTORTURE_RDR_RCU) &&
+		    (mask & RCUTORTURE_RDR_IRQ) &&
+		    !(mask & preempts))
+			mask |= RCUTORTURE_RDR_RCU;
+
+		/* Can't modify atomic bh in non-atomic context */
+		if ((oldmask & atomic_bhs) && (mask & atomic_bhs) &&
+		    !(mask & preempts_irq)) {
+			mask |= oldmask & preempts_irq;
+			if (mask & RCUTORTURE_RDR_IRQ)
+				mask |= oldmask & tmp;
+		}
+		if ((mask & atomic_bhs) && !(mask & preempts_irq))
+			mask |= RCUTORTURE_RDR_PREEMPT;
+
+		/* Can't modify non-atomic bh in atomic context */
+		tmp = nonatomic_bhs;
+		if (oldmask & preempts_irq)
+			mask &= ~tmp;
+		if ((oldmask | mask) & preempts_irq)
+			mask |= oldmask & tmp;
+	}
+
 	return mask ?: RCUTORTURE_RDR_RCU;
 }
 
@@ -1725,7 +1793,7 @@
 // Give the scheduler a chance, even on nohz_full CPUs.
 static void rcu_torture_fwd_prog_cond_resched(unsigned long iter)
 {
-	if (IS_ENABLED(CONFIG_PREEMPT) && IS_ENABLED(CONFIG_NO_HZ_FULL)) {
+	if (IS_ENABLED(CONFIG_PREEMPTION) && IS_ENABLED(CONFIG_NO_HZ_FULL)) {
 		// Real call_rcu() floods hit userspace, so emulate that.
 		if (need_resched() || (iter & 0xfff))
 			schedule();
diff -Nur linux-5.4.5/kernel/rcu/srcutiny.c linux-5.4.5-new/kernel/rcu/srcutiny.c
--- linux-5.4.5/kernel/rcu/srcutiny.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/srcutiny.c	2020-06-15 16:12:25.835715425 +0300
@@ -103,7 +103,7 @@
 
 /*
  * Workqueue handler to drive one grace period and invoke any callbacks
- * that become ready as a result.  Single-CPU and !PREEMPT operation
+ * that become ready as a result.  Single-CPU and !PREEMPTION operation
  * means that we get away with murder on synchronization.  ;-)
  */
 void srcu_drive_gp(struct work_struct *wp)
diff -Nur linux-5.4.5/kernel/rcu/srcutree.c linux-5.4.5-new/kernel/rcu/srcutree.c
--- linux-5.4.5/kernel/rcu/srcutree.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/srcutree.c	2020-06-15 16:12:25.835715425 +0300
@@ -25,6 +25,7 @@
 #include <linux/delay.h>
 #include <linux/module.h>
 #include <linux/srcu.h>
+#include <linux/locallock.h>
 
 #include "rcu.h"
 #include "rcu_segcblist.h"
@@ -735,6 +736,7 @@
 	smp_mb(); /* D */  /* Pairs with C. */
 }
 
+static DEFINE_LOCAL_IRQ_LOCK(sp_llock);
 /*
  * If SRCU is likely idle, return true, otherwise return false.
  *
@@ -764,13 +766,13 @@
 	unsigned long t;
 
 	/* If the local srcu_data structure has callbacks, not idle.  */
-	local_irq_save(flags);
+	local_lock_irqsave(sp_llock, flags);
 	sdp = this_cpu_ptr(ssp->sda);
 	if (rcu_segcblist_pend_cbs(&sdp->srcu_cblist)) {
-		local_irq_restore(flags);
+		local_unlock_irqrestore(sp_llock, flags);
 		return false; /* Callbacks already present, so not idle. */
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(sp_llock, flags);
 
 	/*
 	 * No local callbacks, so probabalistically probe global state.
@@ -850,7 +852,7 @@
 	}
 	rhp->func = func;
 	idx = srcu_read_lock(ssp);
-	local_irq_save(flags);
+	local_lock_irqsave(sp_llock, flags);
 	sdp = this_cpu_ptr(ssp->sda);
 	spin_lock_rcu_node(sdp);
 	rcu_segcblist_enqueue(&sdp->srcu_cblist, rhp, false);
@@ -866,7 +868,8 @@
 		sdp->srcu_gp_seq_needed_exp = s;
 		needexp = true;
 	}
-	spin_unlock_irqrestore_rcu_node(sdp, flags);
+	spin_unlock_rcu_node(sdp);
+	local_unlock_irqrestore(sp_llock, flags);
 	if (needgp)
 		srcu_funnel_gp_start(ssp, sdp, s, do_norm);
 	else if (needexp)
diff -Nur linux-5.4.5/kernel/rcu/tree.c linux-5.4.5-new/kernel/rcu/tree.c
--- linux-5.4.5/kernel/rcu/tree.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/tree.c	2020-06-15 16:12:25.835715425 +0300
@@ -100,8 +100,10 @@
 static bool dump_tree;
 module_param(dump_tree, bool, 0444);
 /* By default, use RCU_SOFTIRQ instead of rcuc kthreads. */
-static bool use_softirq = 1;
+static bool use_softirq = !IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(use_softirq, bool, 0444);
+#endif
 /* Control rcu_node-tree auto-balancing at boot time. */
 static bool rcu_fanout_exact;
 module_param(rcu_fanout_exact, bool, 0444);
@@ -1094,6 +1096,7 @@
 		    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&
 		    (rnp->ffmask & rdp->grpmask)) {
 			init_irq_work(&rdp->rcu_iw, rcu_iw_handler);
+			rdp->rcu_iw.flags = IRQ_WORK_HARD_IRQ;
 			rdp->rcu_iw_pending = true;
 			rdp->rcu_iw_gp_seq = rnp->gp_seq;
 			irq_work_queue_on(&rdp->rcu_iw, rdp->cpu);
@@ -2667,9 +2670,9 @@
 
 /*
  * During early boot, any blocking grace-period wait automatically
- * implies a grace period.  Later on, this is never the case for PREEMPT.
+ * implies a grace period.  Later on, this is never the case for PREEMPTION.
  *
- * Howevr, because a context switch is a grace period for !PREEMPT, any
+ * Howevr, because a context switch is a grace period for !PREEMPTION, any
  * blocking grace-period wait automatically implies a grace period if
  * there is only one CPU online at any point time during execution of
  * either synchronize_rcu() or synchronize_rcu_expedited().  It is OK to
diff -Nur linux-5.4.5/kernel/rcu/tree_exp.h linux-5.4.5-new/kernel/rcu/tree_exp.h
--- linux-5.4.5/kernel/rcu/tree_exp.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/tree_exp.h	2020-06-15 16:12:25.835715425 +0300
@@ -670,7 +670,7 @@
 	}
 }
 
-/* PREEMPT=y, so no PREEMPT=n expedited grace period to clean up after. */
+/* PREEMPTION=y, so no PREEMPTION=n expedited grace period to clean up after. */
 static void sync_sched_exp_online_cleanup(int cpu)
 {
 }
diff -Nur linux-5.4.5/kernel/rcu/tree_plugin.h linux-5.4.5-new/kernel/rcu/tree_plugin.h
--- linux-5.4.5/kernel/rcu/tree_plugin.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/tree_plugin.h	2020-06-15 16:12:25.839715410 +0300
@@ -287,10 +287,14 @@
 	struct task_struct *t = current;
 	struct rcu_data *rdp = this_cpu_ptr(&rcu_data);
 	struct rcu_node *rnp;
+	int sleeping_l = 0;
 
 	trace_rcu_utilization(TPS("Start context switch"));
 	lockdep_assert_irqs_disabled();
-	WARN_ON_ONCE(!preempt && t->rcu_read_lock_nesting > 0);
+#if defined(CONFIG_PREEMPT_RT)
+	sleeping_l = t->sleeping_lock;
+#endif
+	WARN_ON_ONCE(!preempt && t->rcu_read_lock_nesting > 0 && !sleeping_l);
 	if (t->rcu_read_lock_nesting > 0 &&
 	    !t->rcu_read_unlock_special.b.blocked) {
 
@@ -788,7 +792,7 @@
 }
 
 /*
- * Note a quiescent state for PREEMPT=n.  Because we do not need to know
+ * Note a quiescent state for PREEMPTION=n.  Because we do not need to know
  * how many quiescent states passed, just if there was at least one since
  * the start of the grace period, this just sets a flag.  The caller must
  * have disabled preemption.
@@ -838,7 +842,7 @@
 EXPORT_SYMBOL_GPL(rcu_all_qs);
 
 /*
- * Note a PREEMPT=n context switch.  The caller must have disabled interrupts.
+ * Note a PREEMPTION=n context switch.  The caller must have disabled interrupts.
  */
 void rcu_note_context_switch(bool preempt)
 {
diff -Nur linux-5.4.5/kernel/rcu/update.c linux-5.4.5-new/kernel/rcu/update.c
--- linux-5.4.5/kernel/rcu/update.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/rcu/update.c	2020-06-15 16:12:25.839715410 +0300
@@ -55,8 +55,10 @@
 module_param(rcu_expedited, int, 0);
 extern int rcu_normal; /* from sysctl */
 module_param(rcu_normal, int, 0);
-static int rcu_normal_after_boot;
+static int rcu_normal_after_boot = IS_ENABLED(CONFIG_PREEMPT_RT);
+#ifndef CONFIG_PREEMPT_RT
 module_param(rcu_normal_after_boot, int, 0);
+#endif
 #endif /* #ifndef CONFIG_TINY_RCU */
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
diff -Nur linux-5.4.5/kernel/sched/completion.c linux-5.4.5-new/kernel/sched/completion.c
--- linux-5.4.5/kernel/sched/completion.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/completion.c	2020-06-15 16:12:25.875715284 +0300
@@ -29,12 +29,12 @@
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 
 	if (x->done != UINT_MAX)
 		x->done++;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	swake_up_locked(&x->wait);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete);
 
@@ -58,10 +58,10 @@
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	x->done = UINT_MAX;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	swake_up_all_locked(&x->wait);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete_all);
 
@@ -70,20 +70,20 @@
 		   long (*action)(long), long timeout, int state)
 {
 	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
+		DECLARE_SWAITQUEUE(wait);
 
-		__add_wait_queue_entry_tail_exclusive(&x->wait, &wait);
 		do {
 			if (signal_pending_state(state, current)) {
 				timeout = -ERESTARTSYS;
 				break;
 			}
+			__prepare_to_swait(&x->wait, &wait);
 			__set_current_state(state);
-			spin_unlock_irq(&x->wait.lock);
+			raw_spin_unlock_irq(&x->wait.lock);
 			timeout = action(timeout);
-			spin_lock_irq(&x->wait.lock);
+			raw_spin_lock_irq(&x->wait.lock);
 		} while (!x->done && timeout);
-		__remove_wait_queue(&x->wait, &wait);
+		__finish_swait(&x->wait, &wait);
 		if (!x->done)
 			return timeout;
 	}
@@ -100,9 +100,9 @@
 
 	complete_acquire(x);
 
-	spin_lock_irq(&x->wait.lock);
+	raw_spin_lock_irq(&x->wait.lock);
 	timeout = do_wait_for_common(x, action, timeout, state);
-	spin_unlock_irq(&x->wait.lock);
+	raw_spin_unlock_irq(&x->wait.lock);
 
 	complete_release(x);
 
@@ -291,12 +291,12 @@
 	if (!READ_ONCE(x->done))
 		return false;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = false;
 	else if (x->done != UINT_MAX)
 		x->done--;
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(try_wait_for_completion);
@@ -322,8 +322,8 @@
 	 * otherwise we can end up freeing the completion before complete()
 	 * is done referencing it.
 	 */
-	spin_lock_irqsave(&x->wait.lock, flags);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return true;
 }
 EXPORT_SYMBOL(completion_done);
diff -Nur linux-5.4.5/kernel/sched/core.c linux-5.4.5-new/kernel/sched/core.c
--- linux-5.4.5/kernel/sched/core.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/core.c	2020-06-15 16:12:25.875715284 +0300
@@ -56,7 +56,11 @@
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -410,9 +414,15 @@
 #endif
 #endif
 
-static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
+static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task,
+			 bool sleeper)
 {
-	struct wake_q_node *node = &task->wake_q;
+	struct wake_q_node *node;
+
+	if (sleeper)
+		node = &task->wake_q_sleeper;
+	else
+		node = &task->wake_q;
 
 	/*
 	 * Atomically grab the task, if ->wake_q is !nil already it means
@@ -448,7 +458,13 @@
  */
 void wake_q_add(struct wake_q_head *head, struct task_struct *task)
 {
-	if (__wake_q_add(head, task))
+	if (__wake_q_add(head, task, false))
+		get_task_struct(task);
+}
+
+void wake_q_add_sleeper(struct wake_q_head *head, struct task_struct *task)
+{
+	if (__wake_q_add(head, task, true))
 		get_task_struct(task);
 }
 
@@ -471,28 +487,39 @@
  */
 void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)
 {
-	if (!__wake_q_add(head, task))
+	if (!__wake_q_add(head, task, false))
 		put_task_struct(task);
 }
 
-void wake_up_q(struct wake_q_head *head)
+void __wake_up_q(struct wake_q_head *head, bool sleeper)
 {
 	struct wake_q_node *node = head->first;
 
 	while (node != WAKE_Q_TAIL) {
 		struct task_struct *task;
 
-		task = container_of(node, struct task_struct, wake_q);
+		if (sleeper)
+			task = container_of(node, struct task_struct, wake_q_sleeper);
+		else
+			task = container_of(node, struct task_struct, wake_q);
+
 		BUG_ON(!task);
 		/* Task can safely be re-inserted now: */
 		node = node->next;
-		task->wake_q.next = NULL;
 
+		if (sleeper)
+			task->wake_q_sleeper.next = NULL;
+		else
+			task->wake_q.next = NULL;
 		/*
 		 * wake_up_process() executes a full barrier, which pairs with
 		 * the queueing in wake_q_add() so as not to miss wakeups.
 		 */
-		wake_up_process(task);
+		if (sleeper)
+			wake_up_lock_sleeper(task);
+		else
+			wake_up_process(task);
+
 		put_task_struct(task);
 	}
 }
@@ -528,6 +555,48 @@
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+
+static int tsk_is_polling(struct task_struct *p)
+{
+#ifdef TIF_POLLING_NRFLAG
+	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
+#else
+	return 0;
+#endif
+}
+
+void resched_curr_lazy(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	int cpu;
+
+	if (!sched_feat(PREEMPT_LAZY)) {
+		resched_curr(rq);
+		return;
+	}
+
+	lockdep_assert_held(&rq->lock);
+
+	if (test_tsk_need_resched(curr))
+		return;
+
+	if (test_tsk_need_resched_lazy(curr))
+		return;
+
+	set_tsk_need_resched_lazy(curr);
+
+	cpu = cpu_of(rq);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED_LAZY must be visible before we test polling */
+	smp_mb();
+	if (!tsk_is_polling(curr))
+		smp_send_reschedule(cpu);
+}
+#endif
+
 void resched_cpu(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -1460,7 +1529,7 @@
 	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 		return false;
 
-	if (is_per_cpu_kthread(p))
+	if (is_per_cpu_kthread(p) || __migrate_disabled(p))
 		return cpu_online(cpu);
 
 	return cpu_active(cpu);
@@ -1509,6 +1578,7 @@
 struct migration_arg {
 	struct task_struct *task;
 	int dest_cpu;
+	bool done;
 };
 
 /*
@@ -1544,6 +1614,11 @@
 	struct task_struct *p = arg->task;
 	struct rq *rq = this_rq();
 	struct rq_flags rf;
+	int dest_cpu = arg->dest_cpu;
+
+	/* We don't look at arg after this point. */
+	smp_mb();
+	arg->done = true;
 
 	/*
 	 * The original target CPU might have gone down and we might
@@ -1566,9 +1641,9 @@
 	 */
 	if (task_rq(p) == rq) {
 		if (task_on_rq_queued(p))
-			rq = __migrate_task(rq, &rf, p, arg->dest_cpu);
+			rq = __migrate_task(rq, &rf, p, dest_cpu);
 		else
-			p->wake_cpu = arg->dest_cpu;
+			p->wake_cpu = dest_cpu;
 	}
 	rq_unlock(rq, &rf);
 	raw_spin_unlock(&p->pi_lock);
@@ -1584,9 +1659,18 @@
 void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask)
 {
 	cpumask_copy(&p->cpus_mask, new_mask);
-	p->nr_cpus_allowed = cpumask_weight(new_mask);
+	if (p->cpus_ptr == &p->cpus_mask)
+		p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+int __migrate_disabled(struct task_struct *p)
+{
+	return p->migrate_disable;
+}
+EXPORT_SYMBOL_GPL(__migrate_disabled);
+#endif
+
 void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
 	struct rq *rq = task_rq(p);
@@ -1653,7 +1737,7 @@
 		goto out;
 	}
 
-	if (cpumask_equal(p->cpus_ptr, new_mask))
+	if (cpumask_equal(&p->cpus_mask, new_mask))
 		goto out;
 
 	dest_cpu = cpumask_any_and(cpu_valid_mask, new_mask);
@@ -1675,7 +1759,8 @@
 	}
 
 	/* Can the task run on the task's current CPU? If so, we're done */
-	if (cpumask_test_cpu(task_cpu(p), new_mask))
+	if (cpumask_test_cpu(task_cpu(p), new_mask) ||
+	    p->cpus_ptr != &p->cpus_mask)
 		goto out;
 
 	if (task_running(rq, p) || p->state == TASK_WAKING) {
@@ -1872,6 +1957,18 @@
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+static bool check_task_state(struct task_struct *p, long match_state)
+{
+	bool match = false;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	if (p->state == match_state || p->saved_state == match_state)
+		match = true;
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	return match;
+}
+
 /*
  * wait_task_inactive - wait for a thread to unschedule.
  *
@@ -1916,7 +2013,7 @@
 		 * is actually now running somewhere else!
 		 */
 		while (task_running(rq, p)) {
-			if (match_state && unlikely(p->state != match_state))
+			if (match_state && !check_task_state(p, match_state))
 				return 0;
 			cpu_relax();
 		}
@@ -1931,7 +2028,8 @@
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
-		if (!match_state || p->state == match_state)
+		if (!match_state || p->state == match_state ||
+		    p->saved_state == match_state)
 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
 		task_rq_unlock(rq, p, &rf);
 
@@ -2519,6 +2617,8 @@
 	int cpu, success = 0;
 
 	preempt_disable();
+
+#ifndef CONFIG_PREEMPT_RT
 	if (p == current) {
 		/*
 		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
@@ -2541,7 +2641,7 @@
 		trace_sched_wakeup(p);
 		goto out;
 	}
-
+#endif
 	/*
 	 * If we are going to wake up a thread waiting for CONDITION we
 	 * need to ensure that CONDITION=1 done by the caller can not be
@@ -2550,8 +2650,27 @@
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	smp_mb__after_spinlock();
-	if (!(p->state & state))
-		goto unlock;
+	if (!(p->state & state)) {
+		/*
+		 * The task might be running due to a spinlock sleeper
+		 * wakeup. Check the saved state and set it to running
+		 * if the wakeup condition is true.
+		 */
+		if (!(wake_flags & WF_LOCK_SLEEPER)) {
+			if (p->saved_state & state) {
+				p->saved_state = TASK_RUNNING;
+				success = 1;
+			}
+		}
+		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+		goto out_nostat;
+	}
+	/*
+	 * If this is a regular wakeup, then we can unconditionally
+	 * clear the saved state of a "lock sleeper".
+	 */
+	if (!(wake_flags & WF_LOCK_SLEEPER))
+		p->saved_state = TASK_RUNNING;
 
 	trace_sched_waking(p);
 
@@ -2643,9 +2762,12 @@
 	ttwu_queue(p, cpu, wake_flags);
 unlock:
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+#ifndef CONFIG_PREEMPT_RT
 out:
+#endif
 	if (success)
 		ttwu_stat(p, cpu, wake_flags);
+out_nostat:
 	preempt_enable();
 
 	return success;
@@ -2668,6 +2790,18 @@
 }
 EXPORT_SYMBOL(wake_up_process);
 
+/**
+ * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
+ * @p: The process to be woken up.
+ *
+ * Same as wake_up_process() above, but wake_flags=WF_LOCK_SLEEPER to indicate
+ * the nature of the wakeup.
+ */
+int wake_up_lock_sleeper(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_UNINTERRUPTIBLE, WF_LOCK_SLEEPER);
+}
+
 int wake_up_state(struct task_struct *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
@@ -2910,6 +3044,9 @@
 	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(p)->preempt_lazy_count = 0;
+#endif
 #ifdef CONFIG_SMP
 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
@@ -3237,23 +3374,18 @@
 	 *   provided by mmdrop(),
 	 * - a sync_core for SYNC_CORE.
 	 */
+	/*
+	 * We use mmdrop_delayed() here so we don't have to do the
+	 * full __mmdrop() when we are the last user.
+	 */
 	if (mm) {
 		membarrier_mm_sync_core_before_usermode(mm);
-		mmdrop(mm);
+		mmdrop_delayed(mm);
 	}
 	if (unlikely(prev_state == TASK_DEAD)) {
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 
-		/*
-		 * Remove function-return probe instances associated with this
-		 * task and put them back on the free list.
-		 */
-		kprobe_flush_task(prev);
-
-		/* Task is done with its stack. */
-		put_task_stack(prev);
-
 		put_task_struct_rcu_user(prev);
 	}
 
@@ -3956,6 +4088,8 @@
 	BUG();
 }
 
+static void migrate_disabled_sched(struct task_struct *p);
+
 /*
  * __schedule() is the main scheduler function.
  *
@@ -4026,6 +4160,9 @@
 	rq_lock(rq, &rf);
 	smp_mb__after_spinlock();
 
+	if (__migrate_disabled(prev))
+		migrate_disabled_sched(prev);
+
 	/* Promote REQ to ACT */
 	rq->clock_update_flags <<= 1;
 	update_rq_clock(rq);
@@ -4047,6 +4184,7 @@
 
 	next = pick_next_task(rq, prev, &rf);
 	clear_tsk_need_resched(prev);
+	clear_tsk_need_resched_lazy(prev);
 	clear_preempt_need_resched();
 
 	if (likely(prev != next)) {
@@ -4234,6 +4372,30 @@
 	} while (need_resched());
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+/*
+ * If TIF_NEED_RESCHED is then we allow to be scheduled away since this is
+ * set by a RT task. Oterwise we try to avoid beeing scheduled out as long as
+ * preempt_lazy_count counter >0.
+ */
+static __always_inline int preemptible_lazy(void)
+{
+	if (test_thread_flag(TIF_NEED_RESCHED))
+		return 1;
+	if (current_thread_info()->preempt_lazy_count)
+		return 0;
+	return 1;
+}
+
+#else
+
+static inline int preemptible_lazy(void)
+{
+	return 1;
+}
+
+#endif
+
 #ifdef CONFIG_PREEMPTION
 /*
  * This is the entry point to schedule() from in-kernel preemption
@@ -4247,7 +4409,8 @@
 	 */
 	if (likely(!preemptible()))
 		return;
-
+	if (!preemptible_lazy())
+		return;
 	preempt_schedule_common();
 }
 NOKPROBE_SYMBOL(preempt_schedule);
@@ -4274,6 +4437,9 @@
 	if (likely(!preemptible()))
 		return;
 
+	if (!preemptible_lazy())
+		return;
+
 	do {
 		/*
 		 * Because the function tracer can trace preempt_count_sub()
@@ -6064,7 +6230,9 @@
 
 	/* Set the preempt count _outside_ the spinlocks! */
 	init_idle_preempt_count(idle, cpu);
-
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(idle)->preempt_lazy_count = 0;
+#endif
 	/*
 	 * The idle tasks have their own, simple scheduling class:
 	 */
@@ -6169,6 +6337,8 @@
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_HOTPLUG_CPU
+static DEFINE_PER_CPU(struct mm_struct *, idle_last_mm);
+
 /*
  * Ensure that the idle task is using init_mm right before its CPU goes
  * offline.
@@ -6184,7 +6354,11 @@
 		current->active_mm = &init_mm;
 		finish_arch_post_lock_switch();
 	}
-	mmdrop(mm);
+	/*
+	 * Defer the cleanup to an alive cpu. On RT we can neither
+	 * call mmdrop() nor mmdrop_delayed() from here.
+	 */
+	per_cpu(idle_last_mm, smp_processor_id()) = mm;
 }
 
 /*
@@ -6262,6 +6436,7 @@
 			break;
 
 		next = __pick_migrate_task(rq);
+		WARN_ON_ONCE(__migrate_disabled(next));
 
 		/*
 		 * Rules for changing task_struct::cpus_mask are holding
@@ -6490,6 +6665,10 @@
 	update_max_interval();
 	nohz_balance_exit_idle(rq);
 	hrtick_clear(rq);
+	if (per_cpu(idle_last_mm, cpu)) {
+		mmdrop_delayed(per_cpu(idle_last_mm, cpu));
+		per_cpu(idle_last_mm, cpu) = NULL;
+	}
 	return 0;
 }
 #endif
@@ -6721,7 +6900,7 @@
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 static inline int preempt_count_equals(int preempt_offset)
 {
-	int nested = preempt_count() + rcu_preempt_depth();
+	int nested = preempt_count() + sched_rcu_preempt_depth();
 
 	return (nested == preempt_offset);
 }
@@ -7947,3 +8126,167 @@
 };
 
 #undef CREATE_TRACE_POINTS
+
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+
+static inline void
+update_nr_migratory(struct task_struct *p, long delta)
+{
+	if (unlikely((p->sched_class == &rt_sched_class ||
+		      p->sched_class == &dl_sched_class) &&
+		      p->nr_cpus_allowed > 1)) {
+		if (p->sched_class == &rt_sched_class)
+			task_rq(p)->rt.rt_nr_migratory += delta;
+		else
+			task_rq(p)->dl.dl_nr_migratory += delta;
+	}
+}
+
+static inline void
+migrate_disable_update_cpus_allowed(struct task_struct *p)
+{
+	p->cpus_ptr = cpumask_of(smp_processor_id());
+	update_nr_migratory(p, -1);
+	p->nr_cpus_allowed = 1;
+}
+
+static inline void
+migrate_enable_update_cpus_allowed(struct task_struct *p)
+{
+	struct rq *rq;
+	struct rq_flags rf;
+
+	rq = task_rq_lock(p, &rf);
+	p->cpus_ptr = &p->cpus_mask;
+	p->nr_cpus_allowed = cpumask_weight(&p->cpus_mask);
+	update_nr_migratory(p, 1);
+	task_rq_unlock(rq, p, &rf);
+}
+
+void migrate_disable(void)
+{
+	preempt_disable();
+
+	if (++current->migrate_disable == 1) {
+		this_rq()->nr_pinned++;
+		preempt_lazy_disable();
+#ifdef CONFIG_SCHED_DEBUG
+		WARN_ON_ONCE(current->pinned_on_cpu >= 0);
+		current->pinned_on_cpu = smp_processor_id();
+#endif
+	}
+
+	preempt_enable();
+}
+EXPORT_SYMBOL(migrate_disable);
+
+static void migrate_disabled_sched(struct task_struct *p)
+{
+	if (p->migrate_disable_scheduled)
+		return;
+
+	migrate_disable_update_cpus_allowed(p);
+	p->migrate_disable_scheduled = 1;
+}
+
+void migrate_enable(void)
+{
+	struct task_struct *p = current;
+	struct rq *rq = this_rq();
+	int cpu = task_cpu(p);
+
+	WARN_ON_ONCE(p->migrate_disable <= 0);
+	if (p->migrate_disable > 1) {
+		p->migrate_disable--;
+		return;
+	}
+
+	preempt_disable();
+
+#ifdef CONFIG_SCHED_DEBUG
+	WARN_ON_ONCE(current->pinned_on_cpu != cpu);
+	current->pinned_on_cpu = -1;
+#endif
+
+	WARN_ON_ONCE(rq->nr_pinned < 1);
+
+	p->migrate_disable = 0;
+	rq->nr_pinned--;
+#ifdef CONFIG_HOTPLUG_CPU
+	if (rq->nr_pinned == 0 && unlikely(!cpu_active(cpu)) &&
+	    takedown_cpu_task)
+		wake_up_process(takedown_cpu_task);
+#endif
+
+	if (!p->migrate_disable_scheduled)
+		goto out;
+
+	p->migrate_disable_scheduled = 0;
+
+	migrate_enable_update_cpus_allowed(p);
+
+	WARN_ON(smp_processor_id() != cpu);
+	if (!is_cpu_allowed(p, cpu)) {
+		struct migration_arg arg = { .task = p };
+		struct cpu_stop_work work;
+		struct rq_flags rf;
+
+		rq = task_rq_lock(p, &rf);
+		update_rq_clock(rq);
+		arg.dest_cpu = select_fallback_rq(cpu, p);
+		task_rq_unlock(rq, p, &rf);
+
+		stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
+				    &arg, &work);
+		__schedule(true);
+		if (!work.disabled) {
+			while (!arg.done)
+				cpu_relax();
+		}
+	}
+
+out:
+	preempt_lazy_enable();
+	preempt_enable();
+}
+EXPORT_SYMBOL(migrate_enable);
+
+int cpu_nr_pinned(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+
+	return rq->nr_pinned;
+}
+
+#elif !defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+static void migrate_disabled_sched(struct task_struct *p)
+{
+}
+
+void migrate_disable(void)
+{
+#ifdef CONFIG_SCHED_DEBUG
+	current->migrate_disable++;
+#endif
+	barrier();
+}
+EXPORT_SYMBOL(migrate_disable);
+
+void migrate_enable(void)
+{
+#ifdef CONFIG_SCHED_DEBUG
+	struct task_struct *p = current;
+
+	WARN_ON_ONCE(p->migrate_disable <= 0);
+	p->migrate_disable--;
+#endif
+	barrier();
+}
+EXPORT_SYMBOL(migrate_enable);
+
+#else
+static void migrate_disabled_sched(struct task_struct *p)
+{
+}
+
+#endif
diff -Nur linux-5.4.5/kernel/sched/debug.c linux-5.4.5-new/kernel/sched/debug.c
--- linux-5.4.5/kernel/sched/debug.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/debug.c	2020-06-15 16:12:25.875715284 +0300
@@ -958,6 +958,10 @@
 		P(dl.runtime);
 		P(dl.deadline);
 	}
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT_RT)
+	P(migrate_disable);
+#endif
+	P(nr_cpus_allowed);
 #undef PN_SCHEDSTAT
 #undef PN
 #undef __PN
diff -Nur linux-5.4.5/kernel/sched/fair.c linux-5.4.5-new/kernel/sched/fair.c
--- linux-5.4.5/kernel/sched/fair.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/fair.c	2020-06-15 16:12:25.875715284 +0300
@@ -4122,7 +4122,7 @@
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
 	if (delta_exec > ideal_runtime) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		/*
 		 * The current task ran long enough, ensure it doesn't get
 		 * re-elected due to buddy favours.
@@ -4146,7 +4146,7 @@
 		return;
 
 	if (delta > ideal_runtime)
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static void
@@ -4289,7 +4289,7 @@
 	 * validating it and just reschedule.
 	 */
 	if (queued) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		return;
 	}
 	/*
@@ -4414,7 +4414,7 @@
 	 * hierarchy can be throttled
 	 */
 	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static __always_inline
@@ -5127,7 +5127,7 @@
 
 		if (delta < 0) {
 			if (rq->curr == p)
-				resched_curr(rq);
+				resched_curr_lazy(rq);
 			return;
 		}
 		hrtick_start(rq, delta);
@@ -6729,7 +6729,7 @@
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 	/*
 	 * Only set the backward buddy when the current task is still
 	 * on the rq. This can happen when a wakeup gets interleaved
@@ -9984,7 +9984,7 @@
 		 * 'current' within the tree based on its new key value.
 		 */
 		swap(curr->vruntime, se->vruntime);
-		resched_curr(rq);
+		resched_curr_lazy(rq);
 	}
 
 	se->vruntime -= cfs_rq->min_vruntime;
@@ -10008,7 +10008,7 @@
 	 */
 	if (rq->curr == p) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff -Nur linux-5.4.5/kernel/sched/features.h linux-5.4.5-new/kernel/sched/features.h
--- linux-5.4.5/kernel/sched/features.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/features.h	2020-06-15 16:12:25.875715284 +0300
@@ -45,11 +45,19 @@
  */
 SCHED_FEAT(NONTASK_CAPACITY, true)
 
+#ifdef CONFIG_PREEMPT_RT
+SCHED_FEAT(TTWU_QUEUE, false)
+# ifdef CONFIG_PREEMPT_LAZY
+SCHED_FEAT(PREEMPT_LAZY, true)
+# endif
+#else
+
 /*
  * Queue remote wakeups on the target CPU and process them
  * using the scheduler IPI. Reduces rq->lock contention/bounces.
  */
 SCHED_FEAT(TTWU_QUEUE, true)
+#endif
 
 /*
  * When doing wakeups, attempt to limit superfluous scans of the LLC domain.
diff -Nur linux-5.4.5/kernel/sched/isolation.c linux-5.4.5-new/kernel/sched/isolation.c
--- linux-5.4.5/kernel/sched/isolation.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/isolation.c	2020-06-15 16:12:25.879715270 +0300
@@ -8,6 +8,7 @@
  *
  */
 #include "sched.h"
+#include "../../mm/internal.h"
 
 DEFINE_STATIC_KEY_FALSE(housekeeping_overridden);
 EXPORT_SYMBOL_GPL(housekeeping_overridden);
@@ -139,10 +140,21 @@
 static int __init housekeeping_nohz_full_setup(char *str)
 {
 	unsigned int flags;
+	int ret;
 
 	flags = HK_FLAG_TICK | HK_FLAG_WQ | HK_FLAG_TIMER | HK_FLAG_RCU | HK_FLAG_MISC;
 
-	return housekeeping_setup(str, flags);
+	ret = housekeeping_setup(str, flags);
+
+	/*
+	 * Protect struct pagevec with a lock instead using preemption disable;
+	 * with lock protection, remote handling of events instead of queue
+	 * work on remote cpu is default behavior.
+	 */
+	if (ret)
+		static_branch_enable(&use_pvec_lock);
+
+	return ret;
 }
 __setup("nohz_full=", housekeeping_nohz_full_setup);
 
diff -Nur linux-5.4.5/kernel/sched/sched.h linux-5.4.5-new/kernel/sched/sched.h
--- linux-5.4.5/kernel/sched/sched.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/sched.h	2020-06-15 16:12:25.879715270 +0300
@@ -999,6 +999,10 @@
 	/* Must be inspected within a rcu lock section */
 	struct cpuidle_state	*idle_state;
 #endif
+
+#if defined(CONFIG_PREEMPT_RT) && defined(CONFIG_SMP)
+	int			nr_pinned;
+#endif
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1644,6 +1648,7 @@
 #define WF_SYNC			0x01		/* Waker goes to sleep after wakeup */
 #define WF_FORK			0x02		/* Child wakeup after fork */
 #define WF_MIGRATED		0x4		/* Internal use, task got migrated */
+#define WF_LOCK_SLEEPER		0x08		/* wakeup spinlock "sleeper" */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
@@ -1871,6 +1876,15 @@
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
+#ifdef CONFIG_PREEMPT_LAZY
+extern void resched_curr_lazy(struct rq *rq);
+#else
+static inline void resched_curr_lazy(struct rq *rq)
+{
+	resched_curr(rq);
+}
+#endif
+
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
diff -Nur linux-5.4.5/kernel/sched/swait.c linux-5.4.5-new/kernel/sched/swait.c
--- linux-5.4.5/kernel/sched/swait.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/swait.c	2020-06-15 16:12:25.879715270 +0300
@@ -32,6 +32,25 @@
 }
 EXPORT_SYMBOL(swake_up_locked);
 
+void swake_up_all_locked(struct swait_queue_head *q)
+{
+	struct swait_queue *curr;
+	int wakes = 0;
+
+	while (!list_empty(&q->task_list)) {
+
+		curr = list_first_entry(&q->task_list, typeof(*curr),
+					task_list);
+		wake_up_process(curr->task);
+		list_del_init(&curr->task_list);
+		wakes++;
+	}
+	if (pm_in_action)
+		return;
+	WARN(wakes > 2, "complete_all() with %d waiters\n", wakes);
+}
+EXPORT_SYMBOL(swake_up_all_locked);
+
 void swake_up_one(struct swait_queue_head *q)
 {
 	unsigned long flags;
@@ -51,6 +70,7 @@
 	struct swait_queue *curr;
 	LIST_HEAD(tmp);
 
+	WARN_ON(irqs_disabled());
 	raw_spin_lock_irq(&q->lock);
 	list_splice_init(&q->task_list, &tmp);
 	while (!list_empty(&tmp)) {
@@ -69,7 +89,7 @@
 }
 EXPORT_SYMBOL(swake_up_all);
 
-static void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait)
+void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait)
 {
 	wait->task = current;
 	if (list_empty(&wait->task_list))
diff -Nur linux-5.4.5/kernel/sched/topology.c linux-5.4.5-new/kernel/sched/topology.c
--- linux-5.4.5/kernel/sched/topology.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sched/topology.c	2020-06-15 16:12:25.879715270 +0300
@@ -502,6 +502,7 @@
 	rd->rto_cpu = -1;
 	raw_spin_lock_init(&rd->rto_lock);
 	init_irq_work(&rd->rto_push_work, rto_push_irq_work_func);
+	rd->rto_push_work.flags |= IRQ_WORK_HARD_IRQ;
 #endif
 
 	init_dl_bw(&rd->dl_bw);
diff -Nur linux-5.4.5/kernel/signal.c linux-5.4.5-new/kernel/signal.c
--- linux-5.4.5/kernel/signal.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/signal.c	2020-06-15 16:12:26.139714355 +0300
@@ -20,6 +20,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/rt.h>
 #include <linux/file.h>
 #include <linux/fs.h>
 #include <linux/proc_fs.h>
@@ -403,13 +404,30 @@
 	}
 }
 
+static inline struct sigqueue *get_task_cache(struct task_struct *t)
+{
+	struct sigqueue *q = t->sigqueue_cache;
+
+	if (cmpxchg(&t->sigqueue_cache, q, NULL) != q)
+		return NULL;
+	return q;
+}
+
+static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
+{
+	if (cmpxchg(&t->sigqueue_cache, NULL, q) == NULL)
+		return 0;
+	return 1;
+}
+
 /*
  * allocate a new signal queue record
  * - this may be called without locks if and only if t == current, otherwise an
  *   appropriate lock must be held to stop the target task from exiting
  */
 static struct sigqueue *
-__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
+__sigqueue_do_alloc(int sig, struct task_struct *t, gfp_t flags,
+		    int override_rlimit, int fromslab)
 {
 	struct sigqueue *q = NULL;
 	struct user_struct *user;
@@ -426,7 +444,10 @@
 	if (override_rlimit ||
 	    atomic_read(&user->sigpending) <=
 			task_rlimit(t, RLIMIT_SIGPENDING)) {
-		q = kmem_cache_alloc(sigqueue_cachep, flags);
+		if (!fromslab)
+			q = get_task_cache(t);
+		if (!q)
+			q = kmem_cache_alloc(sigqueue_cachep, flags);
 	} else {
 		print_dropped_signal(sig);
 	}
@@ -443,6 +464,13 @@
 	return q;
 }
 
+static struct sigqueue *
+__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags,
+		 int override_rlimit)
+{
+	return __sigqueue_do_alloc(sig, t, flags, override_rlimit, 0);
+}
+
 static void __sigqueue_free(struct sigqueue *q)
 {
 	if (q->flags & SIGQUEUE_PREALLOC)
@@ -452,6 +480,21 @@
 	kmem_cache_free(sigqueue_cachep, q);
 }
 
+static void sigqueue_free_current(struct sigqueue *q)
+{
+	struct user_struct *up;
+
+	if (q->flags & SIGQUEUE_PREALLOC)
+		return;
+
+	up = q->user;
+	if (rt_prio(current->normal_prio) && !put_task_cache(current, q)) {
+		atomic_dec(&up->sigpending);
+		free_uid(up);
+	} else
+		  __sigqueue_free(q);
+}
+
 void flush_sigqueue(struct sigpending *queue)
 {
 	struct sigqueue *q;
@@ -465,6 +508,21 @@
 }
 
 /*
+ * Called from __exit_signal. Flush tsk->pending and
+ * tsk->sigqueue_cache
+ */
+void flush_task_sigqueue(struct task_struct *tsk)
+{
+	struct sigqueue *q;
+
+	flush_sigqueue(&tsk->pending);
+
+	q = get_task_cache(tsk);
+	if (q)
+		kmem_cache_free(sigqueue_cachep, q);
+}
+
+/*
  * Flush all pending signals for this kthread.
  */
 void flush_signals(struct task_struct *t)
@@ -588,7 +646,7 @@
 			(info->si_code == SI_TIMER) &&
 			(info->si_sys_private);
 
-		__sigqueue_free(first);
+		sigqueue_free_current(first);
 	} else {
 		/*
 		 * Ok, it wasn't in the queue.  This must be
@@ -625,6 +683,8 @@
 	bool resched_timer = false;
 	int signr;
 
+	WARN_ON_ONCE(tsk != current);
+
 	/* We only dequeue private signals from ourselves, we don't let
 	 * signalfd steal them
 	 */
@@ -1308,6 +1368,34 @@
 	struct k_sigaction *action;
 	int sig = info->si_signo;
 
+	/*
+	 * On some archs, PREEMPT_RT has to delay sending a signal from a trap
+	 * since it can not enable preemption, and the signal code's spin_locks
+	 * turn into mutexes. Instead, it must set TIF_NOTIFY_RESUME which will
+	 * send the signal on exit of the trap.
+	 */
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+	if (in_atomic()) {
+		struct task_struct *t = current;
+
+		if (WARN_ON_ONCE(t->forced_info.si_signo))
+			return 0;
+
+		if (is_si_special(info)) {
+			WARN_ON_ONCE(info != SEND_SIG_PRIV);
+			t->forced_info.si_signo = info->si_signo;
+			t->forced_info.si_errno = 0;
+			t->forced_info.si_code = SI_KERNEL;
+			t->forced_info.si_pid = 0;
+			t->forced_info.si_uid = 0;
+		} else {
+			t->forced_info = *info;
+		}
+
+		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
+		return 0;
+	}
+#endif
 	spin_lock_irqsave(&t->sighand->siglock, flags);
 	action = &t->sighand->action[sig-1];
 	ignored = action->sa.sa_handler == SIG_IGN;
@@ -1805,7 +1893,8 @@
  */
 struct sigqueue *sigqueue_alloc(void)
 {
-	struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);
+	/* Preallocated sigqueue objects always from the slabcache ! */
+	struct sigqueue *q = __sigqueue_do_alloc(-1, current, GFP_KERNEL, 0, 1);
 
 	if (q)
 		q->flags |= SIGQUEUE_PREALLOC;
@@ -2197,16 +2286,8 @@
 		if (gstop_done && ptrace_reparented(current))
 			do_notify_parent_cldstop(current, false, why);
 
-		/*
-		 * Don't want to allow preemption here, because
-		 * sys_ptrace() needs this task to be inactive.
-		 *
-		 * XXX: implement read_unlock_no_resched().
-		 */
-		preempt_disable();
 		read_unlock(&tasklist_lock);
 		cgroup_enter_frozen();
-		preempt_enable_no_resched();
 		freezable_schedule();
 		cgroup_leave_frozen(true);
 	} else {
diff -Nur linux-5.4.5/kernel/softirq.c linux-5.4.5-new/kernel/softirq.c
--- linux-5.4.5/kernel/softirq.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/softirq.c	2020-06-15 16:12:26.139714355 +0300
@@ -25,6 +25,9 @@
 #include <linux/smpboot.h>
 #include <linux/tick.h>
 #include <linux/irq.h>
+#ifdef CONFIG_PREEMPT_RT
+#include <linux/locallock.h>
+#endif
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
@@ -102,6 +105,104 @@
  * softirq and whether we just have bh disabled.
  */
 
+#ifdef CONFIG_PREEMPT_RT
+static DEFINE_LOCAL_IRQ_LOCK(bh_lock);
+static DEFINE_PER_CPU(long, softirq_counter);
+
+void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+{
+	unsigned long __maybe_unused flags;
+	long soft_cnt;
+
+	WARN_ON_ONCE(in_irq());
+	if (!in_atomic()) {
+		local_lock(bh_lock);
+		rcu_read_lock();
+	}
+	soft_cnt = this_cpu_inc_return(softirq_counter);
+	WARN_ON_ONCE(soft_cnt == 0);
+	current->softirq_count += SOFTIRQ_DISABLE_OFFSET;
+
+#ifdef CONFIG_TRACE_IRQFLAGS
+	local_irq_save(flags);
+	if (soft_cnt == 1)
+		trace_softirqs_off(ip);
+	local_irq_restore(flags);
+#endif
+}
+EXPORT_SYMBOL(__local_bh_disable_ip);
+
+static void local_bh_disable_rt(void)
+{
+	local_bh_disable();
+}
+
+void _local_bh_enable(void)
+{
+	unsigned long __maybe_unused flags;
+	long soft_cnt;
+
+	soft_cnt = this_cpu_dec_return(softirq_counter);
+	WARN_ON_ONCE(soft_cnt < 0);
+
+#ifdef CONFIG_TRACE_IRQFLAGS
+	local_irq_save(flags);
+	if (soft_cnt == 0)
+		trace_softirqs_on(_RET_IP_);
+	local_irq_restore(flags);
+#endif
+
+	current->softirq_count -= SOFTIRQ_DISABLE_OFFSET;
+	if (!in_atomic()) {
+		rcu_read_unlock();
+		local_unlock(bh_lock);
+	}
+}
+
+void _local_bh_enable_rt(void)
+{
+	_local_bh_enable();
+}
+
+void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
+{
+	u32 pending;
+	long count;
+
+	WARN_ON_ONCE(in_irq());
+	lockdep_assert_irqs_enabled();
+
+	local_irq_disable();
+	count = this_cpu_read(softirq_counter);
+
+	if (unlikely(count == 1)) {
+		pending = local_softirq_pending();
+		if (pending && !ksoftirqd_running(pending)) {
+			if (!in_atomic())
+				__do_softirq();
+			else
+				wakeup_softirqd();
+		}
+		trace_softirqs_on(ip);
+	}
+	count = this_cpu_dec_return(softirq_counter);
+	WARN_ON_ONCE(count < 0);
+	local_irq_enable();
+
+	if (!in_atomic()) {
+		rcu_read_unlock();
+		local_unlock(bh_lock);
+	}
+
+	current->softirq_count -= SOFTIRQ_DISABLE_OFFSET;
+	preempt_check_resched();
+}
+EXPORT_SYMBOL(__local_bh_enable_ip);
+
+#else
+static void local_bh_disable_rt(void) { }
+static void _local_bh_enable_rt(void) { }
+
 /*
  * This one is for softirq.c-internal use,
  * where hardirqs are disabled legitimately:
@@ -196,6 +297,7 @@
 	preempt_check_resched();
 }
 EXPORT_SYMBOL(__local_bh_enable_ip);
+#endif
 
 /*
  * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,
@@ -266,7 +368,11 @@
 	pending = local_softirq_pending();
 	account_irq_enter_time(current);
 
+#ifdef CONFIG_PREEMPT_RT
+	current->softirq_count |= SOFTIRQ_OFFSET;
+#else
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+#endif
 	in_hardirq = lockdep_softirq_start();
 
 restart:
@@ -300,9 +406,10 @@
 		h++;
 		pending >>= softirq_bit;
 	}
-
+#ifndef CONFIG_PREEMPT_RT
 	if (__this_cpu_read(ksoftirqd) == current)
 		rcu_softirq_qs();
+#endif
 	local_irq_disable();
 
 	pending = local_softirq_pending();
@@ -316,11 +423,16 @@
 
 	lockdep_softirq_end(in_hardirq);
 	account_irq_exit_time(current);
+#ifdef CONFIG_PREEMPT_RT
+	current->softirq_count &= ~SOFTIRQ_OFFSET;
+#else
 	__local_bh_enable(SOFTIRQ_OFFSET);
+#endif
 	WARN_ON_ONCE(in_interrupt());
 	current_restore_flags(old_flags, PF_MEMALLOC);
 }
 
+#ifndef CONFIG_PREEMPT_RT
 asmlinkage __visible void do_softirq(void)
 {
 	__u32 pending;
@@ -338,6 +450,7 @@
 
 	local_irq_restore(flags);
 }
+#endif
 
 /*
  * Enter an interrupt context.
@@ -358,6 +471,16 @@
 	__irq_enter();
 }
 
+#ifdef CONFIG_PREEMPT_RT
+
+static inline void invoke_softirq(void)
+{
+	if (this_cpu_read(softirq_counter) == 0)
+		wakeup_softirqd();
+}
+
+#else
+
 static inline void invoke_softirq(void)
 {
 	if (ksoftirqd_running(local_softirq_pending()))
@@ -383,6 +506,7 @@
 		wakeup_softirqd();
 	}
 }
+#endif
 
 static inline void tick_irq_exit(void)
 {
@@ -420,6 +544,27 @@
 /*
  * This function must run with irqs disabled!
  */
+#ifdef CONFIG_PREEMPT_RT
+void raise_softirq_irqoff(unsigned int nr)
+{
+	__raise_softirq_irqoff(nr);
+
+	/*
+	 * If we're in an hard interrupt we let irq return code deal
+	 * with the wakeup of ksoftirqd.
+	 */
+	if (in_irq())
+		return;
+	/*
+	 * If were are not in BH-disabled section then we have to wake
+	 * ksoftirqd.
+	 */
+	if (this_cpu_read(softirq_counter) == 0)
+		wakeup_softirqd();
+}
+
+#else
+
 inline void raise_softirq_irqoff(unsigned int nr)
 {
 	__raise_softirq_irqoff(nr);
@@ -437,6 +582,8 @@
 		wakeup_softirqd();
 }
 
+#endif
+
 void raise_softirq(unsigned int nr)
 {
 	unsigned long flags;
@@ -564,7 +711,8 @@
 
 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
 		do {
-			yield();
+			local_bh_disable();
+			local_bh_enable();
 		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
 	}
 	tasklet_unlock_wait(t);
@@ -594,6 +742,7 @@
 
 static void run_ksoftirqd(unsigned int cpu)
 {
+	local_bh_disable_rt();
 	local_irq_disable();
 	if (local_softirq_pending()) {
 		/*
@@ -602,10 +751,12 @@
 		 */
 		__do_softirq();
 		local_irq_enable();
+		_local_bh_enable_rt();
 		cond_resched();
 		return;
 	}
 	local_irq_enable();
+	_local_bh_enable_rt();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -679,6 +830,13 @@
 
 static __init int spawn_ksoftirqd(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		lockdep_set_novalidate_class(per_cpu_ptr(&bh_lock.lock, cpu));
+#endif
+
 	cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
 				  takeover_tasklets);
 	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
@@ -687,6 +845,75 @@
 }
 early_initcall(spawn_ksoftirqd);
 
+#ifdef CONFIG_PREEMPT_RT
+
+/*
+ * On preempt-rt a softirq running context might be blocked on a
+ * lock. There might be no other runnable task on this CPU because the
+ * lock owner runs on some other CPU. So we have to go into idle with
+ * the pending bit set. Therefor we need to check this otherwise we
+ * warn about false positives which confuses users and defeats the
+ * whole purpose of this test.
+ *
+ * This code is called with interrupts disabled.
+ */
+void softirq_check_pending_idle(void)
+{
+	struct task_struct *tsk = __this_cpu_read(ksoftirqd);
+	static int rate_limit;
+	bool okay = false;
+	u32 warnpending;
+
+	if (rate_limit >= 10)
+		return;
+
+	warnpending = local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK;
+	if (!warnpending)
+		return;
+
+	if (!tsk)
+		return;
+	/*
+	 * If ksoftirqd is blocked on a lock then we may go idle with pending
+	 * softirq.
+	 */
+	raw_spin_lock(&tsk->pi_lock);
+	if (tsk->pi_blocked_on || tsk->state == TASK_RUNNING ||
+	    (tsk->state == TASK_UNINTERRUPTIBLE && tsk->sleeping_lock)) {
+		okay = true;
+	}
+	raw_spin_unlock(&tsk->pi_lock);
+	if (okay)
+		return;
+	/*
+	 * The softirq lock is held in non-atomic context and the owner is
+	 * blocking on a lock. It will schedule softirqs once the counter goes
+	 * back to zero.
+	 */
+	if (this_cpu_read(softirq_counter) > 0)
+		return;
+
+	printk(KERN_ERR "NOHZ: local_softirq_pending %02x\n",
+	       warnpending);
+	rate_limit++;
+}
+
+#else
+
+void softirq_check_pending_idle(void)
+{
+	static int ratelimit;
+
+	if (ratelimit < 10 &&
+	    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
+		pr_warn("NOHZ: local_softirq_pending %02x\n",
+			(unsigned int) local_softirq_pending());
+		ratelimit++;
+	}
+}
+
+#endif
+
 /*
  * [ These __weak aliases are kept in a separate compilation unit, so that
  *   GCC does not inline them incorrectly. ]
diff -Nur linux-5.4.5/kernel/stop_machine.c linux-5.4.5-new/kernel/stop_machine.c
--- linux-5.4.5/kernel/stop_machine.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/stop_machine.c	2020-06-15 16:12:26.139714355 +0300
@@ -86,8 +86,11 @@
 	enabled = stopper->enabled;
 	if (enabled)
 		__cpu_stop_queue_work(stopper, work, &wakeq);
-	else if (work->done)
-		cpu_stop_signal_done(work->done);
+	else {
+		work->disabled = true;
+		if (work->done)
+			cpu_stop_signal_done(work->done);
+	}
 	raw_spin_unlock_irqrestore(&stopper->lock, flags);
 
 	wake_up_q(&wakeq);
diff -Nur linux-5.4.5/kernel/sysctl.c linux-5.4.5-new/kernel/sysctl.c
--- linux-5.4.5/kernel/sysctl.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/sysctl.c	2020-06-15 16:12:26.139714355 +0300
@@ -1488,6 +1488,7 @@
 		.extra1		= &min_extfrag_threshold,
 		.extra2		= &max_extfrag_threshold,
 	},
+#ifndef CONFIG_PREEMPT_RT
 	{
 		.procname	= "compact_unevictable_allowed",
 		.data		= &sysctl_compact_unevictable_allowed,
@@ -1497,7 +1498,7 @@
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
-
+#endif
 #endif /* CONFIG_COMPACTION */
 	{
 		.procname	= "min_free_kbytes",
diff -Nur linux-5.4.5/kernel/time/hrtimer.c linux-5.4.5-new/kernel/time/hrtimer.c
--- linux-5.4.5/kernel/time/hrtimer.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/hrtimer.c	2020-06-15 16:12:25.967714960 +0300
@@ -1816,7 +1816,7 @@
 	 * expiry.
 	 */
 	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
-		if (task_is_realtime(current) && !(mode & HRTIMER_MODE_SOFT))
+		if ((task_is_realtime(current) && !(mode & HRTIMER_MODE_SOFT)) || system_state != SYSTEM_RUNNING)
 			mode |= HRTIMER_MODE_HARD;
 	}
 
@@ -1870,12 +1870,12 @@
 		if (likely(t->task))
 			freezable_schedule();
 
+		__set_current_state(TASK_RUNNING);
 		hrtimer_cancel(&t->timer);
 		mode = HRTIMER_MODE_ABS;
 
 	} while (t->task && !signal_pending(current));
 
-	__set_current_state(TASK_RUNNING);
 
 	if (!t->task)
 		return 0;
@@ -1979,6 +1979,38 @@
 }
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Sleep for 1 ms in hope whoever holds what we want will let it go.
+ */
+void cpu_chill(void)
+{
+	unsigned int freeze_flag = current->flags & PF_NOFREEZE;
+	struct task_struct *self = current;
+	ktime_t chill_time;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state_no_track(TASK_UNINTERRUPTIBLE);
+	raw_spin_unlock_irq(&self->pi_lock);
+
+	chill_time = ktime_set(0, NSEC_PER_MSEC);
+
+	current->flags |= PF_NOFREEZE;
+	sleeping_lock_inc();
+	schedule_hrtimeout(&chill_time, HRTIMER_MODE_REL_HARD);
+	sleeping_lock_dec();
+	if (!freeze_flag)
+		current->flags &= ~PF_NOFREEZE;
+
+	raw_spin_lock_irq(&self->pi_lock);
+	__set_current_state_no_track(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	raw_spin_unlock_irq(&self->pi_lock);
+}
+EXPORT_SYMBOL(cpu_chill);
+#endif
+
 /*
  * Functions related to boot-time initialization:
  */
diff -Nur linux-5.4.5/kernel/time/jiffies.c linux-5.4.5-new/kernel/time/jiffies.c
--- linux-5.4.5/kernel/time/jiffies.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/jiffies.c	2020-06-15 16:12:25.967714960 +0300
@@ -58,7 +58,8 @@
 	.max_cycles	= 10,
 };
 
-__cacheline_aligned_in_smp DEFINE_SEQLOCK(jiffies_lock);
+__cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(jiffies_lock);
+__cacheline_aligned_in_smp seqcount_t jiffies_seq;
 
 #if (BITS_PER_LONG < 64)
 u64 get_jiffies_64(void)
@@ -67,9 +68,9 @@
 	u64 ret;
 
 	do {
-		seq = read_seqbegin(&jiffies_lock);
+		seq = read_seqcount_begin(&jiffies_seq);
 		ret = jiffies_64;
-	} while (read_seqretry(&jiffies_lock, seq));
+	} while (read_seqcount_retry(&jiffies_seq, seq));
 	return ret;
 }
 EXPORT_SYMBOL(get_jiffies_64);
diff -Nur linux-5.4.5/kernel/time/posix-cpu-timers.c linux-5.4.5-new/kernel/time/posix-cpu-timers.c
--- linux-5.4.5/kernel/time/posix-cpu-timers.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/posix-cpu-timers.c	2020-06-15 16:12:25.967714960 +0300
@@ -3,8 +3,10 @@
  * Implement CPU time clocks for the POSIX clock interface.
  */
 
+#include <uapi/linux/sched/types.h>
 #include <linux/sched/signal.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/rt.h>
 #include <linux/posix-timers.h>
 #include <linux/errno.h>
 #include <linux/math64.h>
@@ -15,6 +17,7 @@
 #include <linux/workqueue.h>
 #include <linux/compat.h>
 #include <linux/sched/deadline.h>
+#include <linux/smpboot.h>
 
 #include "posix-timers.h"
 
@@ -27,6 +30,9 @@
 		pct->bases[CPUCLOCK_PROF].nextevt = cpu_limit * NSEC_PER_SEC;
 		pct->timers_active = true;
 	}
+#ifdef CONFIG_PREEMPT_RT
+	pct->posix_timer_list = NULL;
+#endif
 }
 
 /*
@@ -440,6 +446,20 @@
 	return ret;
 }
 
+static DEFINE_PER_CPU(spinlock_t, cpu_timer_expiry_lock) = __SPIN_LOCK_UNLOCKED(cpu_timer_expiry_lock);
+
+static void posix_cpu_wait_running(struct k_itimer *timer)
+{
+	int cpu = timer->it.cpu.firing_cpu;
+
+	if (cpu >= 0) {
+		spinlock_t *expiry_lock = per_cpu_ptr(&cpu_timer_expiry_lock, cpu);
+
+		spin_lock_irq(expiry_lock);
+		spin_unlock_irq(expiry_lock);
+	}
+}
+
 static void cleanup_timerqueue(struct timerqueue_head *head)
 {
 	struct timerqueue_node *node;
@@ -777,6 +797,7 @@
 			return expires;
 
 		ctmr->firing = 1;
+		ctmr->firing_cpu = smp_processor_id();
 		cpu_timer_dequeue(ctmr);
 		list_add_tail(&ctmr->elist, firing);
 	}
@@ -804,7 +825,8 @@
 	}
 }
 
-static bool check_rlimit(u64 time, u64 limit, int signo, bool rt, bool hard)
+static bool check_rlimit(struct task_struct *tsk, u64 time, u64 limit,
+			 int signo, bool rt, bool hard)
 {
 	if (time < limit)
 		return false;
@@ -812,9 +834,9 @@
 	if (print_fatal_signals) {
 		pr_info("%s Watchdog Timeout (%s): %s[%d]\n",
 			rt ? "RT" : "CPU", hard ? "hard" : "soft",
-			current->comm, task_pid_nr(current));
+			tsk->comm, task_pid_nr(tsk));
 	}
-	__group_send_sig_info(signo, SEND_SIG_PRIV, current);
+	__group_send_sig_info(signo, SEND_SIG_PRIV, tsk);
 	return true;
 }
 
@@ -850,11 +872,11 @@
 
 		/* At the hard limit, send SIGKILL. No further action. */
 		if (hard != RLIM_INFINITY &&
-		    check_rlimit(rttime, hard, SIGKILL, true, true))
+		    check_rlimit(tsk, rttime, hard, SIGKILL, true, true))
 			return;
 
 		/* At the soft limit, send a SIGXCPU every second */
-		if (check_rlimit(rttime, soft, SIGXCPU, true, false)) {
+		if (check_rlimit(tsk, rttime, soft, SIGXCPU, true, false)) {
 			soft += USEC_PER_SEC;
 			tsk->signal->rlim[RLIMIT_RTTIME].rlim_cur = soft;
 		}
@@ -949,11 +971,11 @@
 
 		/* At the hard limit, send SIGKILL. No further action. */
 		if (hard != RLIM_INFINITY &&
-		    check_rlimit(ptime, hardns, SIGKILL, false, true))
+		    check_rlimit(tsk, ptime, hardns, SIGKILL, false, true))
 			return;
 
 		/* At the soft limit, send a SIGXCPU every second */
-		if (check_rlimit(ptime, softns, SIGXCPU, false, false)) {
+		if (check_rlimit(tsk, ptime, softns, SIGXCPU, false, false)) {
 			sig->rlim[RLIMIT_CPU].rlim_cur = soft + 1;
 			softns += NSEC_PER_SEC;
 		}
@@ -1110,15 +1132,13 @@
  * already updated our counts.  We need to check if any timers fire now.
  * Interrupts are disabled.
  */
-void run_posix_cpu_timers(void)
+static void __run_posix_cpu_timers(struct task_struct *tsk)
 {
-	struct task_struct *tsk = current;
 	struct k_itimer *timer, *next;
 	unsigned long flags;
+	spinlock_t *expiry_lock;
 	LIST_HEAD(firing);
 
-	lockdep_assert_irqs_disabled();
-
 	/*
 	 * The fast path checks that there are no expired thread or thread
 	 * group timers.  If that's so, just return.
@@ -1126,8 +1146,13 @@
 	if (!fastpath_timer_check(tsk))
 		return;
 
-	if (!lock_task_sighand(tsk, &flags))
+	expiry_lock = this_cpu_ptr(&cpu_timer_expiry_lock);
+	spin_lock(expiry_lock);
+
+	if (!lock_task_sighand(tsk, &flags)) {
+		spin_unlock(expiry_lock);
 		return;
+	}
 	/*
 	 * Here we take off tsk->signal->cpu_timers[N] and
 	 * tsk->cpu_timers[N] all the timers that are firing, and
@@ -1160,6 +1185,7 @@
 		list_del_init(&timer->it.cpu.elist);
 		cpu_firing = timer->it.cpu.firing;
 		timer->it.cpu.firing = 0;
+		timer->it.cpu.firing_cpu = -1;
 		/*
 		 * The firing flag is -1 if we collided with a reset
 		 * of the timer, which already reported this
@@ -1169,8 +1195,158 @@
 			cpu_timer_fire(timer);
 		spin_unlock(&timer->it_lock);
 	}
+	spin_unlock(expiry_lock);
 }
 
+#ifdef CONFIG_PREEMPT_RT
+#include <linux/kthread.h>
+#include <linux/cpu.h>
+DEFINE_PER_CPU(struct task_struct *, posix_timer_task);
+DEFINE_PER_CPU(struct task_struct *, posix_timer_tasklist);
+DEFINE_PER_CPU(bool, posix_timer_th_active);
+
+static void posix_cpu_kthread_fn(unsigned int cpu)
+{
+	struct task_struct *tsk = NULL;
+	struct task_struct *next = NULL;
+
+	BUG_ON(per_cpu(posix_timer_task, cpu) != current);
+
+	/* grab task list */
+	raw_local_irq_disable();
+	tsk = per_cpu(posix_timer_tasklist, cpu);
+	per_cpu(posix_timer_tasklist, cpu) = NULL;
+	raw_local_irq_enable();
+
+	/* its possible the list is empty, just return */
+	if (!tsk)
+		return;
+
+	/* Process task list */
+	while (1) {
+		/* save next */
+		next = tsk->posix_cputimers.posix_timer_list;
+
+		/* run the task timers, clear its ptr and
+		 * unreference it
+		 */
+		__run_posix_cpu_timers(tsk);
+		tsk->posix_cputimers.posix_timer_list = NULL;
+		put_task_struct(tsk);
+
+		/* check if this is the last on the list */
+		if (next == tsk)
+			break;
+		tsk = next;
+	}
+}
+
+static inline int __fastpath_timer_check(struct task_struct *tsk)
+{
+	/* tsk == current, ensure it is safe to use ->signal/sighand */
+	if (unlikely(tsk->exit_state))
+		return 0;
+
+	if (!expiry_cache_is_inactive(&tsk->posix_cputimers))
+		return 1;
+
+	if (!expiry_cache_is_inactive(&tsk->signal->posix_cputimers))
+		return 1;
+
+	return 0;
+}
+
+void run_posix_cpu_timers(void)
+{
+	unsigned int cpu = smp_processor_id();
+	struct task_struct *tsk = current;
+	struct task_struct *tasklist;
+
+	BUG_ON(!irqs_disabled());
+
+	if (per_cpu(posix_timer_th_active, cpu) != true)
+		return;
+
+	/* get per-cpu references */
+	tasklist = per_cpu(posix_timer_tasklist, cpu);
+
+	/* check to see if we're already queued */
+	if (!tsk->posix_cputimers.posix_timer_list && __fastpath_timer_check(tsk)) {
+		get_task_struct(tsk);
+		if (tasklist) {
+			tsk->posix_cputimers.posix_timer_list = tasklist;
+		} else {
+			/*
+			 * The list is terminated by a self-pointing
+			 * task_struct
+			 */
+			tsk->posix_cputimers.posix_timer_list = tsk;
+		}
+		per_cpu(posix_timer_tasklist, cpu) = tsk;
+
+		wake_up_process(per_cpu(posix_timer_task, cpu));
+	}
+}
+
+static int posix_cpu_kthread_should_run(unsigned int cpu)
+{
+	return __this_cpu_read(posix_timer_tasklist) != NULL;
+}
+
+static void posix_cpu_kthread_park(unsigned int cpu)
+{
+	this_cpu_write(posix_timer_th_active, false);
+}
+
+static void posix_cpu_kthread_unpark(unsigned int cpu)
+{
+	this_cpu_write(posix_timer_th_active, true);
+}
+
+static void posix_cpu_kthread_setup(unsigned int cpu)
+{
+	struct sched_param sp;
+
+	sp.sched_priority = MAX_RT_PRIO - 1;
+	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+	posix_cpu_kthread_unpark(cpu);
+}
+
+static struct smp_hotplug_thread posix_cpu_thread = {
+	.store			= &posix_timer_task,
+	.thread_should_run	= posix_cpu_kthread_should_run,
+	.thread_fn		= posix_cpu_kthread_fn,
+	.thread_comm		= "posixcputmr/%u",
+	.setup			= posix_cpu_kthread_setup,
+	.park			= posix_cpu_kthread_park,
+	.unpark			= posix_cpu_kthread_unpark,
+};
+
+static int __init posix_cpu_thread_init(void)
+{
+	/* Start one for boot CPU. */
+	unsigned long cpu;
+	int ret;
+
+	/* init the per-cpu posix_timer_tasklets */
+	for_each_possible_cpu(cpu)
+		per_cpu(posix_timer_tasklist, cpu) = NULL;
+
+	ret = smpboot_register_percpu_thread(&posix_cpu_thread);
+	WARN_ON(ret);
+
+	return 0;
+}
+early_initcall(posix_cpu_thread_init);
+
+#else /* CONFIG_PREEMPT_RT */
+void run_posix_cpu_timers(void)
+{
+	lockdep_assert_irqs_disabled();
+	__run_posix_cpu_timers(current);
+}
+#endif /* CONFIG_PREEMPT_RT */
+
 /*
  * Set one of the process-wide special case CPU timers or RLIMIT_CPU.
  * The tsk->sighand->siglock must be held by the caller.
@@ -1282,6 +1458,8 @@
 		spin_unlock_irq(&timer.it_lock);
 
 		while (error == TIMER_RETRY) {
+
+			posix_cpu_wait_running(&timer);
 			/*
 			 * We need to handle case when timer was or is in the
 			 * middle of firing. In other cases we already freed
@@ -1400,6 +1578,7 @@
 	.timer_del	= posix_cpu_timer_del,
 	.timer_get	= posix_cpu_timer_get,
 	.timer_rearm	= posix_cpu_timer_rearm,
+	.timer_wait_running	= posix_cpu_wait_running,
 };
 
 const struct k_clock clock_process = {
diff -Nur linux-5.4.5/kernel/time/tick-common.c linux-5.4.5-new/kernel/time/tick-common.c
--- linux-5.4.5/kernel/time/tick-common.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/tick-common.c	2020-06-15 16:12:25.967714960 +0300
@@ -83,13 +83,15 @@
 static void tick_periodic(int cpu)
 {
 	if (tick_do_timer_cpu == cpu) {
-		write_seqlock(&jiffies_lock);
+		raw_spin_lock(&jiffies_lock);
+		write_seqcount_begin(&jiffies_seq);
 
 		/* Keep track of the next tick event */
 		tick_next_period = ktime_add(tick_next_period, tick_period);
 
 		do_timer(1);
-		write_sequnlock(&jiffies_lock);
+		write_seqcount_end(&jiffies_seq);
+		raw_spin_unlock(&jiffies_lock);
 		update_wall_time();
 	}
 
@@ -161,9 +163,9 @@
 		ktime_t next;
 
 		do {
-			seq = read_seqbegin(&jiffies_lock);
+			seq = read_seqcount_begin(&jiffies_seq);
 			next = tick_next_period;
-		} while (read_seqretry(&jiffies_lock, seq));
+		} while (read_seqcount_retry(&jiffies_seq, seq));
 
 		clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 
diff -Nur linux-5.4.5/kernel/time/tick-sched.c linux-5.4.5-new/kernel/time/tick-sched.c
--- linux-5.4.5/kernel/time/tick-sched.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/tick-sched.c	2020-06-15 16:12:25.967714960 +0300
@@ -64,7 +64,8 @@
 		return;
 
 	/* Reevaluate with jiffies_lock held */
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 
 	delta = ktime_sub(now, last_jiffies_update);
 	if (delta >= tick_period) {
@@ -87,10 +88,12 @@
 		/* Keep the tick_next_period variable up to date */
 		tick_next_period = ktime_add(last_jiffies_update, tick_period);
 	} else {
-		write_sequnlock(&jiffies_lock);
+		write_seqcount_end(&jiffies_seq);
+		raw_spin_unlock(&jiffies_lock);
 		return;
 	}
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	update_wall_time();
 }
 
@@ -101,12 +104,14 @@
 {
 	ktime_t period;
 
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 	/* Did we start the jiffies update yet ? */
 	if (last_jiffies_update == 0)
 		last_jiffies_update = tick_next_period;
 	period = last_jiffies_update;
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	return period;
 }
 
@@ -230,6 +235,7 @@
 
 static DEFINE_PER_CPU(struct irq_work, nohz_full_kick_work) = {
 	.func = nohz_full_kick_func,
+	.flags = IRQ_WORK_HARD_IRQ,
 };
 
 /*
@@ -661,10 +667,10 @@
 
 	/* Read jiffies and the time when jiffies were updated last */
 	do {
-		seq = read_seqbegin(&jiffies_lock);
+		seq = read_seqcount_begin(&jiffies_seq);
 		basemono = last_jiffies_update;
 		basejiff = jiffies;
-	} while (read_seqretry(&jiffies_lock, seq));
+	} while (read_seqcount_retry(&jiffies_seq, seq));
 	ts->last_jiffies = basejiff;
 	ts->timer_expires_base = basemono;
 
@@ -894,14 +900,7 @@
 		return false;
 
 	if (unlikely(local_softirq_pending())) {
-		static int ratelimit;
-
-		if (ratelimit < 10 &&
-		    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
-			pr_warn("NOHZ: local_softirq_pending %02x\n",
-				(unsigned int) local_softirq_pending());
-			ratelimit++;
-		}
+		softirq_check_pending_idle();
 		return false;
 	}
 
diff -Nur linux-5.4.5/kernel/time/timekeeping.c linux-5.4.5-new/kernel/time/timekeeping.c
--- linux-5.4.5/kernel/time/timekeeping.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/timekeeping.c	2020-06-15 16:12:25.967714960 +0300
@@ -2397,8 +2397,10 @@
  */
 void xtime_update(unsigned long ticks)
 {
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 	do_timer(ticks);
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	update_wall_time();
 }
diff -Nur linux-5.4.5/kernel/time/timekeeping.h linux-5.4.5-new/kernel/time/timekeeping.h
--- linux-5.4.5/kernel/time/timekeeping.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/timekeeping.h	2020-06-15 16:12:25.971714946 +0300
@@ -25,7 +25,8 @@
 extern void do_timer(unsigned long ticks);
 extern void update_wall_time(void);
 
-extern seqlock_t jiffies_lock;
+extern raw_spinlock_t jiffies_lock;
+extern seqcount_t jiffies_seq;
 
 #define CS_NAME_LEN	32
 
diff -Nur linux-5.4.5/kernel/time/timer.c linux-5.4.5-new/kernel/time/timer.c
--- linux-5.4.5/kernel/time/timer.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/time/timer.c	2020-06-15 16:12:25.971714946 +0300
@@ -1783,6 +1783,8 @@
 {
 	struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);
 
+	irq_work_tick_soft();
+
 	__run_timers(base);
 	if (IS_ENABLED(CONFIG_NO_HZ_COMMON))
 		__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));
diff -Nur linux-5.4.5/kernel/trace/trace.c linux-5.4.5-new/kernel/trace/trace.c
--- linux-5.4.5/kernel/trace/trace.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/trace/trace.c	2020-06-15 16:12:26.071714594 +0300
@@ -2335,6 +2335,7 @@
 	struct task_struct *tsk = current;
 
 	entry->preempt_count		= pc & 0xff;
+	entry->preempt_lazy_count	= preempt_lazy_count();
 	entry->pid			= (tsk) ? tsk->pid : 0;
 	entry->type			= type;
 	entry->flags =
@@ -2346,8 +2347,11 @@
 		((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |
 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
 		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
-		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(tif_need_resched_now() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(need_resched_lazy() ? TRACE_FLAG_NEED_RESCHED_LAZY : 0) |
 		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
+
+	entry->migrate_disable = (tsk) ? __migrate_disabled(tsk) & 0xFF : 0;
 }
 EXPORT_SYMBOL_GPL(tracing_generic_entry_update);
 
@@ -3573,14 +3577,17 @@
 
 static void print_lat_help_header(struct seq_file *m)
 {
-	seq_puts(m, "#                  _------=> CPU#            \n"
-		    "#                 / _-----=> irqs-off        \n"
-		    "#                | / _----=> need-resched    \n"
-		    "#                || / _---=> hardirq/softirq \n"
-		    "#                ||| / _--=> preempt-depth   \n"
-		    "#                |||| /     delay            \n"
-		    "#  cmd     pid   ||||| time  |   caller      \n"
-		    "#     \\   /      |||||  \\    |   /         \n");
+	seq_puts(m, "#                  _--------=> CPU#              \n"
+		    "#                 / _-------=> irqs-off          \n"
+		    "#                | / _------=> need-resched      \n"
+		    "#                || / _-----=> need-resched_lazy \n"
+		    "#                ||| / _----=> hardirq/softirq   \n"
+		    "#                |||| / _---=> preempt-depth     \n"
+		    "#                ||||| / _--=> preempt-lazy-depth\n"
+		    "#                |||||| / _-=> migrate-disable   \n"
+		    "#                ||||||| /     delay             \n"
+		    "# cmd     pid    |||||||| time   |  caller       \n"
+		    "#     \\   /      ||||||||   \\    |  /            \n");
 }
 
 static void print_event_info(struct trace_buffer *buf, struct seq_file *m)
@@ -3616,11 +3623,12 @@
 
 	seq_printf(m, "#                          %.*s  _-----=> irqs-off\n", prec, space);
 	seq_printf(m, "#                          %.*s / _----=> need-resched\n", prec, space);
-	seq_printf(m, "#                          %.*s| / _---=> hardirq/softirq\n", prec, space);
-	seq_printf(m, "#                          %.*s|| / _--=> preempt-depth\n", prec, space);
-	seq_printf(m, "#                          %.*s||| /     delay\n", prec, space);
-	seq_printf(m, "#           TASK-PID %.*sCPU#  ||||    TIMESTAMP  FUNCTION\n", prec, "   TGID   ");
-	seq_printf(m, "#              | |   %.*s  |   ||||       |         |\n", prec, "     |    ");
+	seq_printf(m, "#                          %.*s| / _----=> need-resched\n", prec, space);
+	seq_printf(m, "#                          %.*s|| / _---=> hardirq/softirq\n", prec, space);
+	seq_printf(m, "#                          %.*s||| / _--=> preempt-depth\n", prec, space);
+	seq_printf(m, "#                          %.*s||||/     delay\n", prec, space);
+	seq_printf(m, "#           TASK-PID %.*sCPU#  |||||   TIMESTAMP  FUNCTION\n", prec, "   TGID   ");
+	seq_printf(m, "#              | |   %.*s  |   |||||      |         |\n", prec, "     |    ");
 }
 
 void
@@ -3654,6 +3662,8 @@
 		   "desktop",
 #elif defined(CONFIG_PREEMPT)
 		   "preempt",
+#elif defined(CONFIG_PREEMPT_RT)
+		   "preempt_rt",
 #else
 		   "unknown",
 #endif
@@ -8908,7 +8918,6 @@
 	tracing_off();
 
 	local_irq_save(flags);
-	printk_nmi_direct_enter();
 
 	/* Simulate the iterator */
 	trace_init_global_iter(&iter);
@@ -8985,7 +8994,6 @@
 		atomic_dec(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);
 	}
 	atomic_dec(&dump_running);
-	printk_nmi_direct_exit();
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(ftrace_dump);
diff -Nur linux-5.4.5/kernel/trace/trace_events.c linux-5.4.5-new/kernel/trace/trace_events.c
--- linux-5.4.5/kernel/trace/trace_events.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/trace/trace_events.c	2020-06-15 16:12:26.071714594 +0300
@@ -181,6 +181,8 @@
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(unsigned short, migrate_disable);
+	__common_field(unsigned short, padding);
 
 	return ret;
 }
diff -Nur linux-5.4.5/kernel/trace/trace.h linux-5.4.5-new/kernel/trace/trace.h
--- linux-5.4.5/kernel/trace/trace.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/trace/trace.h	2020-06-15 16:12:26.071714594 +0300
@@ -126,6 +126,7 @@
  *  NEED_RESCHED	- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
+ *  NEED_RESCHED_LAZY	- lazy reschedule is requested
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
@@ -135,6 +136,7 @@
 	TRACE_FLAG_SOFTIRQ		= 0x10,
 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
 	TRACE_FLAG_NMI			= 0x40,
+	TRACE_FLAG_NEED_RESCHED_LAZY	= 0x80,
 };
 
 #define TRACE_BUF_SIZE		1024
diff -Nur linux-5.4.5/kernel/trace/trace_output.c linux-5.4.5-new/kernel/trace/trace_output.c
--- linux-5.4.5/kernel/trace/trace_output.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/trace/trace_output.c	2020-06-15 16:12:26.071714594 +0300
@@ -426,6 +426,7 @@
 {
 	char hardsoft_irq;
 	char need_resched;
+	char need_resched_lazy;
 	char irqs_off;
 	int hardirq;
 	int softirq;
@@ -456,6 +457,9 @@
 		break;
 	}
 
+	need_resched_lazy =
+		(entry->flags & TRACE_FLAG_NEED_RESCHED_LAZY) ? 'L' : '.';
+
 	hardsoft_irq =
 		(nmi && hardirq)     ? 'Z' :
 		nmi                  ? 'z' :
@@ -464,14 +468,25 @@
 		softirq              ? 's' :
 		                       '.' ;
 
-	trace_seq_printf(s, "%c%c%c",
-			 irqs_off, need_resched, hardsoft_irq);
+	trace_seq_printf(s, "%c%c%c%c",
+			 irqs_off, need_resched, need_resched_lazy,
+			 hardsoft_irq);
 
 	if (entry->preempt_count)
 		trace_seq_printf(s, "%x", entry->preempt_count);
 	else
 		trace_seq_putc(s, '.');
 
+	if (entry->preempt_lazy_count)
+		trace_seq_printf(s, "%x", entry->preempt_lazy_count);
+	else
+		trace_seq_putc(s, '.');
+
+	if (entry->migrate_disable)
+		trace_seq_printf(s, "%x", entry->migrate_disable);
+	else
+		trace_seq_putc(s, '.');
+
 	return !trace_seq_has_overflowed(s);
 }
 
diff -Nur linux-5.4.5/kernel/workqueue.c linux-5.4.5-new/kernel/workqueue.c
--- linux-5.4.5/kernel/workqueue.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/kernel/workqueue.c	2020-06-15 16:12:26.139714355 +0300
@@ -50,6 +50,7 @@
 #include <linux/uaccess.h>
 #include <linux/sched/isolation.h>
 #include <linux/nmi.h>
+#include <linux/swait.h>
 
 #include "workqueue_internal.h"
 
@@ -145,7 +146,7 @@
 /* struct worker is defined in workqueue_internal.h */
 
 struct worker_pool {
-	spinlock_t		lock;		/* the pool lock */
+	raw_spinlock_t		lock;		/* the pool lock */
 	int			cpu;		/* I: the associated cpu */
 	int			node;		/* I: the associated node ID */
 	int			id;		/* I: pool ID */
@@ -300,8 +301,8 @@
 
 static DEFINE_MUTEX(wq_pool_mutex);	/* protects pools and workqueues list */
 static DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */
-static DEFINE_SPINLOCK(wq_mayday_lock);	/* protects wq->maydays list */
-static DECLARE_WAIT_QUEUE_HEAD(wq_manager_wait); /* wait for manager to go away */
+static DEFINE_RAW_SPINLOCK(wq_mayday_lock);	/* protects wq->maydays list */
+static DECLARE_SWAIT_QUEUE_HEAD(wq_manager_wait); /* wait for manager to go away */
 
 static LIST_HEAD(workqueues);		/* PR: list of all workqueues */
 static bool workqueue_freezing;		/* PL: have wqs started freezing? */
@@ -364,11 +365,6 @@
 			 !lockdep_is_held(&wq_pool_mutex),		\
 			 "RCU or wq_pool_mutex should be held")
 
-#define assert_rcu_or_wq_mutex(wq)					\
-	RCU_LOCKDEP_WARN(!rcu_read_lock_held() &&			\
-			 !lockdep_is_held(&wq->mutex),			\
-			 "RCU or wq->mutex should be held")
-
 #define assert_rcu_or_wq_mutex_or_pool_mutex(wq)			\
 	RCU_LOCKDEP_WARN(!rcu_read_lock_held() &&			\
 			 !lockdep_is_held(&wq->mutex) &&		\
@@ -425,9 +421,8 @@
  * ignored.
  */
 #define for_each_pwq(pwq, wq)						\
-	list_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node)		\
-		if (({ assert_rcu_or_wq_mutex(wq); false; })) { }	\
-		else
+	list_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,		\
+				 lockdep_is_held(&(wq->mutex)))
 
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 
@@ -831,7 +826,7 @@
  * Wake up the first idle worker of @pool.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void wake_up_worker(struct worker_pool *pool)
 {
@@ -884,7 +879,7 @@
 		return;
 
 	worker->sleeping = 1;
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 
 	/*
 	 * The counterpart of the following dec_and_test, implied mb,
@@ -903,7 +898,7 @@
 		if (next)
 			wake_up_process(next->task);
 	}
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 }
 
 /**
@@ -914,7 +909,7 @@
  * the scheduler to get a worker's last known identity.
  *
  * CONTEXT:
- * spin_lock_irq(rq->lock)
+ * raw_spin_lock_irq(rq->lock)
  *
  * This function is called during schedule() when a kworker is going
  * to sleep. It's used by psi to identify aggregation workers during
@@ -945,7 +940,7 @@
  * Set @flags in @worker->flags and adjust nr_running accordingly.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock)
+ * raw_spin_lock_irq(pool->lock)
  */
 static inline void worker_set_flags(struct worker *worker, unsigned int flags)
 {
@@ -970,7 +965,7 @@
  * Clear @flags in @worker->flags and adjust nr_running accordingly.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock)
+ * raw_spin_lock_irq(pool->lock)
  */
 static inline void worker_clr_flags(struct worker *worker, unsigned int flags)
 {
@@ -1018,7 +1013,7 @@
  * actually occurs, it should be easy to locate the culprit work function.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  *
  * Return:
  * Pointer to worker which is executing @work if found, %NULL
@@ -1053,7 +1048,7 @@
  * nested inside outer list_for_each_entry_safe().
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void move_linked_works(struct work_struct *work, struct list_head *head,
 			      struct work_struct **nextp)
@@ -1131,9 +1126,9 @@
 		 * As both pwqs and pools are RCU protected, the
 		 * following lock operations are safe.
 		 */
-		spin_lock_irq(&pwq->pool->lock);
+		raw_spin_lock_irq(&pwq->pool->lock);
 		put_pwq(pwq);
-		spin_unlock_irq(&pwq->pool->lock);
+		raw_spin_unlock_irq(&pwq->pool->lock);
 	}
 }
 
@@ -1166,7 +1161,7 @@
  * decrement nr_in_flight of its pwq and handle workqueue flushing.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, int color)
 {
@@ -1265,7 +1260,7 @@
 	if (!pool)
 		goto fail;
 
-	spin_lock(&pool->lock);
+	raw_spin_lock(&pool->lock);
 	/*
 	 * work->data is guaranteed to point to pwq only while the work
 	 * item is queued on pwq->wq, and both updating work->data to point
@@ -1294,11 +1289,11 @@
 		/* work->data points to pwq iff queued, point to pool */
 		set_work_pool_and_keep_pending(work, pool->id);
 
-		spin_unlock(&pool->lock);
+		raw_spin_unlock(&pool->lock);
 		rcu_read_unlock();
 		return 1;
 	}
-	spin_unlock(&pool->lock);
+	raw_spin_unlock(&pool->lock);
 fail:
 	rcu_read_unlock();
 	local_irq_restore(*flags);
@@ -1319,7 +1314,7 @@
  * work_struct flags.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void insert_work(struct pool_workqueue *pwq, struct work_struct *work,
 			struct list_head *head, unsigned int extra_flags)
@@ -1434,7 +1429,7 @@
 	if (last_pool && last_pool != pwq->pool) {
 		struct worker *worker;
 
-		spin_lock(&last_pool->lock);
+		raw_spin_lock(&last_pool->lock);
 
 		worker = find_worker_executing_work(last_pool, work);
 
@@ -1442,11 +1437,11 @@
 			pwq = worker->current_pwq;
 		} else {
 			/* meh... not running there, queue here */
-			spin_unlock(&last_pool->lock);
-			spin_lock(&pwq->pool->lock);
+			raw_spin_unlock(&last_pool->lock);
+			raw_spin_lock(&pwq->pool->lock);
 		}
 	} else {
-		spin_lock(&pwq->pool->lock);
+		raw_spin_lock(&pwq->pool->lock);
 	}
 
 	/*
@@ -1459,7 +1454,7 @@
 	 */
 	if (unlikely(!pwq->refcnt)) {
 		if (wq->flags & WQ_UNBOUND) {
-			spin_unlock(&pwq->pool->lock);
+			raw_spin_unlock(&pwq->pool->lock);
 			cpu_relax();
 			goto retry;
 		}
@@ -1491,7 +1486,7 @@
 	insert_work(pwq, work, worklist, work_flags);
 
 out:
-	spin_unlock(&pwq->pool->lock);
+	raw_spin_unlock(&pwq->pool->lock);
 	rcu_read_unlock();
 }
 
@@ -1611,9 +1606,11 @@
 void delayed_work_timer_fn(struct timer_list *t)
 {
 	struct delayed_work *dwork = from_timer(dwork, t, timer);
+	unsigned long flags;
 
-	/* should have been called from irqsafe timer with irq already off */
+	local_irq_save(flags);
 	__queue_work(dwork->cpu, dwork->wq, &dwork->work);
+	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(delayed_work_timer_fn);
 
@@ -1760,7 +1757,7 @@
  * necessary.
  *
  * LOCKING:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void worker_enter_idle(struct worker *worker)
 {
@@ -1800,7 +1797,7 @@
  * @worker is leaving idle state.  Update stats.
  *
  * LOCKING:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void worker_leave_idle(struct worker *worker)
 {
@@ -1938,11 +1935,11 @@
 	worker_attach_to_pool(worker, pool);
 
 	/* start the newly created worker */
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 	worker->pool->nr_workers++;
 	worker_enter_idle(worker);
 	wake_up_process(worker->task);
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	return worker;
 
@@ -1961,7 +1958,7 @@
  * be idle.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void destroy_worker(struct worker *worker)
 {
@@ -1987,7 +1984,7 @@
 {
 	struct worker_pool *pool = from_timer(pool, t, idle_timer);
 
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 
 	while (too_many_workers(pool)) {
 		struct worker *worker;
@@ -2005,7 +2002,7 @@
 		destroy_worker(worker);
 	}
 
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 }
 
 static void send_mayday(struct work_struct *work)
@@ -2036,8 +2033,8 @@
 	struct worker_pool *pool = from_timer(pool, t, mayday_timer);
 	struct work_struct *work;
 
-	spin_lock_irq(&pool->lock);
-	spin_lock(&wq_mayday_lock);		/* for wq->maydays */
+	raw_spin_lock_irq(&pool->lock);
+	raw_spin_lock(&wq_mayday_lock);		/* for wq->maydays */
 
 	if (need_to_create_worker(pool)) {
 		/*
@@ -2050,8 +2047,8 @@
 			send_mayday(work);
 	}
 
-	spin_unlock(&wq_mayday_lock);
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock(&wq_mayday_lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);
 }
@@ -2070,7 +2067,7 @@
  * may_start_working() %true.
  *
  * LOCKING:
- * spin_lock_irq(pool->lock) which may be released and regrabbed
+ * raw_spin_lock_irq(pool->lock) which may be released and regrabbed
  * multiple times.  Does GFP_KERNEL allocations.  Called only from
  * manager.
  */
@@ -2079,7 +2076,7 @@
 __acquires(&pool->lock)
 {
 restart:
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	/* if we don't make progress in MAYDAY_INITIAL_TIMEOUT, call for help */
 	mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);
@@ -2095,7 +2092,7 @@
 	}
 
 	del_timer_sync(&pool->mayday_timer);
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 	/*
 	 * This is necessary even after a new worker was just successfully
 	 * created as @pool->lock was dropped and the new worker might have
@@ -2118,7 +2115,7 @@
  * and may_start_working() is true.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock) which may be released and regrabbed
+ * raw_spin_lock_irq(pool->lock) which may be released and regrabbed
  * multiple times.  Does GFP_KERNEL allocations.
  *
  * Return:
@@ -2141,7 +2138,7 @@
 
 	pool->manager = NULL;
 	pool->flags &= ~POOL_MANAGER_ACTIVE;
-	wake_up(&wq_manager_wait);
+	swake_up_one(&wq_manager_wait);
 	return true;
 }
 
@@ -2157,7 +2154,7 @@
  * call this function to process a work.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock) which is released and regrabbed.
+ * raw_spin_lock_irq(pool->lock) which is released and regrabbed.
  */
 static void process_one_work(struct worker *worker, struct work_struct *work)
 __releases(&pool->lock)
@@ -2239,7 +2236,7 @@
 	 */
 	set_work_pool_and_clear_pending(work, pool->id);
 
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	lock_map_acquire(&pwq->wq->lockdep_map);
 	lock_map_acquire(&lockdep_map);
@@ -2285,7 +2282,7 @@
 	}
 
 	/*
-	 * The following prevents a kworker from hogging CPU on !PREEMPT
+	 * The following prevents a kworker from hogging CPU on !PREEMPTION
 	 * kernels, where a requeueing work item waiting for something to
 	 * happen could deadlock with stop_machine as such work item could
 	 * indefinitely requeue itself while all other CPUs are trapped in
@@ -2294,7 +2291,7 @@
 	 */
 	cond_resched();
 
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 
 	/* clear cpu intensive status */
 	if (unlikely(cpu_intensive))
@@ -2320,7 +2317,7 @@
  * fetches a work from the top and executes it.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock) which may be released and regrabbed
+ * raw_spin_lock_irq(pool->lock) which may be released and regrabbed
  * multiple times.
  */
 static void process_scheduled_works(struct worker *worker)
@@ -2362,11 +2359,11 @@
 	/* tell the scheduler that this is a workqueue worker */
 	set_pf_worker(true);
 woke_up:
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 
 	/* am I supposed to die? */
 	if (unlikely(worker->flags & WORKER_DIE)) {
-		spin_unlock_irq(&pool->lock);
+		raw_spin_unlock_irq(&pool->lock);
 		WARN_ON_ONCE(!list_empty(&worker->entry));
 		set_pf_worker(false);
 
@@ -2432,7 +2429,7 @@
 	 */
 	worker_enter_idle(worker);
 	__set_current_state(TASK_IDLE);
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 	schedule();
 	goto woke_up;
 }
@@ -2486,7 +2483,7 @@
 	should_stop = kthread_should_stop();
 
 	/* see whether any pwq is asking for help */
-	spin_lock_irq(&wq_mayday_lock);
+	raw_spin_lock_irq(&wq_mayday_lock);
 
 	while (!list_empty(&wq->maydays)) {
 		struct pool_workqueue *pwq = list_first_entry(&wq->maydays,
@@ -2498,11 +2495,11 @@
 		__set_current_state(TASK_RUNNING);
 		list_del_init(&pwq->mayday_node);
 
-		spin_unlock_irq(&wq_mayday_lock);
+		raw_spin_unlock_irq(&wq_mayday_lock);
 
 		worker_attach_to_pool(rescuer, pool);
 
-		spin_lock_irq(&pool->lock);
+		raw_spin_lock_irq(&pool->lock);
 
 		/*
 		 * Slurp in all works issued via this workqueue and
@@ -2531,7 +2528,7 @@
 			 * incur MAYDAY_INTERVAL delay inbetween.
 			 */
 			if (need_to_create_worker(pool)) {
-				spin_lock(&wq_mayday_lock);
+				raw_spin_lock(&wq_mayday_lock);
 				/*
 				 * Queue iff we aren't racing destruction
 				 * and somebody else hasn't queued it already.
@@ -2540,7 +2537,7 @@
 					get_pwq(pwq);
 					list_add_tail(&pwq->mayday_node, &wq->maydays);
 				}
-				spin_unlock(&wq_mayday_lock);
+				raw_spin_unlock(&wq_mayday_lock);
 			}
 		}
 
@@ -2558,14 +2555,14 @@
 		if (need_more_worker(pool))
 			wake_up_worker(pool);
 
-		spin_unlock_irq(&pool->lock);
+		raw_spin_unlock_irq(&pool->lock);
 
 		worker_detach_from_pool(rescuer);
 
-		spin_lock_irq(&wq_mayday_lock);
+		raw_spin_lock_irq(&wq_mayday_lock);
 	}
 
-	spin_unlock_irq(&wq_mayday_lock);
+	raw_spin_unlock_irq(&wq_mayday_lock);
 
 	if (should_stop) {
 		__set_current_state(TASK_RUNNING);
@@ -2645,7 +2642,7 @@
  * underneath us, so we can't reliably determine pwq from @target.
  *
  * CONTEXT:
- * spin_lock_irq(pool->lock).
+ * raw_spin_lock_irq(pool->lock).
  */
 static void insert_wq_barrier(struct pool_workqueue *pwq,
 			      struct wq_barrier *barr,
@@ -2732,7 +2729,7 @@
 	for_each_pwq(pwq, wq) {
 		struct worker_pool *pool = pwq->pool;
 
-		spin_lock_irq(&pool->lock);
+		raw_spin_lock_irq(&pool->lock);
 
 		if (flush_color >= 0) {
 			WARN_ON_ONCE(pwq->flush_color != -1);
@@ -2749,7 +2746,7 @@
 			pwq->work_color = work_color;
 		}
 
-		spin_unlock_irq(&pool->lock);
+		raw_spin_unlock_irq(&pool->lock);
 	}
 
 	if (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))
@@ -2949,9 +2946,9 @@
 	for_each_pwq(pwq, wq) {
 		bool drained;
 
-		spin_lock_irq(&pwq->pool->lock);
+		raw_spin_lock_irq(&pwq->pool->lock);
 		drained = !pwq->nr_active && list_empty(&pwq->delayed_works);
-		spin_unlock_irq(&pwq->pool->lock);
+		raw_spin_unlock_irq(&pwq->pool->lock);
 
 		if (drained)
 			continue;
@@ -2987,7 +2984,7 @@
 		return false;
 	}
 
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 	/* see the comment in try_to_grab_pending() with the same code */
 	pwq = get_work_pwq(work);
 	if (pwq) {
@@ -3003,7 +3000,7 @@
 	check_flush_dependency(pwq->wq, work);
 
 	insert_wq_barrier(pwq, barr, work, worker);
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	/*
 	 * Force a lock recursion deadlock when using flush_work() inside a
@@ -3022,7 +3019,7 @@
 	rcu_read_unlock();
 	return true;
 already_gone:
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 	rcu_read_unlock();
 	return false;
 }
@@ -3415,7 +3412,7 @@
  */
 static int init_worker_pool(struct worker_pool *pool)
 {
-	spin_lock_init(&pool->lock);
+	raw_spin_lock_init(&pool->lock);
 	pool->id = -1;
 	pool->cpu = -1;
 	pool->node = NUMA_NO_NODE;
@@ -3541,15 +3538,15 @@
 	 * @pool's workers from blocking on attach_mutex.  We're the last
 	 * manager and @pool gets freed with the flag set.
 	 */
-	spin_lock_irq(&pool->lock);
-	wait_event_lock_irq(wq_manager_wait,
+	raw_spin_lock_irq(&pool->lock);
+	swait_event_lock_irq(wq_manager_wait,
 			    !(pool->flags & POOL_MANAGER_ACTIVE), pool->lock);
 	pool->flags |= POOL_MANAGER_ACTIVE;
 
 	while ((worker = first_idle_worker(pool)))
 		destroy_worker(worker);
 	WARN_ON(pool->nr_workers || pool->nr_idle);
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 
 	mutex_lock(&wq_pool_attach_mutex);
 	if (!list_empty(&pool->workers))
@@ -3705,7 +3702,7 @@
 		return;
 
 	/* this function can be called during early boot w/ irq disabled */
-	spin_lock_irqsave(&pwq->pool->lock, flags);
+	raw_spin_lock_irqsave(&pwq->pool->lock, flags);
 
 	/*
 	 * During [un]freezing, the caller is responsible for ensuring that
@@ -3728,7 +3725,7 @@
 		pwq->max_active = 0;
 	}
 
-	spin_unlock_irqrestore(&pwq->pool->lock, flags);
+	raw_spin_unlock_irqrestore(&pwq->pool->lock, flags);
 }
 
 /* initialize newly alloced @pwq which is associated with @wq and @pool */
@@ -4130,9 +4127,9 @@
 
 use_dfl_pwq:
 	mutex_lock(&wq->mutex);
-	spin_lock_irq(&wq->dfl_pwq->pool->lock);
+	raw_spin_lock_irq(&wq->dfl_pwq->pool->lock);
 	get_pwq(wq->dfl_pwq);
-	spin_unlock_irq(&wq->dfl_pwq->pool->lock);
+	raw_spin_unlock_irq(&wq->dfl_pwq->pool->lock);
 	old_pwq = numa_pwq_tbl_install(wq, node, wq->dfl_pwq);
 out_unlock:
 	mutex_unlock(&wq->mutex);
@@ -4345,9 +4342,9 @@
 		struct worker *rescuer = wq->rescuer;
 
 		/* this prevents new queueing */
-		spin_lock_irq(&wq_mayday_lock);
+		raw_spin_lock_irq(&wq_mayday_lock);
 		wq->rescuer = NULL;
-		spin_unlock_irq(&wq_mayday_lock);
+		raw_spin_unlock_irq(&wq_mayday_lock);
 
 		/* rescuer will empty maydays list before exiting */
 		kthread_stop(rescuer->task);
@@ -4543,10 +4540,10 @@
 	rcu_read_lock();
 	pool = get_work_pool(work);
 	if (pool) {
-		spin_lock_irqsave(&pool->lock, flags);
+		raw_spin_lock_irqsave(&pool->lock, flags);
 		if (find_worker_executing_work(pool, work))
 			ret |= WORK_BUSY_RUNNING;
-		spin_unlock_irqrestore(&pool->lock, flags);
+		raw_spin_unlock_irqrestore(&pool->lock, flags);
 	}
 	rcu_read_unlock();
 
@@ -4753,10 +4750,10 @@
 		pr_info("workqueue %s: flags=0x%x\n", wq->name, wq->flags);
 
 		for_each_pwq(pwq, wq) {
-			spin_lock_irqsave(&pwq->pool->lock, flags);
+			raw_spin_lock_irqsave(&pwq->pool->lock, flags);
 			if (pwq->nr_active || !list_empty(&pwq->delayed_works))
 				show_pwq(pwq);
-			spin_unlock_irqrestore(&pwq->pool->lock, flags);
+			raw_spin_unlock_irqrestore(&pwq->pool->lock, flags);
 			/*
 			 * We could be printing a lot from atomic context, e.g.
 			 * sysrq-t -> show_workqueue_state(). Avoid triggering
@@ -4770,7 +4767,7 @@
 		struct worker *worker;
 		bool first = true;
 
-		spin_lock_irqsave(&pool->lock, flags);
+		raw_spin_lock_irqsave(&pool->lock, flags);
 		if (pool->nr_workers == pool->nr_idle)
 			goto next_pool;
 
@@ -4789,7 +4786,7 @@
 		}
 		pr_cont("\n");
 	next_pool:
-		spin_unlock_irqrestore(&pool->lock, flags);
+		raw_spin_unlock_irqrestore(&pool->lock, flags);
 		/*
 		 * We could be printing a lot from atomic context, e.g.
 		 * sysrq-t -> show_workqueue_state(). Avoid triggering
@@ -4819,7 +4816,7 @@
 		struct worker_pool *pool = worker->pool;
 
 		if (pool) {
-			spin_lock_irq(&pool->lock);
+			raw_spin_lock_irq(&pool->lock);
 			/*
 			 * ->desc tracks information (wq name or
 			 * set_worker_desc()) for the latest execution.  If
@@ -4833,7 +4830,7 @@
 					scnprintf(buf + off, size - off, "-%s",
 						  worker->desc);
 			}
-			spin_unlock_irq(&pool->lock);
+			raw_spin_unlock_irq(&pool->lock);
 		}
 	}
 
@@ -4864,7 +4861,7 @@
 
 	for_each_cpu_worker_pool(pool, cpu) {
 		mutex_lock(&wq_pool_attach_mutex);
-		spin_lock_irq(&pool->lock);
+		raw_spin_lock_irq(&pool->lock);
 
 		/*
 		 * We've blocked all attach/detach operations. Make all workers
@@ -4878,7 +4875,7 @@
 
 		pool->flags |= POOL_DISASSOCIATED;
 
-		spin_unlock_irq(&pool->lock);
+		raw_spin_unlock_irq(&pool->lock);
 		mutex_unlock(&wq_pool_attach_mutex);
 
 		/*
@@ -4904,9 +4901,9 @@
 		 * worker blocking could lead to lengthy stalls.  Kick off
 		 * unbound chain execution of currently pending work items.
 		 */
-		spin_lock_irq(&pool->lock);
+		raw_spin_lock_irq(&pool->lock);
 		wake_up_worker(pool);
-		spin_unlock_irq(&pool->lock);
+		raw_spin_unlock_irq(&pool->lock);
 	}
 }
 
@@ -4933,7 +4930,7 @@
 		WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,
 						  pool->attrs->cpumask) < 0);
 
-	spin_lock_irq(&pool->lock);
+	raw_spin_lock_irq(&pool->lock);
 
 	pool->flags &= ~POOL_DISASSOCIATED;
 
@@ -4972,7 +4969,7 @@
 		WRITE_ONCE(worker->flags, worker_flags);
 	}
 
-	spin_unlock_irq(&pool->lock);
+	raw_spin_unlock_irq(&pool->lock);
 }
 
 /**
diff -Nur linux-5.4.5/lib/bust_spinlocks.c linux-5.4.5-new/lib/bust_spinlocks.c
--- linux-5.4.5/lib/bust_spinlocks.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/bust_spinlocks.c	2020-06-15 16:12:26.955711482 +0300
@@ -26,7 +26,6 @@
 		unblank_screen();
 #endif
 		console_unblank();
-		if (--oops_in_progress == 0)
-			wake_up_klogd();
+		--oops_in_progress;
 	}
 }
diff -Nur linux-5.4.5/lib/debugobjects.c linux-5.4.5-new/lib/debugobjects.c
--- linux-5.4.5/lib/debugobjects.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/debugobjects.c	2020-06-15 16:12:26.955711482 +0300
@@ -533,7 +533,10 @@
 	struct debug_obj *obj;
 	unsigned long flags;
 
-	fill_pool();
+#ifdef CONFIG_PREEMPT_RT
+	if (preempt_count() == 0 && !irqs_disabled())
+#endif
+		fill_pool();
 
 	db = get_bucket((unsigned long) addr);
 
diff -Nur linux-5.4.5/lib/irq_poll.c linux-5.4.5-new/lib/irq_poll.c
--- linux-5.4.5/lib/irq_poll.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/irq_poll.c	2020-06-15 16:12:26.955711482 +0300
@@ -37,6 +37,7 @@
 	list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll));
 	raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_sched);
 
@@ -72,6 +73,7 @@
 	local_irq_save(flags);
 	__irq_poll_complete(iop);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(irq_poll_complete);
 
@@ -96,6 +98,7 @@
 		}
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Even though interrupts have been re-enabled, this
 		 * access is safe because interrupts can only add new
@@ -133,6 +136,7 @@
 		__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 
 	local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 /**
@@ -196,6 +200,7 @@
 			 this_cpu_ptr(&blk_cpu_iopoll));
 	__raise_softirq_irqoff(IRQ_POLL_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	return 0;
 }
diff -Nur linux-5.4.5/lib/Kconfig.debug linux-5.4.5-new/lib/Kconfig.debug
--- linux-5.4.5/lib/Kconfig.debug	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/Kconfig.debug	2020-06-15 16:12:26.955711482 +0300
@@ -61,6 +61,23 @@
 	  will be used as the loglevel. IOW passing "quiet" will be the
 	  equivalent of passing "loglevel=<CONSOLE_LOGLEVEL_QUIET>"
 
+config CONSOLE_LOGLEVEL_EMERGENCY
+	int "Emergency console loglevel (1-15)"
+	range 1 15
+	default "5"
+	help
+	  The loglevel to determine if a console message is an emergency
+	  message.
+
+	  If supported by the console driver, emergency messages will be
+	  flushed to the console immediately. This can cause significant system
+	  latencies so the value should be set such that only significant
+	  messages are classified as emergency messages.
+
+	  Setting a default here is equivalent to passing in
+	  emergency_loglevel=<x> in the kernel bootargs. emergency_loglevel=<x>
+	  continues to override whatever value is specified here as well.
+
 config MESSAGE_LOGLEVEL_DEFAULT
 	int "Default message log level (1-7)"
 	range 1 7
@@ -1053,7 +1070,7 @@
 
 config DEBUG_PREEMPT
 	bool "Debug preemptible kernel"
-	depends on DEBUG_KERNEL && PREEMPT && TRACE_IRQFLAGS_SUPPORT
+	depends on DEBUG_KERNEL && PREEMPTION && TRACE_IRQFLAGS_SUPPORT
 	default y
 	help
 	  If you say Y here then the kernel will use a debug variant of the
@@ -1231,7 +1248,7 @@
 
 config DEBUG_LOCKING_API_SELFTESTS
 	bool "Locking API boot-time self-tests"
-	depends on DEBUG_KERNEL
+	depends on DEBUG_KERNEL && !PREEMPT_RT
 	help
 	  Say Y here if you want the kernel to run a short self-test during
 	  bootup. The self-test checks whether common types of locking bugs
diff -Nur linux-5.4.5/lib/locking-selftest.c linux-5.4.5-new/lib/locking-selftest.c
--- linux-5.4.5/lib/locking-selftest.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/locking-selftest.c	2020-06-15 16:12:26.955711482 +0300
@@ -742,6 +742,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_rlock)
 
@@ -757,9 +759,12 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
+#ifndef CONFIG_PREEMPT_RT
 /*
  * Enabling hardirqs with a softirq-safe lock held:
  */
@@ -792,6 +797,8 @@
 #undef E1
 #undef E2
 
+#endif
+
 /*
  * Enabling irqs with an irq-safe lock held:
  */
@@ -815,6 +822,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_rlock)
 
@@ -830,6 +839,8 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
@@ -861,6 +872,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_rlock)
 
@@ -876,6 +889,8 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
@@ -909,6 +924,8 @@
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_rlock)
 
@@ -924,10 +941,14 @@
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
 
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock irq inversion.
  *
@@ -990,6 +1011,10 @@
 #undef E2
 #undef E3
 
+#endif
+
+#ifndef CONFIG_PREEMPT_RT
+
 /*
  * read-lock / write-lock recursion that is actually safe.
  */
@@ -1028,6 +1053,8 @@
 #undef E2
 #undef E3
 
+#endif
+
 /*
  * read-lock / write-lock recursion that is unsafe.
  */
@@ -2058,6 +2085,7 @@
 
 	printk("  --------------------------------------------------------------------------\n");
 
+#ifndef CONFIG_PREEMPT_RT
 	/*
 	 * irq-context testcases:
 	 */
@@ -2070,6 +2098,28 @@
 
 	DO_TESTCASE_6x2("irq read-recursion", irq_read_recursion);
 //	DO_TESTCASE_6x2B("irq read-recursion #2", irq_read_recursion2);
+#else
+	/* On -rt, we only do hardirq context test for raw spinlock */
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 12);
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 12);
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 321);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 321);
+#endif
 
 	ww_tests();
 
diff -Nur linux-5.4.5/lib/Makefile linux-5.4.5-new/lib/Makefile
--- linux-5.4.5/lib/Makefile	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/Makefile	2020-06-15 16:12:26.955711482 +0300
@@ -26,7 +26,7 @@
 
 lib-y := ctype.o string.o vsprintf.o cmdline.o \
 	 rbtree.o radix-tree.o timerqueue.o xarray.o \
-	 idr.o extable.o \
+	 idr.o extable.o printk_ringbuffer.o \
 	 sha1.o chacha.o irq_regs.o argv_split.o \
 	 flex_proportions.o ratelimit.o show_mem.o \
 	 is_single_threaded.o plist.o decompress.o kobject_uevent.o \
diff -Nur linux-5.4.5/lib/nmi_backtrace.c linux-5.4.5-new/lib/nmi_backtrace.c
--- linux-5.4.5/lib/nmi_backtrace.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/nmi_backtrace.c	2020-06-15 16:12:26.955711482 +0300
@@ -75,12 +75,6 @@
 		touch_softlockup_watchdog();
 	}
 
-	/*
-	 * Force flush any remote buffers that might be stuck in IRQ context
-	 * and therefore could not run their irq_work.
-	 */
-	printk_safe_flush();
-
 	clear_bit_unlock(0, &backtrace_flag);
 	put_cpu();
 }
diff -Nur linux-5.4.5/lib/printk_ringbuffer.c linux-5.4.5-new/lib/printk_ringbuffer.c
--- linux-5.4.5/lib/printk_ringbuffer.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/lib/printk_ringbuffer.c	2020-06-15 16:12:26.955711482 +0300
@@ -0,0 +1,589 @@
+// SPDX-License-Identifier: GPL-2.0
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/printk_ringbuffer.h>
+
+#define PRB_SIZE(rb) (1 << rb->size_bits)
+#define PRB_SIZE_BITMASK(rb) (PRB_SIZE(rb) - 1)
+#define PRB_INDEX(rb, lpos) (lpos & PRB_SIZE_BITMASK(rb))
+#define PRB_WRAPS(rb, lpos) (lpos >> rb->size_bits)
+#define PRB_WRAP_LPOS(rb, lpos, xtra) \
+	((PRB_WRAPS(rb, lpos) + xtra) << rb->size_bits)
+#define PRB_DATA_SIZE(e) (e->size - sizeof(struct prb_entry))
+#define PRB_DATA_ALIGN sizeof(long)
+
+static bool __prb_trylock(struct prb_cpulock *cpu_lock,
+			  unsigned int *cpu_store)
+{
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = get_cpu();
+
+	*cpu_store = atomic_read(&cpu_lock->owner);
+	/* memory barrier to ensure the current lock owner is visible */
+	smp_rmb();
+	if (*cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_save(*flags);
+		if (atomic_try_cmpxchg_acquire(&cpu_lock->owner,
+					       cpu_store, cpu)) {
+			return true;
+		}
+		local_irq_restore(*flags);
+	} else if (*cpu_store == cpu) {
+		return true;
+	}
+
+	put_cpu();
+	return false;
+}
+
+/*
+ * prb_lock: Perform a processor-reentrant spin lock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" pointer to store lock status information.
+ *
+ * If no processor has the lock, the calling processor takes the lock and
+ * becomes the owner. If the calling processor is already the owner of the
+ * lock, this function succeeds immediately. If lock is locked by another
+ * processor, this function spins until the calling processor becomes the
+ * owner.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_lock(struct prb_cpulock *cpu_lock, unsigned int *cpu_store)
+{
+	for (;;) {
+		if (__prb_trylock(cpu_lock, cpu_store))
+			break;
+		cpu_relax();
+	}
+}
+
+/*
+ * prb_unlock: Perform a processor-reentrant spin unlock.
+ * @cpu_lock: A pointer to the lock object.
+ * @cpu_store: A "flags" object storing lock status information.
+ *
+ * Release the lock. The calling processor must be the owner of the lock.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_unlock(struct prb_cpulock *cpu_lock, unsigned int cpu_store)
+{
+	unsigned long *flags;
+	unsigned int cpu;
+
+	cpu = atomic_read(&cpu_lock->owner);
+	atomic_set_release(&cpu_lock->owner, cpu_store);
+
+	if (cpu_store == -1) {
+		flags = per_cpu_ptr(cpu_lock->irqflags, cpu);
+		local_irq_restore(*flags);
+	}
+
+	put_cpu();
+}
+
+static struct prb_entry *to_entry(struct printk_ringbuffer *rb,
+				  unsigned long lpos)
+{
+	char *buffer = rb->buffer;
+	buffer += PRB_INDEX(rb, lpos);
+	return (struct prb_entry *)buffer;
+}
+
+static int calc_next(struct printk_ringbuffer *rb, unsigned long tail,
+		     unsigned long lpos, int size, unsigned long *calced_next)
+{
+	unsigned long next_lpos;
+	int ret = 0;
+again:
+	next_lpos = lpos + size;
+	if (next_lpos - tail > PRB_SIZE(rb))
+		return -1;
+
+	if (PRB_WRAPS(rb, lpos) != PRB_WRAPS(rb, next_lpos)) {
+		lpos = PRB_WRAP_LPOS(rb, next_lpos, 0);
+		ret |= 1;
+		goto again;
+	}
+
+	*calced_next = next_lpos;
+	return ret;
+}
+
+static bool push_tail(struct printk_ringbuffer *rb, unsigned long tail)
+{
+	unsigned long new_tail;
+	struct prb_entry *e;
+	unsigned long head;
+
+	if (tail != atomic_long_read(&rb->tail))
+		return true;
+
+	e = to_entry(rb, tail);
+	if (e->size != -1)
+		new_tail = tail + e->size;
+	else
+		new_tail = PRB_WRAP_LPOS(rb, tail, 1);
+
+	/* make sure the new tail does not overtake the head */
+	head = atomic_long_read(&rb->head);
+	if (head - new_tail > PRB_SIZE(rb))
+		return false;
+
+	atomic_long_cmpxchg(&rb->tail, tail, new_tail);
+	return true;
+}
+
+/*
+ * prb_commit: Commit a reserved entry to the ring buffer.
+ * @h: An entry handle referencing the data entry to commit.
+ *
+ * Commit data that has been reserved using prb_reserve(). Once the data
+ * block has been committed, it can be invalidated at any time. If a writer
+ * is interested in using the data after committing, the writer should make
+ * its own copy first or use the prb_iter_ reader functions to access the
+ * data in the ring buffer.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_commit(struct prb_handle *h)
+{
+	struct printk_ringbuffer *rb = h->rb;
+	bool changed = false;
+	struct prb_entry *e;
+	unsigned long head;
+	unsigned long res;
+
+	for (;;) {
+		if (atomic_read(&rb->ctx) != 1) {
+			/* the interrupted context will fixup head */
+			atomic_dec(&rb->ctx);
+			break;
+		}
+		/* assign sequence numbers before moving head */
+		head = atomic_long_read(&rb->head);
+		res = atomic_long_read(&rb->reserve);
+		while (head != res) {
+			e = to_entry(rb, head);
+			if (e->size == -1) {
+				head = PRB_WRAP_LPOS(rb, head, 1);
+				continue;
+			}
+			while (atomic_long_read(&rb->lost)) {
+				atomic_long_dec(&rb->lost);
+				rb->seq++;
+			}
+			e->seq = ++rb->seq;
+			head += e->size;
+			changed = true;
+		}
+		atomic_long_set_release(&rb->head, res);
+
+		atomic_dec(&rb->ctx);
+
+		if (atomic_long_read(&rb->reserve) == res)
+			break;
+		atomic_inc(&rb->ctx);
+	}
+
+	prb_unlock(rb->cpulock, h->cpu);
+
+	if (changed) {
+		atomic_long_inc(&rb->wq_counter);
+		if (wq_has_sleeper(rb->wq)) {
+#ifdef CONFIG_IRQ_WORK
+			irq_work_queue(rb->wq_work);
+#else
+			if (!in_nmi())
+				wake_up_interruptible_all(rb->wq);
+#endif
+		}
+	}
+}
+
+/*
+ * prb_reserve: Reserve an entry within a ring buffer.
+ * @h: An entry handle to be setup and reference an entry.
+ * @rb: A ring buffer to reserve data within.
+ * @size: The number of bytes to reserve.
+ *
+ * Reserve an entry of at least @size bytes to be used by the caller. If
+ * successful, the data region of the entry belongs to the caller and cannot
+ * be invalidated by any other task/context. For this reason, the caller
+ * should call prb_commit() as quickly as possible in order to avoid preventing
+ * other tasks/contexts from reserving data in the case that the ring buffer
+ * has wrapped.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns a pointer to the reserved entry (and @h is setup to reference that
+ * entry) or NULL if it was not possible to reserve data.
+ */
+char *prb_reserve(struct prb_handle *h, struct printk_ringbuffer *rb,
+		  unsigned int size)
+{
+	unsigned long tail, res1, res2;
+	int ret;
+
+	if (size == 0)
+		return NULL;
+	size += sizeof(struct prb_entry);
+	size += PRB_DATA_ALIGN - 1;
+	size &= ~(PRB_DATA_ALIGN - 1);
+	if (size >= PRB_SIZE(rb))
+		return NULL;
+
+	h->rb = rb;
+	prb_lock(rb->cpulock, &h->cpu);
+
+	atomic_inc(&rb->ctx);
+
+	do {
+		for (;;) {
+			tail = atomic_long_read(&rb->tail);
+			res1 = atomic_long_read(&rb->reserve);
+			ret = calc_next(rb, tail, res1, size, &res2);
+			if (ret >= 0)
+				break;
+			if (!push_tail(rb, tail)) {
+				prb_commit(h);
+				return NULL;
+			}
+		}
+	} while (!atomic_long_try_cmpxchg_acquire(&rb->reserve, &res1, res2));
+
+	h->entry = to_entry(rb, res1);
+
+	if (ret) {
+		/* handle wrap */
+		h->entry->size = -1;
+		h->entry = to_entry(rb, PRB_WRAP_LPOS(rb, res2, 0));
+	}
+
+	h->entry->size = size;
+
+	return &h->entry->data[0];
+}
+
+/*
+ * prb_iter_copy: Copy an iterator.
+ * @dest: The iterator to copy to.
+ * @src: The iterator to copy from.
+ *
+ * Make a deep copy of an iterator. This is particularly useful for making
+ * backup copies of an iterator in case a form of rewinding it needed.
+ *
+ * It is safe to call this function from any context and state. But
+ * note that this function is not atomic. Callers should not make copies
+ * to/from iterators that can be accessed by other tasks/contexts.
+ */
+void prb_iter_copy(struct prb_iterator *dest, struct prb_iterator *src)
+{
+	memcpy(dest, src, sizeof(*dest));
+}
+
+/*
+ * prb_iter_init: Initialize an iterator for a ring buffer.
+ * @iter: The iterator to initialize.
+ * @rb: A ring buffer to that @iter should iterate.
+ * @seq: The sequence number of the position preceding the first record.
+ *       May be NULL.
+ *
+ * Initialize an iterator to be used with a specified ring buffer. If @seq
+ * is non-NULL, it will be set such that prb_iter_next() will provide a
+ * sequence value of "@seq + 1" if no records were missed.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_iter_init(struct prb_iterator *iter, struct printk_ringbuffer *rb,
+		   u64 *seq)
+{
+	memset(iter, 0, sizeof(*iter));
+	iter->rb = rb;
+	iter->lpos = PRB_INIT;
+
+	if (!seq)
+		return;
+
+	for (;;) {
+		struct prb_iterator tmp_iter;
+		int ret;
+
+		prb_iter_copy(&tmp_iter, iter);
+
+		ret = prb_iter_next(&tmp_iter, NULL, 0, seq);
+		if (ret < 0)
+			continue;
+
+		if (ret == 0)
+			*seq = 0;
+		else
+			(*seq)--;
+		break;
+	}
+}
+
+static bool is_valid(struct printk_ringbuffer *rb, unsigned long lpos)
+{
+	unsigned long head, tail;
+
+	tail = atomic_long_read(&rb->tail);
+	head = atomic_long_read(&rb->head);
+	head -= tail;
+	lpos -= tail;
+
+	if (lpos >= head)
+		return false;
+	return true;
+}
+
+/*
+ * prb_iter_data: Retrieve the record data at the current position.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the record. May be NULL.
+ *
+ * If @iter is at a record, provide the data and/or sequence number of that
+ * record (if specified by the caller).
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns >=0 if the current record contains valid data (returns 0 if @buf
+ * is NULL or returns the size of the data block if @buf is non-NULL) or
+ * -EINVAL if @iter is now invalid.
+ */
+int prb_iter_data(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	struct printk_ringbuffer *rb = iter->rb;
+	unsigned long lpos = iter->lpos;
+	unsigned int datsize = 0;
+	struct prb_entry *e;
+
+	if (buf || seq) {
+		e = to_entry(rb, lpos);
+		if (!is_valid(rb, lpos))
+			return -EINVAL;
+		/* memory barrier to ensure valid lpos */
+		smp_rmb();
+		if (buf) {
+			datsize = PRB_DATA_SIZE(e);
+			/* memory barrier to ensure load of datsize */
+			smp_rmb();
+			if (!is_valid(rb, lpos))
+				return -EINVAL;
+			if (PRB_INDEX(rb, lpos) + datsize >
+			    PRB_SIZE(rb) - PRB_DATA_ALIGN) {
+				return -EINVAL;
+			}
+			if (size > datsize)
+				size = datsize;
+			memcpy(buf, &e->data[0], size);
+		}
+		if (seq)
+			*seq = e->seq;
+		/* memory barrier to ensure loads of entry data */
+		smp_rmb();
+	}
+
+	if (!is_valid(rb, lpos))
+		return -EINVAL;
+
+	return datsize;
+}
+
+/*
+ * prb_iter_next: Advance to the next record.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the next record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the next record. May be NULL.
+ *
+ * If a next record is available, @iter is advanced and (if specified)
+ * the data and/or sequence number of that record are provided.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns 1 if @iter was advanced, 0 if @iter is at the end of the list, or
+ * -EINVAL if @iter is now invalid.
+ */
+int prb_iter_next(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	struct printk_ringbuffer *rb = iter->rb;
+	unsigned long next_lpos;
+	struct prb_entry *e;
+	unsigned int esize;
+
+	if (iter->lpos == PRB_INIT) {
+		next_lpos = atomic_long_read(&rb->tail);
+	} else {
+		if (!is_valid(rb, iter->lpos))
+			return -EINVAL;
+		/* memory barrier to ensure valid lpos */
+		smp_rmb();
+		e = to_entry(rb, iter->lpos);
+		esize = e->size;
+		/* memory barrier to ensure load of size */
+		smp_rmb();
+		if (!is_valid(rb, iter->lpos))
+			return -EINVAL;
+		next_lpos = iter->lpos + esize;
+	}
+	if (next_lpos == atomic_long_read(&rb->head))
+		return 0;
+	if (!is_valid(rb, next_lpos))
+		return -EINVAL;
+	/* memory barrier to ensure valid lpos */
+	smp_rmb();
+
+	iter->lpos = next_lpos;
+	e = to_entry(rb, iter->lpos);
+	esize = e->size;
+	/* memory barrier to ensure load of size */
+	smp_rmb();
+	if (!is_valid(rb, iter->lpos))
+		return -EINVAL;
+	if (esize == -1)
+		iter->lpos = PRB_WRAP_LPOS(rb, iter->lpos, 1);
+
+	if (prb_iter_data(iter, buf, size, seq) < 0)
+		return -EINVAL;
+
+	return 1;
+}
+
+/*
+ * prb_iter_wait_next: Advance to the next record, blocking if none available.
+ * @iter: Iterator tracking the current position.
+ * @buf: A buffer to store the data of the next record. May be NULL.
+ * @size: The size of @buf. (Ignored if @buf is NULL.)
+ * @seq: The sequence number of the next record. May be NULL.
+ *
+ * If a next record is already available, this function works like
+ * prb_iter_next(). Otherwise block interruptible until a next record is
+ * available.
+ *
+ * When a next record is available, @iter is advanced and (if specified)
+ * the data and/or sequence number of that record are provided.
+ *
+ * This function might sleep.
+ *
+ * Returns 1 if @iter was advanced, -EINVAL if @iter is now invalid, or
+ * -ERESTARTSYS if interrupted by a signal.
+ */
+int prb_iter_wait_next(struct prb_iterator *iter, char *buf, int size, u64 *seq)
+{
+	unsigned long last_seen;
+	int ret;
+
+	for (;;) {
+		last_seen = atomic_long_read(&iter->rb->wq_counter);
+
+		ret = prb_iter_next(iter, buf, size, seq);
+		if (ret != 0)
+			break;
+
+		ret = wait_event_interruptible(*iter->rb->wq,
+			last_seen != atomic_long_read(&iter->rb->wq_counter));
+		if (ret < 0)
+			break;
+	}
+
+	return ret;
+}
+
+/*
+ * prb_iter_seek: Seek forward to a specific record.
+ * @iter: Iterator to advance.
+ * @seq: Record number to advance to.
+ *
+ * Advance @iter such that a following call to prb_iter_data() will provide
+ * the contents of the specified record. If a record is specified that does
+ * not yet exist, advance @iter to the end of the record list.
+ *
+ * Note that iterators cannot be rewound. So if a record is requested that
+ * exists but is previous to @iter in position, @iter is considered invalid.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns 1 on succces, 0 if specified record does not yet exist (@iter is
+ * now at the end of the list), or -EINVAL if @iter is now invalid.
+ */
+int prb_iter_seek(struct prb_iterator *iter, u64 seq)
+{
+	u64 cur_seq;
+	int ret;
+
+	/* first check if the iterator is already at the wanted seq */
+	if (seq == 0) {
+		if (iter->lpos == PRB_INIT)
+			return 1;
+		else
+			return -EINVAL;
+	}
+	if (iter->lpos != PRB_INIT) {
+		if (prb_iter_data(iter, NULL, 0, &cur_seq) >= 0) {
+			if (cur_seq == seq)
+				return 1;
+			if (cur_seq > seq)
+				return -EINVAL;
+		}
+	}
+
+	/* iterate to find the wanted seq */
+	for (;;) {
+		ret = prb_iter_next(iter, NULL, 0, &cur_seq);
+		if (ret <= 0)
+			break;
+
+		if (cur_seq == seq)
+			break;
+
+		if (cur_seq > seq) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+/*
+ * prb_buffer_size: Get the size of the ring buffer.
+ * @rb: The ring buffer to get the size of.
+ *
+ * Return the number of bytes used for the ring buffer entry storage area.
+ * Note that this area stores both entry header and entry data. Therefore
+ * this represents an upper bound to the amount of data that can be stored
+ * in the ring buffer.
+ *
+ * It is safe to call this function from any context and state.
+ *
+ * Returns the size in bytes of the entry storage area.
+ */
+int prb_buffer_size(struct printk_ringbuffer *rb)
+{
+	return PRB_SIZE(rb);
+}
+
+/*
+ * prb_inc_lost: Increment the seq counter to signal a lost record.
+ * @rb: The ring buffer to increment the seq of.
+ *
+ * Increment the seq counter so that a seq number is intentially missing
+ * for the readers. This allows readers to identify that a record is
+ * missing. A writer will typically use this function if prb_reserve()
+ * fails.
+ *
+ * It is safe to call this function from any context and state.
+ */
+void prb_inc_lost(struct printk_ringbuffer *rb)
+{
+	atomic_long_inc(&rb->lost);
+}
diff -Nur linux-5.4.5/lib/radix-tree.c linux-5.4.5-new/lib/radix-tree.c
--- linux-5.4.5/lib/radix-tree.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/radix-tree.c	2020-06-15 16:12:26.955711482 +0300
@@ -26,7 +26,7 @@
 #include <linux/slab.h>
 #include <linux/string.h>
 #include <linux/xarray.h>
-
+#include <linux/locallock.h>
 
 /*
  * Radix tree node cache.
@@ -72,6 +72,7 @@
 	struct radix_tree_node *nodes;
 };
 static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };
+static DEFINE_LOCAL_IRQ_LOCK(radix_tree_preloads_lock);
 
 static inline struct radix_tree_node *entry_to_node(void *ptr)
 {
@@ -269,12 +270,13 @@
 		 * succeed in getting a node here (and never reach
 		 * kmem_cache_alloc)
 		 */
-		rtp = this_cpu_ptr(&radix_tree_preloads);
+		rtp = &get_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
 		if (rtp->nr) {
 			ret = rtp->nodes;
 			rtp->nodes = ret->parent;
 			rtp->nr--;
 		}
+		put_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
 		/*
 		 * Update the allocation stack trace as this is more useful
 		 * for debugging.
@@ -340,14 +342,14 @@
 	 */
 	gfp_mask &= ~__GFP_ACCOUNT;
 
-	preempt_disable();
+	local_lock(radix_tree_preloads_lock);
 	rtp = this_cpu_ptr(&radix_tree_preloads);
 	while (rtp->nr < nr) {
-		preempt_enable();
+		local_unlock(radix_tree_preloads_lock);
 		node = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
 		if (node == NULL)
 			goto out;
-		preempt_disable();
+		local_lock(radix_tree_preloads_lock);
 		rtp = this_cpu_ptr(&radix_tree_preloads);
 		if (rtp->nr < nr) {
 			node->parent = rtp->nodes;
@@ -389,11 +391,17 @@
 	if (gfpflags_allow_blocking(gfp_mask))
 		return __radix_tree_preload(gfp_mask, RADIX_TREE_PRELOAD_SIZE);
 	/* Preloading doesn't help anything with this gfp mask, skip it */
-	preempt_disable();
+	local_lock(radix_tree_preloads_lock);
 	return 0;
 }
 EXPORT_SYMBOL(radix_tree_maybe_preload);
 
+void radix_tree_preload_end(void)
+{
+	local_unlock(radix_tree_preloads_lock);
+}
+EXPORT_SYMBOL(radix_tree_preload_end);
+
 static unsigned radix_tree_load_root(const struct radix_tree_root *root,
 		struct radix_tree_node **nodep, unsigned long *maxindex)
 {
@@ -1478,10 +1486,16 @@
 void idr_preload(gfp_t gfp_mask)
 {
 	if (__radix_tree_preload(gfp_mask, IDR_PRELOAD_SIZE))
-		preempt_disable();
+		local_lock(radix_tree_preloads_lock);
 }
 EXPORT_SYMBOL(idr_preload);
 
+void idr_preload_end(void)
+{
+	local_unlock(radix_tree_preloads_lock);
+}
+EXPORT_SYMBOL(idr_preload_end);
+
 void __rcu **idr_get_free(struct radix_tree_root *root,
 			      struct radix_tree_iter *iter, gfp_t gfp,
 			      unsigned long max)
diff -Nur linux-5.4.5/lib/scatterlist.c linux-5.4.5-new/lib/scatterlist.c
--- linux-5.4.5/lib/scatterlist.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/scatterlist.c	2020-06-15 16:12:26.955711482 +0300
@@ -811,7 +811,7 @@
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
-			WARN_ON_ONCE(preemptible());
+			WARN_ON_ONCE(!pagefault_disabled());
 			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);
diff -Nur linux-5.4.5/lib/smp_processor_id.c linux-5.4.5-new/lib/smp_processor_id.c
--- linux-5.4.5/lib/smp_processor_id.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/smp_processor_id.c	2020-06-15 16:12:26.955711482 +0300
@@ -23,7 +23,12 @@
 	 * Kernel threads bound to a single CPU can safely use
 	 * smp_processor_id():
 	 */
-	if (cpumask_equal(current->cpus_ptr, cpumask_of(this_cpu)))
+#if defined(CONFIG_PREEMPT_RT) && (defined(CONFIG_SMP) || defined(CONFIG_SCHED_DEBUG))
+	if (current->migrate_disable)
+		goto out;
+#endif
+
+	if (current->nr_cpus_allowed == 1)
 		goto out;
 
 	/*
diff -Nur linux-5.4.5/lib/ubsan.c linux-5.4.5-new/lib/ubsan.c
--- linux-5.4.5/lib/ubsan.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/lib/ubsan.c	2020-06-15 16:12:26.955711482 +0300
@@ -140,25 +140,21 @@
 	}
 }
 
-static DEFINE_SPINLOCK(report_lock);
-
-static void ubsan_prologue(struct source_location *location,
-			unsigned long *flags)
+static void ubsan_prologue(struct source_location *location)
 {
 	current->in_ubsan++;
-	spin_lock_irqsave(&report_lock, *flags);
 
 	pr_err("========================================"
 		"========================================\n");
 	print_source_location("UBSAN: Undefined behaviour in", location);
 }
 
-static void ubsan_epilogue(unsigned long *flags)
+static void ubsan_epilogue(void)
 {
 	dump_stack();
 	pr_err("========================================"
 		"========================================\n");
-	spin_unlock_irqrestore(&report_lock, *flags);
+
 	current->in_ubsan--;
 }
 
@@ -167,14 +163,13 @@
 {
 
 	struct type_descriptor *type = data->type;
-	unsigned long flags;
 	char lhs_val_str[VALUE_LENGTH];
 	char rhs_val_str[VALUE_LENGTH];
 
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(lhs_val_str, sizeof(lhs_val_str), type, lhs);
 	val_to_string(rhs_val_str, sizeof(rhs_val_str), type, rhs);
@@ -186,7 +181,7 @@
 		rhs_val_str,
 		type->type_name);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 
 void __ubsan_handle_add_overflow(struct overflow_data *data,
@@ -214,20 +209,19 @@
 void __ubsan_handle_negate_overflow(struct overflow_data *data,
 				void *old_val)
 {
-	unsigned long flags;
 	char old_val_str[VALUE_LENGTH];
 
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(old_val_str, sizeof(old_val_str), data->type, old_val);
 
 	pr_err("negation of %s cannot be represented in type %s:\n",
 		old_val_str, data->type->type_name);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 EXPORT_SYMBOL(__ubsan_handle_negate_overflow);
 
@@ -235,13 +229,12 @@
 void __ubsan_handle_divrem_overflow(struct overflow_data *data,
 				void *lhs, void *rhs)
 {
-	unsigned long flags;
 	char rhs_val_str[VALUE_LENGTH];
 
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(rhs_val_str, sizeof(rhs_val_str), data->type, rhs);
 
@@ -251,58 +244,52 @@
 	else
 		pr_err("division by zero\n");
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 EXPORT_SYMBOL(__ubsan_handle_divrem_overflow);
 
 static void handle_null_ptr_deref(struct type_mismatch_data_common *data)
 {
-	unsigned long flags;
-
 	if (suppress_report(data->location))
 		return;
 
-	ubsan_prologue(data->location, &flags);
+	ubsan_prologue(data->location);
 
 	pr_err("%s null pointer of type %s\n",
 		type_check_kinds[data->type_check_kind],
 		data->type->type_name);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 
 static void handle_misaligned_access(struct type_mismatch_data_common *data,
 				unsigned long ptr)
 {
-	unsigned long flags;
-
 	if (suppress_report(data->location))
 		return;
 
-	ubsan_prologue(data->location, &flags);
+	ubsan_prologue(data->location);
 
 	pr_err("%s misaligned address %p for type %s\n",
 		type_check_kinds[data->type_check_kind],
 		(void *)ptr, data->type->type_name);
 	pr_err("which requires %ld byte alignment\n", data->alignment);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 
 static void handle_object_size_mismatch(struct type_mismatch_data_common *data,
 					unsigned long ptr)
 {
-	unsigned long flags;
-
 	if (suppress_report(data->location))
 		return;
 
-	ubsan_prologue(data->location, &flags);
+	ubsan_prologue(data->location);
 	pr_err("%s address %p with insufficient space\n",
 		type_check_kinds[data->type_check_kind],
 		(void *) ptr);
 	pr_err("for an object of type %s\n", data->type->type_name);
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 
 static void ubsan_type_mismatch_common(struct type_mismatch_data_common *data,
@@ -351,25 +338,23 @@
 
 void __ubsan_handle_out_of_bounds(struct out_of_bounds_data *data, void *index)
 {
-	unsigned long flags;
 	char index_str[VALUE_LENGTH];
 
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(index_str, sizeof(index_str), data->index_type, index);
 	pr_err("index %s is out of range for type %s\n", index_str,
 		data->array_type->type_name);
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 EXPORT_SYMBOL(__ubsan_handle_out_of_bounds);
 
 void __ubsan_handle_shift_out_of_bounds(struct shift_out_of_bounds_data *data,
 					void *lhs, void *rhs)
 {
-	unsigned long flags;
 	struct type_descriptor *rhs_type = data->rhs_type;
 	struct type_descriptor *lhs_type = data->lhs_type;
 	char rhs_str[VALUE_LENGTH];
@@ -378,7 +363,7 @@
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(rhs_str, sizeof(rhs_str), rhs_type, rhs);
 	val_to_string(lhs_str, sizeof(lhs_str), lhs_type, lhs);
@@ -401,18 +386,16 @@
 			lhs_str, rhs_str,
 			lhs_type->type_name);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 EXPORT_SYMBOL(__ubsan_handle_shift_out_of_bounds);
 
 
 void __ubsan_handle_builtin_unreachable(struct unreachable_data *data)
 {
-	unsigned long flags;
-
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 	pr_err("calling __builtin_unreachable()\n");
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 	panic("can't return from __builtin_unreachable()");
 }
 EXPORT_SYMBOL(__ubsan_handle_builtin_unreachable);
@@ -420,19 +403,18 @@
 void __ubsan_handle_load_invalid_value(struct invalid_value_data *data,
 				void *val)
 {
-	unsigned long flags;
 	char val_str[VALUE_LENGTH];
 
 	if (suppress_report(&data->location))
 		return;
 
-	ubsan_prologue(&data->location, &flags);
+	ubsan_prologue(&data->location);
 
 	val_to_string(val_str, sizeof(val_str), data->type, val);
 
 	pr_err("load of value %s is not a valid value for type %s\n",
 		val_str, data->type->type_name);
 
-	ubsan_epilogue(&flags);
+	ubsan_epilogue();
 }
 EXPORT_SYMBOL(__ubsan_handle_load_invalid_value);
diff -Nur linux-5.4.5/localversion-rt linux-5.4.5-new/localversion-rt
--- linux-5.4.5/localversion-rt	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/localversion-rt	2020-06-15 16:12:03.499793846 +0300
@@ -0,0 +1 @@
+-rt3
diff -Nur linux-5.4.5/mm/compaction.c linux-5.4.5-new/mm/compaction.c
--- linux-5.4.5/mm/compaction.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/compaction.c	2020-06-15 16:12:27.263710398 +0300
@@ -1590,7 +1590,11 @@
  * Allow userspace to control policy on scanning the unevictable LRU for
  * compactable pages.
  */
+#ifdef CONFIG_PREEMPT_RT
+#define sysctl_compact_unevictable_allowed 0
+#else
 int sysctl_compact_unevictable_allowed __read_mostly = 1;
+#endif
 
 static inline void
 update_fast_start_pfn(struct compact_control *cc, unsigned long pfn)
@@ -2240,10 +2244,16 @@
 				block_start_pfn(cc->migrate_pfn, cc->order);
 
 			if (last_migrated_pfn < current_block_start) {
-				cpu = get_cpu();
-				lru_add_drain_cpu(cpu);
-				drain_local_pages(cc->zone);
-				put_cpu();
+				if (static_branch_likely(&use_pvec_lock)) {
+					cpu = raw_smp_processor_id();
+					lru_add_drain_cpu(cpu);
+					drain_cpu_pages(cpu, cc->zone);
+				} else {
+					cpu = get_cpu();
+					lru_add_drain_cpu(cpu);
+					drain_local_pages(cc->zone);
+					put_cpu();
+				}
 				/* No more flushing until we migrate again */
 				last_migrated_pfn = 0;
 			}
diff -Nur linux-5.4.5/mm/highmem.c linux-5.4.5-new/mm/highmem.c
--- linux-5.4.5/mm/highmem.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/highmem.c	2020-06-15 16:12:27.263710398 +0300
@@ -30,9 +30,11 @@
 #include <linux/kgdb.h>
 #include <asm/tlbflush.h>
 
-
+#ifndef CONFIG_PREEMPT_RT
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 DEFINE_PER_CPU(int, __kmap_atomic_idx);
+EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
+#endif
 #endif
 
 /*
@@ -108,8 +110,6 @@
 atomic_long_t _totalhigh_pages __read_mostly;
 EXPORT_SYMBOL(_totalhigh_pages);
 
-EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
-
 unsigned int nr_free_highpages (void)
 {
 	struct zone *zone;
diff -Nur linux-5.4.5/mm/internal.h linux-5.4.5-new/mm/internal.h
--- linux-5.4.5/mm/internal.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/internal.h	2020-06-15 16:12:27.263710398 +0300
@@ -32,6 +32,12 @@
 /* Do not use these with a slab allocator */
 #define GFP_SLAB_BUG_MASK (__GFP_DMA32|__GFP_HIGHMEM|~__GFP_BITS_MASK)
 
+#ifdef CONFIG_PREEMPT_RT
+extern struct static_key_true use_pvec_lock;
+#else
+extern struct static_key_false use_pvec_lock;
+#endif
+
 void page_writeback_init(void);
 
 vm_fault_t do_swap_page(struct vm_fault *vmf);
diff -Nur linux-5.4.5/mm/Kconfig linux-5.4.5-new/mm/Kconfig
--- linux-5.4.5/mm/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/Kconfig	2020-06-15 16:12:27.259710412 +0300
@@ -369,7 +369,7 @@
 
 config TRANSPARENT_HUGEPAGE
 	bool "Transparent Hugepage Support"
-	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE
+	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE && !PREEMPT_RT
 	select COMPACTION
 	select XARRAY_MULTI
 	help
diff -Nur linux-5.4.5/mm/kmemleak.c linux-5.4.5-new/mm/kmemleak.c
--- linux-5.4.5/mm/kmemleak.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/kmemleak.c	2020-06-15 16:12:27.263710398 +0300
@@ -13,7 +13,7 @@
  *
  * The following locks and mutexes are used by kmemleak:
  *
- * - kmemleak_lock (rwlock): protects the object_list modifications and
+ * - kmemleak_lock (raw_spinlock_t): protects the object_list modifications and
  *   accesses to the object_tree_root. The object_list is the main list
  *   holding the metadata (struct kmemleak_object) for the allocated memory
  *   blocks. The object_tree_root is a red black tree used to look-up
@@ -22,13 +22,13 @@
  *   object_tree_root in the create_object() function called from the
  *   kmemleak_alloc() callback and removed in delete_object() called from the
  *   kmemleak_free() callback
- * - kmemleak_object.lock (spinlock): protects a kmemleak_object. Accesses to
- *   the metadata (e.g. count) are protected by this lock. Note that some
- *   members of this structure may be protected by other means (atomic or
- *   kmemleak_lock). This lock is also held when scanning the corresponding
- *   memory block to avoid the kernel freeing it via the kmemleak_free()
- *   callback. This is less heavyweight than holding a global lock like
- *   kmemleak_lock during scanning
+ * - kmemleak_object.lock (raw_spinlock_t): protects a kmemleak_object.
+ *   Accesses to the metadata (e.g. count) are protected by this lock. Note
+ *   that some members of this structure may be protected by other means
+ *   (atomic or kmemleak_lock). This lock is also held when scanning the
+ *   corresponding memory block to avoid the kernel freeing it via the
+ *   kmemleak_free() callback. This is less heavyweight than holding a global
+ *   lock like kmemleak_lock during scanning.
  * - scan_mutex (mutex): ensures that only one thread may scan the memory for
  *   unreferenced objects at a time. The gray_list contains the objects which
  *   are already referenced or marked as false positives and need to be
@@ -135,7 +135,7 @@
  * (use_count) and freed using the RCU mechanism.
  */
 struct kmemleak_object {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned int flags;		/* object status flags */
 	struct list_head object_list;
 	struct list_head gray_list;
@@ -191,8 +191,8 @@
 static LIST_HEAD(mem_pool_free_list);
 /* search tree for object boundaries */
 static struct rb_root object_tree_root = RB_ROOT;
-/* rw_lock protecting the access to object_list and object_tree_root */
-static DEFINE_RWLOCK(kmemleak_lock);
+/* protecting the access to object_list and object_tree_root */
+static DEFINE_RAW_SPINLOCK(kmemleak_lock);
 
 /* allocation caches for kmemleak internal data */
 static struct kmem_cache *object_cache;
@@ -426,7 +426,7 @@
 	}
 
 	/* slab allocation failed, try the memory pool */
-	write_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 	object = list_first_entry_or_null(&mem_pool_free_list,
 					  typeof(*object), object_list);
 	if (object)
@@ -435,7 +435,7 @@
 		object = &mem_pool[--mem_pool_free_count];
 	else
 		pr_warn_once("Memory pool empty, consider increasing CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE\n");
-	write_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 
 	return object;
 }
@@ -453,9 +453,9 @@
 	}
 
 	/* add the object to the memory pool free list */
-	write_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 	list_add(&object->object_list, &mem_pool_free_list);
-	write_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 }
 
 /*
@@ -514,9 +514,9 @@
 	struct kmemleak_object *object;
 
 	rcu_read_lock();
-	read_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 	object = lookup_object(ptr, alias);
-	read_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 
 	/* check whether the object is still available */
 	if (object && !get_object(object))
@@ -546,11 +546,11 @@
 	unsigned long flags;
 	struct kmemleak_object *object;
 
-	write_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 	object = lookup_object(ptr, alias);
 	if (object)
 		__remove_object(object);
-	write_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 
 	return object;
 }
@@ -585,7 +585,7 @@
 	INIT_LIST_HEAD(&object->object_list);
 	INIT_LIST_HEAD(&object->gray_list);
 	INIT_HLIST_HEAD(&object->area_list);
-	spin_lock_init(&object->lock);
+	raw_spin_lock_init(&object->lock);
 	atomic_set(&object->use_count, 1);
 	object->flags = OBJECT_ALLOCATED;
 	object->pointer = ptr;
@@ -617,7 +617,7 @@
 	/* kernel backtrace */
 	object->trace_len = __save_stack_trace(object->trace);
 
-	write_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 
 	untagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);
 	min_addr = min(min_addr, untagged_ptr);
@@ -649,7 +649,7 @@
 
 	list_add_tail_rcu(&object->object_list, &object_list);
 out:
-	write_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 	return object;
 }
 
@@ -667,9 +667,9 @@
 	 * Locking here also ensures that the corresponding memory block
 	 * cannot be freed when it is being scanned.
 	 */
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	object->flags &= ~OBJECT_ALLOCATED;
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 	put_object(object);
 }
 
@@ -739,9 +739,9 @@
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	__paint_it(object, color);
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 }
 
 static void paint_ptr(unsigned long ptr, int color)
@@ -798,7 +798,7 @@
 	if (scan_area_cache)
 		area = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	if (!area) {
 		pr_warn_once("Cannot allocate a scan area, scanning the full object\n");
 		/* mark the object for full scan to avoid false positives */
@@ -820,7 +820,7 @@
 
 	hlist_add_head(&area->node, &object->area_list);
 out_unlock:
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 	put_object(object);
 }
 
@@ -842,9 +842,9 @@
 		return;
 	}
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	object->excess_ref = excess_ref;
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 	put_object(object);
 }
 
@@ -864,9 +864,9 @@
 		return;
 	}
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	object->flags |= OBJECT_NO_SCAN;
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 	put_object(object);
 }
 
@@ -1026,9 +1026,9 @@
 		return;
 	}
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	object->trace_len = __save_stack_trace(object->trace);
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 
 	put_object(object);
 }
@@ -1233,7 +1233,7 @@
 	unsigned long flags;
 	unsigned long untagged_ptr;
 
-	read_lock_irqsave(&kmemleak_lock, flags);
+	raw_spin_lock_irqsave(&kmemleak_lock, flags);
 	for (ptr = start; ptr < end; ptr++) {
 		struct kmemleak_object *object;
 		unsigned long pointer;
@@ -1268,7 +1268,7 @@
 		 * previously acquired in scan_object(). These locks are
 		 * enclosed by scan_mutex.
 		 */
-		spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
+		raw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
 		/* only pass surplus references (object already gray) */
 		if (color_gray(object)) {
 			excess_ref = object->excess_ref;
@@ -1277,7 +1277,7 @@
 			excess_ref = 0;
 			update_refs(object);
 		}
-		spin_unlock(&object->lock);
+		raw_spin_unlock(&object->lock);
 
 		if (excess_ref) {
 			object = lookup_object(excess_ref, 0);
@@ -1286,12 +1286,12 @@
 			if (object == scanned)
 				/* circular reference, ignore */
 				continue;
-			spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
+			raw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
 			update_refs(object);
-			spin_unlock(&object->lock);
+			raw_spin_unlock(&object->lock);
 		}
 	}
-	read_unlock_irqrestore(&kmemleak_lock, flags);
+	raw_spin_unlock_irqrestore(&kmemleak_lock, flags);
 }
 
 /*
@@ -1324,7 +1324,7 @@
 	 * Once the object->lock is acquired, the corresponding memory block
 	 * cannot be freed (the same lock is acquired in delete_object).
 	 */
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	if (object->flags & OBJECT_NO_SCAN)
 		goto out;
 	if (!(object->flags & OBJECT_ALLOCATED))
@@ -1344,9 +1344,9 @@
 			if (start >= end)
 				break;
 
-			spin_unlock_irqrestore(&object->lock, flags);
+			raw_spin_unlock_irqrestore(&object->lock, flags);
 			cond_resched();
-			spin_lock_irqsave(&object->lock, flags);
+			raw_spin_lock_irqsave(&object->lock, flags);
 		} while (object->flags & OBJECT_ALLOCATED);
 	} else
 		hlist_for_each_entry(area, &object->area_list, node)
@@ -1354,7 +1354,7 @@
 				   (void *)(area->start + area->size),
 				   object);
 out:
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 }
 
 /*
@@ -1407,7 +1407,7 @@
 	/* prepare the kmemleak_object's */
 	rcu_read_lock();
 	list_for_each_entry_rcu(object, &object_list, object_list) {
-		spin_lock_irqsave(&object->lock, flags);
+		raw_spin_lock_irqsave(&object->lock, flags);
 #ifdef DEBUG
 		/*
 		 * With a few exceptions there should be a maximum of
@@ -1424,7 +1424,7 @@
 		if (color_gray(object) && get_object(object))
 			list_add_tail(&object->gray_list, &gray_list);
 
-		spin_unlock_irqrestore(&object->lock, flags);
+		raw_spin_unlock_irqrestore(&object->lock, flags);
 	}
 	rcu_read_unlock();
 
@@ -1492,14 +1492,14 @@
 	 */
 	rcu_read_lock();
 	list_for_each_entry_rcu(object, &object_list, object_list) {
-		spin_lock_irqsave(&object->lock, flags);
+		raw_spin_lock_irqsave(&object->lock, flags);
 		if (color_white(object) && (object->flags & OBJECT_ALLOCATED)
 		    && update_checksum(object) && get_object(object)) {
 			/* color it gray temporarily */
 			object->count = object->min_count;
 			list_add_tail(&object->gray_list, &gray_list);
 		}
-		spin_unlock_irqrestore(&object->lock, flags);
+		raw_spin_unlock_irqrestore(&object->lock, flags);
 	}
 	rcu_read_unlock();
 
@@ -1519,7 +1519,7 @@
 	 */
 	rcu_read_lock();
 	list_for_each_entry_rcu(object, &object_list, object_list) {
-		spin_lock_irqsave(&object->lock, flags);
+		raw_spin_lock_irqsave(&object->lock, flags);
 		if (unreferenced_object(object) &&
 		    !(object->flags & OBJECT_REPORTED)) {
 			object->flags |= OBJECT_REPORTED;
@@ -1529,7 +1529,7 @@
 
 			new_leaks++;
 		}
-		spin_unlock_irqrestore(&object->lock, flags);
+		raw_spin_unlock_irqrestore(&object->lock, flags);
 	}
 	rcu_read_unlock();
 
@@ -1681,10 +1681,10 @@
 	struct kmemleak_object *object = v;
 	unsigned long flags;
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	if ((object->flags & OBJECT_REPORTED) && unreferenced_object(object))
 		print_unreferenced(seq, object);
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 	return 0;
 }
 
@@ -1714,9 +1714,9 @@
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&object->lock, flags);
+	raw_spin_lock_irqsave(&object->lock, flags);
 	dump_object_info(object);
-	spin_unlock_irqrestore(&object->lock, flags);
+	raw_spin_unlock_irqrestore(&object->lock, flags);
 
 	put_object(object);
 	return 0;
@@ -1735,11 +1735,11 @@
 
 	rcu_read_lock();
 	list_for_each_entry_rcu(object, &object_list, object_list) {
-		spin_lock_irqsave(&object->lock, flags);
+		raw_spin_lock_irqsave(&object->lock, flags);
 		if ((object->flags & OBJECT_REPORTED) &&
 		    unreferenced_object(object))
 			__paint_it(object, KMEMLEAK_GREY);
-		spin_unlock_irqrestore(&object->lock, flags);
+		raw_spin_unlock_irqrestore(&object->lock, flags);
 	}
 	rcu_read_unlock();
 
diff -Nur linux-5.4.5/mm/memcontrol.c linux-5.4.5-new/mm/memcontrol.c
--- linux-5.4.5/mm/memcontrol.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/memcontrol.c	2020-06-15 16:12:27.263710398 +0300
@@ -63,6 +63,7 @@
 #include <net/sock.h>
 #include <net/ip.h>
 #include "slab.h"
+#include <linux/locallock.h>
 
 #include <linux/uaccess.h>
 
@@ -92,6 +93,8 @@
 static DECLARE_WAIT_QUEUE_HEAD(memcg_cgwb_frn_waitq);
 #endif
 
+static DEFINE_LOCAL_IRQ_LOCK(event_lock);
+
 /* Whether legacy memory+swap accounting is active */
 static bool do_memsw_account(void)
 {
@@ -2265,7 +2268,7 @@
 	 * as well as workers from this path always operate on the local
 	 * per-cpu data. CPU up doesn't touch memcg_stock at all.
 	 */
-	curcpu = get_cpu();
+	curcpu = get_cpu_light();
 	for_each_online_cpu(cpu) {
 		struct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);
 		struct mem_cgroup *memcg;
@@ -2286,7 +2289,7 @@
 				schedule_work_on(cpu, &stock->work);
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 	mutex_unlock(&percpu_charge_mutex);
 }
 
@@ -5515,12 +5518,12 @@
 
 	ret = 0;
 
-	local_irq_disable();
+	local_lock_irq(event_lock);
 	mem_cgroup_charge_statistics(to, page, compound, nr_pages);
 	memcg_check_events(to, page);
 	mem_cgroup_charge_statistics(from, page, compound, -nr_pages);
 	memcg_check_events(from, page);
-	local_irq_enable();
+	local_unlock_irq(event_lock);
 out_unlock:
 	unlock_page(page);
 out:
@@ -6568,10 +6571,10 @@
 
 	commit_charge(page, memcg, lrucare);
 
-	local_irq_disable();
+	local_lock_irq(event_lock);
 	mem_cgroup_charge_statistics(memcg, page, compound, nr_pages);
 	memcg_check_events(memcg, page);
-	local_irq_enable();
+	local_unlock_irq(event_lock);
 
 	if (do_memsw_account() && PageSwapCache(page)) {
 		swp_entry_t entry = { .val = page_private(page) };
@@ -6640,7 +6643,7 @@
 		memcg_oom_recover(ug->memcg);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(event_lock, flags);
 	__mod_memcg_state(ug->memcg, MEMCG_RSS, -ug->nr_anon);
 	__mod_memcg_state(ug->memcg, MEMCG_CACHE, -ug->nr_file);
 	__mod_memcg_state(ug->memcg, MEMCG_RSS_HUGE, -ug->nr_huge);
@@ -6648,7 +6651,7 @@
 	__count_memcg_events(ug->memcg, PGPGOUT, ug->pgpgout);
 	__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, nr_pages);
 	memcg_check_events(ug->memcg, ug->dummy_page);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(event_lock, flags);
 
 	if (!mem_cgroup_is_root(ug->memcg))
 		css_put_many(&ug->memcg->css, nr_pages);
@@ -6811,10 +6814,10 @@
 
 	commit_charge(newpage, memcg, false);
 
-	local_irq_save(flags);
+	local_lock_irqsave(event_lock, flags);
 	mem_cgroup_charge_statistics(memcg, newpage, compound, nr_pages);
 	memcg_check_events(memcg, newpage);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(event_lock, flags);
 }
 
 DEFINE_STATIC_KEY_FALSE(memcg_sockets_enabled_key);
@@ -7006,6 +7009,7 @@
 	struct mem_cgroup *memcg, *swap_memcg;
 	unsigned int nr_entries;
 	unsigned short oldid;
+	unsigned long flags;
 
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 	VM_BUG_ON_PAGE(page_count(page), page);
@@ -7051,13 +7055,17 @@
 	 * important here to have the interrupts disabled because it is the
 	 * only synchronisation we have for updating the per-CPU variables.
 	 */
+	local_lock_irqsave(event_lock, flags);
+#ifndef CONFIG_PREEMPT_RT
 	VM_BUG_ON(!irqs_disabled());
+#endif
 	mem_cgroup_charge_statistics(memcg, page, PageTransHuge(page),
 				     -nr_entries);
 	memcg_check_events(memcg, page);
 
 	if (!mem_cgroup_is_root(memcg))
 		css_put_many(&memcg->css, nr_entries);
+	local_unlock_irqrestore(event_lock, flags);
 }
 
 /**
diff -Nur linux-5.4.5/mm/memory.c linux-5.4.5-new/mm/memory.c
--- linux-5.4.5/mm/memory.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/memory.c	2020-06-15 16:12:27.267710384 +0300
@@ -2133,7 +2133,7 @@
 				pte_t *page_table, pte_t orig_pte)
 {
 	int same = 1;
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPTION)
 	if (sizeof(pte_t) > sizeof(unsigned long)) {
 		spinlock_t *ptl = pte_lockptr(mm, pmd);
 		spin_lock(ptl);
diff -Nur linux-5.4.5/mm/page_alloc.c linux-5.4.5-new/mm/page_alloc.c
--- linux-5.4.5/mm/page_alloc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/page_alloc.c	2020-06-15 16:12:27.291710299 +0300
@@ -61,6 +61,7 @@
 #include <linux/hugetlb.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/mm.h>
+#include <linux/locallock.h>
 #include <linux/page_owner.h>
 #include <linux/kthread.h>
 #include <linux/memcontrol.h>
@@ -357,6 +358,18 @@
 EXPORT_SYMBOL(nr_online_nodes);
 #endif
 
+static DEFINE_LOCAL_IRQ_LOCK(pa_lock);
+
+#ifdef CONFIG_PREEMPT_RT
+# define cpu_lock_irqsave(cpu, flags)		\
+	local_lock_irqsave_on(pa_lock, flags, cpu)
+# define cpu_unlock_irqrestore(cpu, flags)	\
+	local_unlock_irqrestore_on(pa_lock, flags, cpu)
+#else
+# define cpu_lock_irqsave(cpu, flags)		local_irq_save(flags)
+# define cpu_unlock_irqrestore(cpu, flags)	local_irq_restore(flags)
+#endif
+
 int page_group_by_mobility_disabled __read_mostly;
 
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
@@ -1243,7 +1256,7 @@
 }
 
 /*
- * Frees a number of pages from the PCP lists
+ * Frees a number of pages which have been collected from the pcp lists.
  * Assumes all pages on list are in same zone, and of same order.
  * count is the number of pages to free.
  *
@@ -1253,15 +1266,57 @@
  * And clear the zone's pages_scanned counter, to hold off the "all pages are
  * pinned" detection logic.
  */
-static void free_pcppages_bulk(struct zone *zone, int count,
-					struct per_cpu_pages *pcp)
+static void free_pcppages_bulk(struct zone *zone, struct list_head *head,
+			       bool zone_retry)
+{
+	bool isolated_pageblocks;
+	struct page *page, *tmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&zone->lock, flags);
+	isolated_pageblocks = has_isolate_pageblock(zone);
+
+	/*
+	 * Use safe version since after __free_one_page(),
+	 * page->lru.next will not point to original list.
+	 */
+	list_for_each_entry_safe(page, tmp, head, lru) {
+		int mt = get_pcppage_migratetype(page);
+
+		if (page_zone(page) != zone) {
+			/*
+			 * free_unref_page_list() sorts pages by zone. If we end
+			 * up with pages from a different NUMA nodes belonging
+			 * to the same ZONE index then we need to redo with the
+			 * correct ZONE pointer. Skip the page for now, redo it
+			 * on the next iteration.
+			 */
+			WARN_ON_ONCE(zone_retry == false);
+			if (zone_retry)
+				continue;
+		}
+
+		/* MIGRATE_ISOLATE page should not go to pcplists */
+		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
+		/* Pageblock could have been isolated meanwhile */
+		if (unlikely(isolated_pageblocks))
+			mt = get_pageblock_migratetype(page);
+
+		list_del(&page->lru);
+		__free_one_page(page, page_to_pfn(page), zone, 0, mt);
+		trace_mm_page_pcpu_drain(page, 0, mt);
+	}
+	spin_unlock_irqrestore(&zone->lock, flags);
+}
+
+static void isolate_pcp_pages(int count, struct per_cpu_pages *pcp,
+			      struct list_head *dst)
+
 {
 	int migratetype = 0;
 	int batch_free = 0;
 	int prefetch_nr = 0;
-	bool isolated_pageblocks;
-	struct page *page, *tmp;
-	LIST_HEAD(head);
+	struct page *page;
 
 	while (count) {
 		struct list_head *list;
@@ -1293,7 +1348,7 @@
 			if (bulkfree_pcp_prepare(page))
 				continue;
 
-			list_add_tail(&page->lru, &head);
+			list_add_tail(&page->lru, dst);
 
 			/*
 			 * We are going to put the page back to the global
@@ -1308,26 +1363,6 @@
 				prefetch_buddy(page);
 		} while (--count && --batch_free && !list_empty(list));
 	}
-
-	spin_lock(&zone->lock);
-	isolated_pageblocks = has_isolate_pageblock(zone);
-
-	/*
-	 * Use safe version since after __free_one_page(),
-	 * page->lru.next will not point to original list.
-	 */
-	list_for_each_entry_safe(page, tmp, &head, lru) {
-		int mt = get_pcppage_migratetype(page);
-		/* MIGRATE_ISOLATE page should not go to pcplists */
-		VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
-		/* Pageblock could have been isolated meanwhile */
-		if (unlikely(isolated_pageblocks))
-			mt = get_pageblock_migratetype(page);
-
-		__free_one_page(page, page_to_pfn(page), zone, 0, mt);
-		trace_mm_page_pcpu_drain(page, 0, mt);
-	}
-	spin_unlock(&zone->lock);
 }
 
 static void free_one_page(struct zone *zone,
@@ -1428,10 +1463,10 @@
 		return;
 
 	migratetype = get_pfnblock_migratetype(page, pfn);
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	__count_vm_events(PGFREE, 1 << order);
 	free_one_page(page_zone(page), page, pfn, order, migratetype);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 }
 
 void __free_pages_core(struct page *page, unsigned int order)
@@ -2799,13 +2834,18 @@
 {
 	unsigned long flags;
 	int to_drain, batch;
+	LIST_HEAD(dst);
 
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	batch = READ_ONCE(pcp->batch);
 	to_drain = min(pcp->count, batch);
 	if (to_drain > 0)
-		free_pcppages_bulk(zone, to_drain, pcp);
-	local_irq_restore(flags);
+		isolate_pcp_pages(to_drain, pcp, &dst);
+
+	local_unlock_irqrestore(pa_lock, flags);
+
+	if (to_drain > 0)
+		free_pcppages_bulk(zone, &dst, false);
 }
 #endif
 
@@ -2821,14 +2861,21 @@
 	unsigned long flags;
 	struct per_cpu_pageset *pset;
 	struct per_cpu_pages *pcp;
+	LIST_HEAD(dst);
+	int count;
 
-	local_irq_save(flags);
+	cpu_lock_irqsave(cpu, flags);
 	pset = per_cpu_ptr(zone->pageset, cpu);
 
 	pcp = &pset->pcp;
-	if (pcp->count)
-		free_pcppages_bulk(zone, pcp->count, pcp);
-	local_irq_restore(flags);
+	count = pcp->count;
+	if (count)
+		isolate_pcp_pages(count, pcp, &dst);
+
+	cpu_unlock_irqrestore(cpu, flags);
+
+	if (count)
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -2847,6 +2894,14 @@
 	}
 }
 
+void drain_cpu_pages(unsigned int cpu, struct zone *zone)
+{
+	if (zone)
+		drain_pages_zone(cpu, zone);
+	else
+		drain_pages(cpu);
+}
+
 /*
  * Spill all of this CPU's per-cpu pages back into the buddy allocator.
  *
@@ -2857,10 +2912,7 @@
 {
 	int cpu = smp_processor_id();
 
-	if (zone)
-		drain_pages_zone(cpu, zone);
-	else
-		drain_pages(cpu);
+	drain_cpu_pages(cpu, zone);
 }
 
 static void drain_local_pages_wq(struct work_struct *work)
@@ -2947,15 +2999,20 @@
 			cpumask_clear_cpu(cpu, &cpus_with_pcps);
 	}
 
-	for_each_cpu(cpu, &cpus_with_pcps) {
-		struct pcpu_drain *drain = per_cpu_ptr(&pcpu_drain, cpu);
+	if (static_branch_likely(&use_pvec_lock)) {
+		for_each_cpu(cpu, &cpus_with_pcps)
+			drain_cpu_pages(cpu, zone);
+	} else {
+		for_each_cpu(cpu, &cpus_with_pcps) {
+			struct pcpu_drain *drain = per_cpu_ptr(&pcpu_drain, cpu);
 
-		drain->zone = zone;
-		INIT_WORK(&drain->work, drain_local_pages_wq);
-		queue_work_on(cpu, mm_percpu_wq, &drain->work);
+			drain->zone = zone;
+			INIT_WORK(&drain->work, drain_local_pages_wq);
+			queue_work_on(cpu, mm_percpu_wq, &drain->work);
+		}
+		for_each_cpu(cpu, &cpus_with_pcps)
+			flush_work(&per_cpu_ptr(&pcpu_drain, cpu)->work);
 	}
-	for_each_cpu(cpu, &cpus_with_pcps)
-		flush_work(&per_cpu_ptr(&pcpu_drain, cpu)->work);
 
 	mutex_unlock(&pcpu_drain_mutex);
 }
@@ -3027,7 +3084,8 @@
 	return true;
 }
 
-static void free_unref_page_commit(struct page *page, unsigned long pfn)
+static void free_unref_page_commit(struct page *page, unsigned long pfn,
+				   struct list_head *dst)
 {
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
@@ -3056,7 +3114,8 @@
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = READ_ONCE(pcp->batch);
-		free_pcppages_bulk(zone, batch, pcp);
+
+		isolate_pcp_pages(batch, pcp, dst);
 	}
 }
 
@@ -3067,13 +3126,17 @@
 {
 	unsigned long flags;
 	unsigned long pfn = page_to_pfn(page);
+	struct zone *zone = page_zone(page);
+	LIST_HEAD(dst);
 
 	if (!free_unref_page_prepare(page, pfn))
 		return;
 
-	local_irq_save(flags);
-	free_unref_page_commit(page, pfn);
-	local_irq_restore(flags);
+	local_lock_irqsave(pa_lock, flags);
+	free_unref_page_commit(page, pfn, &dst);
+	local_unlock_irqrestore(pa_lock, flags);
+	if (!list_empty(&dst))
+		free_pcppages_bulk(zone, &dst, false);
 }
 
 /*
@@ -3084,6 +3147,11 @@
 	struct page *page, *next;
 	unsigned long flags, pfn;
 	int batch_count = 0;
+	struct list_head dsts[__MAX_NR_ZONES];
+	int i;
+
+	for (i = 0; i < __MAX_NR_ZONES; i++)
+		INIT_LIST_HEAD(&dsts[i]);
 
 	/* Prepare pages for freeing */
 	list_for_each_entry_safe(page, next, list, lru) {
@@ -3093,25 +3161,42 @@
 		set_page_private(page, pfn);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	list_for_each_entry_safe(page, next, list, lru) {
 		unsigned long pfn = page_private(page);
+		enum zone_type type;
 
 		set_page_private(page, 0);
 		trace_mm_page_free_batched(page);
-		free_unref_page_commit(page, pfn);
+		type = page_zonenum(page);
+		free_unref_page_commit(page, pfn, &dsts[type]);
 
 		/*
 		 * Guard against excessive IRQ disabled times when we get
 		 * a large list of pages to free.
 		 */
 		if (++batch_count == SWAP_CLUSTER_MAX) {
-			local_irq_restore(flags);
+			local_unlock_irqrestore(pa_lock, flags);
 			batch_count = 0;
-			local_irq_save(flags);
+			local_lock_irqsave(pa_lock, flags);
 		}
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
+
+	for (i = 0; i < __MAX_NR_ZONES; ) {
+		struct page *page;
+		struct zone *zone;
+
+		if (list_empty(&dsts[i])) {
+			i++;
+			continue;
+		}
+
+		page = list_first_entry(&dsts[i], struct page, lru);
+		zone = page_zone(page);
+
+		free_pcppages_bulk(zone, &dsts[i], true);
+	}
 }
 
 /*
@@ -3246,7 +3331,7 @@
 	struct page *page;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
 	list = &pcp->lists[migratetype];
 	page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
@@ -3254,7 +3339,7 @@
 		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
 		zone_statistics(preferred_zone, zone);
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 	return page;
 }
 
@@ -3281,7 +3366,7 @@
 	 * allocate greater than order-1 page units with __GFP_NOFAIL.
 	 */
 	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
-	spin_lock_irqsave(&zone->lock, flags);
+	local_spin_lock_irqsave(pa_lock, &zone->lock, flags);
 
 	do {
 		page = NULL;
@@ -3301,7 +3386,7 @@
 
 	__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
 	zone_statistics(preferred_zone, zone);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 
 out:
 	/* Separate test+clear to avoid unnecessary atomics */
@@ -3314,7 +3399,7 @@
 	return page;
 
 failed:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 	return NULL;
 }
 
@@ -8538,7 +8623,7 @@
 	struct per_cpu_pageset *pset;
 
 	/* avoid races with drain_pages()  */
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	if (zone->pageset != &boot_pageset) {
 		for_each_online_cpu(cpu) {
 			pset = per_cpu_ptr(zone->pageset, cpu);
@@ -8547,7 +8632,7 @@
 		free_percpu(zone->pageset);
 		zone->pageset = &boot_pageset;
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
diff -Nur linux-5.4.5/mm/slab.c linux-5.4.5-new/mm/slab.c
--- linux-5.4.5/mm/slab.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/slab.c	2020-06-15 16:12:27.319710201 +0300
@@ -233,7 +233,7 @@
 	parent->shared = NULL;
 	parent->alien = NULL;
 	parent->colour_next = 0;
-	spin_lock_init(&parent->list_lock);
+	raw_spin_lock_init(&parent->list_lock);
 	parent->free_objects = 0;
 	parent->free_touched = 0;
 }
@@ -558,9 +558,9 @@
 	page_node = page_to_nid(page);
 	n = get_node(cachep, page_node);
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, &objp, 1, page_node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -688,7 +688,7 @@
 	struct kmem_cache_node *n = get_node(cachep, node);
 
 	if (ac->avail) {
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		/*
 		 * Stuff objects into the remote nodes shared array first.
 		 * That way we could avoid the overhead of putting the objects
@@ -699,7 +699,7 @@
 
 		free_block(cachep, ac->entry, ac->avail, node, list);
 		ac->avail = 0;
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 	}
 }
 
@@ -772,9 +772,9 @@
 		slabs_destroy(cachep, &list);
 	} else {
 		n = get_node(cachep, page_node);
-		spin_lock(&n->list_lock);
+		raw_spin_lock(&n->list_lock);
 		free_block(cachep, &objp, 1, page_node, &list);
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	return 1;
@@ -815,10 +815,10 @@
 	 */
 	n = get_node(cachep, node);
 	if (n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		n->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +
 				cachep->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		return 0;
 	}
@@ -897,7 +897,7 @@
 		goto fail;
 
 	n = get_node(cachep, node);
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	if (n->shared && force_change) {
 		free_block(cachep, n->shared->entry,
 				n->shared->avail, node, &list);
@@ -915,7 +915,7 @@
 		new_alien = NULL;
 	}
 
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 	slabs_destroy(cachep, &list);
 
 	/*
@@ -954,7 +954,7 @@
 		if (!n)
 			continue;
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		/* Free limit for this kmem_cache_node */
 		n->free_limit -= cachep->batchcount;
@@ -965,7 +965,7 @@
 		nc->avail = 0;
 
 		if (!cpumask_empty(mask)) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto free_slab;
 		}
 
@@ -979,7 +979,7 @@
 		alien = n->alien;
 		n->alien = NULL;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		kfree(shared);
 		if (alien) {
@@ -1163,7 +1163,7 @@
 	/*
 	 * Do not assume that spinlocks can be initialized via memcpy:
 	 */
-	spin_lock_init(&ptr->list_lock);
+	raw_spin_lock_init(&ptr->list_lock);
 
 	MAKE_ALL_LISTS(cachep, ptr, nodeid);
 	cachep->node[nodeid] = ptr;
@@ -1334,11 +1334,11 @@
 	for_each_kmem_cache_node(cachep, node, n) {
 		unsigned long total_slabs, free_slabs, free_objs;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		total_slabs = n->total_slabs;
 		free_slabs = n->free_slabs;
 		free_objs = n->free_objects;
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		pr_warn("  node %d: slabs: %ld/%ld, objs: %ld/%ld\n",
 			node, total_slabs - free_slabs, total_slabs,
@@ -2096,7 +2096,7 @@
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
 #endif
 }
 
@@ -2104,7 +2104,7 @@
 {
 #ifdef CONFIG_SMP
 	check_irq_off();
-	assert_spin_locked(&get_node(cachep, node)->list_lock);
+	assert_raw_spin_locked(&get_node(cachep, node)->list_lock);
 #endif
 }
 
@@ -2144,9 +2144,9 @@
 	check_irq_off();
 	ac = cpu_cache_get(cachep);
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	free_block(cachep, ac->entry, ac->avail, node, &list);
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	slabs_destroy(cachep, &list);
 	ac->avail = 0;
 }
@@ -2164,9 +2164,9 @@
 			drain_alien_cache(cachep, n->alien);
 
 	for_each_kmem_cache_node(cachep, node, n) {
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		drain_array_locked(cachep, n->shared, node, true, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 
 		slabs_destroy(cachep, &list);
 	}
@@ -2188,10 +2188,10 @@
 	nr_freed = 0;
 	while (nr_freed < tofree && !list_empty(&n->slabs_free)) {
 
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		p = n->slabs_free.prev;
 		if (p == &n->slabs_free) {
-			spin_unlock_irq(&n->list_lock);
+			raw_spin_unlock_irq(&n->list_lock);
 			goto out;
 		}
 
@@ -2204,7 +2204,7 @@
 		 * to the cache.
 		 */
 		n->free_objects -= cache->num;
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slab_destroy(cache, page);
 		nr_freed++;
 	}
@@ -2657,7 +2657,7 @@
 	INIT_LIST_HEAD(&page->slab_list);
 	n = get_node(cachep, page_to_nid(page));
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	n->total_slabs++;
 	if (!page->active) {
 		list_add_tail(&page->slab_list, &n->slabs_free);
@@ -2667,7 +2667,7 @@
 
 	STATS_INC_GROWN(cachep);
 	n->free_objects += cachep->num - page->active;
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 
 	fixup_objfreelist_debug(cachep, &list);
 }
@@ -2833,7 +2833,7 @@
 {
 	struct page *page;
 
-	assert_spin_locked(&n->list_lock);
+	assert_raw_spin_locked(&n->list_lock);
 	page = list_first_entry_or_null(&n->slabs_partial, struct page,
 					slab_list);
 	if (!page) {
@@ -2860,10 +2860,10 @@
 	if (!gfp_pfmemalloc_allowed(flags))
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, true);
 	if (!page) {
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 		return NULL;
 	}
 
@@ -2872,7 +2872,7 @@
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 	return obj;
@@ -2931,7 +2931,7 @@
 	if (!n->free_objects && (!shared || !shared->avail))
 		goto direct_grow;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	shared = READ_ONCE(n->shared);
 
 	/* See if we can refill from the shared array */
@@ -2955,7 +2955,7 @@
 must_grow:
 	n->free_objects -= ac->avail;
 alloc_done:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 
 direct_grow:
@@ -3180,7 +3180,7 @@
 	BUG_ON(!n);
 
 	check_irq_off();
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	page = get_first_slab(n, false);
 	if (!page)
 		goto must_grow;
@@ -3198,12 +3198,12 @@
 
 	fixup_slab_list(cachep, n, page, &list);
 
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	fixup_objfreelist_debug(cachep, &list);
 	return obj;
 
 must_grow:
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	page = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);
 	if (page) {
 		/* This slab isn't counted yet so don't update free_objects */
@@ -3379,7 +3379,7 @@
 
 	check_irq_off();
 	n = get_node(cachep, node);
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	if (n->shared) {
 		struct array_cache *shared_array = n->shared;
 		int max = shared_array->limit - shared_array->avail;
@@ -3408,7 +3408,7 @@
 		STATS_SET_FREEABLE(cachep, i);
 	}
 #endif
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	slabs_destroy(cachep, &list);
 	ac->avail -= batchcount;
 	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
@@ -3830,9 +3830,9 @@
 
 		node = cpu_to_mem(cpu);
 		n = get_node(cachep, node);
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 		free_block(cachep, ac->entry, ac->avail, node, &list);
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 		slabs_destroy(cachep, &list);
 	}
 	free_percpu(prev);
@@ -3957,9 +3957,9 @@
 		return;
 	}
 
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	drain_array_locked(cachep, ac, node, false, &list);
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	slabs_destroy(cachep, &list);
 }
@@ -4043,7 +4043,7 @@
 
 	for_each_kmem_cache_node(cachep, node, n) {
 		check_irq_on();
-		spin_lock_irq(&n->list_lock);
+		raw_spin_lock_irq(&n->list_lock);
 
 		total_slabs += n->total_slabs;
 		free_slabs += n->free_slabs;
@@ -4052,7 +4052,7 @@
 		if (n->shared)
 			shared_avail += n->shared->avail;
 
-		spin_unlock_irq(&n->list_lock);
+		raw_spin_unlock_irq(&n->list_lock);
 	}
 	num_objs = total_slabs * cachep->num;
 	active_slabs = total_slabs - free_slabs;
diff -Nur linux-5.4.5/mm/slab.h linux-5.4.5-new/mm/slab.h
--- linux-5.4.5/mm/slab.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/slab.h	2020-06-15 16:12:27.343710115 +0300
@@ -596,7 +596,7 @@
  * The slab lists for all objects.
  */
 struct kmem_cache_node {
-	spinlock_t list_lock;
+	raw_spinlock_t list_lock;
 
 #ifdef CONFIG_SLAB
 	struct list_head slabs_partial;	/* partial list first, better asm code */
diff -Nur linux-5.4.5/mm/slub.c linux-5.4.5-new/mm/slub.c
--- linux-5.4.5/mm/slub.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/slub.c	2020-06-15 16:12:27.343710115 +0300
@@ -1176,7 +1176,7 @@
 	unsigned long uninitialized_var(flags);
 	int ret = 0;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	slab_lock(page);
 
 	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
@@ -1211,7 +1211,7 @@
 			 bulk_cnt, cnt);
 
 	slab_unlock(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	if (!ret)
 		slab_fix(s, "Object at 0x%p not freed", object);
 	return ret;
@@ -1381,6 +1381,12 @@
 
 #endif /* CONFIG_SLUB_DEBUG */
 
+struct slub_free_list {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+static DEFINE_PER_CPU(struct slub_free_list, slub_free_list);
+
 /*
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
@@ -1621,10 +1627,18 @@
 	void *start, *p, *next;
 	int idx;
 	bool shuffle;
+	bool enableirqs = false;
 
 	flags &= gfp_allowed_mask;
 
 	if (gfpflags_allow_blocking(flags))
+		enableirqs = true;
+
+#ifdef CONFIG_PREEMPT_RT
+	if (system_state > SYSTEM_BOOTING)
+		enableirqs = true;
+#endif
+	if (enableirqs)
 		local_irq_enable();
 
 	flags |= s->allocflags;
@@ -1683,7 +1697,7 @@
 	page->frozen = 1;
 
 out:
-	if (gfpflags_allow_blocking(flags))
+	if (enableirqs)
 		local_irq_disable();
 	if (!page)
 		return NULL;
@@ -1731,6 +1745,16 @@
 	__free_pages(page, order);
 }
 
+static void free_delayed(struct list_head *h)
+{
+	while (!list_empty(h)) {
+		struct page *page = list_first_entry(h, struct page, lru);
+
+		list_del(&page->lru);
+		__free_slab(page->slab_cache, page);
+	}
+}
+
 static void rcu_free_slab(struct rcu_head *h)
 {
 	struct page *page = container_of(h, struct page, rcu_head);
@@ -1742,6 +1766,12 @@
 {
 	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
 		call_rcu(&page->rcu_head, rcu_free_slab);
+	} else if (irqs_disabled()) {
+		struct slub_free_list *f = this_cpu_ptr(&slub_free_list);
+
+		raw_spin_lock(&f->lock);
+		list_add(&page->lru, &f->list);
+		raw_spin_unlock(&f->lock);
 	} else
 		__free_slab(s, page);
 }
@@ -1849,7 +1879,7 @@
 	if (!n || !n->nr_partial)
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	list_for_each_entry_safe(page, page2, &n->partial, slab_list) {
 		void *t;
 
@@ -1874,7 +1904,7 @@
 			break;
 
 	}
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	return object;
 }
 
@@ -1963,7 +1993,7 @@
 	return get_any_partial(s, flags, c);
 }
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 /*
  * Calculate the next globally unique transaction for disambiguiation
  * during cmpxchg. The transactions start with the cpu number and are then
@@ -2008,7 +2038,7 @@
 
 	pr_info("%s %s: cmpxchg redo ", n, s->name);
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	if (tid_to_cpu(tid) != tid_to_cpu(actual_tid))
 		pr_warn("due to cpu change %d -> %d\n",
 			tid_to_cpu(tid), tid_to_cpu(actual_tid));
@@ -2122,7 +2152,7 @@
 			 * that acquire_slab() will see a slab page that
 			 * is frozen
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	} else {
 		m = M_FULL;
@@ -2133,7 +2163,7 @@
 			 * slabs from diagnostic functions will not see
 			 * any frozen slabs.
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	}
 
@@ -2157,7 +2187,7 @@
 		goto redo;
 
 	if (lock)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	if (m == M_PARTIAL)
 		stat(s, tail);
@@ -2196,10 +2226,10 @@
 		n2 = get_node(s, page_to_nid(page));
 		if (n != n2) {
 			if (n)
-				spin_unlock(&n->list_lock);
+				raw_spin_unlock(&n->list_lock);
 
 			n = n2;
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 
 		do {
@@ -2228,7 +2258,7 @@
 	}
 
 	if (n)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	while (discard_page) {
 		page = discard_page;
@@ -2265,14 +2295,21 @@
 			pobjects = oldpage->pobjects;
 			pages = oldpage->pages;
 			if (drain && pobjects > s->cpu_partial) {
+				struct slub_free_list *f;
 				unsigned long flags;
+				LIST_HEAD(tofree);
 				/*
 				 * partial array is full. Move the existing
 				 * set to the per node partial list.
 				 */
 				local_irq_save(flags);
 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+				f = this_cpu_ptr(&slub_free_list);
+				raw_spin_lock(&f->lock);
+				list_splice_init(&f->list, &tofree);
+				raw_spin_unlock(&f->lock);
 				local_irq_restore(flags);
+				free_delayed(&tofree);
 				oldpage = NULL;
 				pobjects = 0;
 				pages = 0;
@@ -2340,7 +2377,22 @@
 
 static void flush_all(struct kmem_cache *s)
 {
+	LIST_HEAD(tofree);
+	int cpu;
+
 	on_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1, GFP_ATOMIC);
+	for_each_online_cpu(cpu) {
+		struct slub_free_list *f;
+
+		if (!has_cpu_slab(cpu, s))
+			continue;
+
+		f = &per_cpu(slub_free_list, cpu);
+		raw_spin_lock_irq(&f->lock);
+		list_splice_init(&f->list, &tofree);
+		raw_spin_unlock_irq(&f->lock);
+		free_delayed(&tofree);
+	}
 }
 
 /*
@@ -2395,10 +2447,10 @@
 	unsigned long x = 0;
 	struct page *page;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	list_for_each_entry(page, &n->partial, slab_list)
 		x += get_count(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return x;
 }
 #endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */
@@ -2537,8 +2589,10 @@
  * already disabled (which is the case for bulk allocation).
  */
 static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
-			  unsigned long addr, struct kmem_cache_cpu *c)
+			  unsigned long addr, struct kmem_cache_cpu *c,
+			  struct list_head *to_free)
 {
+	struct slub_free_list *f;
 	void *freelist;
 	struct page *page;
 
@@ -2594,6 +2648,13 @@
 	VM_BUG_ON(!c->page->frozen);
 	c->freelist = get_freepointer(s, freelist);
 	c->tid = next_tid(c->tid);
+
+out:
+	f = this_cpu_ptr(&slub_free_list);
+	raw_spin_lock(&f->lock);
+	list_splice_init(&f->list, to_free);
+	raw_spin_unlock(&f->lock);
+
 	return freelist;
 
 new_slab:
@@ -2609,7 +2670,7 @@
 
 	if (unlikely(!freelist)) {
 		slab_out_of_memory(s, gfpflags, node);
-		return NULL;
+		goto out;
 	}
 
 	page = c->page;
@@ -2622,7 +2683,7 @@
 		goto new_slab;	/* Slab failed checks. Next slab needed */
 
 	deactivate_slab(s, page, get_freepointer(s, freelist), c);
-	return freelist;
+	goto out;
 }
 
 /*
@@ -2634,9 +2695,10 @@
 {
 	void *p;
 	unsigned long flags;
+	LIST_HEAD(tofree);
 
 	local_irq_save(flags);
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	/*
 	 * We may have been preempted and rescheduled on a different
 	 * cpu before disabling interrupts. Need to reload cpu area
@@ -2645,8 +2707,9 @@
 	c = this_cpu_ptr(s->cpu_slab);
 #endif
 
-	p = ___slab_alloc(s, gfpflags, node, addr, c);
+	p = ___slab_alloc(s, gfpflags, node, addr, c, &tofree);
 	local_irq_restore(flags);
+	free_delayed(&tofree);
 	return p;
 }
 
@@ -2690,13 +2753,13 @@
 	 * as we end up on the original cpu again when doing the cmpxchg.
 	 *
 	 * We should guarantee that tid and kmem_cache are retrieved on
-	 * the same cpu. It could be different if CONFIG_PREEMPT so we need
+	 * the same cpu. It could be different if CONFIG_PREEMPTION so we need
 	 * to check if it is matched or not.
 	 */
 	do {
 		tid = this_cpu_read(s->cpu_slab->tid);
 		c = raw_cpu_ptr(s->cpu_slab);
-	} while (IS_ENABLED(CONFIG_PREEMPT) &&
+	} while (IS_ENABLED(CONFIG_PREEMPTION) &&
 		 unlikely(tid != READ_ONCE(c->tid)));
 
 	/*
@@ -2845,7 +2908,7 @@
 
 	do {
 		if (unlikely(n)) {
-			spin_unlock_irqrestore(&n->list_lock, flags);
+			raw_spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
 		prior = page->freelist;
@@ -2877,7 +2940,7 @@
 				 * Otherwise the list_lock will synchronize with
 				 * other processors updating the list of slabs.
 				 */
-				spin_lock_irqsave(&n->list_lock, flags);
+				raw_spin_lock_irqsave(&n->list_lock, flags);
 
 			}
 		}
@@ -2918,7 +2981,7 @@
 		add_partial(n, page, DEACTIVATE_TO_TAIL);
 		stat(s, FREE_ADD_PARTIAL);
 	}
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return;
 
 slab_empty:
@@ -2933,7 +2996,7 @@
 		remove_full(s, n, page);
 	}
 
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	stat(s, FREE_SLAB);
 	discard_slab(s, page);
 }
@@ -2970,7 +3033,7 @@
 	do {
 		tid = this_cpu_read(s->cpu_slab->tid);
 		c = raw_cpu_ptr(s->cpu_slab);
-	} while (IS_ENABLED(CONFIG_PREEMPT) &&
+	} while (IS_ENABLED(CONFIG_PREEMPTION) &&
 		 unlikely(tid != READ_ONCE(c->tid)));
 
 	/* Same with comment on barrier() in slab_alloc_node() */
@@ -3136,6 +3199,7 @@
 			  void **p)
 {
 	struct kmem_cache_cpu *c;
+	LIST_HEAD(to_free);
 	int i;
 
 	/* memcg and kmem_cache debug support */
@@ -3159,7 +3223,7 @@
 			 * of re-populating per CPU c->freelist
 			 */
 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
-					    _RET_IP_, c);
+					    _RET_IP_, c, &to_free);
 			if (unlikely(!p[i]))
 				goto error;
 
@@ -3174,6 +3238,7 @@
 	}
 	c->tid = next_tid(c->tid);
 	local_irq_enable();
+	free_delayed(&to_free);
 
 	/* Clear memory outside IRQ disabled fastpath loop */
 	if (unlikely(slab_want_init_on_alloc(flags, s))) {
@@ -3188,6 +3253,7 @@
 	return i;
 error:
 	local_irq_enable();
+	free_delayed(&to_free);
 	slab_post_alloc_hook(s, flags, i, p);
 	__kmem_cache_free_bulk(s, i, p);
 	return 0;
@@ -3323,7 +3389,7 @@
 init_kmem_cache_node(struct kmem_cache_node *n)
 {
 	n->nr_partial = 0;
-	spin_lock_init(&n->list_lock);
+	raw_spin_lock_init(&n->list_lock);
 	INIT_LIST_HEAD(&n->partial);
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_set(&n->nr_slabs, 0);
@@ -3672,6 +3738,11 @@
 							const char *text)
 {
 #ifdef CONFIG_SLUB_DEBUG
+#ifdef CONFIG_PREEMPT_RT
+	/* XXX move out of irq-off section */
+	slab_err(s, page, text, s->name);
+#else
+
 	void *addr = page_address(page);
 	void *p;
 	unsigned long *map = bitmap_zalloc(page->objects, GFP_ATOMIC);
@@ -3691,8 +3762,10 @@
 	slab_unlock(page);
 	bitmap_free(map);
 #endif
+#endif
 }
 
+
 /*
  * Attempt to free all partial slabs on a node.
  * This is called from __kmem_cache_shutdown(). We must take list_lock
@@ -3704,7 +3777,7 @@
 	struct page *page, *h;
 
 	BUG_ON(irqs_disabled());
-	spin_lock_irq(&n->list_lock);
+	raw_spin_lock_irq(&n->list_lock);
 	list_for_each_entry_safe(page, h, &n->partial, slab_list) {
 		if (!page->inuse) {
 			remove_partial(n, page);
@@ -3714,7 +3787,7 @@
 			"Objects remaining in %s on __kmem_cache_shutdown()");
 		}
 	}
-	spin_unlock_irq(&n->list_lock);
+	raw_spin_unlock_irq(&n->list_lock);
 
 	list_for_each_entry_safe(page, h, &discard, slab_list)
 		discard_slab(s, page);
@@ -3986,7 +4059,7 @@
 		for (i = 0; i < SHRINK_PROMOTE_MAX; i++)
 			INIT_LIST_HEAD(promote + i);
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 
 		/*
 		 * Build lists of slabs to discard or promote.
@@ -4017,7 +4090,7 @@
 		for (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)
 			list_splice(promote + i, &n->partial);
 
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		/* Release empty slabs */
 		list_for_each_entry_safe(page, t, &discard, slab_list)
@@ -4224,6 +4297,12 @@
 {
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		raw_spin_lock_init(&per_cpu(slub_free_list, cpu).lock);
+		INIT_LIST_HEAD(&per_cpu(slub_free_list, cpu).list);
+	}
 
 	if (debug_guardpage_minorder())
 		slub_max_order = 0;
@@ -4425,7 +4504,7 @@
 	struct page *page;
 	unsigned long flags;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 
 	list_for_each_entry(page, &n->partial, slab_list) {
 		validate_slab_slab(s, page, map);
@@ -4447,7 +4526,7 @@
 		       s->name, count, atomic_long_read(&n->nr_slabs));
 
 out:
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return count;
 }
 
@@ -4633,12 +4712,12 @@
 		if (!atomic_long_read(&n->nr_slabs))
 			continue;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		list_for_each_entry(page, &n->partial, slab_list)
 			process_slab(&t, s, page, alloc, map);
 		list_for_each_entry(page, &n->full, slab_list)
 			process_slab(&t, s, page, alloc, map);
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	}
 
 	for (i = 0; i < t.count; i++) {
diff -Nur linux-5.4.5/mm/swap.c linux-5.4.5-new/mm/swap.c
--- linux-5.4.5/mm/swap.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/swap.c	2020-06-15 16:12:27.343710115 +0300
@@ -44,15 +44,112 @@
 /* How many pages do we try to swap or page in/out together? */
 int page_cluster;
 
-static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
-static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
-static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);
-static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);
-static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);
+#ifdef CONFIG_PREEMPT_RT
+DEFINE_STATIC_KEY_TRUE(use_pvec_lock);
+#else
+DEFINE_STATIC_KEY_FALSE(use_pvec_lock);
+#endif
+
+struct swap_pagevec {
+	spinlock_t	lock;
+	struct pagevec	pvec;
+};
+
+#define DEFINE_PER_CPU_PAGEVEC(lvar)				\
+	DEFINE_PER_CPU(struct swap_pagevec, lvar) = {		\
+		.lock = __SPIN_LOCK_UNLOCKED((lvar).lock) }
+
+static DEFINE_PER_CPU_PAGEVEC(lru_add_pvec);
+static DEFINE_PER_CPU_PAGEVEC(lru_rotate_pvecs);
+static DEFINE_PER_CPU_PAGEVEC(lru_deactivate_file_pvecs);
+static DEFINE_PER_CPU_PAGEVEC(lru_deactivate_pvecs);
+static DEFINE_PER_CPU_PAGEVEC(lru_lazyfree_pvecs);
 #ifdef CONFIG_SMP
-static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);
+static DEFINE_PER_CPU_PAGEVEC(activate_page_pvecs);
 #endif
 
+static inline
+struct swap_pagevec *lock_swap_pvec(struct swap_pagevec __percpu *p)
+{
+	struct swap_pagevec *swpvec;
+
+	if (static_branch_likely(&use_pvec_lock)) {
+		swpvec = raw_cpu_ptr(p);
+
+		spin_lock(&swpvec->lock);
+	} else {
+		swpvec = &get_cpu_var(*p);
+	}
+	return swpvec;
+}
+
+static inline struct swap_pagevec *
+lock_swap_pvec_cpu(struct swap_pagevec __percpu *p, int cpu)
+{
+	struct swap_pagevec *swpvec = per_cpu_ptr(p, cpu);
+
+	if (static_branch_likely(&use_pvec_lock))
+		spin_lock(&swpvec->lock);
+
+	return swpvec;
+}
+
+static inline struct swap_pagevec *
+lock_swap_pvec_irqsave(struct swap_pagevec __percpu *p, unsigned long *flags)
+{
+	struct swap_pagevec *swpvec;
+
+	if (static_branch_likely(&use_pvec_lock)) {
+		swpvec = raw_cpu_ptr(p);
+
+		spin_lock_irqsave(&swpvec->lock, (*flags));
+	} else {
+		local_irq_save(*flags);
+
+		swpvec = this_cpu_ptr(p);
+	}
+	return swpvec;
+}
+
+static inline struct swap_pagevec *
+lock_swap_pvec_cpu_irqsave(struct swap_pagevec __percpu *p, int cpu,
+			   unsigned long *flags)
+{
+	struct swap_pagevec *swpvec = per_cpu_ptr(p, cpu);
+
+	if (static_branch_likely(&use_pvec_lock))
+		spin_lock_irqsave(&swpvec->lock, *flags);
+	else
+		local_irq_save(*flags);
+
+	return swpvec;
+}
+
+static inline void unlock_swap_pvec(struct swap_pagevec *swpvec,
+				    struct swap_pagevec __percpu *p)
+{
+	if (static_branch_likely(&use_pvec_lock))
+		spin_unlock(&swpvec->lock);
+	else
+		put_cpu_var(*p);
+
+}
+
+static inline void unlock_swap_pvec_cpu(struct swap_pagevec *swpvec)
+{
+	if (static_branch_likely(&use_pvec_lock))
+		spin_unlock(&swpvec->lock);
+}
+
+static inline void
+unlock_swap_pvec_irqrestore(struct swap_pagevec *swpvec, unsigned long flags)
+{
+	if (static_branch_likely(&use_pvec_lock))
+		spin_unlock_irqrestore(&swpvec->lock, flags);
+	else
+		local_irq_restore(flags);
+}
+
 /*
  * This path almost never happens for VM activity - pages are normally
  * freed via pagevecs.  But it gets used by networking.
@@ -250,15 +347,17 @@
 {
 	if (!PageLocked(page) && !PageDirty(page) &&
 	    !PageUnevictable(page) && PageLRU(page)) {
+		struct swap_pagevec *swpvec;
 		struct pagevec *pvec;
 		unsigned long flags;
 
 		get_page(page);
-		local_irq_save(flags);
-		pvec = this_cpu_ptr(&lru_rotate_pvecs);
+
+		swpvec = lock_swap_pvec_irqsave(&lru_rotate_pvecs, &flags);
+		pvec = &swpvec->pvec;
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_move_tail(pvec);
-		local_irq_restore(flags);
+		unlock_swap_pvec_irqrestore(swpvec, flags);
 	}
 }
 
@@ -293,27 +392,32 @@
 #ifdef CONFIG_SMP
 static void activate_page_drain(int cpu)
 {
-	struct pagevec *pvec = &per_cpu(activate_page_pvecs, cpu);
+	struct swap_pagevec *swpvec = lock_swap_pvec_cpu(&activate_page_pvecs, cpu);
+	struct pagevec *pvec = &swpvec->pvec;
 
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, __activate_page, NULL);
+	unlock_swap_pvec_cpu(swpvec);
 }
 
 static bool need_activate_page_drain(int cpu)
 {
-	return pagevec_count(&per_cpu(activate_page_pvecs, cpu)) != 0;
+	return pagevec_count(per_cpu_ptr(&activate_page_pvecs.pvec, cpu)) != 0;
 }
 
 void activate_page(struct page *page)
 {
 	page = compound_head(page);
 	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
-		struct pagevec *pvec = &get_cpu_var(activate_page_pvecs);
+		struct swap_pagevec *swpvec;
+		struct pagevec *pvec;
 
 		get_page(page);
+		swpvec = lock_swap_pvec(&activate_page_pvecs);
+		pvec = &swpvec->pvec;
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_lru_move_fn(pvec, __activate_page, NULL);
-		put_cpu_var(activate_page_pvecs);
+		unlock_swap_pvec(swpvec, &activate_page_pvecs);
 	}
 }
 
@@ -335,7 +439,8 @@
 
 static void __lru_cache_activate_page(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
+	struct swap_pagevec *swpvec = lock_swap_pvec(&lru_add_pvec);
+	struct pagevec *pvec = &swpvec->pvec;
 	int i;
 
 	/*
@@ -357,7 +462,7 @@
 		}
 	}
 
-	put_cpu_var(lru_add_pvec);
+	unlock_swap_pvec(swpvec, &lru_add_pvec);
 }
 
 /*
@@ -399,12 +504,13 @@
 
 static void __lru_cache_add(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
+	struct swap_pagevec *swpvec = lock_swap_pvec(&lru_add_pvec);
+	struct pagevec *pvec = &swpvec->pvec;
 
 	get_page(page);
 	if (!pagevec_add(pvec, page) || PageCompound(page))
 		__pagevec_lru_add(pvec);
-	put_cpu_var(lru_add_pvec);
+	unlock_swap_pvec(swpvec, &lru_add_pvec);
 }
 
 /**
@@ -588,32 +694,40 @@
  */
 void lru_add_drain_cpu(int cpu)
 {
-	struct pagevec *pvec = &per_cpu(lru_add_pvec, cpu);
+	struct swap_pagevec *swpvec = lock_swap_pvec_cpu(&lru_add_pvec, cpu);
+	struct pagevec *pvec = &swpvec->pvec;
+	unsigned long flags;
 
 	if (pagevec_count(pvec))
 		__pagevec_lru_add(pvec);
+	unlock_swap_pvec_cpu(swpvec);
 
-	pvec = &per_cpu(lru_rotate_pvecs, cpu);
+	swpvec = lock_swap_pvec_cpu_irqsave(&lru_rotate_pvecs, cpu, &flags);
+	pvec = &swpvec->pvec;
 	if (pagevec_count(pvec)) {
-		unsigned long flags;
 
 		/* No harm done if a racing interrupt already did this */
-		local_irq_save(flags);
 		pagevec_move_tail(pvec);
-		local_irq_restore(flags);
 	}
+	unlock_swap_pvec_irqrestore(swpvec, flags);
 
-	pvec = &per_cpu(lru_deactivate_file_pvecs, cpu);
+	swpvec = lock_swap_pvec_cpu(&lru_deactivate_file_pvecs, cpu);
+	pvec = &swpvec->pvec;
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
+	unlock_swap_pvec_cpu(swpvec);
 
-	pvec = &per_cpu(lru_deactivate_pvecs, cpu);
+	swpvec = lock_swap_pvec_cpu(&lru_deactivate_pvecs, cpu);
+	pvec = &swpvec->pvec;
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);
+	unlock_swap_pvec_cpu(swpvec);
 
-	pvec = &per_cpu(lru_lazyfree_pvecs, cpu);
+	swpvec = lock_swap_pvec_cpu(&lru_lazyfree_pvecs, cpu);
+	pvec = &swpvec->pvec;
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);
+	unlock_swap_pvec_cpu(swpvec);
 
 	activate_page_drain(cpu);
 }
@@ -628,6 +742,9 @@
  */
 void deactivate_file_page(struct page *page)
 {
+	struct swap_pagevec *swpvec;
+	struct pagevec *pvec;
+
 	/*
 	 * In a workload with many unevictable page such as mprotect,
 	 * unevictable page deactivation for accelerating reclaim is pointless.
@@ -636,11 +753,12 @@
 		return;
 
 	if (likely(get_page_unless_zero(page))) {
-		struct pagevec *pvec = &get_cpu_var(lru_deactivate_file_pvecs);
+		swpvec = lock_swap_pvec(&lru_deactivate_file_pvecs);
+		pvec = &swpvec->pvec;
 
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
-		put_cpu_var(lru_deactivate_file_pvecs);
+		unlock_swap_pvec(swpvec, &lru_deactivate_file_pvecs);
 	}
 }
 
@@ -655,12 +773,16 @@
 void deactivate_page(struct page *page)
 {
 	if (PageLRU(page) && PageActive(page) && !PageUnevictable(page)) {
-		struct pagevec *pvec = &get_cpu_var(lru_deactivate_pvecs);
+		struct swap_pagevec *swpvec;
+		struct pagevec *pvec;
+
+		swpvec = lock_swap_pvec(&lru_deactivate_pvecs);
+		pvec = &swpvec->pvec;
 
 		get_page(page);
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);
-		put_cpu_var(lru_deactivate_pvecs);
+		unlock_swap_pvec(swpvec, &lru_deactivate_pvecs);
 	}
 }
 
@@ -673,21 +795,29 @@
  */
 void mark_page_lazyfree(struct page *page)
 {
+	struct swap_pagevec *swpvec;
+	struct pagevec *pvec;
+
 	if (PageLRU(page) && PageAnon(page) && PageSwapBacked(page) &&
 	    !PageSwapCache(page) && !PageUnevictable(page)) {
-		struct pagevec *pvec = &get_cpu_var(lru_lazyfree_pvecs);
+		swpvec = lock_swap_pvec(&lru_lazyfree_pvecs);
+		pvec = &swpvec->pvec;
 
 		get_page(page);
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);
-		put_cpu_var(lru_lazyfree_pvecs);
+		unlock_swap_pvec(swpvec, &lru_lazyfree_pvecs);
 	}
 }
 
 void lru_add_drain(void)
 {
-	lru_add_drain_cpu(get_cpu());
-	put_cpu();
+	if (static_branch_likely(&use_pvec_lock)) {
+		lru_add_drain_cpu(raw_smp_processor_id());
+	} else {
+		lru_add_drain_cpu(get_cpu());
+		put_cpu();
+	}
 }
 
 #ifdef CONFIG_SMP
@@ -708,39 +838,54 @@
  */
 void lru_add_drain_all(void)
 {
-	static DEFINE_MUTEX(lock);
-	static struct cpumask has_work;
-	int cpu;
+	if (static_branch_likely(&use_pvec_lock)) {
+		int cpu;
 
-	/*
-	 * Make sure nobody triggers this path before mm_percpu_wq is fully
-	 * initialized.
-	 */
-	if (WARN_ON(!mm_percpu_wq))
-		return;
+		for_each_online_cpu(cpu) {
+			if (pagevec_count(&per_cpu(lru_add_pvec.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_rotate_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_deactivate_file_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_deactivate_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_lazyfree_pvecs.pvec, cpu)) ||
+			    need_activate_page_drain(cpu)) {
+				lru_add_drain_cpu(cpu);
+			}
+		}
+	} else {
+		static DEFINE_MUTEX(lock);
+		static struct cpumask has_work;
+		int cpu;
 
-	mutex_lock(&lock);
-	cpumask_clear(&has_work);
+		/*
+		 * Make sure nobody triggers this path before mm_percpu_wq
+		 * is fully initialized.
+		 */
+		if (WARN_ON(!mm_percpu_wq))
+			return;
 
-	for_each_online_cpu(cpu) {
-		struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
+		mutex_lock(&lock);
+		cpumask_clear(&has_work);
 
-		if (pagevec_count(&per_cpu(lru_add_pvec, cpu)) ||
-		    pagevec_count(&per_cpu(lru_rotate_pvecs, cpu)) ||
-		    pagevec_count(&per_cpu(lru_deactivate_file_pvecs, cpu)) ||
-		    pagevec_count(&per_cpu(lru_deactivate_pvecs, cpu)) ||
-		    pagevec_count(&per_cpu(lru_lazyfree_pvecs, cpu)) ||
-		    need_activate_page_drain(cpu)) {
-			INIT_WORK(work, lru_add_drain_per_cpu);
-			queue_work_on(cpu, mm_percpu_wq, work);
-			cpumask_set_cpu(cpu, &has_work);
+		for_each_online_cpu(cpu) {
+			struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
+
+			if (pagevec_count(&per_cpu(lru_add_pvec.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_rotate_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_deactivate_file_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_deactivate_pvecs.pvec, cpu)) ||
+			    pagevec_count(&per_cpu(lru_lazyfree_pvecs.pvec, cpu)) ||
+			    need_activate_page_drain(cpu)) {
+				INIT_WORK(work, lru_add_drain_per_cpu);
+				queue_work_on(cpu, mm_percpu_wq, work);
+				cpumask_set_cpu(cpu, &has_work);
+			}
 		}
-	}
 
-	for_each_cpu(cpu, &has_work)
-		flush_work(&per_cpu(lru_add_drain_work, cpu));
+		for_each_cpu(cpu, &has_work)
+			flush_work(&per_cpu(lru_add_drain_work, cpu));
 
-	mutex_unlock(&lock);
+		mutex_unlock(&lock);
+	}
 }
 #else
 void lru_add_drain_all(void)
diff -Nur linux-5.4.5/mm/vmalloc.c linux-5.4.5-new/mm/vmalloc.c
--- linux-5.4.5/mm/vmalloc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/vmalloc.c	2020-06-15 16:12:27.343710115 +0300
@@ -1077,31 +1077,34 @@
 
 retry:
 	/*
-	 * Preload this CPU with one extra vmap_area object to ensure
-	 * that we have it available when fit type of free area is
-	 * NE_FIT_TYPE.
+	 * Preload this CPU with one extra vmap_area object. It is used
+	 * when fit type of free area is NE_FIT_TYPE. Please note, it
+	 * does not guarantee that an allocation occurs on a CPU that
+	 * is preloaded, instead we minimize the case when it is not.
+	 * It can happen because of cpu migration, because there is a
+	 * race until the below spinlock is taken.
 	 *
 	 * The preload is done in non-atomic context, thus it allows us
 	 * to use more permissive allocation masks to be more stable under
-	 * low memory condition and high memory pressure.
+	 * low memory condition and high memory pressure. In rare case,
+	 * if not preloaded, GFP_NOWAIT is used.
 	 *
-	 * Even if it fails we do not really care about that. Just proceed
-	 * as it is. "overflow" path will refill the cache we allocate from.
+	 * Set "pva" to NULL here, because of "retry" path.
 	 */
-	preempt_disable();
-	if (!__this_cpu_read(ne_fit_preload_node)) {
-		preempt_enable();
-		pva = kmem_cache_alloc_node(vmap_area_cachep, GFP_KERNEL, node);
-		preempt_disable();
+	pva = NULL;
 
-		if (__this_cpu_cmpxchg(ne_fit_preload_node, NULL, pva)) {
-			if (pva)
-				kmem_cache_free(vmap_area_cachep, pva);
-		}
-	}
+	if (!this_cpu_read(ne_fit_preload_node))
+		/*
+		 * Even if it fails we do not really care about that.
+		 * Just proceed as it is. If needed "overflow" path
+		 * will refill the cache we allocate from.
+		 */
+		pva = kmem_cache_alloc_node(vmap_area_cachep, GFP_KERNEL, node);
 
 	spin_lock(&vmap_area_lock);
-	preempt_enable();
+
+	if (pva && __this_cpu_cmpxchg(ne_fit_preload_node, NULL, pva))
+		kmem_cache_free(vmap_area_cachep, pva);
 
 	/*
 	 * If an allocation fails, the "vend" address is
@@ -1459,7 +1462,7 @@
 	struct vmap_block *vb;
 	struct vmap_area *va;
 	unsigned long vb_idx;
-	int node, err;
+	int node, err, cpu;
 	void *vaddr;
 
 	node = numa_node_id();
@@ -1502,11 +1505,12 @@
 	BUG_ON(err);
 	radix_tree_preload_end();
 
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	spin_lock(&vbq->lock);
 	list_add_tail_rcu(&vb->free_list, &vbq->free);
 	spin_unlock(&vbq->lock);
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 
 	return vaddr;
 }
@@ -1575,6 +1579,7 @@
 	struct vmap_block *vb;
 	void *vaddr = NULL;
 	unsigned int order;
+	int cpu;
 
 	BUG_ON(offset_in_page(size));
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
@@ -1589,7 +1594,8 @@
 	order = get_order(size);
 
 	rcu_read_lock();
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = this_cpu_ptr(&vmap_block_queue);
 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
 		unsigned long pages_off;
 
@@ -1612,7 +1618,7 @@
 		break;
 	}
 
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 	rcu_read_unlock();
 
 	/* Allocate new block if nothing was found */
diff -Nur linux-5.4.5/mm/vmstat.c linux-5.4.5-new/mm/vmstat.c
--- linux-5.4.5/mm/vmstat.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/vmstat.c	2020-06-15 16:12:27.371710017 +0300
@@ -321,6 +321,7 @@
 	long x;
 	long t;
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -330,6 +331,7 @@
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
@@ -341,6 +343,7 @@
 	long x;
 	long t;
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -350,6 +353,7 @@
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_node_page_state);
 
@@ -382,6 +386,7 @@
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -390,6 +395,7 @@
 		zone_page_state_add(v + overstep, zone, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -398,6 +404,7 @@
 	s8 __percpu *p = pcp->vm_node_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -406,6 +413,7 @@
 		node_page_state_add(v + overstep, pgdat, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
@@ -426,6 +434,7 @@
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -434,6 +443,7 @@
 		zone_page_state_add(v - overstep, zone, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
@@ -442,6 +452,7 @@
 	s8 __percpu *p = pcp->vm_node_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -450,6 +461,7 @@
 		node_page_state_add(v - overstep, pgdat, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
diff -Nur linux-5.4.5/mm/workingset.c linux-5.4.5-new/mm/workingset.c
--- linux-5.4.5/mm/workingset.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/workingset.c	2020-06-15 16:12:27.371710017 +0300
@@ -367,6 +367,8 @@
 
 void workingset_update_node(struct xa_node *node)
 {
+	struct address_space *mapping;
+
 	/*
 	 * Track non-empty nodes that contain only shadow entries;
 	 * unlink those that contain pages or are being freed.
@@ -375,7 +377,8 @@
 	 * already where they should be. The list_empty() test is safe
 	 * as node->private_list is protected by the i_pages lock.
 	 */
-	VM_WARN_ON_ONCE(!irqs_disabled());  /* For __inc_lruvec_page_state */
+	mapping = container_of(node->array, struct address_space, i_pages);
+	lockdep_assert_held(&mapping->i_pages.xa_lock);
 
 	if (node->count && node->count == node->nr_values) {
 		if (list_empty(&node->private_list)) {
diff -Nur linux-5.4.5/mm/zsmalloc.c linux-5.4.5-new/mm/zsmalloc.c
--- linux-5.4.5/mm/zsmalloc.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/zsmalloc.c	2020-06-15 16:12:27.371710017 +0300
@@ -57,6 +57,7 @@
 #include <linux/wait.h>
 #include <linux/pagemap.h>
 #include <linux/fs.h>
+#include <linux/locallock.h>
 
 #define ZSPAGE_MAGIC	0x58
 
@@ -74,9 +75,22 @@
  */
 #define ZS_MAX_ZSPAGE_ORDER 2
 #define ZS_MAX_PAGES_PER_ZSPAGE (_AC(1, UL) << ZS_MAX_ZSPAGE_ORDER)
-
 #define ZS_HANDLE_SIZE (sizeof(unsigned long))
 
+#ifdef CONFIG_PREEMPT_RT
+
+struct zsmalloc_handle {
+	unsigned long addr;
+	struct mutex lock;
+};
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(struct zsmalloc_handle))
+
+#else
+
+#define ZS_HANDLE_ALLOC_SIZE (sizeof(unsigned long))
+#endif
+
 /*
  * Object location (<PFN>, <obj_idx>) is encoded as
  * as single (unsigned long) handle value.
@@ -326,7 +340,7 @@
 
 static int create_cache(struct zs_pool *pool)
 {
-	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,
+	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_ALLOC_SIZE,
 					0, 0, NULL);
 	if (!pool->handle_cachep)
 		return 1;
@@ -350,10 +364,27 @@
 
 static unsigned long cache_alloc_handle(struct zs_pool *pool, gfp_t gfp)
 {
-	return (unsigned long)kmem_cache_alloc(pool->handle_cachep,
-			gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+	void *p;
+
+	p = kmem_cache_alloc(pool->handle_cachep,
+			     gfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));
+#ifdef CONFIG_PREEMPT_RT
+	if (p) {
+		struct zsmalloc_handle *zh = p;
+
+		mutex_init(&zh->lock);
+	}
+#endif
+	return (unsigned long)p;
 }
 
+#ifdef CONFIG_PREEMPT_RT
+static struct zsmalloc_handle *zs_get_pure_handle(unsigned long handle)
+{
+	return (void *)(handle &~((1 << OBJ_TAG_BITS) - 1));
+}
+#endif
+
 static void cache_free_handle(struct zs_pool *pool, unsigned long handle)
 {
 	kmem_cache_free(pool->handle_cachep, (void *)handle);
@@ -372,12 +403,18 @@
 
 static void record_obj(unsigned long handle, unsigned long obj)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	WRITE_ONCE(zh->addr, obj);
+#else
 	/*
 	 * lsb of @obj represents handle lock while other bits
 	 * represent object value the handle is pointing so
 	 * updating shouldn't do store tearing.
 	 */
 	WRITE_ONCE(*(unsigned long *)handle, obj);
+#endif
 }
 
 /* zpool driver */
@@ -460,6 +497,7 @@
 
 /* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
 static DEFINE_PER_CPU(struct mapping_area, zs_map_area);
+static DEFINE_LOCAL_IRQ_LOCK(zs_map_area_lock);
 
 static bool is_zspage_isolated(struct zspage *zspage)
 {
@@ -869,7 +907,13 @@
 
 static unsigned long handle_to_obj(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return zh->addr;
+#else
 	return *(unsigned long *)handle;
+#endif
 }
 
 static unsigned long obj_to_head(struct page *page, void *obj)
@@ -883,22 +927,46 @@
 
 static inline int testpin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_is_locked(&zh->lock);
+#else
 	return bit_spin_is_locked(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static inline int trypin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_trylock(&zh->lock);
+#else
 	return bit_spin_trylock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void pin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_lock(&zh->lock);
+#else
 	bit_spin_lock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void unpin_tag(unsigned long handle)
 {
+#ifdef CONFIG_PREEMPT_RT
+	struct zsmalloc_handle *zh = zs_get_pure_handle(handle);
+
+	return mutex_unlock(&zh->lock);
+#else
 	bit_spin_unlock(HANDLE_PIN_BIT, (unsigned long *)handle);
+#endif
 }
 
 static void reset_page(struct page *page)
@@ -1324,7 +1392,7 @@
 	class = pool->size_class[class_idx];
 	off = (class->size * obj_idx) & ~PAGE_MASK;
 
-	area = &get_cpu_var(zs_map_area);
+	area = &get_locked_var(zs_map_area_lock, zs_map_area);
 	area->vm_mm = mm;
 	if (off + class->size <= PAGE_SIZE) {
 		/* this object is contained entirely within a page */
@@ -1378,7 +1446,7 @@
 
 		__zs_unmap_object(area, pages, off, class->size);
 	}
-	put_cpu_var(zs_map_area);
+	put_locked_var(zs_map_area_lock, zs_map_area);
 
 	migrate_read_unlock(zspage);
 	unpin_tag(handle);
diff -Nur linux-5.4.5/mm/zswap.c linux-5.4.5-new/mm/zswap.c
--- linux-5.4.5/mm/zswap.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/mm/zswap.c	2020-06-15 16:12:27.371710017 +0300
@@ -18,6 +18,7 @@
 #include <linux/highmem.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
+#include <linux/locallock.h>
 #include <linux/types.h>
 #include <linux/atomic.h>
 #include <linux/frontswap.h>
@@ -980,6 +981,8 @@
 	memset_l(page, value, PAGE_SIZE / sizeof(unsigned long));
 }
 
+/* protect zswap_dstmem from concurrency */
+static DEFINE_LOCAL_IRQ_LOCK(zswap_dstmem_lock);
 /*********************************
 * frontswap hooks
 **********************************/
@@ -1057,12 +1060,11 @@
 	}
 
 	/* compress */
-	dst = get_cpu_var(zswap_dstmem);
-	tfm = *get_cpu_ptr(entry->pool->tfm);
+	dst = get_locked_var(zswap_dstmem_lock, zswap_dstmem);
+	tfm = *this_cpu_ptr(entry->pool->tfm);
 	src = kmap_atomic(page);
 	ret = crypto_comp_compress(tfm, src, PAGE_SIZE, dst, &dlen);
 	kunmap_atomic(src);
-	put_cpu_ptr(entry->pool->tfm);
 	if (ret) {
 		ret = -EINVAL;
 		goto put_dstmem;
@@ -1086,7 +1088,7 @@
 	memcpy(buf, &zhdr, hlen);
 	memcpy(buf + hlen, dst, dlen);
 	zpool_unmap_handle(entry->pool->zpool, handle);
-	put_cpu_var(zswap_dstmem);
+	put_locked_var(zswap_dstmem_lock, zswap_dstmem);
 
 	/* populate entry */
 	entry->offset = offset;
@@ -1114,7 +1116,7 @@
 	return 0;
 
 put_dstmem:
-	put_cpu_var(zswap_dstmem);
+	put_locked_var(zswap_dstmem_lock, zswap_dstmem);
 	zswap_pool_put(entry->pool);
 freepage:
 	zswap_entry_cache_free(entry);
diff -Nur linux-5.4.5/net/core/dev.c linux-5.4.5-new/net/core/dev.c
--- linux-5.4.5/net/core/dev.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/core/dev.c	2020-06-15 16:12:29.355703031 +0300
@@ -196,6 +196,7 @@
 static DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);
 
 static seqcount_t devnet_rename_seq;
+static DEFINE_MUTEX(devnet_rename_mutex);
 
 static inline void dev_base_seq_inc(struct net *net)
 {
@@ -218,14 +219,14 @@
 static inline void rps_lock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_lock(&sd->input_pkt_queue.lock);
+	raw_spin_lock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
 static inline void rps_unlock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_unlock(&sd->input_pkt_queue.lock);
+	raw_spin_unlock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
@@ -820,7 +821,7 @@
  *
  *	The use of raw_seqcount_begin() and cond_resched() before
  *	retrying is required as we want to give the writers a chance
- *	to complete when CONFIG_PREEMPT is not set.
+ *	to complete when CONFIG_PREEMPTION is not set.
  */
 int netdev_get_name(struct net *net, char *name, int ifindex)
 {
@@ -839,7 +840,8 @@
 	strcpy(name, dev->name);
 	rcu_read_unlock();
 	if (read_seqcount_retry(&devnet_rename_seq, seq)) {
-		cond_resched();
+		mutex_lock(&devnet_rename_mutex);
+		mutex_unlock(&devnet_rename_mutex);
 		goto retry;
 	}
 
@@ -1116,20 +1118,17 @@
 	    likely(!(dev->priv_flags & IFF_LIVE_RENAME_OK)))
 		return -EBUSY;
 
-	write_seqcount_begin(&devnet_rename_seq);
+	mutex_lock(&devnet_rename_mutex);
+	__raw_write_seqcount_begin(&devnet_rename_seq);
 
-	if (strncmp(newname, dev->name, IFNAMSIZ) == 0) {
-		write_seqcount_end(&devnet_rename_seq);
-		return 0;
-	}
+	if (strncmp(newname, dev->name, IFNAMSIZ) == 0)
+		goto outunlock;
 
 	memcpy(oldname, dev->name, IFNAMSIZ);
 
 	err = dev_get_valid_name(net, dev, newname);
-	if (err < 0) {
-		write_seqcount_end(&devnet_rename_seq);
-		return err;
-	}
+	if (err < 0)
+		goto outunlock;
 
 	if (oldname[0] && !strchr(oldname, '%'))
 		netdev_info(dev, "renamed from %s\n", oldname);
@@ -1142,11 +1141,12 @@
 	if (ret) {
 		memcpy(dev->name, oldname, IFNAMSIZ);
 		dev->name_assign_type = old_assign_type;
-		write_seqcount_end(&devnet_rename_seq);
-		return ret;
+		err = ret;
+		goto outunlock;
 	}
 
-	write_seqcount_end(&devnet_rename_seq);
+	__raw_write_seqcount_end(&devnet_rename_seq);
+	mutex_unlock(&devnet_rename_mutex);
 
 	netdev_adjacent_rename_links(dev, oldname);
 
@@ -1167,7 +1167,8 @@
 		/* err >= 0 after dev_alloc_name() or stores the first errno */
 		if (err >= 0) {
 			err = ret;
-			write_seqcount_begin(&devnet_rename_seq);
+			mutex_lock(&devnet_rename_mutex);
+			__raw_write_seqcount_begin(&devnet_rename_seq);
 			memcpy(dev->name, oldname, IFNAMSIZ);
 			memcpy(oldname, newname, IFNAMSIZ);
 			dev->name_assign_type = old_assign_type;
@@ -1180,6 +1181,11 @@
 	}
 
 	return err;
+
+outunlock:
+	__raw_write_seqcount_end(&devnet_rename_seq);
+	mutex_unlock(&devnet_rename_mutex);
+	return err;
 }
 
 /**
@@ -2669,6 +2675,7 @@
 	sd->output_queue_tailp = &q->next_sched;
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 void __netif_schedule(struct Qdisc *q)
@@ -2731,6 +2738,7 @@
 	__this_cpu_write(softnet_data.completion_queue, skb);
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__dev_kfree_skb_irq);
 
@@ -3418,7 +3426,11 @@
 	 * This permits qdisc->running owner to get the lock more
 	 * often and dequeue packets faster.
 	 */
+#ifdef CONFIG_PREEMPT_RT
+	contended = true;
+#else
 	contended = qdisc_is_running(q);
+#endif
 	if (unlikely(contended))
 		spin_lock(&q->busylock);
 
@@ -4211,6 +4223,7 @@
 	rps_unlock(sd);
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 
 	atomic_long_inc(&skb->dev->rx_dropped);
 	kfree_skb(skb);
@@ -4425,7 +4438,7 @@
 		struct rps_dev_flow voidflow, *rflow = &voidflow;
 		int cpu;
 
-		preempt_disable();
+		migrate_disable();
 		rcu_read_lock();
 
 		cpu = get_rps_cpu(skb->dev, skb, &rflow);
@@ -4435,14 +4448,14 @@
 		ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
 
 		rcu_read_unlock();
-		preempt_enable();
+		migrate_enable();
 	} else
 #endif
 	{
 		unsigned int qtail;
 
-		ret = enqueue_to_backlog(skb, get_cpu(), &qtail);
-		put_cpu();
+		ret = enqueue_to_backlog(skb, get_cpu_light(), &qtail);
+		put_cpu_light();
 	}
 	return ret;
 }
@@ -4481,11 +4494,9 @@
 
 	trace_netif_rx_ni_entry(skb);
 
-	preempt_disable();
+	local_bh_disable();
 	err = netif_rx_internal(skb);
-	if (local_softirq_pending())
-		do_softirq();
-	preempt_enable();
+	local_bh_enable();
 	trace_netif_rx_ni_exit(err);
 
 	return err;
@@ -5237,7 +5248,7 @@
 	skb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {
 		if (skb->dev->reg_state == NETREG_UNREGISTERING) {
 			__skb_unlink(skb, &sd->input_pkt_queue);
-			kfree_skb(skb);
+			__skb_queue_tail(&sd->tofree_queue, skb);
 			input_queue_head_incr(sd);
 		}
 	}
@@ -5247,11 +5258,14 @@
 	skb_queue_walk_safe(&sd->process_queue, skb, tmp) {
 		if (skb->dev->reg_state == NETREG_UNREGISTERING) {
 			__skb_unlink(skb, &sd->process_queue);
-			kfree_skb(skb);
+			__skb_queue_tail(&sd->tofree_queue, skb);
 			input_queue_head_incr(sd);
 		}
 	}
+	if (!skb_queue_empty(&sd->tofree_queue))
+		raise_softirq_irqoff(NET_RX_SOFTIRQ);
 	local_bh_enable();
+
 }
 
 static void flush_all_backlogs(void)
@@ -5834,12 +5848,14 @@
 		sd->rps_ipi_list = NULL;
 
 		local_irq_enable();
+		preempt_check_resched_rt();
 
 		/* Send pending IPI's to kick RPS processing on remote cpus. */
 		net_rps_send_ipi(remsd);
 	} else
 #endif
 		local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 static bool sd_has_rps_ipi_waiting(struct softnet_data *sd)
@@ -5869,7 +5885,9 @@
 	while (again) {
 		struct sk_buff *skb;
 
+		local_irq_disable();
 		while ((skb = __skb_dequeue(&sd->process_queue))) {
+			local_irq_enable();
 			rcu_read_lock();
 			__netif_receive_skb(skb);
 			rcu_read_unlock();
@@ -5877,9 +5895,9 @@
 			if (++work >= quota)
 				return work;
 
+			local_irq_disable();
 		}
 
-		local_irq_disable();
 		rps_lock(sd);
 		if (skb_queue_empty(&sd->input_pkt_queue)) {
 			/*
@@ -5917,6 +5935,7 @@
 	local_irq_save(flags);
 	____napi_schedule(this_cpu_ptr(&softnet_data), n);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__napi_schedule);
 
@@ -6359,13 +6378,21 @@
 	unsigned long time_limit = jiffies +
 		usecs_to_jiffies(netdev_budget_usecs);
 	int budget = netdev_budget;
+	struct sk_buff_head tofree_q;
+	struct sk_buff *skb;
 	LIST_HEAD(list);
 	LIST_HEAD(repoll);
 
+	__skb_queue_head_init(&tofree_q);
+
 	local_irq_disable();
+	skb_queue_splice_init(&sd->tofree_queue, &tofree_q);
 	list_splice_init(&sd->poll_list, &list);
 	local_irq_enable();
 
+	while ((skb = __skb_dequeue(&tofree_q)))
+		kfree_skb(skb);
+
 	for (;;) {
 		struct napi_struct *n;
 
@@ -9868,6 +9895,7 @@
 
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 #ifdef CONFIG_RPS
 	remsd = oldsd->rps_ipi_list;
@@ -9881,10 +9909,13 @@
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
-	while ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {
+	while ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {
 		netif_rx_ni(skb);
 		input_queue_head_incr(oldsd);
 	}
+	while ((skb = __skb_dequeue(&oldsd->tofree_queue))) {
+		kfree_skb(skb);
+	}
 
 	return 0;
 }
@@ -10195,8 +10226,9 @@
 
 		INIT_WORK(flush, flush_backlog);
 
-		skb_queue_head_init(&sd->input_pkt_queue);
-		skb_queue_head_init(&sd->process_queue);
+		skb_queue_head_init_raw(&sd->input_pkt_queue);
+		skb_queue_head_init_raw(&sd->process_queue);
+		skb_queue_head_init_raw(&sd->tofree_queue);
 #ifdef CONFIG_XFRM_OFFLOAD
 		skb_queue_head_init(&sd->xfrm_backlog);
 #endif
diff -Nur linux-5.4.5/net/core/gen_estimator.c linux-5.4.5-new/net/core/gen_estimator.c
--- linux-5.4.5/net/core/gen_estimator.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/core/gen_estimator.c	2020-06-15 16:12:29.359703016 +0300
@@ -42,7 +42,7 @@
 struct net_rate_estimator {
 	struct gnet_stats_basic_packed	*bstats;
 	spinlock_t		*stats_lock;
-	seqcount_t		*running;
+	net_seqlock_t		*running;
 	struct gnet_stats_basic_cpu __percpu *cpu_bstats;
 	u8			ewma_log;
 	u8			intvl_log; /* period : (250ms << intvl_log) */
@@ -125,7 +125,7 @@
 		      struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 		      struct net_rate_estimator __rcu **rate_est,
 		      spinlock_t *lock,
-		      seqcount_t *running,
+		      net_seqlock_t *running,
 		      struct nlattr *opt)
 {
 	struct gnet_estimator *parm = nla_data(opt);
@@ -223,7 +223,7 @@
 			  struct gnet_stats_basic_cpu __percpu *cpu_bstats,
 			  struct net_rate_estimator __rcu **rate_est,
 			  spinlock_t *lock,
-			  seqcount_t *running, struct nlattr *opt)
+			  net_seqlock_t *running, struct nlattr *opt)
 {
 	return gen_new_estimator(bstats, cpu_bstats, rate_est,
 				 lock, running, opt);
diff -Nur linux-5.4.5/net/core/gen_stats.c linux-5.4.5-new/net/core/gen_stats.c
--- linux-5.4.5/net/core/gen_stats.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/core/gen_stats.c	2020-06-15 16:12:29.359703016 +0300
@@ -138,7 +138,7 @@
 }
 
 void
-__gnet_stats_copy_basic(const seqcount_t *running,
+__gnet_stats_copy_basic(net_seqlock_t *running,
 			struct gnet_stats_basic_packed *bstats,
 			struct gnet_stats_basic_cpu __percpu *cpu,
 			struct gnet_stats_basic_packed *b)
@@ -151,15 +151,15 @@
 	}
 	do {
 		if (running)
-			seq = read_seqcount_begin(running);
+			seq = net_seq_begin(running);
 		bstats->bytes = b->bytes;
 		bstats->packets = b->packets;
-	} while (running && read_seqcount_retry(running, seq));
+	} while (running && net_seq_retry(running, seq));
 }
 EXPORT_SYMBOL(__gnet_stats_copy_basic);
 
 static int
-___gnet_stats_copy_basic(const seqcount_t *running,
+___gnet_stats_copy_basic(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b,
@@ -200,7 +200,7 @@
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic(const seqcount_t *running,
+gnet_stats_copy_basic(net_seqlock_t *running,
 		      struct gnet_dump *d,
 		      struct gnet_stats_basic_cpu __percpu *cpu,
 		      struct gnet_stats_basic_packed *b)
@@ -224,7 +224,7 @@
  * if the room in the socket buffer was not sufficient.
  */
 int
-gnet_stats_copy_basic_hw(const seqcount_t *running,
+gnet_stats_copy_basic_hw(net_seqlock_t *running,
 			 struct gnet_dump *d,
 			 struct gnet_stats_basic_cpu __percpu *cpu,
 			 struct gnet_stats_basic_packed *b)
diff -Nur linux-5.4.5/net/kcm/Kconfig linux-5.4.5-new/net/kcm/Kconfig
--- linux-5.4.5/net/kcm/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/kcm/Kconfig	2020-06-15 16:12:30.347699535 +0300
@@ -3,6 +3,7 @@
 config AF_KCM
 	tristate "KCM sockets"
 	depends on INET
+	depends on !PREEMPT_RT
 	select BPF_SYSCALL
 	select STREAM_PARSER
 	---help---
diff -Nur linux-5.4.5/net/Kconfig linux-5.4.5-new/net/Kconfig
--- linux-5.4.5/net/Kconfig	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/Kconfig	2020-06-15 16:12:30.403699338 +0300
@@ -26,6 +26,8 @@
 
 if NET
 
+source "net/rtnet/Kconfig"
+
 config WANT_COMPAT_NETLINK_MESSAGES
 	bool
 	help
@@ -278,7 +280,7 @@
 
 config NET_RX_BUSY_POLL
 	bool
-	default y
+	default y if !PREEMPT_RT
 
 config BQL
 	bool
diff -Nur linux-5.4.5/net/Makefile linux-5.4.5-new/net/Makefile
--- linux-5.4.5/net/Makefile	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/Makefile	2020-06-15 16:12:30.423699268 +0300
@@ -6,11 +6,15 @@
 # Rewritten to use lists instead of if-statements.
 #
 
+ccflags-y += -Inet/rtnet/stack/include
+
 obj-$(CONFIG_NET)		:= socket.o core/
 
 tmp-$(CONFIG_COMPAT) 		:= compat.o
 obj-$(CONFIG_NET)		+= $(tmp-y)
 
+obj-$(CONFIG_XENO_DRIVERS_NET)  += rtnet/
+
 # LLC has to be linked before the files in net/802/
 obj-$(CONFIG_LLC)		+= llc/
 obj-$(CONFIG_NET)		+= ethernet/ 802/ sched/ netlink/ bpf/
diff -Nur linux-5.4.5/net/packet/af_packet.c linux-5.4.5-new/net/packet/af_packet.c
--- linux-5.4.5/net/packet/af_packet.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/packet/af_packet.c	2020-06-15 16:12:31.063697012 +0300
@@ -57,6 +57,7 @@
 #include <linux/if_packet.h>
 #include <linux/wireless.h>
 #include <linux/kernel.h>
+#include <linux/delay.h>
 #include <linux/kmod.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
@@ -659,7 +660,7 @@
 	if (BLOCK_NUM_PKTS(pbd)) {
 		while (atomic_read(&pkc->blk_fill_in_prog)) {
 			/* Waiting for skb_copy_bits to finish... */
-			cpu_relax();
+			cpu_chill();
 		}
 	}
 
@@ -921,7 +922,7 @@
 		if (!(status & TP_STATUS_BLK_TMO)) {
 			while (atomic_read(&pkc->blk_fill_in_prog)) {
 				/* Waiting for skb_copy_bits to finish... */
-				cpu_relax();
+				cpu_chill();
 			}
 		}
 		prb_close_block(pkc, pbd, po, status);
diff -Nur linux-5.4.5/net/rtnet/addons/cap.c linux-5.4.5-new/net/rtnet/addons/cap.c
--- linux-5.4.5/net/rtnet/addons/cap.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/addons/cap.c	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,518 @@
+/***
+ *
+ *  rtcap/rtcap.c
+ *
+ *  Real-Time Capturing Interface
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/sched.h>
+
+#include <rtdev.h>
+#include <rtnet_chrdev.h>
+#include <rtnet_port.h> /* for netdev_priv() */
+
+MODULE_LICENSE("GPL");
+
+static unsigned int rtcap_rtskbs = 128;
+module_param(rtcap_rtskbs, uint, 0444);
+MODULE_PARM_DESC(rtcap_rtskbs, "Number of real-time socket buffers per "
+		 "real-time device");
+
+#define TAP_DEV             1
+#define RTMAC_TAP_DEV       2
+#define XMIT_HOOK           4
+
+static rtdm_nrtsig_t        cap_signal;
+static struct rtskb_queue   cap_queue;
+static struct rtskb_pool   cap_pool;
+
+static struct tap_device_t {
+    struct net_device       *tap_dev;
+    struct net_device       *rtmac_tap_dev;
+    struct net_device_stats tap_dev_stats;
+    int                     present;
+    int                     (*orig_xmit)(struct rtskb *skb,
+					 struct rtnet_device *dev);
+} tap_device[MAX_RT_DEVICES];
+
+
+
+void rtcap_rx_hook(struct rtskb *rtskb)
+{
+    if ((rtskb->cap_comp_skb = rtskb_pool_dequeue(&cap_pool)) == 0) {
+	tap_device[rtskb->rtdev->ifindex].tap_dev_stats.rx_dropped++;
+	return;
+    }
+
+    if (cap_queue.first == NULL)
+	cap_queue.first = rtskb;
+    else
+	cap_queue.last->cap_next = rtskb;
+    cap_queue.last  = rtskb;
+    rtskb->cap_next = NULL;
+
+    rtskb->cap_flags |= RTSKB_CAP_SHARED;
+
+    rtdm_nrtsig_pend(&cap_signal);
+}
+
+
+
+int rtcap_xmit_hook(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tap_device_t *tap_dev = &tap_device[rtskb->rtdev->ifindex];
+    rtdm_lockctx_t      context;
+
+
+    if ((rtskb->cap_comp_skb = rtskb_pool_dequeue(&cap_pool)) == 0) {
+	tap_dev->tap_dev_stats.rx_dropped++;
+	return tap_dev->orig_xmit(rtskb, rtdev);
+    }
+
+    rtskb->cap_next  = NULL;
+    rtskb->cap_start = rtskb->data;
+    rtskb->cap_len   = rtskb->len;
+    rtskb->cap_flags |= RTSKB_CAP_SHARED;
+
+    rtskb->time_stamp = rtdm_clock_read();
+
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+    if (cap_queue.first == NULL)
+	cap_queue.first = rtskb;
+    else
+	cap_queue.last->cap_next = rtskb;
+    cap_queue.last = rtskb;
+
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    rtdm_nrtsig_pend(&cap_signal);
+
+    return tap_dev->orig_xmit(rtskb, rtdev);
+}
+
+
+
+int rtcap_loopback_xmit_hook(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tap_device_t *tap_dev = &tap_device[rtskb->rtdev->ifindex];
+
+
+    rtskb->time_stamp = rtdm_clock_read();
+
+    return tap_dev->orig_xmit(rtskb, rtdev);
+}
+
+
+
+void rtcap_kfree_rtskb(struct rtskb *rtskb)
+{
+    rtdm_lockctx_t  context;
+    struct rtskb    *comp_skb;
+
+
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+    if (rtskb->cap_flags & RTSKB_CAP_SHARED) {
+	rtskb->cap_flags &= ~RTSKB_CAP_SHARED;
+
+	comp_skb = rtskb->cap_comp_skb;
+
+	rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+	rtskb_pool_queue_tail(comp_skb->pool, comp_skb);
+
+	return;
+    }
+
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    rtskb->chain_end = rtskb;
+    rtskb_pool_queue_tail(rtskb->pool, rtskb);
+}
+
+
+
+static void convert_timestamp(nanosecs_abs_t timestamp, struct sk_buff *skb)
+{
+# ifdef CONFIG_KTIME_SCALAR
+    skb->tstamp.tv64 = timestamp;
+# else /* !CONFIG_KTIME_SCALAR */
+    unsigned long rem;
+
+    rem = do_div(timestamp, NSEC_PER_SEC);
+    skb->tstamp = ktime_set((long)timestamp, rem);
+# endif /* !CONFIG_KTIME_SCALAR */
+}
+
+
+
+static void rtcap_signal_handler(rtdm_nrtsig_t *nrtsig, void *arg)
+{
+    struct rtskb            *rtskb;
+    struct sk_buff          *skb;
+    struct sk_buff          *rtmac_skb;
+    struct net_device_stats *stats;
+    int                     ifindex;
+    int                     active;
+    rtdm_lockctx_t          context;
+
+
+    while (1)
+    {
+	rtdm_lock_get_irqsave(&rtcap_lock, context);
+
+	if ((rtskb = cap_queue.first) == NULL) {
+	    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+	    break;
+	}
+
+	cap_queue.first = rtskb->cap_next;
+
+	rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+	ifindex = rtskb->rtdev->ifindex;
+	active  = tap_device[ifindex].present;
+
+	if (active) {
+		if ((tap_device[ifindex].tap_dev->flags & IFF_UP) == 0)
+			active &= ~TAP_DEV;
+		if (active & RTMAC_TAP_DEV &&
+		    !(tap_device[ifindex].rtmac_tap_dev->flags & IFF_UP))
+			active &= ~RTMAC_TAP_DEV;
+	}
+
+	if (active == 0) {
+	    tap_device[ifindex].tap_dev_stats.rx_dropped++;
+	    rtcap_kfree_rtskb(rtskb);
+	    continue;
+	}
+
+	skb = dev_alloc_skb(rtskb->cap_len);
+	if (skb) {
+	    memcpy(skb_put(skb, rtskb->cap_len),
+		   rtskb->cap_start, rtskb->cap_len);
+
+	    if (active & TAP_DEV) {
+		skb->dev      = tap_device[ifindex].tap_dev;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		convert_timestamp(rtskb->time_stamp, skb);
+
+		rtmac_skb = NULL;
+		if ((rtskb->cap_flags & RTSKB_CAP_RTMAC_STAMP) &&
+		    (active & RTMAC_TAP_DEV)) {
+		    rtmac_skb = skb_clone(skb, GFP_ATOMIC);
+		    if (rtmac_skb != NULL)
+			convert_timestamp(rtskb->cap_rtmac_stamp, rtmac_skb);
+		}
+
+		rtcap_kfree_rtskb(rtskb);
+
+		stats = &tap_device[ifindex].tap_dev_stats;
+		stats->rx_packets++;
+		stats->rx_bytes += skb->len;
+
+		if (rtmac_skb != NULL) {
+		    rtmac_skb->dev = tap_device[ifindex].rtmac_tap_dev;
+		    netif_rx(rtmac_skb);
+		}
+		netif_rx(skb);
+	    } else if (rtskb->cap_flags & RTSKB_CAP_RTMAC_STAMP) {
+		skb->dev      = tap_device[ifindex].rtmac_tap_dev;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		convert_timestamp(rtskb->cap_rtmac_stamp, skb);
+
+		rtcap_kfree_rtskb(rtskb);
+
+		stats = &tap_device[ifindex].tap_dev_stats;
+		stats->rx_packets++;
+		stats->rx_bytes += skb->len;
+
+		netif_rx(skb);
+	    } else {
+		dev_kfree_skb(skb);
+		rtcap_kfree_rtskb(rtskb);
+	    }
+	} else {
+	    printk("RTcap: unable to allocate linux skb\n");
+	    rtcap_kfree_rtskb(rtskb);
+	}
+    }
+}
+
+
+
+static int tap_dev_open(struct net_device *dev)
+{
+    int err;
+
+    err = try_module_get(THIS_MODULE);
+    if (err == 0)
+	return -EIDRM;
+
+    memcpy(dev->dev_addr,
+	   (*(struct rtnet_device **)netdev_priv(dev))->dev_addr,
+	   MAX_ADDR_LEN);
+
+    return 0;
+}
+
+static int tap_dev_stop(struct net_device *dev)
+{
+    module_put(THIS_MODULE);
+    return 0;
+}
+
+static int tap_dev_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+    netif_stop_queue(dev);
+    return 1;
+}
+
+
+
+static struct net_device_stats *tap_dev_get_stats(struct net_device *dev)
+{
+    struct rtnet_device *rtdev = *(struct rtnet_device **)netdev_priv(dev);
+
+    return &tap_device[rtdev->ifindex].tap_dev_stats;
+}
+
+
+
+static int tap_dev_change_mtu(struct net_device *dev, int new_mtu)
+{
+    return -EINVAL;
+}
+
+
+
+static const struct net_device_ops tap_netdev_ops = {
+    .ndo_open       = tap_dev_open,
+    .ndo_stop       = tap_dev_stop,
+    .ndo_start_xmit = tap_dev_xmit,
+    .ndo_get_stats  = tap_dev_get_stats,
+    .ndo_change_mtu = tap_dev_change_mtu,
+};
+
+static void tap_dev_setup(struct net_device *dev)
+{
+    ether_setup(dev);
+
+    dev->netdev_ops      = &tap_netdev_ops;
+    dev->mtu             = 1500;
+    dev->flags           &= ~IFF_MULTICAST;
+}
+
+
+
+void cleanup_tap_devices(void)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++)
+	if ((tap_device[i].present & TAP_DEV) != 0) {
+	    if ((tap_device[i].present & XMIT_HOOK) != 0) {
+		rtdev = *(struct rtnet_device **)
+		    netdev_priv(tap_device[i].tap_dev);
+
+		mutex_lock(&rtdev->nrt_lock);
+		rtdev->hard_start_xmit = tap_device[i].orig_xmit;
+		if (rtdev->features & NETIF_F_LLTX)
+		    rtdev->start_xmit = tap_device[i].orig_xmit;
+		mutex_unlock(&rtdev->nrt_lock);
+
+		rtdev_dereference(rtdev);
+	    }
+
+	    if ((tap_device[i].present & RTMAC_TAP_DEV) != 0) {
+		unregister_netdev(tap_device[i].rtmac_tap_dev);
+		free_netdev(tap_device[i].rtmac_tap_dev);
+	    }
+
+	    unregister_netdev(tap_device[i].tap_dev);
+	    free_netdev(tap_device[i].tap_dev);
+	}
+}
+
+
+
+int __init rtcap_init(void)
+{
+    struct rtnet_device *rtdev;
+    struct net_device   *dev;
+    int                 ret;
+    int                 devices = 0;
+    int                 i;
+
+
+    printk("RTcap: real-time capturing interface\n");
+
+    rtskb_queue_init(&cap_queue);
+
+    rtdm_nrtsig_init(&cap_signal, rtcap_signal_handler, NULL);
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	tap_device[i].present = 0;
+
+	rtdev = rtdev_get_by_index(i);
+	if (rtdev != NULL) {
+	    mutex_lock(&rtdev->nrt_lock);
+
+	    if (test_bit(PRIV_FLAG_UP, &rtdev->priv_flags)) {
+		mutex_unlock(&rtdev->nrt_lock);
+		printk("RTcap: %s busy, skipping device!\n", rtdev->name);
+		rtdev_dereference(rtdev);
+		continue;
+	    }
+
+	    if (rtdev->mac_priv != NULL) {
+		mutex_unlock(&rtdev->nrt_lock);
+
+		printk("RTcap: RTmac discipline already active on device %s. "
+		       "Load RTcap before RTmac!\n", rtdev->name);
+
+		rtdev_dereference(rtdev);
+		continue;
+	    }
+
+	    memset(&tap_device[i].tap_dev_stats, 0,
+		   sizeof(struct net_device_stats));
+
+	    dev = alloc_netdev(sizeof(struct rtnet_device *), rtdev->name,
+			    NET_NAME_UNKNOWN, tap_dev_setup);
+	    if (!dev) {
+		ret = -ENOMEM;
+		goto error3;
+	    }
+
+	    tap_device[i].tap_dev = dev;
+	    *(struct rtnet_device **)netdev_priv(dev) = rtdev;
+
+	    ret = register_netdev(dev);
+	    if (ret < 0)
+		goto error3;
+
+	    tap_device[i].present = TAP_DEV;
+
+	    tap_device[i].orig_xmit = rtdev->hard_start_xmit;
+
+	    if ((rtdev->flags & IFF_LOOPBACK) == 0) {
+		dev = alloc_netdev(sizeof(struct rtnet_device *), rtdev->name,
+				NET_NAME_UNKNOWN, tap_dev_setup);
+		if (!dev) {
+		    ret = -ENOMEM;
+		    goto error3;
+		}
+
+		tap_device[i].rtmac_tap_dev = dev;
+		*(struct rtnet_device **)netdev_priv(dev) = rtdev;
+		strncat(dev->name, "-mac", IFNAMSIZ-strlen(dev->name));
+
+		ret = register_netdev(dev);
+		if (ret < 0)
+		    goto error3;
+
+		tap_device[i].present |= RTMAC_TAP_DEV;
+
+		rtdev->hard_start_xmit = rtcap_xmit_hook;
+	    } else
+		rtdev->hard_start_xmit = rtcap_loopback_xmit_hook;
+
+	    /* If the device requires no xmit_lock, start_xmit points equals
+	     * hard_start_xmit => we have to update this as well
+	     */
+	    if (rtdev->features & NETIF_F_LLTX)
+		rtdev->start_xmit = rtdev->hard_start_xmit;
+
+	    tap_device[i].present |= XMIT_HOOK;
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    devices++;
+	}
+    }
+
+    if (devices == 0) {
+	printk("RTcap: no real-time devices found!\n");
+	ret = -ENODEV;
+	goto error2;
+    }
+
+    if (rtskb_module_pool_init(&cap_pool, rtcap_rtskbs * devices) <
+	    rtcap_rtskbs * devices) {
+	rtskb_pool_release(&cap_pool);
+	ret = -ENOMEM;
+	goto error2;
+    }
+
+    /* register capturing handlers with RTnet core
+     * (adding the handler need no locking) */
+    rtcap_handler = rtcap_rx_hook;
+
+    return 0;
+
+  error3:
+    mutex_unlock(&rtdev->nrt_lock);
+    rtdev_dereference(rtdev);
+    printk("RTcap: unable to register %s!\n", dev->name);
+
+  error2:
+    cleanup_tap_devices();
+    rtdm_nrtsig_destroy(&cap_signal);
+
+    return ret;
+}
+
+
+
+void rtcap_cleanup(void)
+{
+    rtdm_lockctx_t  context;
+
+
+    rtdm_nrtsig_destroy(&cap_signal);
+
+    /* unregister capturing handlers
+     * (take lock to avoid any unloading code before handler was left) */
+    rtdm_lock_get_irqsave(&rtcap_lock, context);
+    rtcap_handler = NULL;
+    rtdm_lock_put_irqrestore(&rtcap_lock, context);
+
+    /* empty queue (should be already empty) */
+    rtcap_signal_handler(0, NULL /* we ignore them anyway */);
+
+    cleanup_tap_devices();
+
+    rtskb_pool_release(&cap_pool);
+
+    printk("RTcap: unloaded\n");
+}
+
+
+
+module_init(rtcap_init);
+module_exit(rtcap_cleanup);
diff -Nur linux-5.4.5/net/rtnet/addons/Kconfig linux-5.4.5-new/net/rtnet/addons/Kconfig
--- linux-5.4.5/net/rtnet/addons/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/addons/Kconfig	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,44 @@
+menu "Add-Ons"
+    depends on XENO_DRIVERS_NET
+
+config XENO_DRIVERS_NET_ADDON_RTCAP
+    depends on XENO_DRIVERS_NET && m
+    select ETHERNET
+    tristate "Real-Time Capturing Support"
+    default n
+    ---help---
+    This feature allows to capture real-time packets traversing the RTnet
+    stack. It can both be used to sniff passively on a network (in this
+    case you may want to enable the promisc mode of your real-time NIC via
+    rtifconfig) and to log the traffic the node receives and transmits
+    during normal operation. RTcap consists of additional hooks in the
+    RTnet stack and a separate module as interface to standard network
+    analysis tools like Ethereal.
+
+    For further information see Documentation/README.rtcap.
+
+config XENO_DRIVERS_NET_ADDON_PROXY
+    depends on XENO_DRIVERS_NET_RTIPV4 && m
+    select ETHERNET
+    tristate "IP protocol proxy for Linux"
+    default n
+    ---help---
+    Enables a forward-to-Linux module for all IP protocols that are not
+    handled by the IPv4 implemenation of RTnet (TCP, UDP, etc.). Only use
+    when you know what you are doing - it can easily break your real-time
+    requirements!
+
+    See Documentation/README.rtnetproxy for further information.
+
+config XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    depends on XENO_DRIVERS_NET_ADDON_PROXY
+    bool "Enable ARP handling via protocol proxy"
+    default n
+    ---help---
+    Enables ARP support for the IP protocol proxy. Incoming ARP replies
+    are then delivered to both, the RTnet and the Linux network stack,
+    but only answered by Linux. The IP protocol proxy gets attached to
+    the RTnet device specified by the module parameter "rtdev_attach",
+    rteth0 by default.
+
+endmenu
diff -Nur linux-5.4.5/net/rtnet/addons/Makefile linux-5.4.5-new/net/rtnet/addons/Makefile
--- linux-5.4.5/net/rtnet/addons/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/addons/Makefile	2020-06-15 16:12:31.587695165 +0300
@@ -0,0 +1,9 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP) += rtcap.o
+
+rtcap-y := cap.o
+
+obj-$(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY) += rtnetproxy.o
+
+rtnetproxy-y := proxy.o
diff -Nur linux-5.4.5/net/rtnet/addons/proxy.c linux-5.4.5-new/net/rtnet/addons/proxy.c
--- linux-5.4.5/net/rtnet/addons/proxy.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/addons/proxy.c	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,447 @@
+/* rtnetproxy.c: a Linux network driver that uses the RTnet driver to
+ * transport IP data from/to Linux kernel mode.
+ * This allows the usage of TCP/IP from linux space using via the RTNET
+ * network adapter.
+ *
+ *
+ * Usage:
+ *
+ * insmod rtnetproxy.o    (only after having rtnet up and running)
+ *
+ * ifconfig rtproxy up IP_ADDRESS netmask NETMASK
+ *
+ * Use it like any other network device from linux.
+ *
+ * Restrictions:
+ * Only IPV4 based protocols are supported, UDP and ICMP can be send out
+ * but not received - as these are handled directly by rtnet!
+ *
+ *
+ *
+ * Based on the linux net driver dummy.c by Nick Holloway
+ *
+ *
+ * Changelog:
+ *
+ * 08-Nov-2002  Mathias Koehrer - Clear separation between rtai context and
+ *                                standard linux driver context.
+ *                                Data exchange via ringbuffers.
+ *                                A RTAI thread is used for rtnet transmission.
+ *
+ * 05-Nov-2002  Mathias Koehrer - Initial version!
+ *                                Development based on rtnet 0.2.6,
+ *                                rtai-24.1.10, kernel 2.4.19
+ *
+ *
+ * Mathias Koehrer - mathias_koehrer@yahoo.de
+*/
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <net/ip.h>
+
+#include <linux/if_ether.h> /* For the statistics structure. */
+#include <linux/if_arp.h>   /* For ARPHRD_ETHER */
+
+
+#include <rtdev.h>
+#include <rtskb.h>
+#include <rtdm/driver.h>
+#include <ipv4/ip_input.h>
+#include <ipv4/route.h>
+#include <rtnet_port.h>
+
+
+static struct net_device *dev_rtnetproxy;
+
+/* **************************************************************************
+ *  SKB pool management (JK):
+ * ************************************************************************ */
+#define DEFAULT_PROXY_RTSKBS        32
+
+static unsigned int proxy_rtskbs = DEFAULT_PROXY_RTSKBS;
+module_param(proxy_rtskbs, uint, 0444);
+MODULE_PARM_DESC(proxy_rtskbs, "Number of realtime socket buffers in proxy pool");
+
+static struct rtskb_pool rtskb_pool;
+
+static struct rtskb_queue tx_queue;
+static struct rtskb_queue rx_queue;
+
+/* handle for non-real-time signal */
+static rtdm_nrtsig_t rtnetproxy_rx_signal;
+
+/* Thread for transmission */
+static rtdm_task_t rtnetproxy_tx_task;
+
+static rtdm_event_t rtnetproxy_tx_event;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+static char* rtdev_attach = "rteth0";
+module_param(rtdev_attach, charp, 0444);
+MODULE_PARM_DESC(rtdev_attach, "Attach to the specified RTnet device");
+
+struct rtnet_device *rtnetproxy_rtdev;
+#endif
+
+/* ************************************************************************
+ * ************************************************************************
+ *   T R A N S M I T
+ * ************************************************************************
+ * ************************************************************************ */
+
+static void rtnetproxy_tx_loop(void *arg)
+{
+    struct rtnet_device *rtdev;
+    struct rtskb *rtskb;
+
+    while (!rtdm_task_should_stop()) {
+	if (rtdm_event_wait(&rtnetproxy_tx_event) < 0)
+	    break;
+
+	while ((rtskb = rtskb_dequeue(&tx_queue)) != NULL) {
+	    rtdev = rtskb->rtdev;
+	    rtdev_xmit_proxy(rtskb);
+	    rtdev_dereference(rtdev);
+	}
+    }
+}
+
+
+/* ************************************************************************
+ *  hard_xmit
+ *
+ *  This function runs in linux kernel context and is executed whenever
+ *  there is a frame to be sent out.
+ * ************************************************************************ */
+static int rtnetproxy_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+    struct ethhdr *eth = (struct ethhdr *)skb->data;
+    struct rtskb *rtskb;
+    int len = skb->len;
+#ifndef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    struct dest_route rt;
+    struct iphdr *iph;
+    u32 saddr, daddr;
+#endif
+
+    switch (ntohs(eth->h_proto)) {
+    case ETH_P_IP:
+	 if (len < sizeof(struct ethhdr) + sizeof(struct iphdr))
+	     goto drop1;
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    case ETH_P_ARP:
+#endif
+	break;
+    default:
+drop1:
+	dev->stats.tx_dropped++;
+	dev_kfree_skb(skb);
+	return NETDEV_TX_OK;
+    }
+
+    rtskb = alloc_rtskb(len, &rtskb_pool);
+    if (!rtskb)
+	return NETDEV_TX_BUSY;
+
+    memcpy(rtskb_put(rtskb, len), skb->data, len);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    dev_kfree_skb(skb);
+
+    rtskb->rtdev = rtnetproxy_rtdev;
+    if (rtdev_reference(rtnetproxy_rtdev) == 0) {
+	dev->stats.tx_dropped++;
+	kfree_rtskb(rtskb);
+	return NETDEV_TX_BUSY;
+    }
+
+#else /* !CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP */
+    iph = (struct iphdr *)(skb->data + sizeof(struct ethhdr));
+    saddr = iph->saddr;
+    daddr = iph->daddr;
+
+    dev_kfree_skb(skb);
+
+    if (rt_ip_route_output(&rt, daddr, INADDR_ANY) < 0) {
+drop2:
+	dev->stats.tx_dropped++;
+	kfree_rtskb(rtskb);
+	return NETDEV_TX_OK;
+    }
+    if (rt.rtdev->local_ip != saddr) {
+	rtdev_dereference(rt.rtdev);
+	goto drop2;
+    }
+
+    eth = (struct ethhdr *)rtskb->data;
+    memcpy(eth->h_source, rt.rtdev->dev_addr, rt.rtdev->addr_len);
+    memcpy(eth->h_dest, rt.dev_addr, rt.rtdev->addr_len);
+
+    rtskb->rtdev = rt.rtdev;
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP */
+
+    dev->stats.tx_packets++;
+    dev->stats.tx_bytes += len;
+
+    rtskb_queue_tail(&tx_queue, rtskb);
+    rtdm_event_signal(&rtnetproxy_tx_event);
+
+    return NETDEV_TX_OK;
+}
+
+
+/* ************************************************************************
+ * ************************************************************************
+ *   R E C E I V E
+ * ************************************************************************
+ * ************************************************************************ */
+
+
+/* ************************************************************************
+ * This function runs in real-time context.
+ *
+ * It is called from inside rtnet whenever a packet has been received that
+ * has to be processed by rtnetproxy.
+ * ************************************************************************ */
+static void rtnetproxy_recv(struct rtskb *rtskb)
+{
+    /* Acquire rtskb (JK) */
+    if (rtskb_acquire(rtskb, &rtskb_pool) != 0) {
+	dev_rtnetproxy->stats.rx_dropped++;
+	rtdm_printk("rtnetproxy_recv: No free rtskb in pool\n");
+	kfree_rtskb(rtskb);
+	return;
+    }
+
+    rtskb_queue_tail(&rx_queue, rtskb);
+    rtdm_nrtsig_pend(&rtnetproxy_rx_signal);
+}
+
+
+/* ************************************************************************
+ * This function runs in kernel mode.
+ * It is activated from rtnetproxy_signal_handler whenever rtnet received a
+ * frame to be processed by rtnetproxy.
+ * ************************************************************************ */
+static inline void rtnetproxy_kernel_recv(struct rtskb *rtskb)
+{
+    struct sk_buff *skb;
+    struct net_device *dev = dev_rtnetproxy;
+
+    int header_len = rtskb->rtdev->hard_header_len;
+    int len        = rtskb->len + header_len;
+
+    /* Copy the realtime skb (rtskb) to the standard skb: */
+    skb = dev_alloc_skb(len+2);
+    skb_reserve(skb, 2);
+
+    memcpy(skb_put(skb, len), rtskb->data-header_len, len);
+
+
+    /* Set some relevant entries in the skb: */
+    skb->protocol=eth_type_trans(skb,dev);
+    skb->dev=dev;
+    skb->ip_summed = CHECKSUM_UNNECESSARY;
+    skb->pkt_type = PACKET_HOST;  /* Extremely important! Why?!? */
+
+    /* the rtskb stamp is useless (different clock), get new one */
+    __net_timestamp(skb);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)
+    dev->last_rx = jiffies;
+#endif
+    dev->stats.rx_bytes+=skb->len;
+    dev->stats.rx_packets++;
+
+    netif_rx(skb);  /* pass it to the received stuff */
+
+}
+
+/* ************************************************************************
+ * This function runs in kernel mode.
+ * It is activated from rtnetproxy_recv whenever rtnet received a frame to
+ * be processed by rtnetproxy.
+ * ************************************************************************ */
+static void rtnetproxy_signal_handler(rtdm_nrtsig_t *nrtsig, void *arg)
+{
+    struct rtskb *rtskb;
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	rtnetproxy_kernel_recv(rtskb);
+	kfree_rtskb(rtskb);
+    }
+}
+
+/* ************************************************************************
+ * ************************************************************************
+ *   G E N E R A L
+ * ************************************************************************
+ * ************************************************************************ */
+
+static void fake_multicast_support(struct net_device *dev)
+{
+}
+
+#ifdef CONFIG_NET_FASTROUTE
+static int rtnetproxy_accept_fastpath(struct net_device *dev, struct dst_entry *dst)
+{
+    return -1;
+}
+#endif
+
+static int rtnetproxy_open(struct net_device *dev)
+{
+    int err = try_module_get(THIS_MODULE);
+    if (err == 0)
+	return -EIDRM;
+
+    return 0;
+}
+
+static int rtnetproxy_stop(struct net_device *dev)
+{
+    module_put(THIS_MODULE);
+    return 0;
+}
+
+static const struct net_device_ops rtnetproxy_netdev_ops = {
+    .ndo_open		    = rtnetproxy_open,
+    .ndo_stop		    = rtnetproxy_stop,
+    .ndo_start_xmit         = rtnetproxy_xmit,
+    .ndo_set_rx_mode        = fake_multicast_support,
+};
+
+/* ************************************************************************
+ *  device init
+ * ************************************************************************ */
+static void __init rtnetproxy_init(struct net_device *dev)
+{
+    /* Fill in device structure with ethernet-generic values. */
+    ether_setup(dev);
+
+    dev->tx_queue_len = 0;
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    memcpy(dev->dev_addr, rtnetproxy_rtdev->dev_addr, MAX_ADDR_LEN);
+#else
+    dev->flags |= IFF_NOARP;
+#endif
+    dev->flags &= ~IFF_MULTICAST;
+
+    dev->netdev_ops      = &rtnetproxy_netdev_ops;
+}
+
+/* ************************************************************************
+ * ************************************************************************
+ *   I N I T
+ * ************************************************************************
+ * ************************************************************************ */
+static int __init rtnetproxy_init_module(void)
+{
+    int err;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    if ((rtnetproxy_rtdev = rtdev_get_by_name(rtdev_attach)) == NULL) {
+	printk("Couldn't attach to %s\n", rtdev_attach);
+	return -EINVAL;
+    }
+    printk("RTproxy attached to %s\n", rtdev_attach);
+#endif
+
+    /* Initialize the proxy's rtskb pool (JK) */
+    if (rtskb_module_pool_init(&rtskb_pool, proxy_rtskbs) < proxy_rtskbs) {
+	err = -ENOMEM;
+	goto err1;
+    }
+
+    dev_rtnetproxy = alloc_netdev(0, "rtproxy", NET_NAME_UNKNOWN, rtnetproxy_init);
+    if (!dev_rtnetproxy) {
+	err = -ENOMEM;
+	goto err1;
+    }
+
+    rtdm_nrtsig_init(&rtnetproxy_rx_signal, rtnetproxy_signal_handler, NULL);
+
+    rtskb_queue_init(&tx_queue);
+    rtskb_queue_init(&rx_queue);
+
+    err = register_netdev(dev_rtnetproxy);
+    if (err < 0)
+	goto err3;
+
+    /* Init the task for transmission */
+    rtdm_event_init(&rtnetproxy_tx_event, 0);
+    err = rtdm_task_init(&rtnetproxy_tx_task, "rtnetproxy",
+			 rtnetproxy_tx_loop, 0,
+			 RTDM_TASK_LOWEST_PRIORITY, 0);
+    if (err)
+	goto err4;
+
+    /* Register with RTnet */
+    rt_ip_fallback_handler = rtnetproxy_recv;
+
+    printk("rtnetproxy installed as \"%s\"\n", dev_rtnetproxy->name);
+
+    return 0;
+
+err4:
+    unregister_netdev(dev_rtnetproxy);
+
+err3:
+    rtdm_nrtsig_destroy(&rtnetproxy_rx_signal);
+
+    free_netdev(dev_rtnetproxy);
+
+err1:
+    rtskb_pool_release(&rtskb_pool);
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    rtdev_dereference(rtnetproxy_rtdev);
+#endif
+    return err;
+}
+
+
+static void __exit rtnetproxy_cleanup_module(void)
+{
+    struct rtskb *rtskb;
+
+    /* Unregister the fallback at rtnet */
+    rt_ip_fallback_handler = NULL;
+
+    /* Unregister the net device: */
+    unregister_netdev(dev_rtnetproxy);
+    free_netdev(dev_rtnetproxy);
+
+    rtdm_event_destroy(&rtnetproxy_tx_event);
+    rtdm_task_destroy(&rtnetproxy_tx_task);
+
+    /* free the non-real-time signal */
+    rtdm_nrtsig_destroy(&rtnetproxy_rx_signal);
+
+    while ((rtskb = rtskb_dequeue(&tx_queue)) != NULL) {
+	rtdev_dereference(rtskb->rtdev);
+	kfree_rtskb(rtskb);
+    }
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	kfree_rtskb(rtskb);
+    }
+
+    rtskb_pool_release(&rtskb_pool);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    rtdev_dereference(rtnetproxy_rtdev);
+#endif
+}
+
+module_init(rtnetproxy_init_module);
+module_exit(rtnetproxy_cleanup_module);
+MODULE_LICENSE("GPL");
diff -Nur linux-5.4.5/net/rtnet/drivers/8139too.c linux-5.4.5-new/net/rtnet/drivers/8139too.c
--- linux-5.4.5/net/rtnet/drivers/8139too.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/8139too.c	2020-06-15 16:12:31.483695532 +0300
@@ -0,0 +1,1728 @@
+/***
+ * rt_8139too.c - Realtime driver for
+ * for more information, look to end of file or '8139too.c'
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+ /*
+  * This Version was modified by Fabian Koch
+  * It includes a different implementation of the 'cards' module parameter
+  * we are using an array of integers to determine which cards to use
+  * for RTnet (e.g. cards=0,1,0)
+  *
+  * Thanks to Jan Kiszka for this idea
+  */
+
+#define DRV_NAME            "rt_8139too"
+#define DRV_VERSION         "0.9.24-rt0.7"
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/compiler.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/ethtool.h>
+#include <linux/rtnetlink.h>
+#include <linux/delay.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/completion.h>
+#include <linux/crc32.h>
+#include <linux/uaccess.h>
+#include <asm/io.h>
+
+/* *** RTnet *** */
+#include <rtnet_port.h>
+
+#define MAX_UNITS               8
+#define DEFAULT_RX_POOL_SIZE    16
+
+static int cards[MAX_UNITS] = { [0 ... (MAX_UNITS-1)] = 1 };
+static int media[MAX_UNITS] = { [0 ... (MAX_UNITS-1)] = -1 };
+static unsigned int rx_pool_size = DEFAULT_RX_POOL_SIZE;
+module_param_array(cards, int, NULL, 0444);
+module_param_array(media, int, NULL, 0444);
+module_param(rx_pool_size, uint, 0444);
+MODULE_PARM_DESC(cards, "array of cards to be supported (e.g. 1,0,1)");
+MODULE_PARM_DESC(media, "8139too: Bits 4+9: force full duplex, bit 5: 100Mbps");
+MODULE_PARM_DESC(rx_pool_size, "number of receive buffers");
+
+/* *** RTnet *** */
+
+
+#define RTL8139_DRIVER_NAME   DRV_NAME " Fast Ethernet driver " DRV_VERSION
+#define PFX DRV_NAME ": "
+
+/* enable PIO instead of MMIO, if CONFIG_8139TOO_PIO is selected */
+/* *** RTnet ***
+#ifdef CONFIG_8139TOO_PIO
+#define USE_IO_OPS 1
+#endif
+ *** RTnet *** */
+
+/* Size of the in-memory receive ring. */
+#define RX_BUF_LEN_IDX        2        /* 0==8K, 1==16K, 2==32K, 3==64K */
+#define RX_BUF_LEN        (8192 << RX_BUF_LEN_IDX)
+#define RX_BUF_PAD        16
+#define RX_BUF_WRAP_PAD 2048 /* spare padding to handle lack of packet wrap */
+#define RX_BUF_TOT_LEN        (RX_BUF_LEN + RX_BUF_PAD + RX_BUF_WRAP_PAD)
+
+/* Number of Tx descriptor registers. */
+#define NUM_TX_DESC        4
+
+/* max supported ethernet frame size -- must be at least (rtdev->mtu+14+4).*/
+#define MAX_ETH_FRAME_SIZE        1536
+
+/* Size of the Tx bounce buffers -- must be at least (rtdev->mtu+14+4). */
+#define TX_BUF_SIZE        MAX_ETH_FRAME_SIZE
+#define TX_BUF_TOT_LEN        (TX_BUF_SIZE * NUM_TX_DESC)
+
+/* PCI Tuning Parameters
+   Threshold is bytes transferred to chip before transmission starts. */
+#define TX_FIFO_THRESH 256        /* In bytes, rounded down to 32 byte units. */
+
+/* The following settings are log_2(bytes)-4:  0 == 16 bytes .. 6==1024, 7==end of packet. */
+#define RX_FIFO_THRESH        7        /* Rx buffer level before first PCI xfer.  */
+#define RX_DMA_BURST        7        /* Maximum PCI burst, '6' is 1024 */
+#define TX_DMA_BURST        6        /* Maximum PCI burst, '6' is 1024 */
+#define TX_RETRY        8        /* 0-15.  retries = 16 + (TX_RETRY * 16) */
+
+/* Operational parameters that usually are not changed. */
+/* Time in jiffies before concluding the transmitter is hung. */
+#define TX_TIMEOUT  (6*HZ)
+
+
+enum {
+	HAS_MII_XCVR = 0x010000,
+	HAS_CHIP_XCVR = 0x020000,
+	HAS_LNK_CHNG = 0x040000,
+};
+
+#define RTL_MIN_IO_SIZE 0x80
+#define RTL8139B_IO_SIZE 256
+
+#define RTL8129_CAPS        HAS_MII_XCVR
+#define RTL8139_CAPS        HAS_CHIP_XCVR|HAS_LNK_CHNG
+
+typedef enum {
+	RTL8139 = 0,
+	RTL8139_CB,
+	SMC1211TX,
+	/*MPX5030,*/
+	DELTA8139,
+	ADDTRON8139,
+	DFE538TX,
+	DFE690TXD,
+	FE2000VX,
+	ALLIED8139,
+	RTL8129,
+} board_t;
+
+
+/* indexed by board_t, above */
+static struct {
+	const char *name;
+	u32 hw_flags;
+} board_info[] = {
+	{ "RealTek RTL8139", RTL8139_CAPS },
+	{ "RealTek RTL8129", RTL8129_CAPS },
+};
+
+
+static struct pci_device_id rtl8139_pci_tbl[] = {
+	{0x10ec, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x10ec, 0x8138, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1113, 0x1211, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1500, 0x1360, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x4033, 0x1360, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1186, 0x1300, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1186, 0x1340, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x13d1, 0xab06, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1259, 0xa117, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1259, 0xa11e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x14ea, 0xab06, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x14ea, 0xab07, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x11db, 0x1234, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1432, 0x9130, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x02ac, 0x1012, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x018a, 0x0106, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x126c, 0x1211, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x1743, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+	{0x021b, 0x8139, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+
+#ifdef CONFIG_SH_SECUREEDGE5410
+	/* Bogus 8139 silicon reports 8129 without external PROM :-( */
+	{0x10ec, 0x8129, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8139 },
+#endif
+#ifdef CONFIG_8139TOO_8129
+	{0x10ec, 0x8129, PCI_ANY_ID, PCI_ANY_ID, 0, 0, RTL8129 },
+#endif
+
+	/* some crazy cards report invalid vendor ids like
+	 * 0x0001 here.  The other ids are valid and constant,
+	 * so we simply don't match on the main vendor id.
+	 */
+	{PCI_ANY_ID, 0x8139, 0x10ec, 0x8139, 0, 0, RTL8139 },
+	{PCI_ANY_ID, 0x8139, 0x1186, 0x1300, 0, 0, RTL8139 },
+	{PCI_ANY_ID, 0x8139, 0x13d1, 0xab06, 0, 0, RTL8139 },
+
+	{0,}
+};
+MODULE_DEVICE_TABLE (pci, rtl8139_pci_tbl);
+
+/* The rest of these values should never change. */
+
+/* Symbolic offsets to registers. */
+enum RTL8139_registers {
+	MAC0 = 0,                /* Ethernet hardware address. */
+	MAR0 = 8,                /* Multicast filter. */
+	TxStatus0 = 0x10,        /* Transmit status (Four 32bit registers). */
+	TxAddr0 = 0x20,                /* Tx descriptors (also four 32bit). */
+	RxBuf = 0x30,
+	ChipCmd = 0x37,
+	RxBufPtr = 0x38,
+	RxBufAddr = 0x3A,
+	IntrMask = 0x3C,
+	IntrStatus = 0x3E,
+	TxConfig = 0x40,
+	ChipVersion = 0x43,
+	RxConfig = 0x44,
+	Timer = 0x48,                /* A general-purpose counter. */
+	RxMissed = 0x4C,        /* 24 bits valid, write clears. */
+	Cfg9346 = 0x50,
+	Config0 = 0x51,
+	Config1 = 0x52,
+	FlashReg = 0x54,
+	MediaStatus = 0x58,
+	Config3 = 0x59,
+	Config4 = 0x5A,                /* absent on RTL-8139A */
+	HltClk = 0x5B,
+	MultiIntr = 0x5C,
+	TxSummary = 0x60,
+	BasicModeCtrl = 0x62,
+	BasicModeStatus = 0x64,
+	NWayAdvert = 0x66,
+	NWayLPAR = 0x68,
+	NWayExpansion = 0x6A,
+	/* Undocumented registers, but required for proper operation. */
+	FIFOTMS = 0x70,                /* FIFO Control and test. */
+	CSCR = 0x74,                /* Chip Status and Configuration Register. */
+	PARA78 = 0x78,
+	PARA7c = 0x7c,                /* Magic transceiver parameter register. */
+	Config5 = 0xD8,                /* absent on RTL-8139A */
+};
+
+enum ClearBitMasks {
+	MultiIntrClear = 0xF000,
+	ChipCmdClear = 0xE2,
+	Config1Clear = (1<<7)|(1<<6)|(1<<3)|(1<<2)|(1<<1),
+};
+
+enum ChipCmdBits {
+	CmdReset = 0x10,
+	CmdRxEnb = 0x08,
+	CmdTxEnb = 0x04,
+	RxBufEmpty = 0x01,
+};
+
+/* Interrupt register bits, using my own meaningful names. */
+enum IntrStatusBits {
+	PCIErr = 0x8000,
+	PCSTimeout = 0x4000,
+	RxFIFOOver = 0x40,
+	RxUnderrun = 0x20,
+	RxOverflow = 0x10,
+	TxErr = 0x08,
+	TxOK = 0x04,
+	RxErr = 0x02,
+	RxOK = 0x01,
+
+	RxAckBits = RxFIFOOver | RxOverflow | RxOK,
+};
+
+enum TxStatusBits {
+	TxHostOwns = 0x2000,
+	TxUnderrun = 0x4000,
+	TxStatOK = 0x8000,
+	TxOutOfWindow = 0x20000000,
+	TxAborted = 0x40000000,
+	TxCarrierLost = 0x80000000,
+};
+enum RxStatusBits {
+	RxMulticast = 0x8000,
+	RxPhysical = 0x4000,
+	RxBroadcast = 0x2000,
+	RxBadSymbol = 0x0020,
+	RxRunt = 0x0010,
+	RxTooLong = 0x0008,
+	RxCRCErr = 0x0004,
+	RxBadAlign = 0x0002,
+	RxStatusOK = 0x0001,
+};
+
+/* Bits in RxConfig. */
+enum rx_mode_bits {
+	AcceptErr = 0x20,
+	AcceptRunt = 0x10,
+	AcceptBroadcast = 0x08,
+	AcceptMulticast = 0x04,
+	AcceptMyPhys = 0x02,
+	AcceptAllPhys = 0x01,
+};
+
+/* Bits in TxConfig. */
+enum tx_config_bits {
+
+	/* Interframe Gap Time. Only TxIFG96 doesn't violate IEEE 802.3 */
+	TxIFGShift = 24,
+	TxIFG84 = (0 << TxIFGShift),    /* 8.4us / 840ns (10 / 100Mbps) */
+	TxIFG88 = (1 << TxIFGShift),    /* 8.8us / 880ns (10 / 100Mbps) */
+	TxIFG92 = (2 << TxIFGShift),    /* 9.2us / 920ns (10 / 100Mbps) */
+	TxIFG96 = (3 << TxIFGShift),    /* 9.6us / 960ns (10 / 100Mbps) */
+
+	TxLoopBack = (1 << 18) | (1 << 17), /* enable loopback test mode */
+	TxCRC = (1 << 16),        /* DISABLE appending CRC to end of Tx packets */
+	TxClearAbt = (1 << 0),        /* Clear abort (WO) */
+	TxDMAShift = 8,                /* DMA burst value (0-7) is shifted this many bits */
+	TxRetryShift = 4,        /* TXRR value (0-15) is shifted this many bits */
+
+	TxVersionMask = 0x7C800000, /* mask out version bits 30-26, 23 */
+};
+
+/* Bits in Config1 */
+enum Config1Bits {
+	Cfg1_PM_Enable = 0x01,
+	Cfg1_VPD_Enable = 0x02,
+	Cfg1_PIO = 0x04,
+	Cfg1_MMIO = 0x08,
+	LWAKE = 0x10,                /* not on 8139, 8139A */
+	Cfg1_Driver_Load = 0x20,
+	Cfg1_LED0 = 0x40,
+	Cfg1_LED1 = 0x80,
+	SLEEP = (1 << 1),        /* only on 8139, 8139A */
+	PWRDN = (1 << 0),        /* only on 8139, 8139A */
+};
+
+/* Bits in Config3 */
+enum Config3Bits {
+	Cfg3_FBtBEn    = (1 << 0), /* 1 = Fast Back to Back */
+	Cfg3_FuncRegEn = (1 << 1), /* 1 = enable CardBus Function registers */
+	Cfg3_CLKRUN_En = (1 << 2), /* 1 = enable CLKRUN */
+	Cfg3_CardB_En  = (1 << 3), /* 1 = enable CardBus registers */
+	Cfg3_LinkUp    = (1 << 4), /* 1 = wake up on link up */
+	Cfg3_Magic     = (1 << 5), /* 1 = wake up on Magic Packet (tm) */
+	Cfg3_PARM_En   = (1 << 6), /* 0 = software can set twister parameters */
+	Cfg3_GNTSel    = (1 << 7), /* 1 = delay 1 clock from PCI GNT signal */
+};
+
+/* Bits in Config4 */
+enum Config4Bits {
+	LWPTN = (1 << 2),        /* not on 8139, 8139A */
+};
+
+/* Bits in Config5 */
+enum Config5Bits {
+	Cfg5_PME_STS     = (1 << 0), /* 1 = PCI reset resets PME_Status */
+	Cfg5_LANWake     = (1 << 1), /* 1 = enable LANWake signal */
+	Cfg5_LDPS        = (1 << 2), /* 0 = save power when link is down */
+	Cfg5_FIFOAddrPtr = (1 << 3), /* Realtek internal SRAM testing */
+	Cfg5_UWF         = (1 << 4), /* 1 = accept unicast wakeup frame */
+	Cfg5_MWF         = (1 << 5), /* 1 = accept multicast wakeup frame */
+	Cfg5_BWF         = (1 << 6), /* 1 = accept broadcast wakeup frame */
+};
+
+enum RxConfigBits {
+	/* rx fifo threshold */
+	RxCfgFIFOShift = 13,
+	RxCfgFIFONone = (7 << RxCfgFIFOShift),
+
+	/* Max DMA burst */
+	RxCfgDMAShift = 8,
+	RxCfgDMAUnlimited = (7 << RxCfgDMAShift),
+
+	/* rx ring buffer length */
+	RxCfgRcv8K = 0,
+	RxCfgRcv16K = (1 << 11),
+	RxCfgRcv32K = (1 << 12),
+	RxCfgRcv64K = (1 << 11) | (1 << 12),
+
+	/* Disable packet wrap at end of Rx buffer */
+	RxNoWrap = (1 << 7),
+};
+
+
+/* Twister tuning parameters from RealTek.
+   Completely undocumented, but required to tune bad links. */
+enum CSCRBits {
+	CSCR_LinkOKBit = 0x0400,
+	CSCR_LinkChangeBit = 0x0800,
+	CSCR_LinkStatusBits = 0x0f000,
+	CSCR_LinkDownOffCmd = 0x003c0,
+	CSCR_LinkDownCmd = 0x0f3c0,
+};
+
+
+enum Cfg9346Bits {
+	Cfg9346_Lock = 0x00,
+	Cfg9346_Unlock = 0xC0,
+};
+
+
+#define PARA78_default        0x78fa8388
+#define PARA7c_default        0xcb38de43        /* param[0][3] */
+#define PARA7c_xxx                0xcb38de43
+/*static const unsigned long param[4][4] = {
+	{0xcb39de43, 0xcb39ce43, 0xfb38de03, 0xcb38de43},
+	{0xcb39de43, 0xcb39ce43, 0xcb39ce83, 0xcb39ce83},
+	{0xcb39de43, 0xcb39ce43, 0xcb39ce83, 0xcb39ce83},
+	{0xbb39de43, 0xbb39ce43, 0xbb39ce83, 0xbb39ce83}
+};*/
+
+typedef enum {
+	CH_8139 = 0,
+	CH_8139_K,
+	CH_8139A,
+	CH_8139B,
+	CH_8130,
+	CH_8139C,
+} chip_t;
+
+enum chip_flags {
+	HasHltClk = (1 << 0),
+	HasLWake = (1 << 1),
+};
+
+
+/* directly indexed by chip_t, above */
+const static struct {
+	const char *name;
+	u8 version; /* from RTL8139C docs */
+	u32 flags;
+} rtl_chip_info[] = {
+	{ "RTL-8139",
+	  0x40,
+	  HasHltClk,
+	},
+
+	{ "RTL-8139 rev K",
+	  0x60,
+	  HasHltClk,
+	},
+
+	{ "RTL-8139A",
+	  0x70,
+	  HasHltClk, /* XXX undocumented? */
+	},
+
+	{ "RTL-8139A rev G",
+	  0x72,
+	  HasHltClk, /* XXX undocumented? */
+	},
+
+	{ "RTL-8139B",
+	  0x78,
+	  HasLWake,
+	},
+
+	{ "RTL-8130",
+	  0x7C,
+	  HasLWake,
+	},
+
+	{ "RTL-8139C",
+	  0x74,
+	  HasLWake,
+	},
+
+	{ "RTL-8100",
+	  0x7A,
+	  HasLWake,
+	 },
+
+	{ "RTL-8100B/8139D",
+	  0x75,
+	  HasHltClk /* XXX undocumented? */
+	  | HasLWake,
+	},
+
+	{ "RTL-8101",
+	  0x77,
+	  HasLWake,
+	},
+};
+
+struct rtl_extra_stats {
+	unsigned long early_rx;
+	unsigned long tx_buf_mapped;
+	unsigned long tx_timeouts;
+	unsigned long rx_lost_in_ring;
+};
+
+struct rtl8139_private {
+	void *mmio_addr;
+	int drv_flags;
+	struct pci_dev *pci_dev;
+	struct net_device_stats stats;
+	unsigned char *rx_ring;
+	unsigned int cur_rx;        /* Index into the Rx buffer of next Rx pkt. */
+	unsigned int tx_flag;
+	unsigned long cur_tx;
+	unsigned long dirty_tx;
+	unsigned char *tx_buf[NUM_TX_DESC];        /* Tx bounce buffers */
+	unsigned char *tx_bufs;        /* Tx bounce buffer region. */
+	dma_addr_t rx_ring_dma;
+	dma_addr_t tx_bufs_dma;
+	signed char phys[4];                /* MII device addresses. */
+	char twistie, twist_row, twist_col;        /* Twister tune state. */
+	unsigned int default_port:4;        /* Last rtdev->if_port value. */
+	unsigned int medialock:1;        /* Don't sense media type. */
+	raw_spinlock_t lock;
+	chip_t chipset;
+	pid_t thr_pid;
+	u32 rx_config;
+	struct rtl_extra_stats xstats;
+	int time_to_die;
+	struct mii_if_info mii;
+};
+
+MODULE_AUTHOR ("Jeff Garzik <jgarzik@mandrakesoft.com>");
+MODULE_DESCRIPTION ("RealTek RTL-8139 Fast Ethernet driver");
+MODULE_LICENSE("GPL");
+
+static int read_eeprom (void *ioaddr, int location, int addr_len);
+static int mdio_read (struct rtnet_device *rtdev, int phy_id, int location);
+static void mdio_write (struct rtnet_device *rtdev, int phy_id, int location, int val);
+
+
+static int rtl8139_open (struct rtnet_device *rtdev);
+static int rtl8139_close (struct rtnet_device *rtdev);
+static irqreturn_t rtl8139_interrupt(int irq, void *data);
+static int rtl8139_start_xmit (struct rtskb *skb, struct rtnet_device *rtdev);
+
+static int rtl8139_ioctl(struct rtnet_device *, struct ifreq *rq, int cmd);
+static struct net_device_stats *rtl8139_get_stats(struct rtnet_device*rtdev);
+
+static void rtl8139_init_ring (struct rtnet_device *rtdev);
+static void rtl8139_set_rx_mode (struct rtnet_device *rtdev);
+static void __set_rx_mode (struct rtnet_device *rtdev);
+static void rtl8139_hw_start (struct rtnet_device *rtdev);
+
+#ifdef USE_IO_OPS
+
+#define RTL_R8(reg)                inb (((unsigned long)ioaddr) + (reg))
+#define RTL_R16(reg)                inw (((unsigned long)ioaddr) + (reg))
+#define RTL_R32(reg)                inl (((unsigned long)ioaddr) + (reg))
+#define RTL_W8(reg, val8)        outb ((val8), ((unsigned long)ioaddr) + (reg))
+#define RTL_W16(reg, val16)        outw ((val16), ((unsigned long)ioaddr) + (reg))
+#define RTL_W32(reg, val32)        outl ((val32), ((unsigned long)ioaddr) + (reg))
+#define RTL_W8_F                RTL_W8
+#define RTL_W16_F                RTL_W16
+#define RTL_W32_F                RTL_W32
+#undef readb
+#undef readw
+#undef readl
+#undef writeb
+#undef writew
+#undef writel
+#define readb(addr) inb((unsigned long)(addr))
+#define readw(addr) inw((unsigned long)(addr))
+#define readl(addr) inl((unsigned long)(addr))
+#define writeb(val,addr) outb((val),(unsigned long)(addr))
+#define writew(val,addr) outw((val),(unsigned long)(addr))
+#define writel(val,addr) outl((val),(unsigned long)(addr))
+
+#else
+
+/* write MMIO register, with flush */
+/* Flush avoids rtl8139 bug w/ posted MMIO writes */
+#define RTL_W8_F(reg, val8)        do { writeb ((val8), ioaddr + (reg)); readb (ioaddr + (reg)); } while (0)
+#define RTL_W16_F(reg, val16)        do { writew ((val16), ioaddr + (reg)); readw (ioaddr + (reg)); } while (0)
+#define RTL_W32_F(reg, val32)        do { writel ((val32), ioaddr + (reg)); readl (ioaddr + (reg)); } while (0)
+
+
+#define MMIO_FLUSH_AUDIT_COMPLETE 1
+#if MMIO_FLUSH_AUDIT_COMPLETE
+
+/* write MMIO register */
+#define RTL_W8(reg, val8)        writeb ((val8), ioaddr + (reg))
+#define RTL_W16(reg, val16)        writew ((val16), ioaddr + (reg))
+#define RTL_W32(reg, val32)        writel ((val32), ioaddr + (reg))
+
+#else
+
+/* write MMIO register, then flush */
+#define RTL_W8                RTL_W8_F
+#define RTL_W16                RTL_W16_F
+#define RTL_W32                RTL_W32_F
+
+#endif /* MMIO_FLUSH_AUDIT_COMPLETE */
+
+/* read MMIO register */
+#define RTL_R8(reg)                readb (ioaddr + (reg))
+#define RTL_R16(reg)                readw (ioaddr + (reg))
+#define RTL_R32(reg)                readl (ioaddr + (reg))
+
+#endif /* USE_IO_OPS */
+
+
+static const u16 rtl8139_intr_mask =
+	PCIErr | PCSTimeout | RxUnderrun | RxOverflow | RxFIFOOver |
+	TxErr | TxOK | RxErr | RxOK;
+
+static const unsigned int rtl8139_rx_config =
+	RxCfgRcv32K | RxNoWrap |
+	(RX_FIFO_THRESH << RxCfgFIFOShift) |
+	(RX_DMA_BURST << RxCfgDMAShift);
+
+static const unsigned int rtl8139_tx_config =
+	TxIFG96 | (TX_DMA_BURST << TxDMAShift) | (TX_RETRY << TxRetryShift);
+
+
+
+
+static void rtl8139_chip_reset (void *ioaddr)
+{
+	int i;
+
+	/* Soft reset the chip. */
+	RTL_W8 (ChipCmd, CmdReset);
+
+	/* Check that the chip has finished the reset. */
+	for (i = 1000; i > 0; i--) {
+		barrier();
+		if ((RTL_R8 (ChipCmd) & CmdReset) == 0)
+			break;
+		udelay (10);
+	}
+}
+
+
+static int rtl8139_init_board (struct pci_dev *pdev,
+					 struct rtnet_device **dev_out)
+{
+	void *ioaddr;
+	struct rtnet_device *rtdev;
+	struct rtl8139_private *tp;
+	u8 tmp8;
+	int rc;
+	unsigned int i;
+#ifdef USE_IO_OPS
+	u32 pio_start, pio_end, pio_flags, pio_len;
+#endif
+	unsigned long mmio_start, mmio_flags, mmio_len;
+	u32 tmp;
+
+
+	*dev_out = NULL;
+
+	/* dev and rtdev->priv zeroed in alloc_etherdev */
+	rtdev=rt_alloc_etherdev(sizeof (struct rtl8139_private),
+				rx_pool_size + NUM_TX_DESC);
+	if (rtdev==NULL) {
+		printk (KERN_ERR "%s: Unable to alloc new net device\n", pci_name(pdev));
+		return -ENOMEM;
+	}
+	rtdev_alloc_name(rtdev, "rteth%d");
+
+	rt_rtdev_connect(rtdev, &RTDEV_manager);
+
+	rtdev->vers = RTDEV_VERS_2_0;
+	tp = rtdev->priv;
+	tp->pci_dev = pdev;
+
+	/* enable device (incl. PCI PM wakeup and hotplug setup) */
+	rc = pci_enable_device (pdev);
+	if (rc)
+		goto err_out;
+
+	rc = pci_request_regions (pdev, "rtnet8139too");
+	if (rc)
+		goto err_out;
+
+	/* enable PCI bus-mastering */
+	pci_set_master (pdev);
+
+	mmio_start = pci_resource_start (pdev, 1);
+	mmio_flags = pci_resource_flags (pdev, 1);
+	mmio_len = pci_resource_len (pdev, 1);
+
+	/* set this immediately, we need to know before
+	 * we talk to the chip directly */
+#ifdef USE_IO_OPS
+	pio_start = pci_resource_start (pdev, 0);
+	pio_end = pci_resource_end (pdev, 0);
+	pio_flags = pci_resource_flags (pdev, 0);
+	pio_len = pci_resource_len (pdev, 0);
+
+	/* make sure PCI base addr 0 is PIO */
+	if (!(pio_flags & IORESOURCE_IO)) {
+		printk (KERN_ERR "%s: region #0 not a PIO resource, aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+	/* check for weird/broken PCI region reporting */
+	if (pio_len < RTL_MIN_IO_SIZE) {
+		printk (KERN_ERR "%s: Invalid PCI I/O region size(s), aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+#else
+	/* make sure PCI base addr 1 is MMIO */
+	if (!(mmio_flags & IORESOURCE_MEM)) {
+		printk(KERN_ERR "%s: region #1 not an MMIO resource, aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+	if (mmio_len < RTL_MIN_IO_SIZE) {
+		printk(KERN_ERR "%s: Invalid PCI mem region size(s), aborting\n", pci_name(pdev));
+		rc = -ENODEV;
+		goto err_out;
+	}
+#endif
+
+#ifdef USE_IO_OPS
+	ioaddr = (void *) pio_start;
+	rtdev->base_addr = pio_start;
+	tp->mmio_addr = ioaddr;
+#else
+	/* ioremap MMIO region */
+	ioaddr = ioremap (mmio_start, mmio_len);
+	if (ioaddr == NULL) {
+		printk(KERN_ERR "%s: cannot remap MMIO, aborting\n", pci_name(pdev));
+		rc = -EIO;
+		goto err_out;
+	}
+	rtdev->base_addr = (long) ioaddr;
+	tp->mmio_addr = ioaddr;
+#endif /* USE_IO_OPS */
+
+	/* Bring old chips out of low-power mode. */
+	RTL_W8 (HltClk, 'R');
+
+	/* check for missing/broken hardware */
+	if (RTL_R32 (TxConfig) == 0xFFFFFFFF) {
+		printk(KERN_ERR "%s: Chip not responding, ignoring board\n", pci_name(pdev));
+		rc = -EIO;
+		goto err_out;
+	}
+
+	/* identify chip attached to board */
+	tmp = RTL_R8 (ChipVersion);
+	for (i = 0; i < ARRAY_SIZE (rtl_chip_info); i++)
+		if (tmp == rtl_chip_info[i].version) {
+			tp->chipset = i;
+			goto match;
+		}
+
+	printk("rt8139too: unknown chip version, assuming RTL-8139\n");
+	printk("rt8139too: TxConfig = 0x%08x\n", RTL_R32 (TxConfig));
+
+	tp->chipset = 0;
+
+match:
+	if (tp->chipset >= CH_8139B) {
+		u8 new_tmp8 = tmp8 = RTL_R8 (Config1);
+		if ((rtl_chip_info[tp->chipset].flags & HasLWake) &&
+		    (tmp8 & LWAKE))
+			new_tmp8 &= ~LWAKE;
+		new_tmp8 |= Cfg1_PM_Enable;
+		if (new_tmp8 != tmp8) {
+			RTL_W8 (Cfg9346, Cfg9346_Unlock);
+			RTL_W8 (Config1, tmp8);
+			RTL_W8 (Cfg9346, Cfg9346_Lock);
+		}
+		if (rtl_chip_info[tp->chipset].flags & HasLWake) {
+			tmp8 = RTL_R8 (Config4);
+			if (tmp8 & LWPTN) {
+				RTL_W8 (Cfg9346, Cfg9346_Unlock);
+				RTL_W8 (Config4, tmp8 & ~LWPTN);
+				RTL_W8 (Cfg9346, Cfg9346_Lock);
+			}
+		}
+	} else {
+		tmp8 = RTL_R8 (Config1);
+		tmp8 &= ~(SLEEP | PWRDN);
+		RTL_W8 (Config1, tmp8);
+	}
+
+	rtl8139_chip_reset (ioaddr);
+
+	*dev_out = rtdev;
+	return 0;
+
+err_out:
+#ifndef USE_IO_OPS
+	if (tp->mmio_addr) iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+	/* it's ok to call this even if we have no regions to free */
+	pci_release_regions (pdev);
+	rtdev_free(rtdev);
+	pci_set_drvdata (pdev, NULL);
+
+	return rc;
+}
+
+
+
+
+static int rtl8139_init_one (struct pci_dev *pdev,
+				       const struct pci_device_id *ent)
+{
+	struct rtnet_device *rtdev = NULL;
+	struct rtl8139_private *tp;
+	int i, addr_len;
+	int option;
+	void *ioaddr;
+	static int board_idx = -1;
+
+	board_idx++;
+
+	if( cards[board_idx] == 0)
+		return -ENODEV;
+
+	/* when we're built into the kernel, the driver version message
+	 * is only printed if at least one 8139 board has been found
+	 */
+#ifndef MODULE
+	{
+		static int printed_version;
+		if (!printed_version++)
+			printk (KERN_INFO RTL8139_DRIVER_NAME "\n");
+	}
+#endif
+
+	if ((i=rtl8139_init_board (pdev, &rtdev)) < 0)
+		return i;
+
+
+	tp = rtdev->priv;
+	ioaddr = tp->mmio_addr;
+
+	addr_len = read_eeprom (ioaddr, 0, 8) == 0x8129 ? 8 : 6;
+	for (i = 0; i < 3; i++)
+		((u16 *) (rtdev->dev_addr))[i] =
+		    le16_to_cpu (read_eeprom (ioaddr, i + 7, addr_len));
+
+	/* The Rtl8139-specific entries in the device structure. */
+	rtdev->open = rtl8139_open;
+	rtdev->stop = rtl8139_close;
+	rtdev->hard_header = &rt_eth_header;
+	rtdev->hard_start_xmit = rtl8139_start_xmit;
+	rtdev->do_ioctl = rtl8139_ioctl;
+	rtdev->get_stats = rtl8139_get_stats;
+
+	/*rtdev->set_multicast_list = rtl8139_set_rx_mode; */
+	rtdev->features |= NETIF_F_SG|NETIF_F_HW_CSUM;
+
+	rtdev->irq = pdev->irq;
+
+	/* rtdev->priv/tp zeroed and aligned in init_etherdev */
+	tp = rtdev->priv;
+
+	/* note: tp->chipset set in rtl8139_init_board */
+	tp->drv_flags = board_info[ent->driver_data].hw_flags;
+	tp->mmio_addr = ioaddr;
+	raw_spin_lock_init(&tp->lock);
+
+	if ( (i=rt_register_rtnetdev(rtdev)) )
+		goto err_out;
+
+	pci_set_drvdata (pdev, rtdev);
+
+	tp->phys[0] = 32;
+
+	/* The lower four bits are the media type. */
+	option = (board_idx >= MAX_UNITS) ? 0 : media[board_idx];
+	if (option > 0) {
+		tp->mii.full_duplex = (option & 0x210) ? 1 : 0;
+		tp->default_port = option & 0xFF;
+		if (tp->default_port)
+			tp->medialock = 1;
+	}
+	if (tp->default_port) {
+		printk(KERN_INFO "  Forcing %dMbps %s-duplex operation.\n",
+			    (option & 0x20 ? 100 : 10),
+			    (option & 0x10 ? "full" : "half"));
+		mdio_write(rtdev, tp->phys[0], 0,
+				   ((option & 0x20) ? 0x2000 : 0) |         /* 100Mbps? */
+				   ((option & 0x10) ? 0x0100 : 0)); /* Full duplex? */
+	}
+
+
+	/* Put the chip into low-power mode. */
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'H');        /* 'R' would leave the clock running. */
+
+	return 0;
+
+
+err_out:
+#ifndef USE_IO_OPS
+	if (tp->mmio_addr) iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+	/* it's ok to call this even if we have no regions to free */
+	pci_release_regions (pdev);
+	rtdev_free(rtdev);
+	pci_set_drvdata (pdev, NULL);
+
+	return i;
+}
+
+
+static void rtl8139_remove_one (struct pci_dev *pdev)
+{
+	struct rtnet_device *rtdev = pci_get_drvdata(pdev);
+
+#ifndef USE_IO_OPS
+	struct rtl8139_private *tp = rtdev->priv;
+
+	if (tp->mmio_addr)
+		iounmap (tp->mmio_addr);
+#endif /* !USE_IO_OPS */
+
+	/* it's ok to call this even if we have no regions to free */
+	rt_unregister_rtnetdev(rtdev);
+	rt_rtdev_disconnect(rtdev);
+
+	pci_release_regions(pdev);
+	pci_set_drvdata(pdev, NULL);
+
+	rtdev_free(rtdev);
+}
+
+
+/* Serial EEPROM section. */
+
+/*  EEPROM_Ctrl bits. */
+#define EE_SHIFT_CLK        0x04        /* EEPROM shift clock. */
+#define EE_CS                        0x08        /* EEPROM chip select. */
+#define EE_DATA_WRITE        0x02        /* EEPROM chip data in. */
+#define EE_WRITE_0                0x00
+#define EE_WRITE_1                0x02
+#define EE_DATA_READ        0x01        /* EEPROM chip data out. */
+#define EE_ENB                        (0x80 | EE_CS)
+
+/* Delay between EEPROM clock transitions.
+   No extra delay is needed with 33Mhz PCI, but 66Mhz may change this.
+ */
+
+#define eeprom_delay()        readl(ee_addr)
+
+/* The EEPROM commands include the alway-set leading bit. */
+#define EE_WRITE_CMD        (5)
+#define EE_READ_CMD                (6)
+#define EE_ERASE_CMD        (7)
+
+static int read_eeprom (void *ioaddr, int location, int addr_len)
+{
+	int i;
+	unsigned retval = 0;
+	void *ee_addr = ioaddr + Cfg9346;
+	int read_cmd = location | (EE_READ_CMD << addr_len);
+
+	writeb (EE_ENB & ~EE_CS, ee_addr);
+	writeb (EE_ENB, ee_addr);
+	eeprom_delay ();
+
+	/* Shift the read command bits out. */
+	for (i = 4 + addr_len; i >= 0; i--) {
+		int dataval = (read_cmd & (1 << i)) ? EE_DATA_WRITE : 0;
+		writeb (EE_ENB | dataval, ee_addr);
+		eeprom_delay ();
+		writeb (EE_ENB | dataval | EE_SHIFT_CLK, ee_addr);
+		eeprom_delay ();
+	}
+	writeb (EE_ENB, ee_addr);
+	eeprom_delay ();
+
+	for (i = 16; i > 0; i--) {
+		writeb (EE_ENB | EE_SHIFT_CLK, ee_addr);
+		eeprom_delay ();
+		retval =
+		    (retval << 1) | ((readb (ee_addr) & EE_DATA_READ) ? 1 :
+				     0);
+		writeb (EE_ENB, ee_addr);
+		eeprom_delay ();
+	}
+
+	/* Terminate the EEPROM access. */
+	writeb (~EE_CS, ee_addr);
+	eeprom_delay ();
+
+	return retval;
+}
+
+/* MII serial management: mostly bogus for now. */
+/* Read and write the MII management registers using software-generated
+   serial MDIO protocol.
+   The maximum data clock rate is 2.5 Mhz.  The minimum timing is usually
+   met by back-to-back PCI I/O cycles, but we insert a delay to avoid
+   "overclocking" issues. */
+#define MDIO_DIR                0x80
+#define MDIO_DATA_OUT        0x04
+#define MDIO_DATA_IN        0x02
+#define MDIO_CLK                0x01
+#define MDIO_WRITE0 (MDIO_DIR)
+#define MDIO_WRITE1 (MDIO_DIR | MDIO_DATA_OUT)
+
+#define mdio_delay(mdio_addr)        readb(mdio_addr)
+
+
+
+static char mii_2_8139_map[8] = {
+	BasicModeCtrl,
+	BasicModeStatus,
+	0,
+	0,
+	NWayAdvert,
+	NWayLPAR,
+	NWayExpansion,
+	0
+};
+
+#ifdef CONFIG_8139TOO_8129
+/* Syncronize the MII management interface by shifting 32 one bits out. */
+static void mdio_sync (void *mdio_addr)
+{
+	int i;
+
+	for (i = 32; i >= 0; i--) {
+		writeb (MDIO_WRITE1, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_WRITE1 | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+}
+#endif
+
+
+static int mdio_read (struct rtnet_device *rtdev, int phy_id, int location)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int retval = 0;
+#ifdef CONFIG_8139TOO_8129
+	void *mdio_addr = tp->mmio_addr + Config4;
+	int mii_cmd = (0xf6 << 10) | (phy_id << 5) | location;
+	int i;
+#endif
+
+	if (phy_id > 31) {        /* Really a 8139.  Use internal registers. */
+		return location < 8 && mii_2_8139_map[location] ?
+		    readw (tp->mmio_addr + mii_2_8139_map[location]) : 0;
+	}
+
+#ifdef CONFIG_8139TOO_8129
+	mdio_sync (mdio_addr);
+	/* Shift the read command bits out. */
+	for (i = 15; i >= 0; i--) {
+		int dataval = (mii_cmd & (1 << i)) ? MDIO_DATA_OUT : 0;
+
+		writeb (MDIO_DIR | dataval, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_DIR | dataval | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+
+	/* Read the two transition, 16 data, and wire-idle bits. */
+	for (i = 19; i > 0; i--) {
+		writeb (0, mdio_addr);
+		mdio_delay (mdio_addr);
+		retval = (retval << 1) | ((readb (mdio_addr) & MDIO_DATA_IN) ? 1 : 0);
+		writeb (MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+#endif
+
+	return (retval >> 1) & 0xffff;
+}
+
+
+static void mdio_write (struct rtnet_device *rtdev, int phy_id, int location,
+			int value)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+#ifdef CONFIG_8139TOO_8129
+	void *mdio_addr = tp->mmio_addr + Config4;
+	int mii_cmd = (0x5002 << 16) | (phy_id << 23) | (location << 18) | value;
+	int i;
+#endif
+
+	if (phy_id > 31) {        /* Really a 8139.  Use internal registers. */
+		void *ioaddr = tp->mmio_addr;
+		if (location == 0) {
+			RTL_W8 (Cfg9346, Cfg9346_Unlock);
+			RTL_W16 (BasicModeCtrl, value);
+			RTL_W8 (Cfg9346, Cfg9346_Lock);
+		} else if (location < 8 && mii_2_8139_map[location])
+			RTL_W16 (mii_2_8139_map[location], value);
+		return;
+	}
+
+#ifdef CONFIG_8139TOO_8129
+	mdio_sync (mdio_addr);
+
+	/* Shift the command bits out. */
+	for (i = 31; i >= 0; i--) {
+		int dataval =
+		    (mii_cmd & (1 << i)) ? MDIO_WRITE1 : MDIO_WRITE0;
+		writeb (dataval, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (dataval | MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+	/* Clear out extra bits. */
+	for (i = 2; i > 0; i--) {
+		writeb (0, mdio_addr);
+		mdio_delay (mdio_addr);
+		writeb (MDIO_CLK, mdio_addr);
+		mdio_delay (mdio_addr);
+	}
+#endif
+}
+
+static int rtl8139_open (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int retval;
+
+	rt_stack_connect(rtdev, &STACK_manager);
+
+	retval = request_irq(rtdev->irq, rtl8139_interrupt,
+		IRQF_NO_THREAD | IRQF_SHARED, rtdev->name, rtdev);
+	if (retval)
+		return retval;
+
+	tp->tx_bufs = pci_alloc_consistent(tp->pci_dev, TX_BUF_TOT_LEN, &tp->tx_bufs_dma);
+	tp->rx_ring = pci_alloc_consistent(tp->pci_dev, RX_BUF_TOT_LEN, &tp->rx_ring_dma);
+
+	if (tp->tx_bufs == NULL || tp->rx_ring == NULL) {
+		free_irq(rtdev->irq, rtdev);
+		if (tp->tx_bufs)
+			pci_free_consistent(tp->pci_dev, TX_BUF_TOT_LEN, tp->tx_bufs, tp->tx_bufs_dma);
+		if (tp->rx_ring)
+			pci_free_consistent(tp->pci_dev, RX_BUF_TOT_LEN, tp->rx_ring, tp->rx_ring_dma);
+
+		return -ENOMEM;
+	}
+	/* FIXME: create wrapper for duplex_lock vs. force_media
+	   tp->mii.full_duplex = tp->mii.duplex_lock; */
+	tp->tx_flag = (TX_FIFO_THRESH << 11) & 0x003f0000;
+	tp->twistie = 1;
+	tp->time_to_die = 0;
+
+	rtl8139_init_ring (rtdev);
+	rtl8139_hw_start (rtdev);
+
+	return 0;
+}
+
+
+static void rtl_check_media (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	u16 mii_lpa;
+
+	if (tp->phys[0] < 0)
+		return;
+
+	mii_lpa = mdio_read(rtdev, tp->phys[0], MII_LPA);
+	if (mii_lpa == 0xffff)
+		return;
+
+	tp->mii.full_duplex = (mii_lpa & LPA_100FULL) == LPA_100FULL ||
+		(mii_lpa & 0x00C0) == LPA_10FULL;
+}
+
+
+/* Start the hardware at open or resume. */
+static void rtl8139_hw_start (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	u32 i;
+	u8 tmp;
+
+	/* Bring old chips out of low-power mode. */
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'R');
+
+	rtl8139_chip_reset(ioaddr);
+
+	/* unlock Config[01234] and BMCR register writes */
+	RTL_W8_F (Cfg9346, Cfg9346_Unlock);
+	/* Restore our idea of the MAC address. */
+	RTL_W32_F (MAC0 + 0, cpu_to_le32 (*(u32 *) (rtdev->dev_addr + 0)));
+	RTL_W32_F (MAC0 + 4, cpu_to_le32 (*(u32 *) (rtdev->dev_addr + 4)));
+
+	tp->cur_rx = 0;
+
+	/* init Rx ring buffer DMA address */
+	RTL_W32_F (RxBuf, tp->rx_ring_dma);
+
+	/* Must enable Tx/Rx before setting transfer thresholds! */
+	RTL_W8 (ChipCmd, CmdRxEnb | CmdTxEnb);
+
+	tp->rx_config = rtl8139_rx_config | AcceptBroadcast | AcceptMyPhys;
+	RTL_W32 (RxConfig, tp->rx_config);
+
+	/* Check this value: the documentation for IFG contradicts ifself. */
+	RTL_W32 (TxConfig, rtl8139_tx_config);
+
+	rtl_check_media (rtdev);
+
+	if (tp->chipset >= CH_8139B) {
+		/* Disable magic packet scanning, which is enabled
+		 * when PM is enabled in Config1.  It can be reenabled
+		 * via ETHTOOL_SWOL if desired.  */
+		RTL_W8 (Config3, RTL_R8 (Config3) & ~Cfg3_Magic);
+	}
+
+	/* Lock Config[01234] and BMCR register writes */
+	RTL_W8 (Cfg9346, Cfg9346_Lock);
+
+	/* init Tx buffer DMA addresses */
+	for (i = 0; i < NUM_TX_DESC; i++)
+		RTL_W32_F (TxAddr0 + (i * 4), tp->tx_bufs_dma + (tp->tx_buf[i] - tp->tx_bufs));
+
+	RTL_W32 (RxMissed, 0);
+
+	rtl8139_set_rx_mode (rtdev);
+
+	/* no early-rx interrupts */
+	RTL_W16 (MultiIntr, RTL_R16 (MultiIntr) & MultiIntrClear);
+
+	/* make sure RxTx has started */
+	tmp = RTL_R8 (ChipCmd);
+	if ((!(tmp & CmdRxEnb)) || (!(tmp & CmdTxEnb)))
+		RTL_W8 (ChipCmd, CmdRxEnb | CmdTxEnb);
+
+	/* Enable all known interrupts by setting the interrupt mask. */
+	RTL_W16 (IntrMask, rtl8139_intr_mask);
+
+	rtnetif_start_queue (rtdev);
+}
+
+
+/* Initialize the Rx and Tx rings, along with various 'dev' bits. */
+static void rtl8139_init_ring (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	int i;
+
+	tp->cur_rx = 0;
+	tp->cur_tx = 0;
+	tp->dirty_tx = 0;
+
+	for (i = 0; i < NUM_TX_DESC; i++)
+		tp->tx_buf[i] = &tp->tx_bufs[i * TX_BUF_SIZE];
+}
+
+
+static void rtl8139_tx_clear (struct rtl8139_private *tp)
+{
+	tp->cur_tx = 0;
+	tp->dirty_tx = 0;
+
+	/* XXX account for unsent Tx packets in tp->stats.tx_dropped */
+}
+
+
+
+static int rtl8139_start_xmit (struct rtskb *skb, struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+
+	void *ioaddr = tp->mmio_addr;
+	unsigned int entry;
+	unsigned int len = skb->len;
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&tp->lock, context);
+	/* Calculate the next Tx descriptor entry. */
+	entry = tp->cur_tx % NUM_TX_DESC;
+
+	if (likely(len < TX_BUF_SIZE)) {
+		if (unlikely(skb->xmit_stamp != NULL)) {
+			//raw_spin_lock_irqsave(&tp->lock, context);
+			*skb->xmit_stamp = cpu_to_be64(ktime_get() +
+						       *skb->xmit_stamp);
+			/* typically, we are only copying a few bytes here */
+			rtskb_copy_and_csum_dev(skb, tp->tx_buf[entry]);
+		} else {
+			/* copy larger packets outside the lock */
+			rtskb_copy_and_csum_dev(skb, tp->tx_buf[entry]);
+			//raw_spin_lock_irqsave(&tp->lock, context);
+		}
+	} else {
+		dev_kfree_rtskb(skb);
+		tp->stats.tx_dropped++;
+		return 0;
+	}
+
+
+	/* Note: the chip doesn't have auto-pad! */
+	RTL_W32_F (TxStatus0 + (entry * sizeof (u32)), tp->tx_flag | max(len, (unsigned int)ETH_ZLEN));
+	tp->cur_tx++;
+	wmb();
+	if ((tp->cur_tx - NUM_TX_DESC) == tp->dirty_tx)
+		rtnetif_stop_queue (rtdev);
+	raw_spin_unlock_irqrestore(&tp->lock, context);
+
+	dev_kfree_rtskb(skb);
+
+#ifdef DEBUG
+	printk ("%s: Queued Tx packet size %u to slot %d.\n", rtdev->name, len, entry);
+#endif
+	return 0;
+}
+
+static int rtl8139_ioctl(struct rtnet_device *rtdev, struct ifreq *ifr, int cmd)
+{
+    struct rtl8139_private *tp = rtdev->priv;
+    void *ioaddr = tp->mmio_addr;
+    int nReturn = 0;
+    struct ethtool_value *value;
+
+    switch (cmd) {
+	case SIOCETHTOOL:
+	    /* TODO: user-safe parameter access, most probably one layer higher */
+	    value = (struct ethtool_value *)ifr->ifr_data;
+	    if (value->cmd == ETHTOOL_GLINK)
+	    {
+		if (RTL_R16(CSCR) & CSCR_LinkOKBit)
+		    value->data = 1;
+		else
+		    value->data = 0;
+	    }
+	    break;
+
+	default:
+	    nReturn = -EOPNOTSUPP;
+	    break;
+    }
+    return nReturn;
+}
+
+static struct net_device_stats *rtl8139_get_stats(struct rtnet_device*rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	return &tp->stats;
+}
+
+static void rtl8139_tx_interrupt (struct rtnet_device *rtdev,
+				  struct rtl8139_private *tp,
+				  void *ioaddr)
+{
+	unsigned long dirty_tx, tx_left;
+
+	dirty_tx = tp->dirty_tx;
+	tx_left = tp->cur_tx - dirty_tx;
+
+	while (tx_left > 0) {
+		int entry = dirty_tx % NUM_TX_DESC;
+		int txstatus;
+
+		txstatus = RTL_R32 (TxStatus0 + (entry * sizeof (u32)));
+
+		if (!(txstatus & (TxStatOK | TxUnderrun | TxAborted)))
+			break;        /* It still hasn't been Txed */
+
+		/* Note: TxCarrierLost is always asserted at 100mbps. */
+		if (txstatus & (TxOutOfWindow | TxAborted)) {
+			/* There was an major error, log it. */
+			printk("%s: Transmit error, Tx status %8.8x.\n",
+				    rtdev->name, txstatus);
+			tp->stats.tx_errors++;
+			if (txstatus & TxAborted) {
+				tp->stats.tx_aborted_errors++;
+				RTL_W32 (TxConfig, TxClearAbt);
+				RTL_W16 (IntrStatus, TxErr);
+				wmb();
+			}
+			if (txstatus & TxCarrierLost)
+				tp->stats.tx_carrier_errors++;
+			if (txstatus & TxOutOfWindow)
+				tp->stats.tx_window_errors++;
+#ifdef ETHER_STATS
+			if ((txstatus & 0x0f000000) == 0x0f000000)
+				tp->stats.collisions16++;
+#endif
+		} else {
+			if (txstatus & TxUnderrun) {
+				/* Add 64 to the Tx FIFO threshold. */
+				if (tp->tx_flag < 0x00300000)
+					tp->tx_flag += 0x00020000;
+				tp->stats.tx_fifo_errors++;
+			}
+			tp->stats.collisions += (txstatus >> 24) & 15;
+			tp->stats.tx_bytes += txstatus & 0x7ff;
+			tp->stats.tx_packets++;
+		}
+
+		dirty_tx++;
+		tx_left--;
+	}
+
+	/* only wake the queue if we did work, and the queue is stopped */
+	if (tp->dirty_tx != dirty_tx) {
+		tp->dirty_tx = dirty_tx;
+		mb();
+		if (rtnetif_queue_stopped (rtdev))
+			rtnetif_wake_queue (rtdev);
+	}
+}
+
+
+/* TODO: clean this up!  Rx reset need not be this intensive */
+static void rtl8139_rx_err
+(u32 rx_status, struct rtnet_device *rtdev, struct rtl8139_private *tp, void *ioaddr)
+{
+/*        u8 tmp8;
+#ifndef CONFIG_8139_NEW_RX_RESET
+	int tmp_work;
+#endif */
+
+	/* RTnet-TODO: We really need an error manager to handle such issues... */
+	printk("%s: FATAL - Ethernet frame had errors, status %8.8x.\n",
+		    rtdev->name, rx_status);
+}
+
+
+static void rtl8139_rx_interrupt (struct rtnet_device *rtdev,
+				  struct rtl8139_private *tp, void *ioaddr,
+				  ktime_t *time_stamp)
+{
+	unsigned char *rx_ring;
+	u16 cur_rx;
+
+	rx_ring = tp->rx_ring;
+	cur_rx = tp->cur_rx;
+
+	while ((RTL_R8 (ChipCmd) & RxBufEmpty) == 0) {
+		int ring_offset = cur_rx % RX_BUF_LEN;
+		u32 rx_status;
+		unsigned int rx_size;
+		unsigned int pkt_size;
+		struct rtskb *skb;
+
+		rmb();
+
+		/* read size+status of next frame from DMA ring buffer */
+		rx_status = le32_to_cpu (*(u32 *) (rx_ring + ring_offset));
+		rx_size = rx_status >> 16;
+		pkt_size = rx_size - 4;
+
+		/* Packet copy from FIFO still in progress.
+		 * Theoretically, this should never happen
+		 * since EarlyRx is disabled.
+		 */
+		if (rx_size == 0xfff0) {
+			tp->xstats.early_rx++;
+			break;
+		}
+
+		/* If Rx err or invalid rx_size/rx_status received
+		 * (which happens if we get lost in the ring),
+		 * Rx process gets reset, so we abort any further
+		 * Rx processing.
+		 */
+		if ((rx_size > (MAX_ETH_FRAME_SIZE+4)) ||
+		    (rx_size < 8) ||
+		    (!(rx_status & RxStatusOK))) {
+			rtl8139_rx_err (rx_status, rtdev, tp, ioaddr);
+			return;
+		}
+
+		/* Malloc up new buffer, compatible with net-2e. */
+		/* Omit the four octet CRC from the length. */
+
+		/* TODO: consider allocating skb's outside of
+		 * interrupt context, both to speed interrupt processing,
+		 * and also to reduce the chances of having to
+		 * drop packets here under memory pressure.
+		 */
+
+		skb = rtnetdev_alloc_rtskb(rtdev, pkt_size + 2);
+		if (skb) {
+			skb->time_stamp = *time_stamp;
+			rtskb_reserve (skb, 2);        /* 16 byte align the IP fields. */
+
+
+			/* eth_copy_and_sum (skb, &rx_ring[ring_offset + 4], pkt_size, 0); */
+			memcpy (skb->data, &rx_ring[ring_offset + 4], pkt_size);
+			rtskb_put (skb, pkt_size);
+			skb->protocol = rt_eth_type_trans (skb, rtdev);
+			rtnetif_rx (skb);
+			tp->stats.rx_bytes += pkt_size;
+			tp->stats.rx_packets++;
+		} else {
+			printk (KERN_WARNING"%s: Memory squeeze, dropping packet.\n", rtdev->name);
+			tp->stats.rx_dropped++;
+		}
+
+		cur_rx = (cur_rx + rx_size + 4 + 3) & ~3;
+		RTL_W16 (RxBufPtr, cur_rx - 16);
+
+		if (RTL_R16 (IntrStatus) & RxAckBits)
+			RTL_W16_F (IntrStatus, RxAckBits);
+	}
+
+	tp->cur_rx = cur_rx;
+}
+
+
+static void rtl8139_weird_interrupt (struct rtnet_device *rtdev,
+				     struct rtl8139_private *tp,
+				     void *ioaddr,
+				     int status, int link_changed)
+{
+	printk ("%s: Abnormal interrupt, status %8.8x.\n",
+		      rtdev->name, status);
+
+	/* Update the error count. */
+	tp->stats.rx_missed_errors += RTL_R32 (RxMissed);
+	RTL_W32 (RxMissed, 0);
+
+	if ((status & RxUnderrun) && link_changed && (tp->drv_flags & HAS_LNK_CHNG)) {
+		/* Really link-change on new chips. */
+		status &= ~RxUnderrun;
+	}
+
+	/* XXX along with rtl8139_rx_err, are we double-counting errors? */
+	if (status &
+	    (RxUnderrun | RxOverflow | RxErr | RxFIFOOver))
+		tp->stats.rx_errors++;
+
+	if (status & PCSTimeout)
+		tp->stats.rx_length_errors++;
+
+	if (status & (RxUnderrun | RxFIFOOver))
+		tp->stats.rx_fifo_errors++;
+
+	if (status & PCIErr) {
+		u16 pci_cmd_status;
+		pci_read_config_word (tp->pci_dev, PCI_STATUS, &pci_cmd_status);
+		pci_write_config_word (tp->pci_dev, PCI_STATUS, pci_cmd_status);
+
+		printk (KERN_ERR "%s: PCI Bus error %4.4x.\n", rtdev->name, pci_cmd_status);
+	}
+}
+
+/* The interrupt handler does all of the Rx thread work and cleans up
+   after the Tx thread. */
+static irqreturn_t rtl8139_interrupt(int irq, void *data)
+{
+	ktime_t time_stamp = ktime_get();
+	struct rtnet_device *rtdev = (struct rtnet_device *)data;
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	int ackstat;
+	int status;
+	int link_changed = 0; /* avoid bogus "uninit" warning */
+	int saved_status = 0;
+	int ret = IRQ_NONE;
+
+	raw_spin_lock(&tp->lock);
+
+	status = RTL_R16(IntrStatus);
+
+	/* h/w no longer present (hotplug?) or major error, bail */
+	if (unlikely(status == 0xFFFF) || unlikely(!(status & rtl8139_intr_mask)))
+		goto out;
+
+	ret = IRQ_HANDLED;
+
+	/* close possible race with dev_close */
+	if (unlikely(!rtnetif_running(rtdev))) {
+		RTL_W16(IntrMask, 0);
+		goto out;
+	}
+
+	/* Acknowledge all of the current interrupt sources ASAP, but
+	   first get an additional status bit from CSCR. */
+	if (unlikely(status & RxUnderrun))
+		link_changed = RTL_R16(CSCR) & CSCR_LinkChangeBit;
+
+	/* The chip takes special action when we clear RxAckBits,
+	 * so we clear them later in rtl8139_rx_interrupt
+	 */
+	ackstat = status & ~(RxAckBits | TxErr);
+	if (ackstat)
+		RTL_W16(IntrStatus, ackstat);
+
+	if (status & RxAckBits) {
+		saved_status |= RxAckBits;
+		rtl8139_rx_interrupt(rtdev, tp, ioaddr, &time_stamp);
+	}
+
+	/* Check uncommon events with one test. */
+	if (unlikely(status & (PCIErr | PCSTimeout | RxUnderrun | RxErr)))
+		rtl8139_weird_interrupt(rtdev, tp, ioaddr, status, link_changed);
+
+	if (status & (TxOK |TxErr)) {
+		rtl8139_tx_interrupt(rtdev, tp, ioaddr);
+		if (status & TxErr) {
+			RTL_W16(IntrStatus, TxErr);
+			saved_status |= TxErr;
+		}
+	}
+ out:
+	raw_spin_unlock(&tp->lock);
+
+	if (saved_status & RxAckBits)
+		rt_mark_stack_mgr(rtdev);
+
+	if (saved_status & TxErr)
+		rtnetif_err_tx(rtdev);
+
+	return ret;
+}
+
+
+static int rtl8139_close (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	unsigned long context;
+
+	printk ("%s: Shutting down ethercard, status was 0x%4.4x.\n", rtdev->name, RTL_R16 (IntrStatus));
+
+	rtnetif_stop_queue (rtdev);
+
+	raw_spin_lock_irqsave (&tp->lock, context);
+	/* Stop the chip's Tx and Rx DMA processes. */
+	RTL_W8 (ChipCmd, 0);
+	/* Disable interrupts by clearing the interrupt mask. */
+	RTL_W16 (IntrMask, 0);
+	/* Update the error counts. */
+	tp->stats.rx_missed_errors += RTL_R32 (RxMissed);
+	RTL_W32 (RxMissed, 0);
+	raw_spin_unlock_irqrestore (&tp->lock, context);
+
+	free_irq(rtdev->irq, rtdev);
+
+	rt_stack_disconnect(rtdev);
+
+	rtl8139_tx_clear (tp);
+
+	pci_free_consistent(tp->pci_dev, RX_BUF_TOT_LEN, tp->rx_ring, tp->rx_ring_dma);
+	pci_free_consistent(tp->pci_dev, TX_BUF_TOT_LEN, tp->tx_bufs, tp->tx_bufs_dma);
+	tp->rx_ring = NULL;
+	tp->tx_bufs = NULL;
+
+	/* Green! Put the chip in low-power mode. */
+	RTL_W8 (Cfg9346, Cfg9346_Unlock);
+
+	if (rtl_chip_info[tp->chipset].flags & HasHltClk)
+		RTL_W8 (HltClk, 'H');        /* 'R' would leave the clock running. */
+
+	return 0;
+}
+
+
+
+/* Set or clear the multicast filter for this adaptor.
+   This routine is not state sensitive and need not be SMP locked. */
+static void __set_rx_mode (struct rtnet_device *rtdev)
+{
+	struct rtl8139_private *tp = rtdev->priv;
+	void *ioaddr = tp->mmio_addr;
+	u32 mc_filter[2];        /* Multicast hash filter */
+	int rx_mode;
+	u32 tmp;
+
+#ifdef DEBUG
+	printk ("%s:   rtl8139_set_rx_mode(%4.4x) done -- Rx config %8.8lx.\n",
+			rtdev->name, rtdev->flags, RTL_R32 (RxConfig));
+#endif
+
+	/* Note: do not reorder, GCC is clever about common statements. */
+	if (rtdev->flags & IFF_PROMISC) {
+		/* Unconditionally log net taps. */
+		/*printk (KERN_NOTICE "%s: Promiscuous mode enabled.\n", rtdev->name);*/
+		rx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys | AcceptAllPhys;
+		mc_filter[1] = mc_filter[0] = 0xffffffff;
+	} else if (rtdev->flags & IFF_ALLMULTI) {
+		/* Too many to filter perfectly -- accept all multicasts. */
+		rx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys;
+		mc_filter[1] = mc_filter[0] = 0xffffffff;
+	} else {
+		rx_mode = AcceptBroadcast | AcceptMyPhys;
+		mc_filter[1] = mc_filter[0] = 0;
+	}
+
+	/* We can safely update without stopping the chip. */
+	tmp = rtl8139_rx_config | rx_mode;
+	if (tp->rx_config != tmp) {
+		RTL_W32_F (RxConfig, tmp);
+		tp->rx_config = tmp;
+	}
+	RTL_W32_F (MAR0 + 0, mc_filter[0]);
+	RTL_W32_F (MAR0 + 4, mc_filter[1]);
+}
+
+static void rtl8139_set_rx_mode (struct rtnet_device *rtdev)
+{
+	unsigned long context;
+	struct rtl8139_private *tp = rtdev->priv;
+
+	raw_spin_lock_irqsave (&tp->lock, context);
+	__set_rx_mode(rtdev);
+	raw_spin_unlock_irqrestore (&tp->lock, context);
+}
+
+static struct pci_driver rtl8139_pci_driver = {
+	name:                   DRV_NAME,
+	id_table:               rtl8139_pci_tbl,
+	probe:                  rtl8139_init_one,
+	remove:                 rtl8139_remove_one,
+	suspend:                NULL,
+	resume:                 NULL,
+};
+
+
+static int __init rtl8139_init_module (void)
+{
+	/* when we're a module, we always print a version message,
+	 * even if no 8139 board is found.
+	 */
+
+#ifdef MODULE
+	printk (KERN_INFO RTL8139_DRIVER_NAME "\n");
+#endif
+
+	return pci_register_driver (&rtl8139_pci_driver);
+}
+
+
+static void __exit rtl8139_cleanup_module (void)
+{
+	pci_unregister_driver (&rtl8139_pci_driver);
+}
+
+
+module_init(rtl8139_init_module);
+module_exit(rtl8139_cleanup_module);
diff -Nur linux-5.4.5/net/rtnet/drivers/Kconfig linux-5.4.5-new/net/rtnet/drivers/Kconfig
--- linux-5.4.5/net/rtnet/drivers/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/Kconfig	2020-06-15 16:12:31.491695504 +0300
@@ -0,0 +1,28 @@
+menu "Drivers"
+    depends on XENO_DRIVERS_NET
+
+comment "Common PCI Drivers"
+    depends on PCI
+
+config XENO_DRIVERS_NET_DRV_LOOPBACK
+    depends on XENO_DRIVERS_NET
+    tristate "Loopback"
+    default y
+
+config XENO_DRIVERS_NET_DRV_TI_CPSW
+    depends on XENO_DRIVERS_NET
+    tristate "TI CPSW"
+    default y if ARM
+    help
+    Texas Instruments am335x real-time network driver
+
+config XENO_DRIVERS_NET_DRV_8139
+    depends on XENO_DRIVERS_NET && PCI
+    tristate "Realtek 8139"
+    default y if X86 || X86_64
+    help
+    Realtek 8139too real-time network driver
+
+#source "drivers/rtnet/drivers/experimental/Kconfig"
+
+endmenu
diff -Nur linux-5.4.5/net/rtnet/drivers/loopback.c linux-5.4.5-new/net/rtnet/drivers/loopback.c
--- linux-5.4.5/net/rtnet/drivers/loopback.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/loopback.c	2020-06-15 16:12:31.491695504 +0300
@@ -0,0 +1,148 @@
+/* loopback.c
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ * extended by Jose Carlos Billalabeitia and Jan Kiszka
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+#include <linux/netdevice.h>
+
+#include <rtnet_port.h>
+#include <stack_mgr.h>
+
+MODULE_AUTHOR("Maintainer: Jan Kiszka <Jan.Kiszka@web.de>");
+MODULE_DESCRIPTION("RTnet loopback driver");
+MODULE_LICENSE("GPL");
+
+static struct rtnet_device* rt_loopback_dev;
+
+/***
+ *  rt_loopback_open
+ *  @rtdev
+ */
+static int rt_loopback_open (struct rtnet_device *rtdev)
+{
+    rt_stack_connect(rtdev, &STACK_manager);
+    rtnetif_start_queue(rtdev);
+
+    return 0;
+}
+
+
+/***
+ *  rt_loopback_close
+ *  @rtdev
+ */
+static int rt_loopback_close (struct rtnet_device *rtdev)
+{
+    rtnetif_stop_queue(rtdev);
+    rt_stack_disconnect(rtdev);
+
+    return 0;
+}
+
+
+/***
+ *  rt_loopback_xmit - begin packet transmission
+ *  @skb: packet to be sent
+ *  @dev: network device to which packet is sent
+ *
+ */
+static int rt_loopback_xmit(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    /* write transmission stamp - in case any protocol ever gets the idea to
+       ask the lookback device for this service... */
+    if (rtskb->xmit_stamp)
+	*rtskb->xmit_stamp =
+	    cpu_to_be64(ktime_get() + *rtskb->xmit_stamp);
+
+    /* make sure that critical fields are re-intialised */
+    rtskb->chain_end = rtskb;
+
+    /* parse the Ethernet header as usual */
+    rtskb->protocol = rt_eth_type_trans(rtskb, rtdev);
+
+    rt_stack_deliver(rtskb);
+
+    return 0;
+}
+
+
+/***
+ *  loopback_init
+ */
+static int __init loopback_init(void)
+{
+    int err;
+    struct rtnet_device *rtdev;
+
+    printk("initializing loopback...\n");
+
+    if ((rtdev = rt_alloc_etherdev(0, 1)) == NULL) {
+	printk(KERN_ERR "%s rt_alloc_etherdev error\n", __func__);
+	return -ENODEV;
+    }
+
+    rt_rtdev_connect(rtdev, &RTDEV_manager);
+
+    strcpy(rtdev->name, "rtlo");
+
+    rtdev->vers = RTDEV_VERS_2_0;
+    rtdev->open = &rt_loopback_open;
+    rtdev->stop = &rt_loopback_close;
+    rtdev->hard_start_xmit = &rt_loopback_xmit;
+    rtdev->flags |= IFF_LOOPBACK;
+    rtdev->flags &= ~IFF_BROADCAST;
+    rtdev->features |= NETIF_F_LLTX;
+
+    if ((err = rt_register_rtnetdev(rtdev)) != 0)
+    {
+	printk(KERN_ERR "%s rt_register_rtnetdev error %d\n", __func__, err);
+	rtdev_free(rtdev);
+	return err;
+    }
+
+    rt_loopback_dev = rtdev;
+
+    return 0;
+}
+
+
+/***
+ *  loopback_cleanup
+ */
+static void __exit loopback_cleanup(void)
+{
+    struct rtnet_device *rtdev = rt_loopback_dev;
+
+    printk("removing loopback...\n");
+
+    rt_unregister_rtnetdev(rtdev);
+    rt_rtdev_disconnect(rtdev);
+
+    rtdev_free(rtdev);
+}
+
+module_init(loopback_init);
+module_exit(loopback_cleanup);
diff -Nur linux-5.4.5/net/rtnet/drivers/Makefile linux-5.4.5-new/net/rtnet/drivers/Makefile
--- linux-5.4.5/net/rtnet/drivers/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/Makefile	2020-06-15 16:12:31.483695532 +0300
@@ -0,0 +1,14 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK) += rt_loopback.o
+
+rt_loopback-y := loopback.o
+
+obj-$(CONFIG_XENO_DRIVERS_NET_DRV_TI_CPSW) += ticpsw/ \
+					   rt_davinci_mdio.o \
+					   rt_smsc.o
+
+obj-$(CONFIG_XENO_DRIVERS_NET_DRV_8139) += rt_8139too.o
+
+rt_8139too-y := 8139too.o
+
diff -Nur linux-5.4.5/net/rtnet/drivers/rt_davinci_mdio.c linux-5.4.5-new/net/rtnet/drivers/rt_davinci_mdio.c
--- linux-5.4.5/net/rtnet/drivers/rt_davinci_mdio.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/rt_davinci_mdio.c	2020-06-15 16:12:31.483695532 +0300
@@ -0,0 +1,534 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * 2019/09/20
+ * Small modifications to Geoffrey Bonneville
+ * and Hidde Verstoep RTNet port to beaglebone-black,
+ * modifications made by Laurentiu-Cristian Duca
+ * in order to compile on kernel 4.14.71
+ *
+ * DaVinci MDIO Module driver
+ *
+ * Copyright (C) 2010 Texas Instruments.
+ *
+ * Shamelessly ripped out of davinci_emac.c, original copyrights follow:
+ *
+ * Copyright (C) 2009 Texas Instruments.
+ *
+ * ---------------------------------------------------------------------------
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ * ---------------------------------------------------------------------------
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/pm_runtime.h>
+#include <linux/davinci_emac.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include <rtnet_port.h>
+
+/*
+ * This timeout definition is a worst-case ultra defensive measure against
+ * unexpected controller lock ups.  Ideally, we should never ever hit this
+ * scenario in practice.
+ */
+#define MDIO_TIMEOUT		100 /* msecs */
+
+#define PHY_REG_MASK		0x1f
+#define PHY_ID_MASK		0x1f
+
+#define DEF_OUT_FREQ		2200000		/* 2.2 MHz */
+
+struct davinci_mdio_regs {
+	u32	version;
+	u32	control;
+#define CONTROL_IDLE		BIT(31)
+#define CONTROL_ENABLE		BIT(30)
+#define CONTROL_MAX_DIV		(0xffff)
+
+	u32	alive;
+	u32	link;
+	u32	linkintraw;
+	u32	linkintmasked;
+	u32	__reserved_0[2];
+	u32	userintraw;
+	u32	userintmasked;
+	u32	userintmaskset;
+	u32	userintmaskclr;
+	u32	__reserved_1[20];
+
+	struct {
+		u32	access;
+#define USERACCESS_GO		BIT(31)
+#define USERACCESS_WRITE	BIT(30)
+#define USERACCESS_ACK		BIT(29)
+#define USERACCESS_READ		(0)
+#define USERACCESS_DATA		(0xffff)
+
+		u32	physel;
+	}	user[0];
+};
+
+struct mdio_platform_data default_pdata = {
+	.bus_freq = DEF_OUT_FREQ,
+};
+
+struct davinci_mdio_data {
+	struct mdio_platform_data pdata;
+	struct davinci_mdio_regs __iomem *regs;
+	raw_spinlock_t	lock;
+	struct clk	*clk;
+	struct device	*dev;
+	struct mii_bus	*bus;
+	bool		suspended;
+	unsigned long	access_time; /* jiffies */
+};
+
+static void __davinci_mdio_reset(struct davinci_mdio_data *data)
+{
+	u32 mdio_in, div, mdio_out_khz, access_time;
+
+	mdio_in = clk_get_rate(data->clk);
+	div = (mdio_in / data->pdata.bus_freq) - 1;
+	if (div > CONTROL_MAX_DIV)
+		div = CONTROL_MAX_DIV;
+
+	/* set enable and clock divider */
+	__raw_writel(div | CONTROL_ENABLE, &data->regs->control);
+
+	/*
+	 * One mdio transaction consists of:
+	 *	32 bits of preamble
+	 *	32 bits of transferred data
+	 *	24 bits of bus yield (not needed unless shared?)
+	 */
+	mdio_out_khz = mdio_in / (1000 * (div + 1));
+	access_time  = (88 * 1000) / mdio_out_khz;
+
+	/*
+	 * In the worst case, we could be kicking off a user-access immediately
+	 * after the mdio bus scan state-machine triggered its own read.  If
+	 * so, our request could get deferred by one access cycle.  We
+	 * defensively allow for 4 access cycles.
+	 */
+	data->access_time = usecs_to_jiffies(access_time * 4);
+	if (!data->access_time)
+		data->access_time = 1;
+}
+
+static int davinci_mdio_reset(struct mii_bus *bus)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 phy_mask, ver;
+
+	__davinci_mdio_reset(data);
+
+	/* wait for scan logic to settle */
+	msleep(PHY_MAX_ADDR * data->access_time);
+
+	/* dump hardware version info */
+	ver = __raw_readl(&data->regs->version);
+	dev_info(data->dev, "davinci mdio revision %d.%d\n",
+		 (ver >> 8) & 0xff, ver & 0xff);
+
+	/* get phy mask from the alive register */
+	phy_mask = __raw_readl(&data->regs->alive);
+	if (phy_mask) {
+		/* restrict mdio bus to live phys only */
+		dev_info(data->dev, "detected phy mask %x\n", ~phy_mask);
+		phy_mask = ~phy_mask;
+	} else {
+		/* desperately scan all phys */
+		dev_warn(data->dev, "no live phy, scanning all\n");
+		phy_mask = 0;
+	}
+	data->bus->phy_mask = phy_mask;
+
+	return 0;
+}
+
+/* wait until hardware is ready for another user access */
+static inline int wait_for_user_access(struct davinci_mdio_data *data)
+{
+	struct davinci_mdio_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+	u32 reg;
+
+	while (time_after(timeout, jiffies)) {
+		reg = __raw_readl(&regs->user[0].access);
+		if ((reg & USERACCESS_GO) == 0)
+			return 0;
+
+		reg = __raw_readl(&regs->control);
+		if ((reg & CONTROL_IDLE) == 0)
+			continue;
+
+		/*
+		 * An emac soft_reset may have clobbered the mdio controller's
+		 * state machine.  We need to reset and retry the current
+		 * operation
+		 */
+		dev_warn(data->dev, "resetting idled controller\n");
+		__davinci_mdio_reset(data);
+		return -EAGAIN;
+	}
+
+	reg = __raw_readl(&regs->user[0].access);
+	if ((reg & USERACCESS_GO) == 0)
+		return 0;
+
+	dev_err(data->dev, "timed out waiting for user access\n");
+	return -ETIMEDOUT;
+}
+
+/* wait until hardware state machine is idle */
+static inline int wait_for_idle(struct davinci_mdio_data *data)
+{
+	struct davinci_mdio_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+
+	while (time_after(timeout, jiffies)) {
+		if (__raw_readl(&regs->control) & CONTROL_IDLE)
+			return 0;
+	}
+	dev_err(data->dev, "timed out waiting for idle\n");
+	return -ETIMEDOUT;
+}
+
+static int davinci_mdio_read(struct mii_bus *bus, int phy_id, int phy_reg)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	raw_spin_lock(&data->lock);
+
+	if (data->suspended) {
+		raw_spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_READ | (phy_reg << 21) |
+	       (phy_id << 16));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		reg = __raw_readl(&data->regs->user[0].access);
+		ret = (reg & USERACCESS_ACK) ? (reg & USERACCESS_DATA) : -EIO;
+		break;
+	}
+
+	raw_spin_unlock(&data->lock);
+
+	return ret;
+}
+
+static int davinci_mdio_write(struct mii_bus *bus, int phy_id,
+			      int phy_reg, u16 phy_data)
+{
+	struct davinci_mdio_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	raw_spin_lock(&data->lock);
+
+	if (data->suspended) {
+		raw_spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_WRITE | (phy_reg << 21) |
+		   (phy_id << 16) | (phy_data & USERACCESS_DATA));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		break;
+	}
+
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int davinci_mdio_probe_dt(struct mdio_platform_data *data,
+			 struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	u32 prop;
+
+	if (!node)
+		return -EINVAL;
+
+	if (of_property_read_u32(node, "bus_freq", &prop)) {
+		pr_err("Missing bus_freq property in the DT.\n");
+		return -EINVAL;
+	}
+	data->bus_freq = prop;
+
+	return 0;
+}
+
+
+static int davinci_mdio_probe(struct platform_device *pdev)
+{
+	struct mdio_platform_data *pdata = pdev->dev.platform_data;
+	struct device *dev = &pdev->dev;
+	struct davinci_mdio_data *data;
+	struct resource *res;
+	struct phy_device *phy;
+	int ret, addr;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data) {
+		dev_err(dev, "failed to alloc device data\n");
+		return -ENOMEM;
+	}
+
+	data->bus = mdiobus_alloc();
+	if (!data->bus) {
+		dev_err(dev, "failed to alloc mii bus\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	if (dev->of_node) {
+		if (davinci_mdio_probe_dt(&data->pdata, pdev))
+			data->pdata = default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s", pdev->name);
+	} else {
+		data->pdata = pdata ? (*pdata) : default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s-%x",
+			 pdev->name, pdev->id);
+	}
+
+	data->bus->name		= dev_name(dev);
+	data->bus->read		= davinci_mdio_read,
+	data->bus->write	= davinci_mdio_write,
+	data->bus->reset	= davinci_mdio_reset,
+	data->bus->parent	= dev;
+	data->bus->priv		= data;
+
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+	data->clk = clk_get(&pdev->dev, "fck");
+	if (IS_ERR(data->clk)) {
+		dev_err(dev, "failed to get device clock\n");
+		ret = PTR_ERR(data->clk);
+		data->clk = NULL;
+		goto bail_out;
+	}
+
+	dev_set_drvdata(dev, data);
+	data->dev = dev;
+	raw_spin_lock_init(&data->lock);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "could not find register map resource\n");
+		ret = -ENOENT;
+		goto bail_out;
+	}
+
+	res = devm_request_mem_region(dev, res->start, resource_size(res),
+					    dev_name(dev));
+	if (!res) {
+		dev_err(dev, "could not allocate register map resource\n");
+		ret = -ENXIO;
+		goto bail_out;
+	}
+
+	data->regs = devm_ioremap_nocache(dev, res->start, resource_size(res));
+	if (!data->regs) {
+		dev_err(dev, "could not map mdio registers\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	/* register the mii bus */
+	ret = mdiobus_register(data->bus);
+	if (ret)
+		goto bail_out;
+
+	/* scan and dump the bus */
+	for (addr = 0; addr < PHY_MAX_ADDR; addr++) {
+		phy = mdiobus_get_phy(data->bus, addr);
+		if (phy) {
+			dev_info(dev, "phy[%d]: device %s, driver %s\n",
+				 phy->mdio.addr, phydev_name(phy),
+				 phy->drv ? phy->drv->name : "unknown");
+		}
+	}
+	
+	return 0;
+
+bail_out:
+	if (data->bus)
+		mdiobus_free(data->bus);
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	kfree(data);
+
+	return ret;
+}
+
+static int davinci_mdio_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+
+	if (data->bus) {
+		mdiobus_unregister(data->bus);
+		mdiobus_free(data->bus);
+	}
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	dev_set_drvdata(dev, NULL);
+
+	kfree(data);
+
+	return 0;
+}
+
+static int davinci_mdio_suspend(struct device *dev)
+{
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	raw_spin_lock(&data->lock);
+
+	/* shutdown the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl &= ~CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+	wait_for_idle(data);
+
+	pm_runtime_put_sync(data->dev);
+
+	data->suspended = true;
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int davinci_mdio_resume(struct device *dev)
+{
+	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	raw_spin_lock(&data->lock);
+	pm_runtime_get_sync(data->dev);
+
+	/* restart the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl |= CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+
+	data->suspended = false;
+	raw_spin_unlock(&data->lock);
+
+	return 0;
+}
+
+int load_davinci_mdio_module(int v)
+{
+	return v;
+}
+/* Every module that calls this function will automatically load this module */
+EXPORT_SYMBOL_GPL(load_davinci_mdio_module);
+
+static const struct dev_pm_ops davinci_mdio_pm_ops = {
+	.suspend	= davinci_mdio_suspend,
+	.resume		= davinci_mdio_resume,
+};
+
+static const struct of_device_id davinci_mdio_of_mtable[] = {
+	{ .compatible = "ti,davinci_mdio", },
+	{ /* sentinel */ },
+};
+
+static struct platform_driver davinci_mdio_driver = {
+	.driver = {
+		.name	 = "davinci_mdio",
+		.owner	 = THIS_MODULE,
+		.pm	 = &davinci_mdio_pm_ops,
+		.of_match_table = of_match_ptr(davinci_mdio_of_mtable),
+	},
+	.probe = davinci_mdio_probe,
+	.remove = davinci_mdio_remove,
+};
+
+static int __init davinci_mdio_init(void)
+{
+	return platform_driver_register(&davinci_mdio_driver);
+}
+device_initcall(davinci_mdio_init);
+
+static void __exit davinci_mdio_exit(void)
+{
+	platform_driver_unregister(&davinci_mdio_driver);
+}
+module_exit(davinci_mdio_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("RT DaVinci MDIO driver");
diff -Nur linux-5.4.5/net/rtnet/drivers/rt_smsc.c linux-5.4.5-new/net/rtnet/drivers/rt_smsc.c
--- linux-5.4.5/net/rtnet/drivers/rt_smsc.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/rt_smsc.c	2020-06-15 16:12:31.483695532 +0300
@@ -0,0 +1,404 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * Copyright (C) 2019 Laurentiu-Cristian Duca
+ * Allmost identical to Hidde Verstoep RTNet port to beaglebone-black,
+ * small modifications made by Laurentiu-Cristian Duca
+ *
+ * Driver for SMSC PHYs
+ *
+ * Author: Herbert Valerio Riedel
+ *
+ * Copyright (c) 2006 Herbert Valerio Riedel <hvr@gnu.org>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * Support added for SMSC LAN8187 and LAN8700 by steve.glendinning@shawell.net
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/of.h>
+#include <linux/phy.h>
+#include <linux/netdevice.h>
+#include <linux/smscphy.h>
+
+struct smsc_hw_stat {
+	const char *string;
+	u8 reg;
+	u8 bits;
+};
+
+static struct smsc_hw_stat smsc_hw_stats[] = {
+	{ "phy_symbol_errors", 26, 16},
+};
+
+struct smsc_phy_priv {
+	bool energy_enable;
+};
+
+static int smsc_phy_config_intr(struct phy_device *phydev)
+{
+	int rc = phy_write (phydev, MII_LAN83C185_IM,
+			((PHY_INTERRUPT_ENABLED == phydev->interrupts)
+			? MII_LAN83C185_ISF_INT_PHYLIB_EVENTS
+			: 0));
+
+	return rc < 0 ? rc : 0;
+}
+
+static int smsc_phy_ack_interrupt(struct phy_device *phydev)
+{
+	int rc = phy_read (phydev, MII_LAN83C185_ISF);
+
+	return rc < 0 ? rc : 0;
+}
+
+static int smsc_phy_config_init(struct phy_device *phydev)
+{
+	struct smsc_phy_priv *priv = phydev->priv;
+
+	int rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+
+	if (rc < 0)
+		return rc;
+
+	if (priv->energy_enable) {
+		/* Enable energy detect mode for this SMSC Transceivers */
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc | MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+	}
+
+	return smsc_phy_ack_interrupt(phydev);
+}
+
+static int smsc_phy_reset(struct phy_device *phydev)
+{
+	int rc = phy_read(phydev, MII_LAN83C185_SPECIAL_MODES);
+	if (rc < 0)
+		return rc;
+
+	/* If the SMSC PHY is in power down mode, then set it
+	 * in all capable mode before using it.
+	 */
+	if ((rc & MII_LAN83C185_MODE_MASK) == MII_LAN83C185_MODE_POWERDOWN) {
+		/* set "all capable" mode */
+		rc |= MII_LAN83C185_MODE_ALL;
+		phy_write(phydev, MII_LAN83C185_SPECIAL_MODES, rc);
+	}
+
+	/* reset the phy */
+	return genphy_soft_reset(phydev);
+}
+
+static int lan911x_config_init(struct phy_device *phydev)
+{
+	return smsc_phy_ack_interrupt(phydev);
+}
+
+/*
+ * The LAN87xx suffers from rare absence of the ENERGYON-bit when Ethernet cable
+ * plugs in while LAN87xx is in Energy Detect Power-Down mode. This leads to
+ * unstable detection of plugging in Ethernet cable.
+ * This workaround disables Energy Detect Power-Down mode and waiting for
+ * response on link pulses to detect presence of plugged Ethernet cable.
+ * The Energy Detect Power-Down mode is enabled again in the end of procedure to
+ * save approximately 220 mW of power if cable is unplugged.
+ */
+static int lan87xx_read_status(struct phy_device *phydev)
+{
+	struct smsc_phy_priv *priv = phydev->priv;
+
+	int err = genphy_read_status(phydev);
+
+	if (!phydev->link && priv->energy_enable) {
+		int i;
+
+		/* Disable EDPD to wake up PHY */
+		int rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+		if (rc < 0)
+			return rc;
+
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc & ~MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+
+		/* Wait max 640 ms to detect energy */
+		for (i = 0; i < 64; i++) {
+			/* Sleep to allow link test pulses to be sent */
+			msleep(10);
+			rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+			if (rc < 0)
+				return rc;
+			if (rc & MII_LAN83C185_ENERGYON)
+				break;
+		}
+
+		/* Re-enable EDPD */
+		rc = phy_read(phydev, MII_LAN83C185_CTRL_STATUS);
+		if (rc < 0)
+			return rc;
+
+		rc = phy_write(phydev, MII_LAN83C185_CTRL_STATUS,
+			       rc | MII_LAN83C185_EDPWRDOWN);
+		if (rc < 0)
+			return rc;
+	}
+
+	return err;
+}
+
+static int smsc_get_sset_count(struct phy_device *phydev)
+{
+	return ARRAY_SIZE(smsc_hw_stats);
+}
+
+static void smsc_get_strings(struct phy_device *phydev, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(smsc_hw_stats); i++) {
+		strncpy(data + i * ETH_GSTRING_LEN,
+		       smsc_hw_stats[i].string, ETH_GSTRING_LEN);
+	}
+}
+
+#ifndef UINT64_MAX
+#define UINT64_MAX              (u64)(~((u64)0))
+#endif
+static u64 smsc_get_stat(struct phy_device *phydev, int i)
+{
+	struct smsc_hw_stat stat = smsc_hw_stats[i];
+	int val;
+	u64 ret;
+
+	val = phy_read(phydev, stat.reg);
+	if (val < 0)
+		ret = UINT64_MAX;
+	else
+		ret = val;
+
+	return ret;
+}
+
+static void smsc_get_stats(struct phy_device *phydev,
+			   struct ethtool_stats *stats, u64 *data)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(smsc_hw_stats); i++)
+		data[i] = smsc_get_stat(phydev, i);
+}
+
+static int smsc_phy_probe(struct phy_device *phydev)
+{
+	struct device *dev = &phydev->mdio.dev;
+	struct device_node *of_node = dev->of_node;
+	struct smsc_phy_priv *priv;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->energy_enable = true;
+
+	if (of_property_read_bool(of_node, "smsc,disable-energy-detect")) {
+		pr_info("smsc,disable-energy-detect found");
+		priv->energy_enable = false;
+	} else
+		pr_info("smsc,disable-energy-detect missing");
+
+	phydev->priv = priv;
+
+	return 0;
+}
+
+int load_smsc_phy_module(int v)
+{
+	return v;
+}
+/* Every module that calls this function will automatically load this module */
+EXPORT_SYMBOL_GPL(load_smsc_phy_module);
+
+static struct phy_driver smsc_phy_driver[] = {
+{
+	.phy_id		= 0x0007c0a0, /* OUI=0x00800f, Model#=0x0a */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN83C185",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0b0, /* OUI=0x00800f, Model#=0x0b */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8187",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0c0, /* OUI=0x00800f, Model#=0x0c */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8700",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0d0, /* OUI=0x00800f, Model#=0x0d */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN911x Internal PHY",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.config_init	= lan911x_config_init,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c0f0, /* OUI=0x00800f, Model#=0x0f */
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "RT SMSC LAN8710/LAN8720",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+}, {
+	.phy_id		= 0x0007c110,
+	.phy_id_mask	= 0xfffffff0,
+	.name		= "SMSC LAN8740",
+
+	.features	= PHY_BASIC_FEATURES,
+	.flags		= 0, //PHY_HAS_INTERRUPT,
+
+	.probe		= smsc_phy_probe,
+
+	/* basic functions */
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= lan87xx_read_status,
+	.config_init	= smsc_phy_config_init,
+	.soft_reset	= smsc_phy_reset,
+
+	/* IRQ related */
+	.ack_interrupt	= smsc_phy_ack_interrupt,
+	.config_intr	= smsc_phy_config_intr,
+
+	/* Statistics */
+	.get_sset_count = smsc_get_sset_count,
+	.get_strings	= smsc_get_strings,
+	.get_stats	= smsc_get_stats,
+
+	.suspend	= genphy_suspend,
+	.resume		= genphy_resume,
+} };
+
+module_phy_driver(smsc_phy_driver);
+
+MODULE_DESCRIPTION("SMSC PHY driver");
+MODULE_AUTHOR("Herbert Valerio Riedel");
+MODULE_LICENSE("GPL");
+
+static struct mdio_device_id __maybe_unused smsc_tbl[] = {
+	{ 0x0007c0a0, 0xfffffff0 },
+	{ 0x0007c0b0, 0xfffffff0 },
+	{ 0x0007c0c0, 0xfffffff0 },
+	{ 0x0007c0d0, 0xfffffff0 },
+	{ 0x0007c0f0, 0xfffffff0 },
+	{ 0x0007c110, 0xfffffff0 },
+	{ }
+};
+
+MODULE_DEVICE_TABLE(mdio, smsc_tbl);
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw_ale.c linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw_ale.c
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw_ale.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw_ale.c	2020-06-15 16:12:31.467695589 +0300
@@ -0,0 +1,665 @@
+/*
+ * Texas Instruments 3-Port Ethernet Switch Address Lookup Engine
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/stat.h>
+#include <linux/sysfs.h>
+#include <linux/etherdevice.h>
+
+#include "cpsw_ale.h"
+
+#define BITMASK(bits)		(BIT(bits) - 1)
+#define ALE_ENTRY_BITS		68
+#define ALE_ENTRY_WORDS	DIV_ROUND_UP(ALE_ENTRY_BITS, 32)
+
+#define ALE_VERSION_MAJOR(rev)	((rev >> 8) & 0xff)
+#define ALE_VERSION_MINOR(rev)	(rev & 0xff)
+
+/* ALE Registers */
+#define ALE_IDVER		0x00
+#define ALE_CONTROL		0x08
+#define ALE_PRESCALE		0x10
+#define ALE_UNKNOWNVLAN		0x18
+#define ALE_TABLE_CONTROL	0x20
+#define ALE_TABLE		0x34
+#define ALE_PORTCTL		0x40
+
+#define ALE_TABLE_WRITE		BIT(31)
+
+#define ALE_TYPE_FREE			0
+#define ALE_TYPE_ADDR			1
+#define ALE_TYPE_VLAN			2
+#define ALE_TYPE_VLAN_ADDR		3
+
+#define ALE_UCAST_PERSISTANT		0
+#define ALE_UCAST_UNTOUCHED		1
+#define ALE_UCAST_OUI			2
+#define ALE_UCAST_TOUCHED		3
+
+static inline int cpsw_ale_get_field(u32 *ale_entry, u32 start, u32 bits)
+{
+	int idx;
+
+	idx    = start / 32;
+	start -= idx * 32;
+	idx    = 2 - idx; /* flip */
+	return (ale_entry[idx] >> start) & BITMASK(bits);
+}
+
+static inline void cpsw_ale_set_field(u32 *ale_entry, u32 start, u32 bits,
+				      u32 value)
+{
+	int idx;
+
+	value &= BITMASK(bits);
+	idx    = start / 32;
+	start -= idx * 32;
+	idx    = 2 - idx; /* flip */
+	ale_entry[idx] &= ~(BITMASK(bits) << start);
+	ale_entry[idx] |=  (value << start);
+}
+
+#define DEFINE_ALE_FIELD(name, start, bits)				\
+static inline int cpsw_ale_get_##name(u32 *ale_entry)			\
+{									\
+	return cpsw_ale_get_field(ale_entry, start, bits);		\
+}									\
+static inline void cpsw_ale_set_##name(u32 *ale_entry, u32 value)	\
+{									\
+	cpsw_ale_set_field(ale_entry, start, bits, value);		\
+}
+
+DEFINE_ALE_FIELD(entry_type,		60,	2)
+DEFINE_ALE_FIELD(vlan_id,		48,	12)
+DEFINE_ALE_FIELD(mcast_state,		62,	2)
+DEFINE_ALE_FIELD(port_mask,		66,     3)
+DEFINE_ALE_FIELD(super,			65,	1)
+DEFINE_ALE_FIELD(ucast_type,		62,     2)
+DEFINE_ALE_FIELD(port_num,		66,     2)
+DEFINE_ALE_FIELD(blocked,		65,     1)
+DEFINE_ALE_FIELD(secure,		64,     1)
+DEFINE_ALE_FIELD(vlan_untag_force,	24,	3)
+DEFINE_ALE_FIELD(vlan_reg_mcast,	16,	3)
+DEFINE_ALE_FIELD(vlan_unreg_mcast,	8,	3)
+DEFINE_ALE_FIELD(vlan_member_list,	0,	3)
+DEFINE_ALE_FIELD(mcast,			40,	1)
+
+/* The MAC address field in the ALE entry cannot be macroized as above */
+static inline void cpsw_ale_get_addr(u32 *ale_entry, u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < 6; i++)
+		addr[i] = cpsw_ale_get_field(ale_entry, 40 - 8*i, 8);
+}
+
+static inline void cpsw_ale_set_addr(u32 *ale_entry, u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < 6; i++)
+		cpsw_ale_set_field(ale_entry, 40 - 8*i, 8, addr[i]);
+}
+
+static int cpsw_ale_read(struct cpsw_ale *ale, int idx, u32 *ale_entry)
+{
+	int i;
+
+	WARN_ON(idx > ale->params.ale_entries);
+
+	__raw_writel(idx, ale->params.ale_regs + ALE_TABLE_CONTROL);
+
+	for (i = 0; i < ALE_ENTRY_WORDS; i++)
+		ale_entry[i] = __raw_readl(ale->params.ale_regs +
+					   ALE_TABLE + 4 * i);
+
+	return idx;
+}
+
+static int cpsw_ale_write(struct cpsw_ale *ale, int idx, u32 *ale_entry)
+{
+	int i;
+
+	WARN_ON(idx > ale->params.ale_entries);
+
+	for (i = 0; i < ALE_ENTRY_WORDS; i++)
+		__raw_writel(ale_entry[i], ale->params.ale_regs +
+			     ALE_TABLE + 4 * i);
+
+	__raw_writel(idx | ALE_TABLE_WRITE, ale->params.ale_regs +
+		     ALE_TABLE_CONTROL);
+
+	return idx;
+}
+
+static int cpsw_ale_match_addr(struct cpsw_ale *ale, u8 *addr)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		u8 entry_addr[6];
+
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
+			continue;
+		cpsw_ale_get_addr(ale_entry, entry_addr);
+		if (memcmp(entry_addr, addr, 6) == 0)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static int cpsw_ale_match_free(struct cpsw_ale *ale)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type == ALE_TYPE_FREE)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static int cpsw_ale_find_ageable(struct cpsw_ale *ale)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		type = cpsw_ale_get_entry_type(ale_entry);
+		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
+			continue;
+		if (cpsw_ale_get_mcast(ale_entry))
+			continue;
+		type = cpsw_ale_get_ucast_type(ale_entry);
+		if (type != ALE_UCAST_PERSISTANT &&
+		    type != ALE_UCAST_OUI)
+			return idx;
+	}
+	return -ENOENT;
+}
+
+static void cpsw_ale_flush_mcast(struct cpsw_ale *ale, u32 *ale_entry,
+				 int port_mask)
+{
+	int mask;
+
+	mask = cpsw_ale_get_port_mask(ale_entry);
+	if ((mask & port_mask) == 0)
+		return; /* ports dont intersect, not interested */
+	mask &= ~port_mask;
+
+	/* free if only remaining port is host port */
+	if (mask)
+		cpsw_ale_set_port_mask(ale_entry, mask);
+	else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+}
+
+int cpsw_ale_flush_multicast(struct cpsw_ale *ale, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int ret, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		ret = cpsw_ale_get_entry_type(ale_entry);
+		if (ret != ALE_TYPE_ADDR && ret != ALE_TYPE_VLAN_ADDR)
+			continue;
+
+		if (cpsw_ale_get_mcast(ale_entry)) {
+			u8 addr[6];
+
+			cpsw_ale_get_addr(ale_entry, addr);
+			if (!is_broadcast_ether_addr(addr))
+				cpsw_ale_flush_mcast(ale, ale_entry, port_mask);
+		}
+
+		cpsw_ale_write(ale, idx, ale_entry);
+	}
+	return 0;
+}
+
+static void cpsw_ale_flush_ucast(struct cpsw_ale *ale, u32 *ale_entry,
+				 int port_mask)
+{
+	int port;
+
+	port = cpsw_ale_get_port_num(ale_entry);
+	if ((BIT(port) & port_mask) == 0)
+		return; /* ports dont intersect, not interested */
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+}
+
+int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int ret, idx;
+
+	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		ret = cpsw_ale_get_entry_type(ale_entry);
+		if (ret != ALE_TYPE_ADDR && ret != ALE_TYPE_VLAN_ADDR)
+			continue;
+
+		if (cpsw_ale_get_mcast(ale_entry))
+			cpsw_ale_flush_mcast(ale, ale_entry, port_mask);
+		else
+			cpsw_ale_flush_ucast(ale, ale_entry, port_mask);
+
+		cpsw_ale_write(ale, idx, ale_entry);
+	}
+	return 0;
+}
+
+int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port, int flags)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_ucast_type(ale_entry, ALE_UCAST_PERSISTANT);
+	cpsw_ale_set_secure(ale_entry, (flags & ALE_SECURE) ? 1 : 0);
+	cpsw_ale_set_blocked(ale_entry, (flags & ALE_BLOCKED) ? 1 : 0);
+	cpsw_ale_set_port_num(ale_entry, port);
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		idx = cpsw_ale_match_free(ale);
+	if (idx < 0)
+		idx = cpsw_ale_find_ageable(ale);
+	if (idx < 0)
+		return -ENOMEM;
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		return -ENOENT;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
+			int super, int mcast_state)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx, mask;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx >= 0)
+		cpsw_ale_read(ale, idx, ale_entry);
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_super(ale_entry, super);
+	cpsw_ale_set_mcast_state(ale_entry, mcast_state);
+
+	mask = cpsw_ale_get_port_mask(ale_entry);
+	port_mask |= mask;
+	cpsw_ale_set_port_mask(ale_entry, port_mask);
+
+	if (idx < 0)
+		idx = cpsw_ale_match_free(ale);
+	if (idx < 0)
+		idx = cpsw_ale_find_ageable(ale);
+	if (idx < 0)
+		return -ENOMEM;
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	idx = cpsw_ale_match_addr(ale, addr);
+	if (idx < 0)
+		return -EINVAL;
+
+	cpsw_ale_read(ale, idx, ale_entry);
+
+	if (port_mask)
+		cpsw_ale_set_port_mask(ale_entry, port_mask);
+	else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+struct ale_control_info {
+	const char	*name;
+	int		offset, port_offset;
+	int		shift, port_shift;
+	int		bits;
+};
+
+static const struct ale_control_info ale_controls[ALE_NUM_CONTROLS] = {
+	[ALE_ENABLE]		= {
+		.name		= "enable",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 31,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_CLEAR]		= {
+		.name		= "clear",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 30,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_AGEOUT]		= {
+		.name		= "ageout",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 29,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_VLAN_NOLEARN]	= {
+		.name		= "vlan_nolearn",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 7,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_NO_PORT_VLAN]	= {
+		.name		= "no_port_vlan",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 6,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_OUI_DENY]		= {
+		.name		= "oui_deny",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 5,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_BYPASS]		= {
+		.name		= "bypass",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 4,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_RATE_LIMIT_TX]	= {
+		.name		= "rate_limit_tx",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 3,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_VLAN_AWARE]	= {
+		.name		= "vlan_aware",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 2,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_AUTH_ENABLE]	= {
+		.name		= "auth_enable",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 1,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_RATE_LIMIT]	= {
+		.name		= "rate_limit",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_STATE]	= {
+		.name		= "port_state",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 2,
+	},
+	[ALE_PORT_DROP_UNTAGGED] = {
+		.name		= "drop_untagged",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 2,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_DROP_UNKNOWN_VLAN] = {
+		.name		= "drop_unknown",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 3,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_NOLEARN]	= {
+		.name		= "nolearn",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 4,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
+	[ALE_PORT_MCAST_LIMIT]	= {
+		.name		= "mcast_limit",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 16,
+		.port_shift	= 0,
+		.bits		= 8,
+	},
+	[ALE_PORT_BCAST_LIMIT]	= {
+		.name		= "bcast_limit",
+		.offset		= ALE_PORTCTL,
+		.port_offset	= 4,
+		.shift		= 24,
+		.port_shift	= 0,
+		.bits		= 8,
+	},
+	[ALE_PORT_UNKNOWN_VLAN_MEMBER] = {
+		.name		= "unknown_vlan_member",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNKNOWN_MCAST_FLOOD] = {
+		.name		= "unknown_mcast_flood",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 8,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNKNOWN_REG_MCAST_FLOOD] = {
+		.name		= "unknown_reg_flood",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 16,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+	[ALE_PORT_UNTAGGED_EGRESS] = {
+		.name		= "untagged_egress",
+		.offset		= ALE_UNKNOWNVLAN,
+		.port_offset	= 0,
+		.shift		= 24,
+		.port_shift	= 0,
+		.bits		= 6,
+	},
+};
+
+int cpsw_ale_control_set(struct cpsw_ale *ale, int port, int control,
+			 int value)
+{
+	const struct ale_control_info *info;
+	int offset, shift;
+	u32 tmp, mask;
+
+	if (control < 0 || control >= ARRAY_SIZE(ale_controls))
+		return -EINVAL;
+
+	info = &ale_controls[control];
+	if (info->port_offset == 0 && info->port_shift == 0)
+		port = 0; /* global, port is a dont care */
+
+	if (port < 0 || port > ale->params.ale_ports)
+		return -EINVAL;
+
+	mask = BITMASK(info->bits);
+	if (value & ~mask)
+		return -EINVAL;
+
+	offset = info->offset + (port * info->port_offset);
+	shift  = info->shift  + (port * info->port_shift);
+
+	tmp = __raw_readl(ale->params.ale_regs + offset);
+	tmp = (tmp & ~(mask << shift)) | (value << shift);
+	__raw_writel(tmp, ale->params.ale_regs + offset);
+
+	return 0;
+}
+
+int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control)
+{
+	const struct ale_control_info *info;
+	int offset, shift;
+	u32 tmp;
+
+	if (control < 0 || control >= ARRAY_SIZE(ale_controls))
+		return -EINVAL;
+
+	info = &ale_controls[control];
+	if (info->port_offset == 0 && info->port_shift == 0)
+		port = 0; /* global, port is a dont care */
+
+	if (port < 0 || port > ale->params.ale_ports)
+		return -EINVAL;
+
+	offset = info->offset + (port * info->port_offset);
+	shift  = info->shift  + (port * info->port_shift);
+
+	tmp = __raw_readl(ale->params.ale_regs + offset) >> shift;
+	return tmp & BITMASK(info->bits);
+}
+
+static enum hrtimer_restart cpsw_ale_timer(struct hrtimer *timer)
+{
+	struct cpsw_ale *ale = container_of(timer, struct cpsw_ale, timer);
+
+	cpsw_ale_control_set(ale, 0, ALE_AGEOUT, 1);
+
+	return HRTIMER_NORESTART;
+}
+
+int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout)
+{
+	hrtimer_cancel(&ale->timer);
+	ale->ageout = ageout * 1000 * 1000 * 1000;
+	if (ale->ageout) {
+		hrtimer_start(&ale->timer, ale->ageout, HRTIMER_MODE_REL);
+	}
+	return 0;
+}
+
+void cpsw_ale_start(struct cpsw_ale *ale)
+{
+	u32 rev;
+
+	rev = __raw_readl(ale->params.ale_regs + ALE_IDVER);
+	dev_dbg(ale->params.dev, "initialized cpsw ale revision %d.%d\n",
+		ALE_VERSION_MAJOR(rev), ALE_VERSION_MINOR(rev));
+	cpsw_ale_control_set(ale, 0, ALE_ENABLE, 1);
+	cpsw_ale_control_set(ale, 0, ALE_CLEAR, 1);
+
+	if (ale->ageout) {
+		hrtimer_start(&ale->timer, ale->ageout, HRTIMER_MODE_REL);
+	}
+}
+
+void cpsw_ale_stop(struct cpsw_ale *ale)
+{
+	hrtimer_cancel(&ale->timer);
+}
+
+struct cpsw_ale *cpsw_ale_create(struct cpsw_ale_params *params)
+{
+	struct cpsw_ale *ale;
+
+	ale = kzalloc(sizeof(*ale), GFP_KERNEL);
+	if (!ale)
+		return NULL;
+	hrtimer_init(&ale->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	ale->timer.function = cpsw_ale_timer;
+
+	ale->params = *params;
+	ale->ageout = ale->params.ale_ageout * 1000 * 1000 * 1000;
+
+	return ale;
+}
+
+int cpsw_ale_destroy(struct cpsw_ale *ale)
+{
+	if (!ale)
+		return -EINVAL;
+	cpsw_ale_stop(ale);
+	cpsw_ale_control_set(ale, 0, ALE_ENABLE, 0);
+	hrtimer_cancel(&ale->timer);
+	kfree(ale);
+	return 0;
+}
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw_ale.h linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw_ale.h
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw_ale.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw_ale.h	2020-06-15 16:12:31.471695575 +0300
@@ -0,0 +1,100 @@
+/*
+ * Texas Instruments 3-Port Ethernet Switch Address Lookup Engine APIs
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __TI_CPSW_ALE_H__
+#define __TI_CPSW_ALE_H__
+
+#include <rtnet_port.h>
+
+struct cpsw_ale_params {
+	struct device		*dev;
+	void __iomem		*ale_regs;
+	unsigned long		ale_ageout;	/* in secs */
+	unsigned long		ale_entries;
+	unsigned long		ale_ports;
+};
+
+struct cpsw_ale {
+	struct cpsw_ale_params	params;
+	struct hrtimer		timer;
+	ktime_t			ageout;
+};
+
+enum cpsw_ale_control {
+	/* global */
+	ALE_ENABLE,
+	ALE_CLEAR,
+	ALE_AGEOUT,
+	ALE_VLAN_NOLEARN,
+	ALE_NO_PORT_VLAN,
+	ALE_OUI_DENY,
+	ALE_BYPASS,
+	ALE_RATE_LIMIT_TX,
+	ALE_VLAN_AWARE,
+	ALE_AUTH_ENABLE,
+	ALE_RATE_LIMIT,
+	/* port controls */
+	ALE_PORT_STATE,
+	ALE_PORT_DROP_UNTAGGED,
+	ALE_PORT_DROP_UNKNOWN_VLAN,
+	ALE_PORT_NOLEARN,
+	ALE_PORT_UNKNOWN_VLAN_MEMBER,
+	ALE_PORT_UNKNOWN_MCAST_FLOOD,
+	ALE_PORT_UNKNOWN_REG_MCAST_FLOOD,
+	ALE_PORT_UNTAGGED_EGRESS,
+	ALE_PORT_BCAST_LIMIT,
+	ALE_PORT_MCAST_LIMIT,
+	ALE_NUM_CONTROLS,
+};
+
+enum cpsw_ale_port_state {
+	ALE_PORT_STATE_DISABLE	= 0x00,
+	ALE_PORT_STATE_BLOCK	= 0x01,
+	ALE_PORT_STATE_LEARN	= 0x02,
+	ALE_PORT_STATE_FORWARD	= 0x03,
+};
+
+/* ALE unicast entry flags - passed into cpsw_ale_add_ucast() */
+#define ALE_SECURE			1
+#define ALE_BLOCKED			2
+
+#define ALE_MCAST_FWD			0
+#define ALE_MCAST_BLOCK_LEARN_FWD	1
+#define ALE_MCAST_FWD_LEARN		2
+#define ALE_MCAST_FWD_2			3
+
+struct cpsw_ale *cpsw_ale_create(struct cpsw_ale_params *params);
+int cpsw_ale_destroy(struct cpsw_ale *ale);
+
+void cpsw_ale_start(struct cpsw_ale *ale);
+void cpsw_ale_stop(struct cpsw_ale *ale);
+
+int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout);
+int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask);
+int cpsw_ale_flush_multicast(struct cpsw_ale *ale, int port_mask);
+int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port, int flags);
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port);
+int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
+			int super, int mcast_state);
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask);
+
+int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control);
+int cpsw_ale_control_set(struct cpsw_ale *ale, int port,
+			 int control, int value);
+
+#endif
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw.c linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw.c
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw.c	2020-06-15 16:12:31.471695575 +0300
@@ -0,0 +1,1844 @@
+/*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * 2019/09/20
+ * Modifications to Geoffrey Bonneville
+ * and Hidde Verstoep RTNet port to beaglebone-black,
+ * modifications made by Laurentiu-Cristian Duca:
+ * - use a dummy net_device as a glue between rtnet_device, 
+ * phy_connect() and cpsw_adjust_link()
+ *
+ * Texas Instruments Ethernet Switch Driver
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/timer.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/irqreturn.h>
+#include <linux/interrupt.h>
+#include <linux/if_ether.h>
+#include <linux/etherdevice.h>
+#include <linux/netdevice.h>
+#include <linux/net_tstamp.h>
+#include <linux/phy.h>
+#include <linux/workqueue.h>
+#include <linux/delay.h>
+#include <linux/pm_runtime.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_device.h>
+#include "cpsw.h"
+
+#include "cpsw_ale.h"
+#include "cpts.h"
+#include "davinci_cpdma.h"
+
+#define RX_RING_SIZE 8
+#define PM_NEGATIVE_DELAY			-2000
+
+/*static inline void rtskb_tx_timestamp(struct rtskb *skb){
+	if (skb->xmit_stamp)
+		*skb->xmit_stamp = cpu_to_be64(rtdm_clock_read() + *skb->xmit_stamp);
+}*/
+
+#define AM33XX_CTRL_MAC_LO_REG(offset, id) ((offset) + 0x8 * (id))
+#define AM33XX_CTRL_MAC_HI_REG(offset, id) ((offset) + 0x8 * (id) + 0x4)
+
+int cpsw_am33xx_cm_get_macid(struct device *dev, u16 offset, int slave,
+			     u8 *mac_addr)
+{
+	u32 macid_lo;
+	u32 macid_hi;
+	struct regmap *syscon;
+
+	syscon = syscon_regmap_lookup_by_phandle(dev->of_node, "syscon");
+	if (IS_ERR(syscon)) {
+		if (PTR_ERR(syscon) == -ENODEV)
+			return 0;
+		return PTR_ERR(syscon);
+	}
+
+	regmap_read(syscon, AM33XX_CTRL_MAC_LO_REG(offset, slave),
+		    &macid_lo);
+	regmap_read(syscon, AM33XX_CTRL_MAC_HI_REG(offset, slave),
+		    &macid_hi);
+
+	mac_addr[5] = (macid_lo >> 8) & 0xff;
+	mac_addr[4] = macid_lo & 0xff;
+	mac_addr[3] = (macid_hi >> 24) & 0xff;
+	mac_addr[2] = (macid_hi >> 16) & 0xff;
+	mac_addr[1] = (macid_hi >> 8) & 0xff;
+	mac_addr[0] = macid_hi & 0xff;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpsw_am33xx_cm_get_macid);
+
+static inline int rtskb_tailroom(const struct rtskb *skb){
+	return skb->end - skb->tail;
+}
+
+#define CPSW_DEBUG	(NETIF_MSG_HW		| NETIF_MSG_WOL		| \
+			 NETIF_MSG_DRV		| NETIF_MSG_LINK	| \
+			 NETIF_MSG_IFUP		| NETIF_MSG_INTR	| \
+			 NETIF_MSG_PROBE	| NETIF_MSG_TIMER	| \
+			 NETIF_MSG_IFDOWN	| NETIF_MSG_RX_ERR	| \
+			 NETIF_MSG_TX_ERR	| NETIF_MSG_TX_DONE	| \
+			 NETIF_MSG_PKTDATA	| NETIF_MSG_TX_QUEUED	| \
+			 NETIF_MSG_RX_STATUS)
+
+#define cpsw_info(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_info(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_err(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_err(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_dbg(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_dbg(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define cpsw_notice(priv, type, format, ...)		\
+do {								\
+	if (netif_msg_##type(priv) && net_ratelimit())		\
+		dev_notice(priv->dev, format, ## __VA_ARGS__);	\
+} while (0)
+
+#define ALE_ALL_PORTS		0x7
+
+#define CPSW_MAJOR_VERSION(reg)		(reg >> 8 & 0x7)
+#define CPSW_MINOR_VERSION(reg)		(reg & 0xff)
+#define CPSW_RTL_VERSION(reg)		((reg >> 11) & 0x1f)
+
+#define CPSW_VERSION_1		0x19010a
+#define CPSW_VERSION_2		0x19010c
+
+#define HOST_PORT_NUM		0
+#define SLIVER_SIZE		0x40
+
+#define CPSW1_HOST_PORT_OFFSET	0x028
+#define CPSW1_SLAVE_OFFSET	0x050
+#define CPSW1_SLAVE_SIZE	0x040
+#define CPSW1_CPDMA_OFFSET	0x100
+#define CPSW1_STATERAM_OFFSET	0x200
+#define CPSW1_CPTS_OFFSET	0x500
+#define CPSW1_ALE_OFFSET	0x600
+#define CPSW1_SLIVER_OFFSET	0x700
+
+#define CPSW2_HOST_PORT_OFFSET	0x108
+#define CPSW2_SLAVE_OFFSET	0x200
+#define CPSW2_SLAVE_SIZE	0x100
+#define CPSW2_CPDMA_OFFSET	0x800
+#define CPSW2_STATERAM_OFFSET	0xa00
+#define CPSW2_CPTS_OFFSET	0xc00
+#define CPSW2_ALE_OFFSET	0xd00
+#define CPSW2_SLIVER_OFFSET	0xd80
+#define CPSW2_BD_OFFSET		0x2000
+
+#define CPDMA_RXTHRESH		0x0c0
+#define CPDMA_RXFREE		0x0e0
+#define CPDMA_TXHDP		0x00
+#define CPDMA_RXHDP		0x20
+#define CPDMA_TXCP		0x40
+#define CPDMA_RXCP		0x60
+
+#define CPSW_POLL_WEIGHT	64
+#define CPSW_MIN_PACKET_SIZE	60
+#define CPSW_MAX_PACKET_SIZE	(1500 + 14 + 4 + 4)
+
+#define RX_PRIORITY_MAPPING	0x76543210
+#define TX_PRIORITY_MAPPING	0x33221100
+#define CPDMA_TX_PRIORITY_MAP	0x76543210
+
+#if 0
+#define cpsw_enable_irq(priv)	\
+	do {			\
+		u32 i;		\
+		for (i = 0; i < priv->num_irqs; i++) \
+			rtdm_irq_enable(&priv->irqs_table[i]); \
+	} while (0);
+#define cpsw_disable_irq(priv)	\
+	do {			\
+		u32 i;		\
+		for (i = 0; i < priv->num_irqs; i++) \
+			rtdm_irq_disable(&priv->irqs_table[i]); \
+	} while (0);
+#endif
+
+static int debug_level;
+module_param(debug_level, int, 0);
+MODULE_PARM_DESC(debug_level, "cpsw debug level (NETIF_MSG bits)");
+
+static int ale_ageout = 10;
+module_param(ale_ageout, int, 0);
+MODULE_PARM_DESC(ale_ageout, "cpsw ale ageout interval (seconds)");
+
+static int rx_packet_max = CPSW_MAX_PACKET_SIZE;
+module_param(rx_packet_max, int, 0);
+MODULE_PARM_DESC(rx_packet_max, "maximum receive packet size (bytes)");
+
+struct cpsw_wr_regs {
+	u32	id_ver;
+	u32	soft_reset;
+	u32	control;
+	u32	int_control;
+	u32	c0_rx_thresh_en;
+	u32	c0_rx_en;
+	u32	c0_tx_en;
+	u32	c0_misc_en;
+	u32	c1_rx_thresh_en;
+	u32	c1_rx_en;
+	u32	c1_tx_en;
+	u32	c1_misc_en;
+	u32	c2_rx_thresh_en;
+	u32	c2_rx_en;
+	u32	c2_tx_en;
+	u32	c2_misc_en;
+	u32	c0_rx_thresh_stat;
+	u32	c0_rx_stat;
+	u32	c0_tx_stat;
+	u32	c0_misc_stat;
+	u32	c1_rx_thresh_stat;
+	u32	c1_rx_stat;
+	u32	c1_tx_stat;
+	u32	c1_misc_stat;
+	u32	c2_rx_thresh_stat;
+	u32	c2_rx_stat;
+	u32	c2_tx_stat;
+	u32	c2_misc_stat;
+	u32	c0_rx_imax;
+	u32	c0_tx_imax;
+	u32	c1_rx_imax;
+	u32	c1_tx_imax;
+	u32	c2_rx_imax;
+	u32	c2_tx_imax;
+	u32	rgmii_ctl;
+};
+
+struct cpsw_ss_regs {
+	u32	id_ver;
+	u32	control;
+	u32	soft_reset;
+	u32	stat_port_en;
+	u32	ptype;
+	u32	soft_idle;
+	u32	thru_rate;
+	u32	gap_thresh;
+	u32	tx_start_wds;
+	u32	flow_control;
+	u32	vlan_ltype;
+	u32	ts_ltype;
+	u32	dlr_ltype;
+};
+
+/* CPSW_PORT_V1 */
+#define CPSW1_MAX_BLKS      0x00 /* Maximum FIFO Blocks */
+#define CPSW1_BLK_CNT       0x04 /* FIFO Block Usage Count (Read Only) */
+#define CPSW1_TX_IN_CTL     0x08 /* Transmit FIFO Control */
+#define CPSW1_PORT_VLAN     0x0c /* VLAN Register */
+#define CPSW1_TX_PRI_MAP    0x10 /* Tx Header Priority to Switch Pri Mapping */
+#define CPSW1_TS_CTL        0x14 /* Time Sync Control */
+#define CPSW1_TS_SEQ_LTYPE  0x18 /* Time Sync Sequence ID Offset and Msg Type */
+#define CPSW1_TS_VLAN       0x1c /* Time Sync VLAN1 and VLAN2 */
+
+/* CPSW_PORT_V2 */
+#define CPSW2_CONTROL       0x00 /* Control Register */
+#define CPSW2_MAX_BLKS      0x08 /* Maximum FIFO Blocks */
+#define CPSW2_BLK_CNT       0x0c /* FIFO Block Usage Count (Read Only) */
+#define CPSW2_TX_IN_CTL     0x10 /* Transmit FIFO Control */
+#define CPSW2_PORT_VLAN     0x14 /* VLAN Register */
+#define CPSW2_TX_PRI_MAP    0x18 /* Tx Header Priority to Switch Pri Mapping */
+#define CPSW2_TS_SEQ_MTYPE  0x1c /* Time Sync Sequence ID Offset and Msg Type */
+
+/* CPSW_PORT_V1 and V2 */
+#define SA_LO               0x20 /* CPGMAC_SL Source Address Low */
+#define SA_HI               0x24 /* CPGMAC_SL Source Address High */
+#define SEND_PERCENT        0x28 /* Transmit Queue Send Percentages */
+
+/* CPSW_PORT_V2 only */
+#define RX_DSCP_PRI_MAP0    0x30 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP1    0x34 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP2    0x38 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP3    0x3c /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP4    0x40 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP5    0x44 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP6    0x48 /* Rx DSCP Priority to Rx Packet Mapping */
+#define RX_DSCP_PRI_MAP7    0x4c /* Rx DSCP Priority to Rx Packet Mapping */
+
+/* Bit definitions for the CPSW2_CONTROL register */
+#define PASS_PRI_TAGGED     (1<<24) /* Pass Priority Tagged */
+#define VLAN_LTYPE2_EN      (1<<21) /* VLAN LTYPE 2 enable */
+#define VLAN_LTYPE1_EN      (1<<20) /* VLAN LTYPE 1 enable */
+#define DSCP_PRI_EN         (1<<16) /* DSCP Priority Enable */
+#define TS_320              (1<<14) /* Time Sync Dest Port 320 enable */
+#define TS_319              (1<<13) /* Time Sync Dest Port 319 enable */
+#define TS_132              (1<<12) /* Time Sync Dest IP Addr 132 enable */
+#define TS_131              (1<<11) /* Time Sync Dest IP Addr 131 enable */
+#define TS_130              (1<<10) /* Time Sync Dest IP Addr 130 enable */
+#define TS_129              (1<<9)  /* Time Sync Dest IP Addr 129 enable */
+#define TS_BIT8             (1<<8)  /* ts_ttl_nonzero? */
+#define TS_ANNEX_D_EN       (1<<4)  /* Time Sync Annex D enable */
+#define TS_LTYPE2_EN        (1<<3)  /* Time Sync LTYPE 2 enable */
+#define TS_LTYPE1_EN        (1<<2)  /* Time Sync LTYPE 1 enable */
+#define TS_TX_EN            (1<<1)  /* Time Sync Transmit Enable */
+#define TS_RX_EN            (1<<0)  /* Time Sync Receive Enable */
+
+#define CTRL_TS_BITS \
+	(TS_320 | TS_319 | TS_132 | TS_131 | TS_130 | TS_129 | TS_BIT8 | \
+	 TS_ANNEX_D_EN | TS_LTYPE1_EN)
+
+#define CTRL_ALL_TS_MASK (CTRL_TS_BITS | TS_TX_EN | TS_RX_EN)
+#define CTRL_TX_TS_BITS  (CTRL_TS_BITS | TS_TX_EN)
+#define CTRL_RX_TS_BITS  (CTRL_TS_BITS | TS_RX_EN)
+
+/* Bit definitions for the CPSW2_TS_SEQ_MTYPE register */
+#define TS_SEQ_ID_OFFSET_SHIFT   (16)    /* Time Sync Sequence ID Offset */
+#define TS_SEQ_ID_OFFSET_MASK    (0x3f)
+#define TS_MSG_TYPE_EN_SHIFT     (0)     /* Time Sync Message Type Enable */
+#define TS_MSG_TYPE_EN_MASK      (0xffff)
+
+/* The PTP event messages - Sync, Delay_Req, Pdelay_Req, and Pdelay_Resp. */
+#define EVENT_MSG_BITS ((1<<0) | (1<<1) | (1<<2) | (1<<3))
+
+/* Bit definitions for the CPSW1_TS_CTL register */
+#define CPSW_V1_TS_RX_EN		BIT(0)
+#define CPSW_V1_TS_TX_EN		BIT(4)
+#define CPSW_V1_MSG_TYPE_OFS		16
+
+/* Bit definitions for the CPSW1_TS_SEQ_LTYPE register */
+#define CPSW_V1_SEQ_ID_OFS_SHIFT	16
+
+struct cpsw_host_regs {
+	u32	max_blks;
+	u32	blk_cnt;
+	u32	flow_thresh;
+	u32	port_vlan;
+	u32	tx_pri_map;
+	u32	cpdma_tx_pri_map;
+	u32	cpdma_rx_chan_map;
+};
+
+struct cpsw_sliver_regs {
+	u32	id_ver;
+	u32	mac_control;
+	u32	mac_status;
+	u32	soft_reset;
+	u32	rx_maxlen;
+	u32	__reserved_0;
+	u32	rx_pause;
+	u32	tx_pause;
+	u32	__reserved_1;
+	u32	rx_pri_map;
+};
+
+struct cpsw_slave {
+	void __iomem			*regs;
+	struct cpsw_sliver_regs __iomem	*sliver;
+	int				slave_num;
+	u32				mac_control;
+	struct cpsw_slave_data		*data;
+	struct phy_device		*phy;
+};
+
+static inline u32 slave_read(struct cpsw_slave *slave, u32 offset)
+{
+	return __raw_readl(slave->regs + offset);
+}
+
+static inline void slave_write(struct cpsw_slave *slave, u32 val, u32 offset)
+{
+	__raw_writel(val, slave->regs + offset);
+}
+
+struct cpsw_priv {
+	raw_spinlock_t			lock;
+	struct platform_device		*pdev;
+	struct rtnet_device		*ndev;
+	struct resource			*cpsw_res;
+	struct resource			*cpsw_wr_res;
+	struct device			*dev;
+	struct cpsw_platform_data	data;
+	struct cpsw_ss_regs __iomem	*regs;
+	struct cpsw_wr_regs __iomem	*wr_regs;
+	struct cpsw_host_regs __iomem	*host_port_regs;
+	u32				msg_enable;
+	u32				version;
+	struct net_device_stats		stats;
+	int				rx_packet_max;
+	int				host_port;
+	struct clk			*clk;
+	u8				mac_addr[ETH_ALEN];
+	struct cpsw_slave		*slaves;
+	struct cpdma_ctlr		*dma;
+	struct cpdma_chan		*txch, *rxch;
+	struct cpsw_ale			*ale;
+	/* snapshot of IRQ numbers */
+	int irqs_table[4];
+	u32 num_irqs;
+	bool irq_enabled;
+	struct cpts cpts;
+
+	struct rtskb_pool skb_pool;
+
+	struct net_device *phy_phony_net_device;
+};
+
+struct dummy_netdev_priv {
+	struct rtnet_device *rtdev;
+};
+
+static inline struct rtskb *dev_alloc_rtskb_ip_align(struct rtnet_device *ndev, unsigned int size){
+	struct cpsw_priv *priv = ndev->priv;
+	struct rtskb_pool *pool = &priv->skb_pool;
+	struct rtskb *skb = alloc_rtskb(size + NET_IP_ALIGN, pool);
+
+	if(skb)
+		skb->rtdev = ndev;
+	if(NET_IP_ALIGN && skb)
+		rtskb_reserve(skb, NET_IP_ALIGN);
+	return skb;
+}
+
+#define napi_to_priv(napi)	container_of(napi, struct cpsw_priv, napi)
+#define for_each_slave(priv, func, arg...)			\
+	do {							\
+		int idx;					\
+		for (idx = 0; idx < (priv)->data.slaves; idx++)	\
+			(func)((priv)->slaves + idx, ##arg);	\
+	} while (0)
+
+#if 0
+static void cpsw_ndo_set_rx_mode(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	if (ndev->flags & IFF_PROMISC) {
+		/* Enable promiscuous mode */
+		dev_err(priv->dev, "Ignoring Promiscuous mode\n");
+		return;
+	}
+
+	/* Clear all mcast from ALE */
+	cpsw_ale_flush_multicast(priv->ale, ALE_ALL_PORTS << priv->host_port);
+
+	if (!netdev_mc_empty(ndev)) {
+		struct netdev_hw_addr *ha;
+
+		/* program multicast address list into ALE register */
+		netdev_for_each_mc_addr(ha, ndev) {
+			cpsw_ale_add_mcast(priv->ale, (u8 *)ha->addr,
+				ALE_ALL_PORTS << priv->host_port, 0, 0);
+		}
+	}
+}
+#endif
+
+static void cpsw_intr_enable(struct cpsw_priv *priv)
+{
+	__raw_writel(0xFF, &priv->wr_regs->c0_tx_en);
+	__raw_writel(0xFF, &priv->wr_regs->c0_rx_en);
+
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	return;
+}
+
+static void cpsw_intr_disable(struct cpsw_priv *priv)
+{
+	__raw_writel(0, &priv->wr_regs->c0_tx_en);
+	__raw_writel(0, &priv->wr_regs->c0_rx_en);
+
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	return;
+}
+
+void cpsw_tx_handler(void *token, int len, int status)
+{
+	struct rtskb		*skb = token;
+	struct rtnet_device	*ndev = skb->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+
+#if 0
+	rtdm_printk("cpsw_tx_handler(%x, %d, %d)\n", token, len, status);
+#endif
+
+	if (unlikely(rtnetif_queue_stopped(ndev)))
+		rtnetif_wake_queue(ndev);
+	cpts_tx_timestamp(&priv->cpts, skb);
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += len;
+	dev_kfree_rtskb(skb);
+}
+
+void cpsw_rx_handler(void *token, int len, int status)
+{
+	struct rtskb		*skb = token;
+	struct rtnet_device	*ndev = skb->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+	int			ret = 0;
+	ktime_t time_stamp = ktime_get();
+
+	/* free and bail if we are shutting down */
+	if (unlikely(!rtnetif_running(ndev)) ||
+			unlikely(!rtnetif_carrier_ok(ndev))) {
+		dev_kfree_rtskb(skb);
+		return;
+	}
+
+	if (likely(status >= 0)) {
+		skb->time_stamp = time_stamp;
+		rtskb_put(skb, len);
+		cpts_rx_timestamp(&priv->cpts, skb);
+		skb->protocol = rt_eth_type_trans(skb, ndev);
+		rtnetif_rx(skb);
+		priv->stats.rx_bytes += len;
+		priv->stats.rx_packets++;
+		skb = NULL;
+	}
+
+	if (unlikely(!rtnetif_running(ndev))) {
+		if (skb)
+			dev_kfree_rtskb(skb);
+		return;
+	}
+
+	if (likely(!skb)) {
+		skb = dev_alloc_rtskb_ip_align(ndev, priv->rx_packet_max);
+		if (WARN_ON(!skb))
+			return;
+
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					rtskb_tailroom(skb), GFP_KERNEL);
+	}
+	WARN_ON(ret < 0);
+}
+
+static inline int cpsw_get_slave_port(struct cpsw_priv *priv, u32 slave_num)
+{
+	if (priv->host_port == 0)
+		return slave_num + 1;
+	else
+		return slave_num;
+}
+
+#if 0
+static int cpsw_poll(struct cpsw_priv *priv, int budget)
+{
+	int			num_tx, num_rx, num_total_tx, num_total_rx;
+	int			budget_left;
+
+	budget_left = budget;
+
+	/* read status and throw away */
+	(void)__raw_readl(&priv->wr_regs->c0_tx_stat);
+
+	/* handle all transmits */
+	num_total_tx = 0;
+	while (budget_left > 0 &&
+		(num_tx = cpdma_chan_process(priv->txch, 128)) > 0) {
+		budget_left -= num_tx;
+		num_total_tx += num_tx;
+	}
+
+	if (num_total_tx > 0 && budget_left > 0)
+		cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	/* read status and throw away */
+	(void)__raw_readl(&priv->wr_regs->c0_rx_stat);
+
+	/* handle all receives */
+	num_total_rx = 0;
+	while (budget_left > 0 &&
+		(num_rx = cpdma_chan_process(priv->rxch, budget_left)) > 0) {
+		budget_left -= num_rx;
+		num_total_rx += num_rx;
+	}
+
+	if (num_total_rx > 0 && budget_left > 0)
+		cpdma_ctlr_eoi(priv->dma, 0x01);
+
+	if ((num_total_rx + num_total_tx) < budget) {
+		cpsw_intr_enable(priv);
+		if (priv->irq_enabled == false) {
+			cpsw_enable_irq(priv);
+			priv->irq_enabled = true;
+		}
+	}
+
+	rtdm_printk("lost packets\n");
+	return num_total_rx + num_total_rx;
+}
+#endif
+
+#if 0
+static int cpsw_interrupt(rtdm_irq_t *irq_handle)
+{
+	struct cpsw_priv *priv = rtdm_irq_get_arg(irq_handle, struct cpsw_priv);
+
+	if (likely(rtnetif_running(priv->ndev))) {
+		cpsw_intr_disable(priv);
+		if (priv->irq_enabled == true) {
+			cpsw_disable_irq(priv);
+			priv->irq_enabled = false;
+		}
+		cpsw_poll(priv, CPSW_POLL_WEIGHT);
+	}
+
+	return IRQ_HANDLED;
+}
+#endif
+
+static irqreturn_t cpsw_rx_thresh_pend_irq(int irq, void *data)
+{
+	/* not handling this interrupt yet */
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_rx_pend_irq(int irq, void *data)
+{
+	struct cpsw_priv *priv = (struct cpsw_priv *)data;
+	int num_rx, total_rx;
+	u32 rx_stat;
+	unsigned long context;
+
+	rx_stat = __raw_readl(&priv->wr_regs->c0_rx_stat) & 0xff;
+	if (rx_stat == 0)
+		return IRQ_NONE;
+
+#if 0
+	rtdm_printk("cpsw_rx_pend_irq: %d\n", rx_stat);
+#endif
+
+	raw_spin_lock_irqsave(&priv->lock, context);
+
+	total_rx = 0;
+	while ((num_rx = cpdma_chan_process(priv->rxch, RX_RING_SIZE)) > 0)
+		total_rx += num_rx;
+
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+
+	raw_spin_unlock_irqrestore(&priv->lock, context);
+
+	if(total_rx > 0)
+		rt_mark_stack_mgr(priv->ndev);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_tx_pend_irq(int irq, void *data)
+{
+	struct cpsw_priv *priv = (struct cpsw_priv *) data;
+	int num_tx, total_tx;
+	u32 tx_stat;
+
+	tx_stat = __raw_readl(&priv->wr_regs->c0_tx_stat) & 0xff;
+	if (tx_stat == 0)
+		return IRQ_NONE;
+
+#if 0
+	rtdm_printk("cpsw_tx_pend_irq: %d\n", tx_stat);
+#endif
+
+	total_tx = 0;
+	while ((num_tx = cpdma_chan_process(priv->txch, 128)) > 0)
+		total_tx += num_tx;
+
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cpsw_misc_pend_irq(int irq, void *data)
+{
+	/* not handling this interrupt yet */
+	return IRQ_HANDLED;
+}
+
+static inline void soft_reset(const char *module, void __iomem *reg)
+{
+	unsigned long timeout = jiffies + HZ;
+
+	__raw_writel(1, reg);
+	do {
+		cpu_relax();
+	} while ((__raw_readl(reg) & 1) && time_after(timeout, jiffies));
+
+	WARN(__raw_readl(reg) & 1, "failed to soft-reset %s\n", module);
+}
+
+#define mac_hi(mac)	(((mac)[0] << 0) | ((mac)[1] << 8) |	\
+			 ((mac)[2] << 16) | ((mac)[3] << 24))
+#define mac_lo(mac)	(((mac)[4] << 0) | ((mac)[5] << 8))
+
+static void cpsw_set_slave_mac(struct cpsw_slave *slave,
+			       struct cpsw_priv *priv)
+{
+	slave_write(slave, mac_hi(priv->mac_addr), SA_HI);
+	slave_write(slave, mac_lo(priv->mac_addr), SA_LO);
+}
+
+static void _cpsw_adjust_link(struct cpsw_slave *slave,
+			      struct cpsw_priv *priv, bool *link)
+{
+	struct phy_device	*phy = slave->phy;
+	u32			mac_control = 0;
+	u32			slave_port;
+
+	if (!phy)
+		return;
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	if (phy->link) {
+		mac_control = priv->data.mac_control;
+
+		/* enable forwarding */
+		cpsw_ale_control_set(priv->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+		if (phy->speed == 1000)
+			mac_control |= BIT(7);	/* GIGABITEN	*/
+		if (phy->duplex)
+			mac_control |= BIT(0);	/* FULLDUPLEXEN	*/
+
+		/* set speed_in input in case RMII mode is used in 100Mbps */
+		if (phy->speed == 100)
+			mac_control |= BIT(15);
+
+		*link = true;
+	} else {
+		mac_control = 0;
+		/* disable forwarding */
+		cpsw_ale_control_set(priv->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_DISABLE);
+	}
+
+	if (mac_control != slave->mac_control) {
+		phy_print_status(phy);
+		__raw_writel(mac_control, &slave->sliver->mac_control);
+	}
+
+	slave->mac_control = mac_control;
+}
+
+static void cpsw_adjust_link(struct net_device *ndev_)
+{
+	struct dummy_netdev_priv *p = netdev_priv(ndev_);
+	struct rtnet_device	*ndev = (struct rtnet_device *)p->rtdev;
+	struct cpsw_priv	*priv = ndev->priv;
+	bool			link = false;
+
+	for_each_slave(priv, _cpsw_adjust_link, priv, &link);
+
+	if (link) {
+		rtnetif_carrier_on(ndev);
+		if (rtnetif_running(ndev))
+			rtnetif_wake_queue(ndev);
+	} else {
+		rtnetif_carrier_off(ndev);
+		rtnetif_stop_queue(ndev);
+	}
+}
+
+static inline int __show_stat(char *buf, int maxlen, const char *name, u32 val)
+{
+	static char *leader = "........................................";
+
+	if (!val)
+		return 0;
+	else
+		return snprintf(buf, maxlen, "%s %s %10d\n", name,
+				leader + strlen(name), val);
+}
+
+static void cpsw_slave_open(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	char name[32];
+	u32 slave_port;
+	struct net_device *dummy;
+	struct dummy_netdev_priv *p;
+
+	sprintf(name, "slave-%d", slave->slave_num);
+
+	soft_reset(name, &slave->sliver->soft_reset);
+
+	/* setup priority mapping */
+	__raw_writel(RX_PRIORITY_MAPPING, &slave->sliver->rx_pri_map);
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		slave_write(slave, TX_PRIORITY_MAPPING, CPSW1_TX_PRI_MAP);
+		break;
+	case CPSW_VERSION_2:
+		slave_write(slave, TX_PRIORITY_MAPPING, CPSW2_TX_PRI_MAP);
+		break;
+	}
+
+	/* setup max packet size, and mac address */
+	__raw_writel(priv->rx_packet_max, &slave->sliver->rx_maxlen);
+	cpsw_set_slave_mac(slave, priv);
+
+	slave->mac_control = 0;	/* no link yet */
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << slave_port, 0, ALE_MCAST_FWD_2);
+
+	/* Do the same as one did in macb.c rtnet driver
+	 * because the rtnet_device structure is much different than net_device.
+	 */
+	dummy = alloc_etherdev(sizeof(struct dummy_netdev_priv));
+	dev_alloc_name(dummy, "dummyeth%d");
+	p = netdev_priv(dummy);
+	p->rtdev = priv->ndev;
+	priv->phy_phony_net_device = dummy;
+	/* set dummy->dev.parent to priv->dev */
+	SET_NETDEV_DEV(dummy, priv->dev);
+	slave->phy = phy_connect(dummy, slave->data->phy_id,
+				 &cpsw_adjust_link, slave->data->phy_if);
+	if (IS_ERR(slave->phy)) {
+		dev_err(priv->dev, "phy %s not found on slave %d\n",
+			slave->data->phy_id, slave->slave_num);
+		slave->phy = NULL;
+	} else {
+		dev_info(priv->dev, "phy found : id is : 0x%x\n",
+			 slave->phy->phy_id);
+		phy_start(slave->phy);
+	}
+}
+
+static void cpsw_init_host_port(struct cpsw_priv *priv)
+{
+	/* soft reset the controller and initialize ale */
+	soft_reset("cpsw", &priv->regs->soft_reset);
+	cpsw_ale_start(priv->ale);
+
+	/* switch to vlan unaware mode */
+	cpsw_ale_control_set(priv->ale, 0, ALE_VLAN_AWARE, 0);
+
+	/* setup host port priority mapping */
+	__raw_writel(CPDMA_TX_PRIORITY_MAP,
+		     &priv->host_port_regs->cpdma_tx_pri_map);
+	__raw_writel(0, &priv->host_port_regs->cpdma_rx_chan_map);
+
+	cpsw_ale_control_set(priv->ale, priv->host_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_add_ucast(priv->ale, priv->mac_addr, priv->host_port, 0);
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << priv->host_port, 0, ALE_MCAST_FWD_2);
+}
+
+static int cpsw_ndo_open(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	int i, ret;
+	u32 reg;
+
+	cpsw_intr_disable(priv);
+	rtnetif_carrier_off(ndev);
+
+#if 0
+	pm_runtime_get_sync(&priv->pdev->dev);
+#endif
+	reg = priv->version;
+
+	dev_info(priv->dev, "initializing cpsw version %d.%d (%d)\n",
+		 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+		 CPSW_RTL_VERSION(reg));
+
+	/* initialize host and slave ports */
+	cpsw_init_host_port(priv);
+	for_each_slave(priv, cpsw_slave_open, priv);
+
+	/* setup tx dma to fixed prio and zero offset */
+	cpdma_control_set(priv->dma, CPDMA_TX_PRIO_FIXED, 1);
+	cpdma_control_set(priv->dma, CPDMA_RX_BUFFER_OFFSET, 0);
+
+	/* disable priority elevation and enable statistics on all ports */
+	__raw_writel(0, &priv->regs->ptype);
+
+	/* enable statistics collection only on the host port */
+	__raw_writel(0x7, &priv->regs->stat_port_en);
+
+	for (i = 0; i < RX_RING_SIZE; i++) {
+		struct rtskb *skb;
+
+		ret = -ENOMEM;
+		skb = dev_alloc_rtskb_ip_align(ndev, priv->rx_packet_max);
+		if (!skb)
+			break;
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					rtskb_tailroom(skb), GFP_KERNEL);
+		if (WARN_ON(ret < 0))
+			break;
+	}
+	/* continue even if we didn't manage to submit all receive descs */
+	cpsw_info(priv, ifup, "submitted %d rx descriptors\n", i);
+
+	cpdma_ctlr_start(priv->dma);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+
+	return 0;
+}
+
+static void cpsw_slave_stop(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	if (!slave->phy)
+		return;
+	phy_stop(slave->phy);
+	phy_disconnect(slave->phy);
+	slave->phy = NULL;
+}
+
+static int cpsw_ndo_stop(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+
+	cpsw_info(priv, ifdown, "shutting down cpsw device\n");
+#if 0
+	if (priv->irq_enabled == true) {
+		cpsw_disable_irq(priv);
+		priv->irq_enabled = false;
+	}
+#endif
+	rtnetif_stop_queue(priv->ndev);
+	rtnetif_carrier_off(priv->ndev);
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	cpdma_ctlr_stop(priv->dma);
+	cpsw_ale_stop(priv->ale);
+	for_each_slave(priv, cpsw_slave_stop, priv);
+#if 0
+	pm_runtime_put_sync(&priv->pdev->dev);
+#endif
+	return 0;
+}
+
+static netdev_tx_t cpsw_ndo_start_xmit(struct rtskb *skb,
+				       struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	int ret;
+
+	if (!rtskb_padto(skb, CPSW_MIN_PACKET_SIZE)) {
+		cpsw_err(priv, tx_err, "packet pad failed\n");
+		priv->stats.tx_dropped++;
+		return NETDEV_TX_OK;
+	}
+
+	#if 0
+	if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP && priv->cpts.tx_enable)
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	#endif
+
+	rtskb_tx_timestamp(skb);
+
+	ret = cpdma_chan_submit(priv->txch, skb, skb->data,
+				skb->len, GFP_KERNEL);
+	if (unlikely(ret != 0)) {
+		cpsw_err(priv, tx_err, "desc submit failed\n");
+		goto fail;
+	}
+
+	return NETDEV_TX_OK;
+fail:
+	priv->stats.tx_dropped++;
+	rtnetif_stop_queue(ndev);
+	return NETDEV_TX_BUSY;
+}
+
+#if 0
+static void cpsw_ndo_change_rx_flags(struct net_device *ndev, int flags)
+{
+	/*
+	 * The switch cannot operate in promiscuous mode without substantial
+	 * headache.  For promiscuous mode to work, we would need to put the
+	 * ALE in bypass mode and route all traffic to the host port.
+	 * Subsequently, the host will need to operate as a "bridge", learn,
+	 * and flood as needed.  For now, we simply complain here and
+	 * do nothing about it :-)
+	 */
+	if ((flags & IFF_PROMISC) && (ndev->flags & IFF_PROMISC))
+		dev_err(&ndev->dev, "promiscuity ignored!\n");
+
+	/*
+	 * The switch cannot filter multicast traffic unless it is configured
+	 * in "VLAN Aware" mode.  Unfortunately, VLAN awareness requires a
+	 * whole bunch of additional logic that this driver does not implement
+	 * at present.
+	 */
+	if ((flags & IFF_ALLMULTI) && !(ndev->flags & IFF_ALLMULTI))
+		dev_err(&ndev->dev, "multicast traffic cannot be filtered!\n");
+}
+#endif
+
+#ifdef CONFIG_TI_CPTS
+
+static void cpsw_hwtstamp_v1(struct cpsw_priv *priv)
+{
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
+	u32 ts_en, seq_id;
+
+	if (!priv->cpts.tx_enable && !priv->cpts.rx_enable) {
+		slave_write(slave, 0, CPSW1_TS_CTL);
+		return;
+	}
+
+	seq_id = (30 << CPSW_V1_SEQ_ID_OFS_SHIFT) | ETH_P_1588;
+	ts_en = EVENT_MSG_BITS << CPSW_V1_MSG_TYPE_OFS;
+
+	if (priv->cpts.tx_enable)
+		ts_en |= CPSW_V1_TS_TX_EN;
+
+	if (priv->cpts.rx_enable)
+		ts_en |= CPSW_V1_TS_RX_EN;
+
+	slave_write(slave, ts_en, CPSW1_TS_CTL);
+	slave_write(slave, seq_id, CPSW1_TS_SEQ_LTYPE);
+}
+
+static void cpsw_hwtstamp_v2(struct cpsw_priv *priv)
+{
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
+	u32 ctrl, mtype;
+
+	ctrl = slave_read(slave, CPSW2_CONTROL);
+	ctrl &= ~CTRL_ALL_TS_MASK;
+
+	if (priv->cpts.tx_enable)
+		ctrl |= CTRL_TX_TS_BITS;
+
+	if (priv->cpts.rx_enable)
+		ctrl |= CTRL_RX_TS_BITS;
+
+	mtype = (30 << TS_SEQ_ID_OFFSET_SHIFT) | EVENT_MSG_BITS;
+
+	slave_write(slave, mtype, CPSW2_TS_SEQ_MTYPE);
+	slave_write(slave, ctrl, CPSW2_CONTROL);
+	__raw_writel(ETH_P_1588, &priv->regs->ts_ltype);
+}
+
+static int cpsw_hwtstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
+{
+	struct cpsw_priv *priv = netdev_priv(dev);
+	struct cpts *cpts = &priv->cpts;
+	struct hwtstamp_config cfg;
+
+	if (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (cfg.flags)
+		return -EINVAL;
+
+	switch (cfg.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		cpts->tx_enable = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		cpts->tx_enable = 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (cfg.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		cpts->rx_enable = 0;
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+		return -ERANGE;
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		cpts->rx_enable = 1;
+		cfg.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		cpsw_hwtstamp_v1(priv);
+		break;
+	case CPSW_VERSION_2:
+		cpsw_hwtstamp_v2(priv);
+		break;
+	default:
+		return -ENOTSUPP;
+	}
+
+	return copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;
+}
+
+#endif /*CONFIG_TI_CPTS*/
+
+#if 0
+static int cpsw_ndo_ioctl(struct net_device *dev, struct ifreq *req, int cmd)
+{
+	if (!rtnetif_running(dev))
+		return -EINVAL;
+
+#ifdef CONFIG_TI_CPTS
+	if (cmd == SIOCSHWTSTAMP)
+		return cpsw_hwtstamp_ioctl(dev, req);
+#endif
+	return -ENOTSUPP;
+}
+#endif
+
+#if 0
+static void cpsw_ndo_tx_timeout(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	cpsw_err(priv, tx_err, "transmit timeout, restarting dma\n");
+	priv->stats.tx_errors++;
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	cpdma_chan_stop(priv->txch);
+	cpdma_chan_start(priv->txch);
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+}
+#endif
+
+static struct net_device_stats *cpsw_ndo_get_stats(struct rtnet_device *ndev)
+{
+	struct cpsw_priv *priv = ndev->priv;
+	return &priv->stats;
+}
+
+#if 0
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void cpsw_ndo_poll_controller(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	if (!priv->data.disable_napi)
+		cpsw_interrupt(ndev->irq, priv);
+	else {
+		/* bah! */
+		cpsw_rx_pend_irq(ndev->irq, priv);
+		cpsw_tx_pend_irq(ndev->irq, priv);
+	}
+	cpdma_ctlr_int_ctrl(priv->dma, true);
+	cpsw_intr_enable(priv);
+	cpdma_ctlr_eoi(priv->dma, 0x01);
+	cpdma_ctlr_eoi(priv->dma, 0x02);
+}
+#endif
+#endif
+
+#if 0
+static struct net_device_ops cpsw_netdev_ops = {
+	.ndo_open		= cpsw_ndo_open,
+	.ndo_stop		= cpsw_ndo_stop,
+	.ndo_start_xmit		= cpsw_ndo_start_xmit,
+	.ndo_change_rx_flags	= cpsw_ndo_change_rx_flags,
+	.ndo_do_ioctl		= cpsw_ndo_ioctl,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_change_mtu		= eth_change_mtu,
+	.ndo_tx_timeout		= cpsw_ndo_tx_timeout,
+	.ndo_get_stats		= cpsw_ndo_get_stats,
+	.ndo_set_rx_mode	= cpsw_ndo_set_rx_mode,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cpsw_ndo_poll_controller,
+#endif
+};
+#endif
+
+static void cpsw_get_drvinfo(struct net_device *ndev,
+			     struct ethtool_drvinfo *info)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	strcpy(info->driver, "TI CPSW Driver v1.0");
+	strcpy(info->version, "1.0");
+	strcpy(info->bus_info, priv->pdev->name);
+}
+
+static u32 cpsw_get_msglevel(struct net_device *ndev)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	return priv->msg_enable;
+}
+
+static void cpsw_set_msglevel(struct net_device *ndev, u32 value)
+{
+	struct cpsw_priv *priv = netdev_priv(ndev);
+	priv->msg_enable = value;
+}
+
+static int cpsw_get_ts_info(struct net_device *ndev,
+			    struct ethtool_ts_info *info)
+{
+#ifdef CONFIG_TI_CPTS
+	struct cpsw_priv *priv = netdev_priv(ndev);
+
+	info->so_timestamping =
+		SOF_TIMESTAMPING_TX_HARDWARE |
+		SOF_TIMESTAMPING_TX_SOFTWARE |
+		SOF_TIMESTAMPING_RX_HARDWARE |
+		SOF_TIMESTAMPING_RX_SOFTWARE |
+		SOF_TIMESTAMPING_SOFTWARE |
+		SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->phc_index = priv->cpts.phc_index;
+	info->tx_types =
+		(1 << HWTSTAMP_TX_OFF) |
+		(1 << HWTSTAMP_TX_ON);
+	info->rx_filters =
+		(1 << HWTSTAMP_FILTER_NONE) |
+		(1 << HWTSTAMP_FILTER_PTP_V2_EVENT);
+#else
+	info->so_timestamping =
+		SOF_TIMESTAMPING_TX_SOFTWARE |
+		SOF_TIMESTAMPING_RX_SOFTWARE |
+		SOF_TIMESTAMPING_SOFTWARE;
+	info->phc_index = -1;
+	info->tx_types = 0;
+	info->rx_filters = 0;
+#endif
+	return 0;
+}
+
+static const struct ethtool_ops cpsw_ethtool_ops = {
+	.get_drvinfo	= cpsw_get_drvinfo,
+	.get_msglevel	= cpsw_get_msglevel,
+	.set_msglevel	= cpsw_set_msglevel,
+	.get_link	= ethtool_op_get_link,
+	.get_ts_info	= cpsw_get_ts_info,
+};
+
+static void cpsw_slave_init(struct cpsw_slave *slave, struct cpsw_priv *priv,
+			    u32 slave_reg_ofs, u32 sliver_reg_ofs)
+{
+	void __iomem		*regs = priv->regs;
+	int			slave_num = slave->slave_num;
+	struct cpsw_slave_data	*data = priv->data.slave_data + slave_num;
+
+	slave->data	= data;
+	slave->regs	= regs + slave_reg_ofs;
+	slave->sliver	= regs + sliver_reg_ofs;
+}
+
+static int cpsw_probe_dt(struct cpsw_platform_data *data,
+			 struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct device_node *slave_node;
+	int i = 0, ret;
+	u32 prop;
+
+	if (!node)
+		return -EINVAL;
+
+	if (of_property_read_u32(node, "slaves", &prop)) {
+		dev_err(&pdev->dev, "Missing slaves property in the DT.\n");
+		return -EINVAL;
+	}
+	data->slaves = prop;
+
+	if (of_property_read_u32(node, "active_slave", &prop)) {
+		dev_err(&pdev->dev, "Missing active_slave property in the DT.\n");
+		return -EINVAL;
+	}
+	data->active_slave = prop;
+
+	if (of_property_read_u32(node, "cpts_clock_mult", &prop)) {
+		dev_err(&pdev->dev, "Missing cpts_clock_mult property in the DT.\n");
+		return -EINVAL;
+	}
+	data->cpts_clock_mult = prop;
+
+	if (of_property_read_u32(node, "cpts_clock_shift", &prop)) {
+		dev_err(&pdev->dev, "Missing cpts_clock_shift property in the DT.\n");
+		return -EINVAL;
+	}
+	data->cpts_clock_shift = prop;
+
+	data->slave_data = devm_kzalloc(&pdev->dev, data->slaves
+					* sizeof(struct cpsw_slave_data),
+					GFP_KERNEL);
+	if (!data->slave_data)
+		return -ENOMEM;
+
+	if (of_property_read_u32(node, "cpdma_channels", &prop)) {
+		dev_err(&pdev->dev, "Missing cpdma_channels property in the DT.\n");
+		return -EINVAL;
+	}
+	data->channels = prop;
+
+	if (of_property_read_u32(node, "ale_entries", &prop)) {
+		dev_err(&pdev->dev, "Missing ale_entries property in the DT.\n");
+		return -EINVAL;
+	}
+	data->ale_entries = prop;
+
+	if (of_property_read_u32(node, "bd_ram_size", &prop)) {
+		dev_err(&pdev->dev, "Missing bd_ram_size property in the DT.\n");
+		return -EINVAL;
+	}
+	data->bd_ram_size = prop;
+
+	if (of_property_read_u32(node, "rx_descs", &prop)) {
+		dev_warn(&pdev->dev, "Missing rx_descs property in the DT; defaults to 64\n");
+		prop = 64;
+	}
+	data->rx_descs = prop;
+
+	if (of_property_read_u32(node, "mac_control", &prop)) {
+		dev_err(&pdev->dev, "Missing mac_control property in the DT.\n");
+		return -EINVAL;
+	}
+	data->mac_control = prop;
+
+	if (of_property_read_bool(node, "dual_emac"))
+		data->dual_emac = 1;
+
+	/*
+	 * Populate all the child nodes here...
+	 */
+	ret = of_platform_populate(node, NULL, NULL, &pdev->dev);
+	/* We do not want to force this, as in some cases may not have child */
+	if (ret)
+		dev_warn(&pdev->dev, "Doesn't have any child node\n");
+
+	for_each_child_of_node(node, slave_node) {
+		struct cpsw_slave_data *slave_data = data->slave_data + i;
+		const void *mac_addr = NULL;
+		u32 phyid;
+		int lenp;
+		const __be32 *parp;
+		struct device_node *mdio_node;
+		struct platform_device *mdio;
+
+		/* This is no slave child node, continue */
+		if (strcmp(slave_node->name, "slave"))
+			continue;
+
+		parp = of_get_property(slave_node, "phy_id", &lenp);
+		if ((parp == NULL) || (lenp != (sizeof(void *) * 2))) {
+			dev_err(&pdev->dev, "Missing slave[%d] phy_id property\n", i);
+			goto no_phy_slave;
+		}
+		mdio_node = of_find_node_by_phandle(be32_to_cpup(parp));
+		phyid = be32_to_cpup(parp+1);
+		mdio = of_find_device_by_node(mdio_node);
+	        of_node_put(mdio_node);
+		if (!mdio) {
+			dev_err(&pdev->dev, "Missing mdio platform device\n");
+			return -EINVAL;
+		}
+		snprintf(slave_data->phy_id, sizeof(slave_data->phy_id),
+			 PHY_ID_FMT, mdio->name, phyid);
+
+		 slave_data->phy_if = of_get_phy_mode(slave_node);
+		 if (slave_data->phy_if < 0) {
+			 dev_err(&pdev->dev, "Missing or malformed slave[%d] phy-mode property\n",
+				 i);
+			 return slave_data->phy_if;
+		 }
+
+no_phy_slave:
+		mac_addr = of_get_mac_address(slave_node);
+		if (mac_addr) {
+			memcpy(slave_data->mac_addr, mac_addr, ETH_ALEN);
+		} else {
+			if (of_machine_is_compatible("ti,am33xx")) {
+				ret = cpsw_am33xx_cm_get_macid(&pdev->dev,
+							0x630, i,
+							slave_data->mac_addr);
+				if (ret)
+					return ret;
+			}
+		}
+		if (data->dual_emac) {
+			if (of_property_read_u32(slave_node, "dual_emac_res_vlan",
+						 &prop)) {
+				dev_err(&pdev->dev, "Missing dual_emac_res_vlan in DT.\n");
+				slave_data->dual_emac_res_vlan = i+1;
+				dev_err(&pdev->dev, "Using %d as Reserved VLAN for %d slave\n",
+					slave_data->dual_emac_res_vlan, i);
+			} else {
+				slave_data->dual_emac_res_vlan = prop;
+			}
+		}
+
+		i++;
+		if (i == data->slaves)
+			break;
+		}
+
+ 	return 0;
+ }
+
+static irq_handler_t cpsw_get_irq_handler(struct cpsw_priv *priv, int irq_idx)
+{
+	static const irq_handler_t non_napi_irq_tab[4] = {
+		cpsw_rx_thresh_pend_irq, cpsw_rx_pend_irq,
+		cpsw_tx_pend_irq, cpsw_misc_pend_irq
+	};
+
+	if ((unsigned int)irq_idx >= 4)
+		return NULL;
+
+#if 0
+	if (!priv->data.disable_napi)
+		return cpsw_interrupt;
+#endif
+
+	return non_napi_irq_tab[irq_idx];
+}
+
+static int cpsw_probe(struct platform_device *pdev)
+{
+	struct cpsw_platform_data	*data = pdev->dev.platform_data;
+	struct rtnet_device		*ndev;
+	struct cpsw_priv		*priv;
+	struct cpdma_params		dma_params;
+	struct cpsw_ale_params		ale_params;
+	void __iomem			*ss_regs, *wr_regs;
+	struct resource			*res;
+	u32 slave_offset, sliver_offset, slave_size;
+	irq_handler_t		irqh;
+	int ret = 0, i, j, k = 0;
+
+	ndev = rt_alloc_etherdev(sizeof(struct cpsw_priv), RX_RING_SIZE*2);
+	if (ndev == NULL){
+		return -ENOMEM;
+	}
+	rtdev_alloc_name(ndev, "rteth%d");
+	rt_rtdev_connect(ndev, &RTDEV_manager);
+	ndev->vers = RTDEV_VERS_2_0;
+
+	priv = ndev->priv;
+	platform_set_drvdata(pdev, ndev);
+	raw_spin_lock_init(&priv->lock);
+	priv->pdev = pdev;
+	priv->ndev = ndev;
+	priv->dev  = &pdev->dev;
+	priv->msg_enable = netif_msg_init(debug_level, CPSW_DEBUG);
+	priv->rx_packet_max = max(rx_packet_max, 128);
+	priv->irq_enabled = false;
+	priv->phy_phony_net_device = NULL;
+
+	if (rtskb_module_pool_init(&priv->skb_pool, RX_RING_SIZE*2) < RX_RING_SIZE*2) {
+		rtskb_pool_release(&priv->skb_pool);
+		ret = -ENOMEM;
+		goto clean_real_ndev_ret;
+	}
+
+	pm_runtime_use_autosuspend(&pdev->dev);
+	/* if delay is negative and the use_autosuspend flag is set
+	 * then runtime suspends are prevented.
+	 */
+	pm_runtime_set_autosuspend_delay(&pdev->dev, PM_NEGATIVE_DELAY);
+	pm_runtime_enable(&pdev->dev);
+	ret = pm_runtime_get_sync(&pdev->dev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "%s: pm_runtime_get_sync error %d\n",
+				__func__, ret);
+		return ret;
+	}
+
+
+	if (cpsw_probe_dt(&priv->data, pdev)) {
+		pr_err("cpsw: platform data missing\n");
+		ret = -ENODEV;
+		goto clean_ndev_ret;
+	}
+	data = &priv->data;
+
+	pr_info("DT probed OK\n");
+
+	if (is_valid_ether_addr(data->slave_data[0].mac_addr)) {
+		memcpy(priv->mac_addr, data->slave_data[0].mac_addr, ETH_ALEN);
+		pr_info("Detected MACID = %pM", priv->mac_addr);
+	} else {
+		eth_random_addr(priv->mac_addr);
+		pr_info("Random MACID = %pM", priv->mac_addr);
+	}
+
+	memcpy(ndev->dev_addr, priv->mac_addr, ETH_ALEN);
+
+	priv->slaves = kzalloc(sizeof(struct cpsw_slave) * data->slaves,
+			       GFP_KERNEL);
+	if (!priv->slaves) {
+		ret = -EBUSY;
+		goto clean_ndev_ret;
+	}
+	for (i = 0; i < data->slaves; i++)
+		priv->slaves[i].slave_num = i;
+
+	priv->clk = clk_get(&pdev->dev, "fck");
+	if (IS_ERR(priv->clk)) {
+		dev_err(&pdev->dev, "fck is not found\n");
+		ret = -ENODEV;
+		goto clean_slave_ret;
+	}
+
+	priv->cpsw_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!priv->cpsw_res) {
+		dev_err(priv->dev, "error getting i/o resource\n");
+		ret = -ENOENT;
+		goto clean_clk_ret;
+	}
+	if (!request_mem_region(priv->cpsw_res->start,
+				resource_size(priv->cpsw_res), ndev->name)) {
+		dev_err(priv->dev, "failed request i/o region\n");
+		ret = -ENXIO;
+		goto clean_clk_ret;
+	}
+	ss_regs = ioremap(priv->cpsw_res->start, resource_size(priv->cpsw_res));
+	if (!ss_regs) {
+		dev_err(priv->dev, "unable to map i/o region\n");
+		goto clean_cpsw_iores_ret;
+	}
+	priv->regs = ss_regs;
+	priv->version = __raw_readl(&priv->regs->id_ver);
+	priv->host_port = HOST_PORT_NUM;
+
+	priv->cpsw_wr_res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	if (!priv->cpsw_wr_res) {
+		dev_err(priv->dev, "error getting i/o resource\n");
+		ret = -ENOENT;
+		goto clean_iomap_ret;
+	}
+	if (!request_mem_region(priv->cpsw_wr_res->start,
+			resource_size(priv->cpsw_wr_res), ndev->name)) {
+		dev_err(priv->dev, "failed request i/o region\n");
+		ret = -ENXIO;
+		goto clean_iomap_ret;
+	}
+	wr_regs = ioremap(priv->cpsw_wr_res->start,
+				resource_size(priv->cpsw_wr_res));
+	if (!wr_regs) {
+		dev_err(priv->dev, "unable to map i/o region\n");
+		goto clean_cpsw_wr_iores_ret;
+	}
+	priv->wr_regs = wr_regs;
+
+	memset(&dma_params, 0, sizeof(dma_params));
+	memset(&ale_params, 0, sizeof(ale_params));
+
+	switch (priv->version) {
+	case CPSW_VERSION_1:
+		priv->host_port_regs = ss_regs + CPSW1_HOST_PORT_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW1_CPTS_OFFSET;
+		dma_params.dmaregs   = ss_regs + CPSW1_CPDMA_OFFSET;
+		dma_params.txhdp     = ss_regs + CPSW1_STATERAM_OFFSET;
+		ale_params.ale_regs  = ss_regs + CPSW1_ALE_OFFSET;
+		slave_offset         = CPSW1_SLAVE_OFFSET;
+		slave_size           = CPSW1_SLAVE_SIZE;
+		sliver_offset        = CPSW1_SLIVER_OFFSET;
+		dma_params.desc_mem_phys = 0;
+		break;
+	case CPSW_VERSION_2:
+		priv->host_port_regs = ss_regs + CPSW2_HOST_PORT_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW2_CPTS_OFFSET;
+		dma_params.dmaregs   = ss_regs + CPSW2_CPDMA_OFFSET;
+		dma_params.txhdp     = ss_regs + CPSW2_STATERAM_OFFSET;
+		ale_params.ale_regs  = ss_regs + CPSW2_ALE_OFFSET;
+		slave_offset         = CPSW2_SLAVE_OFFSET;
+		slave_size           = CPSW2_SLAVE_SIZE;
+		sliver_offset        = CPSW2_SLIVER_OFFSET;
+		dma_params.desc_mem_phys =
+			(u32 __force) priv->cpsw_res->start + CPSW2_BD_OFFSET;
+		break;
+	default:
+		dev_err(priv->dev, "unknown version 0x%08x\n", priv->version);
+		ret = -ENODEV;
+		goto clean_cpsw_wr_iores_ret;
+	}
+	for (i = 0; i < priv->data.slaves; i++) {
+		struct cpsw_slave *slave = &priv->slaves[i];
+		cpsw_slave_init(slave, priv, slave_offset, sliver_offset);
+		slave_offset  += slave_size;
+		sliver_offset += SLIVER_SIZE;
+	}
+
+	dma_params.dev		= &pdev->dev;
+	dma_params.rxthresh	= dma_params.dmaregs + CPDMA_RXTHRESH;
+	dma_params.rxfree	= dma_params.dmaregs + CPDMA_RXFREE;
+	dma_params.rxhdp	= dma_params.txhdp + CPDMA_RXHDP;
+	dma_params.txcp		= dma_params.txhdp + CPDMA_TXCP;
+	dma_params.rxcp		= dma_params.txhdp + CPDMA_RXCP;
+
+	dma_params.num_chan		= data->channels;
+	dma_params.has_soft_reset	= true;
+	dma_params.min_packet_size	= CPSW_MIN_PACKET_SIZE;
+	dma_params.desc_mem_size	= data->bd_ram_size;
+	dma_params.desc_align		= 16;
+	dma_params.has_ext_regs		= true;
+	dma_params.desc_hw_addr         = dma_params.desc_mem_phys;
+
+	priv->dma = cpdma_ctlr_create(&dma_params);
+	if (!priv->dma) {
+		dev_err(priv->dev, "error initializing dma\n");
+		ret = -ENOMEM;
+		goto clean_wr_iomap_ret;
+	}
+
+	priv->txch = cpdma_chan_create(priv->dma, tx_chan_num(0),
+				       cpsw_tx_handler);
+	priv->rxch = cpdma_chan_create(priv->dma, rx_chan_num(0),
+				       cpsw_rx_handler);
+
+	if (WARN_ON(!priv->txch || !priv->rxch)) {
+		dev_err(priv->dev, "error initializing dma channels\n");
+		ret = -ENOMEM;
+		goto clean_dma_ret;
+	}
+
+	ale_params.dev			= &pdev->dev;
+	ale_params.ale_ageout		= ale_ageout;
+	ale_params.ale_entries		= data->ale_entries;
+	ale_params.ale_ports		= data->slaves;
+
+	priv->ale = cpsw_ale_create(&ale_params);
+	if (!priv->ale) {
+		dev_err(priv->dev, "error initializing ale engine\n");
+		ret = -ENODEV;
+		goto clean_dma_ret;
+	}
+
+	ndev->irq = platform_get_irq(pdev, 0);
+	if (ndev->irq < 0) {
+		dev_err(priv->dev, "error getting irq resource\n");
+		ret = -ENOENT;
+		goto clean_ale_ret;
+	}
+
+#if 0
+	dev_info(&pdev->dev, "NAPI %s\n", priv->data.disable_napi ?
+			"disabled" : "enabled");
+#endif
+
+	rt_stack_connect(ndev, &STACK_manager);
+	/* get interrupts */
+	j = k = 0;
+	while ((res = platform_get_resource(pdev, IORESOURCE_IRQ, j++))) {
+		for (i = res->start; k < 4 && i <= res->end; i++) {
+			irqh = cpsw_get_irq_handler(priv, k);
+			if (irqh == NULL) {
+				dev_err(&pdev->dev, "Unable to get handler "
+						"for #%d (%d)\n", k, i);
+				goto clean_ale_ret;
+			}
+			priv->irqs_table[k] = i;
+			if (request_irq(i, irqh, IRQF_NO_THREAD | IRQF_SHARED,
+						dev_name(&pdev->dev), priv)) {
+				dev_err(priv->dev, "error attaching irq\n");
+				goto clean_ale_ret;
+			}
+			k++;
+		}
+	}
+	priv->num_irqs = k;
+
+	ndev->flags |= IFF_ALLMULTI;	/* see cpsw_ndo_change_rx_flags() */
+
+	#if 0
+	ndev->netdev_ops = &cpsw_netdev_ops;
+	SET_ETHTOOL_OPS(ndev, &cpsw_ethtool_ops);
+	#endif
+	ndev->open		= cpsw_ndo_open;
+	ndev->hard_start_xmit 	= cpsw_ndo_start_xmit;
+	ndev->get_stats    	= cpsw_ndo_get_stats;
+	ndev->stop 		= cpsw_ndo_stop;
+
+	/* register the network device */
+	ret = rt_register_rtnetdev(ndev);
+	if (ret) {
+		dev_err(priv->dev, "error registering net device\n");
+		ret = -ENODEV;
+		goto clean_irq_ret;
+	}
+
+	if (cpts_register(&pdev->dev, &priv->cpts,
+			  data->cpts_clock_mult, data->cpts_clock_shift))
+		dev_err(priv->dev, "error registering cpts device\n");
+
+	cpsw_notice(priv, probe, "initialized device (regs %x, irq %d)\n",
+		  priv->cpsw_res->start, ndev->irq);
+
+	return 0;
+
+clean_irq_ret:
+	for(i = 0; i < priv->num_irqs; i++)
+		free_irq(priv->irqs_table[i], priv);
+	rt_stack_disconnect(ndev);
+clean_ale_ret:
+	cpsw_ale_destroy(priv->ale);
+clean_dma_ret:
+	cpdma_chan_destroy(priv->txch);
+	cpdma_chan_destroy(priv->rxch);
+	cpdma_ctlr_destroy(priv->dma);
+clean_wr_iomap_ret:
+	iounmap(priv->wr_regs);
+clean_cpsw_wr_iores_ret:
+	release_mem_region(priv->cpsw_wr_res->start,
+			   resource_size(priv->cpsw_wr_res));
+clean_iomap_ret:
+	iounmap(priv->regs);
+clean_cpsw_iores_ret:
+	release_mem_region(priv->cpsw_res->start,
+			   resource_size(priv->cpsw_res));
+clean_clk_ret:
+	clk_put(priv->clk);
+clean_slave_ret:
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+	kfree(priv->slaves);
+clean_ndev_ret:
+	rtskb_pool_release(&priv->skb_pool);
+clean_real_ndev_ret:
+	rt_rtdev_disconnect(ndev);
+	rtdev_free(ndev);
+	return ret;
+}
+
+static int cpsw_remove(struct platform_device *pdev)
+{
+	struct rtnet_device *ndev = platform_get_drvdata(pdev);
+	struct cpsw_priv *priv = ndev->priv;
+	int i;
+
+	pr_info("removing cpsw device");
+	platform_set_drvdata(pdev, NULL);
+
+	cpts_unregister(&priv->cpts);
+	if (priv->phy_phony_net_device)
+		free_netdev(priv->phy_phony_net_device);
+	for(i = 0; i < priv->num_irqs; i++)
+		free_irq(priv->irqs_table[i], priv);
+	rt_stack_disconnect(ndev);
+	cpsw_ale_destroy(priv->ale);
+	cpdma_chan_destroy(priv->txch);
+	cpdma_chan_destroy(priv->rxch);
+	cpdma_ctlr_destroy(priv->dma);
+	iounmap(priv->regs);
+	release_mem_region(priv->cpsw_res->start,
+			   resource_size(priv->cpsw_res));
+	iounmap(priv->wr_regs);
+	release_mem_region(priv->cpsw_wr_res->start,
+			   resource_size(priv->cpsw_wr_res));
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	clk_put(priv->clk);
+	kfree(priv->slaves);
+	rtskb_pool_release(&priv->skb_pool);
+	rt_rtdev_disconnect(ndev);
+	rtdev_free(ndev);
+
+	return 0;
+}
+#if 0
+static int cpsw_suspend(struct device *dev)
+{
+	struct platform_device	*pdev = to_platform_device(dev);
+	struct rtnet_device	*ndev = platform_get_drvdata(pdev);
+
+	if (rtnetif_running(ndev))
+		cpsw_ndo_stop(ndev);
+	pm_runtime_put_sync(&pdev->dev);
+
+	return 0;
+}
+
+static int cpsw_resume(struct device *dev)
+{
+	struct platform_device	*pdev = to_platform_device(dev);
+	struct rtnet_device	*ndev = platform_get_drvdata(pdev);
+
+	pm_runtime_get_sync(&pdev->dev);
+	if (rtnetif_running(ndev))
+		cpsw_ndo_open(ndev);
+	return 0;
+}
+
+static const struct dev_pm_ops cpsw_pm_ops = {
+	.suspend	= cpsw_suspend,
+	.resume		= cpsw_resume,
+};
+#endif
+
+static const struct of_device_id cpsw_of_mtable[] = {
+	{ .compatible = "ti,cpsw", },
+	{ /* sentinel */ },
+};
+
+static struct platform_driver cpsw_driver = {
+	.driver = {
+		.name	 = "cpsw",
+		.owner	 = THIS_MODULE,
+#if 0
+		.pm	 = &cpsw_pm_ops,
+#endif
+		.of_match_table = of_match_ptr(cpsw_of_mtable),
+	},
+	.probe = cpsw_probe,
+	.remove = cpsw_remove,
+};
+
+static int __init cpsw_init(void)
+{
+	load_smsc_phy_module(0);
+	load_davinci_mdio_module(0);
+	return platform_driver_register(&cpsw_driver);
+}
+late_initcall(cpsw_init);
+
+static void __exit cpsw_exit(void)
+{
+	platform_driver_unregister(&cpsw_driver);
+}
+module_exit(cpsw_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cyril Chemparathy <cyril@ti.com>");
+MODULE_AUTHOR("Mugunthan V N <mugunthanvnm@ti.com>");
+MODULE_DESCRIPTION("RT TI CPSW Ethernet driver");
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw.h linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw.h
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpsw.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpsw.h	2020-06-15 16:12:31.463695602 +0300
@@ -0,0 +1,56 @@
+/*
+ * Texas Instruments Ethernet Switch Driver
+ *
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __CPSW_H__
+#define __CPSW_H__
+
+#include <linux/if_ether.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/ktime.h>
+#include <linux/timekeeping.h>
+
+struct cpsw_slave_data {
+	char		phy_id[MII_BUS_ID_SIZE];
+	int		phy_if;
+	u8		mac_addr[ETH_ALEN];
+	u16		dual_emac_res_vlan;	/* Reserved VLAN for DualEMAC */
+};
+
+struct cpsw_platform_data {
+	struct cpsw_slave_data	*slave_data;
+	u32	ss_reg_ofs;	/* Subsystem control register offset */
+	u32	channels;	/* number of cpdma channels (symmetric) */
+	u32	slaves;		/* number of slave cpgmac ports */
+	u32	active_slave; /* time stamping, ethtool and SIOCGMIIPHY slave */
+	u32	cpts_clock_mult;  /* convert input clock ticks to nanoseconds */
+	u32	cpts_clock_shift; /* convert input clock ticks to nanoseconds */
+	u32	ale_entries;	/* ale table size */
+	u32	bd_ram_size;  /*buffer descriptor ram size */
+	u32	rx_descs;	/* Number of Rx Descriptios */
+	u32	mac_control;	/* Mac control register */
+	u16	default_vlan;	/* Def VLAN for ALE lookup in VLAN aware mode*/
+	bool	dual_emac;	/* Enable Dual EMAC mode */
+};
+
+int cpsw_am33xx_cm_get_macid(struct device *dev, u16 offset, int slave,
+			     u8 *mac_addr);
+int load_davinci_mdio_module(int v);
+int load_smsc_phy_module(int v);
+
+#endif /* __CPSW_H__ */
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpts.c linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpts.c
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpts.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpts.c	2020-06-15 16:12:31.467695589 +0300
@@ -0,0 +1,424 @@
+/*
+ * TI Common Platform Time Sync
+ *
+ * Copyright (C) 2012 Richard Cochran <richardcochran@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#include <linux/err.h>
+#include <linux/if.h>
+#include <linux/hrtimer.h>
+#include <linux/module.h>
+#include <linux/net_tstamp.h>
+#include <linux/ptp_classify.h>
+#include <linux/time.h>
+#include <linux/uaccess.h>
+#include <linux/workqueue.h>
+
+#include "cpts.h"
+
+#ifdef CONFIG_TI_CPTS
+
+static struct sock_filter ptp_filter[] = {
+	PTP_FILTER
+};
+
+#define cpts_read32(c, r)	__raw_readl(&c->reg->r)
+#define cpts_write32(c, v, r)	__raw_writel(v, &c->reg->r)
+
+static int event_expired(struct cpts_event *event)
+{
+	return time_after(jiffies, event->tmo);
+}
+
+static int event_type(struct cpts_event *event)
+{
+	return (event->high >> EVENT_TYPE_SHIFT) & EVENT_TYPE_MASK;
+}
+
+static int cpts_fifo_pop(struct cpts *cpts, u32 *high, u32 *low)
+{
+	u32 r = cpts_read32(cpts, intstat_raw);
+
+	if (r & TS_PEND_RAW) {
+		*high = cpts_read32(cpts, event_high);
+		*low  = cpts_read32(cpts, event_low);
+		cpts_write32(cpts, EVENT_POP, event_pop);
+		return 0;
+	}
+	return -1;
+}
+
+/*
+ * Returns zero if matching event type was found.
+ */
+static int cpts_fifo_read(struct cpts *cpts, int match)
+{
+	int i, type = -1;
+	u32 hi, lo;
+	struct cpts_event *event;
+
+	for (i = 0; i < CPTS_FIFO_DEPTH; i++) {
+		if (cpts_fifo_pop(cpts, &hi, &lo))
+			break;
+		if (list_empty(&cpts->pool)) {
+			pr_err("cpts: event pool is empty\n");
+			return -1;
+		}
+		event = list_first_entry(&cpts->pool, struct cpts_event, list);
+		event->tmo = jiffies + 2;
+		event->high = hi;
+		event->low = lo;
+		type = event_type(event);
+		switch (type) {
+		case CPTS_EV_PUSH:
+		case CPTS_EV_RX:
+		case CPTS_EV_TX:
+			list_del_init(&event->list);
+			list_add_tail(&event->list, &cpts->events);
+			break;
+		case CPTS_EV_ROLL:
+		case CPTS_EV_HALF:
+		case CPTS_EV_HW:
+			break;
+		default:
+			pr_err("cpts: unkown event type\n");
+			break;
+		}
+		if (type == match)
+			break;
+	}
+	return type == match ? 0 : -1;
+}
+
+static cycle_t cpts_systim_read(const struct cyclecounter *cc)
+{
+	u64 val = 0;
+	struct cpts_event *event;
+	struct list_head *this, *next;
+	struct cpts *cpts = container_of(cc, struct cpts, cc);
+
+	cpts_write32(cpts, TS_PUSH, ts_push);
+	if (cpts_fifo_read(cpts, CPTS_EV_PUSH))
+		pr_err("cpts: unable to obtain a time stamp\n");
+
+	list_for_each_safe(this, next, &cpts->events) {
+		event = list_entry(this, struct cpts_event, list);
+		if (event_type(event) == CPTS_EV_PUSH) {
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			val = event->low;
+			break;
+		}
+	}
+
+	return val;
+}
+
+/* PTP clock operations */
+
+static int cpts_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)
+{
+	u64 adj;
+	u32 diff, mult;
+	int neg_adj = 0;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	if (ppb < 0) {
+		neg_adj = 1;
+		ppb = -ppb;
+	}
+	mult = cpts->cc_mult;
+	adj = mult;
+	adj *= ppb;
+	diff = div_u64(adj, 1000000000ULL);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+
+	timecounter_read(&cpts->tc);
+
+	cpts->cc.mult = neg_adj ? mult - diff : mult + diff;
+
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
+{
+	s64 now;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	now = timecounter_read(&cpts->tc);
+	now += delta;
+	timecounter_init(&cpts->tc, &cpts->cc, now);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_gettime(struct ptp_clock_info *ptp, struct timespec *ts)
+{
+	u64 ns;
+	u32 remainder;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	ns = timecounter_read(&cpts->tc);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	ts->tv_sec = div_u64_rem(ns, 1000000000, &remainder);
+	ts->tv_nsec = remainder;
+
+	return 0;
+}
+
+static int cpts_ptp_settime(struct ptp_clock_info *ptp,
+			    const struct timespec *ts)
+{
+	u64 ns;
+	rtdm_lockctx_t context;
+	struct cpts *cpts = container_of(ptp, struct cpts, info);
+
+	ns = ts->tv_sec * 1000000000ULL;
+	ns += ts->tv_nsec;
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	timecounter_init(&cpts->tc, &cpts->cc, ns);
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return 0;
+}
+
+static int cpts_ptp_enable(struct ptp_clock_info *ptp,
+			   struct ptp_clock_request *rq, int on)
+{
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info cpts_info = {
+	.owner		= THIS_MODULE,
+	.name		= "CTPS timer",
+	.max_adj	= 1000000,
+	.n_ext_ts	= 0,
+	.pps		= 0,
+	.adjfreq	= cpts_ptp_adjfreq,
+	.adjtime	= cpts_ptp_adjtime,
+	.gettime	= cpts_ptp_gettime,
+	.settime	= cpts_ptp_settime,
+	.enable		= cpts_ptp_enable,
+};
+
+static void cpts_overflow_check(struct work_struct *work)
+{
+	struct timespec ts;
+	struct cpts *cpts = container_of(work, struct cpts, overflow_work.work);
+
+	cpts_write32(cpts, CPTS_EN, control);
+	cpts_write32(cpts, TS_PEND_EN, int_enable);
+	cpts_ptp_gettime(&cpts->info, &ts);
+	pr_debug("cpts overflow check at %ld.%09lu\n", ts.tv_sec, ts.tv_nsec);
+	schedule_delayed_work(&cpts->overflow_work, CPTS_OVERFLOW_PERIOD);
+}
+
+#define CPTS_REF_CLOCK_NAME "cpsw_cpts_rft_clk"
+
+static void cpts_clk_init(struct cpts *cpts)
+{
+	cpts->refclk = clk_get(NULL, CPTS_REF_CLOCK_NAME);
+	if (IS_ERR(cpts->refclk)) {
+		pr_err("Failed to clk_get %s\n", CPTS_REF_CLOCK_NAME);
+		cpts->refclk = NULL;
+		return;
+	}
+	clk_prepare_enable(cpts->refclk);
+}
+
+static void cpts_clk_release(struct cpts *cpts)
+{
+	clk_disable(cpts->refclk);
+	clk_put(cpts->refclk);
+}
+
+static int cpts_match(struct sk_buff *skb, unsigned int ptp_class,
+		      u16 ts_seqid, u8 ts_msgtype)
+{
+	u16 *seqid;
+	unsigned int offset;
+	u8 *msgtype, *data = skb->data;
+
+	switch (ptp_class) {
+	case PTP_CLASS_V1_IPV4:
+	case PTP_CLASS_V2_IPV4:
+		offset = ETH_HLEN + IPV4_HLEN(data) + UDP_HLEN;
+		break;
+	case PTP_CLASS_V1_IPV6:
+	case PTP_CLASS_V2_IPV6:
+		offset = OFF_PTP6;
+		break;
+	case PTP_CLASS_V2_L2:
+		offset = ETH_HLEN;
+		break;
+	case PTP_CLASS_V2_VLAN:
+		offset = ETH_HLEN + VLAN_HLEN;
+		break;
+	default:
+		return 0;
+	}
+
+	if (skb->len + ETH_HLEN < offset + OFF_PTP_SEQUENCE_ID + sizeof(*seqid))
+		return 0;
+
+	if (unlikely(ptp_class & PTP_CLASS_V1))
+		msgtype = data + offset + OFF_PTP_CONTROL;
+	else
+		msgtype = data + offset;
+
+	seqid = (u16 *)(data + offset + OFF_PTP_SEQUENCE_ID);
+
+	return (ts_msgtype == (*msgtype & 0xf) && ts_seqid == ntohs(*seqid));
+}
+
+static u64 cpts_find_ts(struct cpts *cpts, struct sk_buff *skb, int ev_type)
+{
+	u64 ns = 0;
+	struct cpts_event *event;
+	struct list_head *this, *next;
+	unsigned int class = sk_run_filter(skb, ptp_filter);
+	rtdm_lockctx_t context;
+	u16 seqid;
+	u8 mtype;
+
+	if (class == PTP_CLASS_NONE)
+		return 0;
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	cpts_fifo_read(cpts, CPTS_EV_PUSH);
+	list_for_each_safe(this, next, &cpts->events) {
+		event = list_entry(this, struct cpts_event, list);
+		if (event_expired(event)) {
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			continue;
+		}
+		mtype = (event->high >> MESSAGE_TYPE_SHIFT) & MESSAGE_TYPE_MASK;
+		seqid = (event->high >> SEQUENCE_ID_SHIFT) & SEQUENCE_ID_MASK;
+		if (ev_type == event_type(event) &&
+		    cpts_match(skb, class, seqid, mtype)) {
+			ns = timecounter_cyc2time(&cpts->tc, event->low);
+			list_del_init(&event->list);
+			list_add(&event->list, &cpts->pool);
+			break;
+		}
+	}
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	return ns;
+}
+
+void cpts_rx_timestamp(struct cpts *cpts, struct sk_buff *skb)
+{
+	u64 ns;
+	struct skb_shared_hwtstamps *ssh;
+
+	if (!cpts->rx_enable)
+		return;
+	ns = cpts_find_ts(cpts, skb, CPTS_EV_RX);
+	if (!ns)
+		return;
+	ssh = skb_hwtstamps(skb);
+	memset(ssh, 0, sizeof(*ssh));
+	ssh->hwtstamp = ns_to_ktime(ns);
+}
+
+void cpts_tx_timestamp(struct cpts *cpts, struct sk_buff *skb)
+{
+	u64 ns;
+	struct skb_shared_hwtstamps ssh;
+
+	if (!(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS))
+		return;
+	ns = cpts_find_ts(cpts, skb, CPTS_EV_TX);
+	if (!ns)
+		return;
+	memset(&ssh, 0, sizeof(ssh));
+	ssh.hwtstamp = ns_to_ktime(ns);
+	skb_tstamp_tx(skb, &ssh);
+}
+
+#endif /*CONFIG_TI_CPTS*/
+
+int cpts_register(struct device *dev, struct cpts *cpts,
+		  u32 mult, u32 shift)
+{
+#ifdef CONFIG_TI_CPTS
+	int err, i;
+	rtdm_lockctx_t context;
+
+	if (ptp_filter_init(ptp_filter, ARRAY_SIZE(ptp_filter))) {
+		pr_err("cpts: bad ptp filter\n");
+		return -EINVAL;
+	}
+	cpts->info = cpts_info;
+	cpts->clock = ptp_clock_register(&cpts->info, dev);
+	if (IS_ERR(cpts->clock)) {
+		err = PTR_ERR(cpts->clock);
+		cpts->clock = NULL;
+		return err;
+	}
+	rtdm_lock_init(&cpts->lock);
+
+	cpts->cc.read = cpts_systim_read;
+	cpts->cc.mask = CLOCKSOURCE_MASK(32);
+	cpts->cc_mult = mult;
+	cpts->cc.mult = mult;
+	cpts->cc.shift = shift;
+
+	INIT_LIST_HEAD(&cpts->events);
+	INIT_LIST_HEAD(&cpts->pool);
+	for (i = 0; i < CPTS_MAX_EVENTS; i++)
+		list_add(&cpts->pool_data[i].list, &cpts->pool);
+
+	cpts_clk_init(cpts);
+	cpts_write32(cpts, CPTS_EN, control);
+	cpts_write32(cpts, TS_PEND_EN, int_enable);
+
+	rtdm_lock_get_irqsave(&cpts->lock, context);
+	timecounter_init(&cpts->tc, &cpts->cc, ktime_to_ns(ktime_get_real()));
+	rtdm_lock_put_irqrestore(&cpts->lock, context);
+
+	INIT_DELAYED_WORK(&cpts->overflow_work, cpts_overflow_check);
+	schedule_delayed_work(&cpts->overflow_work, CPTS_OVERFLOW_PERIOD);
+
+	cpts->phc_index = ptp_clock_index(cpts->clock);
+#endif
+	return 0;
+}
+
+void cpts_unregister(struct cpts *cpts)
+{
+#ifdef CONFIG_TI_CPTS
+	if (cpts->clock) {
+		ptp_clock_unregister(cpts->clock);
+		cancel_delayed_work_sync(&cpts->overflow_work);
+	}
+	if (cpts->refclk)
+		cpts_clk_release(cpts);
+#endif
+}
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/cpts.h linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpts.h
--- linux-5.4.5/net/rtnet/drivers/ticpsw/cpts.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/cpts.h	2020-06-15 16:12:31.467695589 +0300
@@ -0,0 +1,147 @@
+/*
+ * TI Common Platform Time Sync
+ *
+ * Copyright (C) 2012 Richard Cochran <richardcochran@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef _TI_CPTS_H_
+#define _TI_CPTS_H_
+
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/clocksource.h>
+#include <linux/device.h>
+#include <linux/list.h>
+#include <linux/ptp_clock_kernel.h>
+#include <linux/skbuff.h>
+
+#include <rtnet_port.h>
+
+struct cpsw_cpts {
+	u32 idver;                /* Identification and version */
+	u32 control;              /* Time sync control */
+	u32 res1;
+	u32 ts_push;              /* Time stamp event push */
+	u32 ts_load_val;          /* Time stamp load value */
+	u32 ts_load_en;           /* Time stamp load enable */
+	u32 res2[2];
+	u32 intstat_raw;          /* Time sync interrupt status raw */
+	u32 intstat_masked;       /* Time sync interrupt status masked */
+	u32 int_enable;           /* Time sync interrupt enable */
+	u32 res3;
+	u32 event_pop;            /* Event interrupt pop */
+	u32 event_low;            /* 32 Bit Event Time Stamp */
+	u32 event_high;           /* Event Type Fields */
+};
+
+/* Bit definitions for the IDVER register */
+#define TX_IDENT_SHIFT       (16)    /* TX Identification Value */
+#define TX_IDENT_MASK        (0xffff)
+#define RTL_VER_SHIFT        (11)    /* RTL Version Value */
+#define RTL_VER_MASK         (0x1f)
+#define MAJOR_VER_SHIFT      (8)     /* Major Version Value */
+#define MAJOR_VER_MASK       (0x7)
+#define MINOR_VER_SHIFT      (0)     /* Minor Version Value */
+#define MINOR_VER_MASK       (0xff)
+
+/* Bit definitions for the CONTROL register */
+#define HW4_TS_PUSH_EN       (1<<11) /* Hardware push 4 enable */
+#define HW3_TS_PUSH_EN       (1<<10) /* Hardware push 3 enable */
+#define HW2_TS_PUSH_EN       (1<<9)  /* Hardware push 2 enable */
+#define HW1_TS_PUSH_EN       (1<<8)  /* Hardware push 1 enable */
+#define INT_TEST             (1<<1)  /* Interrupt Test */
+#define CPTS_EN              (1<<0)  /* Time Sync Enable */
+
+/*
+ * Definitions for the single bit resisters:
+ * TS_PUSH TS_LOAD_EN  INTSTAT_RAW INTSTAT_MASKED INT_ENABLE EVENT_POP
+ */
+#define TS_PUSH             (1<<0)  /* Time stamp event push */
+#define TS_LOAD_EN          (1<<0)  /* Time Stamp Load */
+#define TS_PEND_RAW         (1<<0)  /* int read (before enable) */
+#define TS_PEND             (1<<0)  /* masked interrupt read (after enable) */
+#define TS_PEND_EN          (1<<0)  /* masked interrupt enable */
+#define EVENT_POP           (1<<0)  /* writing discards one event */
+
+/* Bit definitions for the EVENT_HIGH register */
+#define PORT_NUMBER_SHIFT    (24)    /* Indicates Ethernet port or HW pin */
+#define PORT_NUMBER_MASK     (0x1f)
+#define EVENT_TYPE_SHIFT     (20)    /* Time sync event type */
+#define EVENT_TYPE_MASK      (0xf)
+#define MESSAGE_TYPE_SHIFT   (16)    /* PTP message type */
+#define MESSAGE_TYPE_MASK    (0xf)
+#define SEQUENCE_ID_SHIFT    (0)     /* PTP message sequence ID */
+#define SEQUENCE_ID_MASK     (0xffff)
+
+enum {
+	CPTS_EV_PUSH, /* Time Stamp Push Event */
+	CPTS_EV_ROLL, /* Time Stamp Rollover Event */
+	CPTS_EV_HALF, /* Time Stamp Half Rollover Event */
+	CPTS_EV_HW,   /* Hardware Time Stamp Push Event */
+	CPTS_EV_RX,   /* Ethernet Receive Event */
+	CPTS_EV_TX,   /* Ethernet Transmit Event */
+};
+
+/* This covers any input clock up to about 500 MHz. */
+#define CPTS_OVERFLOW_PERIOD (HZ * 8)
+
+#define CPTS_FIFO_DEPTH 16
+#define CPTS_MAX_EVENTS 32
+
+struct cpts_event {
+	struct list_head list;
+	unsigned long tmo;
+	u32 high;
+	u32 low;
+};
+
+struct cpts {
+	struct cpsw_cpts __iomem *reg;
+	int tx_enable;
+	int rx_enable;
+#ifdef CONFIG_TI_CPTS
+	struct ptp_clock_info info;
+	struct ptp_clock *clock;
+	rtdm_lock_t lock; /* protects time registers */
+	u32 cc_mult; /* for the nominal frequency */
+	struct cyclecounter cc;
+	struct timecounter tc;
+	struct delayed_work overflow_work;
+	int phc_index;
+	struct clk *refclk;
+	struct list_head events;
+	struct list_head pool;
+	struct cpts_event pool_data[CPTS_MAX_EVENTS];
+#endif
+};
+
+#ifdef CONFIG_TI_CPTS
+extern void cpts_rx_timestamp(struct cpts *cpts, struct rtskb *skb);
+extern void cpts_tx_timestamp(struct cpts *cpts, struct rtskb *skb);
+#else
+static inline void cpts_rx_timestamp(struct cpts *cpts, struct rtskb *skb)
+{
+}
+static inline void cpts_tx_timestamp(struct cpts *cpts, struct rtskb *skb)
+{
+}
+#endif
+
+extern int cpts_register(struct device *dev, struct cpts *cpts,
+			 u32 mult, u32 shift);
+extern void cpts_unregister(struct cpts *cpts);
+
+#endif
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/davinci_cpdma.c linux-5.4.5-new/net/rtnet/drivers/ticpsw/davinci_cpdma.c
--- linux-5.4.5/net/rtnet/drivers/ticpsw/davinci_cpdma.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/davinci_cpdma.c	2020-06-15 16:12:31.467695589 +0300
@@ -0,0 +1,1003 @@
+/*
+ * Texas Instruments CPDMA Driver
+ *
+ * Copyright (C) 2010 Texas Instruments
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/dma-mapping.h>
+#include <linux/io.h>
+
+#include "davinci_cpdma.h"
+
+#include <rtnet_port.h>
+
+/* DMA Registers */
+#define CPDMA_TXIDVER		0x00
+#define CPDMA_TXCONTROL		0x04
+#define CPDMA_TXTEARDOWN	0x08
+#define CPDMA_RXIDVER		0x10
+#define CPDMA_RXCONTROL		0x14
+#define CPDMA_SOFTRESET		0x1c
+#define CPDMA_RXTEARDOWN	0x18
+#define CPDMA_TXINTSTATRAW	0x80
+#define CPDMA_TXINTSTATMASKED	0x84
+#define CPDMA_TXINTMASKSET	0x88
+#define CPDMA_TXINTMASKCLEAR	0x8c
+#define CPDMA_MACINVECTOR	0x90
+#define CPDMA_MACEOIVECTOR	0x94
+#define CPDMA_RXINTSTATRAW	0xa0
+#define CPDMA_RXINTSTATMASKED	0xa4
+#define CPDMA_RXINTMASKSET	0xa8
+#define CPDMA_RXINTMASKCLEAR	0xac
+#define CPDMA_DMAINTSTATRAW	0xb0
+#define CPDMA_DMAINTSTATMASKED	0xb4
+#define CPDMA_DMAINTMASKSET	0xb8
+#define CPDMA_DMAINTMASKCLEAR	0xbc
+#define CPDMA_DMAINT_HOSTERR	BIT(1)
+
+/* the following exist only if has_ext_regs is set */
+#define CPDMA_DMACONTROL	0x20
+#define CPDMA_DMASTATUS		0x24
+#define CPDMA_RXBUFFOFS		0x28
+#define CPDMA_EM_CONTROL	0x2c
+
+/* Descriptor mode bits */
+#define CPDMA_DESC_SOP		BIT(31)
+#define CPDMA_DESC_EOP		BIT(30)
+#define CPDMA_DESC_OWNER	BIT(29)
+#define CPDMA_DESC_EOQ		BIT(28)
+#define CPDMA_DESC_TD_COMPLETE	BIT(27)
+#define CPDMA_DESC_PASS_CRC	BIT(26)
+
+#define CPDMA_TEARDOWN_VALUE	0xfffffffc
+
+struct cpdma_desc {
+	/* hardware fields */
+	u32			hw_next;
+	u32			hw_buffer;
+	u32			hw_len;
+	u32			hw_mode;
+	/* software fields */
+	void			*sw_token;
+	u32			sw_buffer;
+	u32			sw_len;
+};
+
+struct cpdma_desc_pool {
+	u32			phys;
+	u32			hw_addr;
+	void __iomem		*iomap;		/* ioremap map */
+	void			*cpumap;	/* dma_alloc map */
+	int			desc_size, mem_size;
+	int			num_desc, used_desc;
+	unsigned long		*bitmap;
+	struct device		*dev;
+	raw_spinlock_t		lock;
+};
+
+enum cpdma_state {
+	CPDMA_STATE_IDLE,
+	CPDMA_STATE_ACTIVE,
+	CPDMA_STATE_TEARDOWN,
+};
+
+static const char *cpdma_state_str[] = { "idle", "active", "teardown" };
+
+struct cpdma_ctlr {
+	enum cpdma_state	state;
+	struct cpdma_params	params;
+	struct device		*dev;
+	struct cpdma_desc_pool	*pool;
+	raw_spinlock_t		lock;
+	struct cpdma_chan	*channels[2 * CPDMA_MAX_CHANNELS];
+};
+
+struct cpdma_chan {
+	enum cpdma_state		state;
+	struct cpdma_ctlr		*ctlr;
+	int				chan_num;
+	raw_spinlock_t			lock;
+	struct cpdma_desc __iomem	*head, *tail;
+	int				count;
+	void __iomem			*hdp, *cp, *rxfree;
+	u32				mask;
+	cpdma_handler_fn		handler;
+	enum dma_data_direction		dir;
+	struct cpdma_chan_stats		stats;
+	/* offsets into dmaregs */
+	int	int_set, int_clear, td;
+};
+
+/* The following make access to common cpdma_ctlr params more readable */
+#define dmaregs		params.dmaregs
+#define num_chan	params.num_chan
+
+/* various accessors */
+#define dma_reg_read(ctlr, ofs)		__raw_readl((ctlr)->dmaregs + (ofs))
+#define chan_read(chan, fld)		__raw_readl((chan)->fld)
+#define desc_read(desc, fld)		__raw_readl(&(desc)->fld)
+#define dma_reg_write(ctlr, ofs, v)	__raw_writel(v, (ctlr)->dmaregs + (ofs))
+#define chan_write(chan, fld, v)	__raw_writel(v, (chan)->fld)
+#define desc_write(desc, fld, v)	__raw_writel((u32)(v), &(desc)->fld)
+
+/*
+ * Utility constructs for a cpdma descriptor pool.  Some devices (e.g. davinci
+ * emac) have dedicated on-chip memory for these descriptors.  Some other
+ * devices (e.g. cpsw switches) use plain old memory.  Descriptor pools
+ * abstract out these details
+ */
+static struct cpdma_desc_pool *
+cpdma_desc_pool_create(struct device *dev, u32 phys, u32 hw_addr,
+				int size, int align)
+{
+	int bitmap_size;
+	struct cpdma_desc_pool *pool;
+
+	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
+	if (!pool)
+		return NULL;
+
+	raw_spin_lock_init(&pool->lock);
+
+	pool->dev	= dev;
+	pool->mem_size	= size;
+	pool->desc_size	= ALIGN(sizeof(struct cpdma_desc), align);
+	pool->num_desc	= size / pool->desc_size;
+
+	bitmap_size  = (pool->num_desc / BITS_PER_LONG) * sizeof(long);
+	pool->bitmap = kzalloc(bitmap_size, GFP_KERNEL);
+	if (!pool->bitmap)
+		goto fail;
+
+	if (phys) {
+		pool->phys  = phys;
+		pool->iomap = ioremap(phys, size);
+		pool->hw_addr = hw_addr;
+	} else {
+		pool->cpumap = dma_alloc_coherent(dev, size, &pool->phys,
+						  GFP_KERNEL);
+		pool->iomap = pool->cpumap;
+		pool->hw_addr = pool->phys;
+	}
+
+	if (pool->iomap)
+		return pool;
+
+fail:
+	kfree(pool->bitmap);
+	kfree(pool);
+	return NULL;
+}
+
+static void cpdma_desc_pool_destroy(struct cpdma_desc_pool *pool)
+{
+	unsigned long context;
+
+	if (!pool)
+		return;
+
+	raw_spin_lock_irqsave(&pool->lock, context);
+	WARN_ON(pool->used_desc);
+	kfree(pool->bitmap);
+	if (pool->cpumap) {
+		dma_free_coherent(pool->dev, pool->mem_size, pool->cpumap,
+				  pool->phys);
+	} else {
+		iounmap(pool->iomap);
+	}
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+	kfree(pool);
+}
+
+static inline dma_addr_t desc_phys(struct cpdma_desc_pool *pool,
+		  struct cpdma_desc __iomem *desc)
+{
+	if (!desc)
+		return 0;
+	return pool->hw_addr + (__force dma_addr_t)desc -
+			    (__force dma_addr_t)pool->iomap;
+}
+
+static inline struct cpdma_desc __iomem *
+desc_from_phys(struct cpdma_desc_pool *pool, dma_addr_t dma)
+{
+	return dma ? pool->iomap + dma - pool->hw_addr : NULL;
+}
+
+static struct cpdma_desc __iomem *
+cpdma_desc_alloc(struct cpdma_desc_pool *pool, int num_desc)
+{
+	unsigned long context;
+	int index;
+	struct cpdma_desc __iomem *desc = NULL;
+
+	raw_spin_lock_irqsave(&pool->lock, context);
+
+	index = bitmap_find_next_zero_area(pool->bitmap, pool->num_desc, 0,
+					   num_desc, 0);
+	if (index < pool->num_desc) {
+		bitmap_set(pool->bitmap, index, num_desc);
+		desc = pool->iomap + pool->desc_size * index;
+		pool->used_desc++;
+	}
+
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+	return desc;
+}
+
+static void cpdma_desc_free(struct cpdma_desc_pool *pool,
+			    struct cpdma_desc __iomem *desc, int num_desc)
+{
+	unsigned long index;
+	unsigned long context;
+
+	index = ((unsigned long)desc - (unsigned long)pool->iomap) /
+		pool->desc_size;
+	raw_spin_lock_irqsave(&pool->lock, context);
+	bitmap_clear(pool->bitmap, index, num_desc);
+	pool->used_desc--;
+	raw_spin_unlock_irqrestore(&pool->lock, context);
+}
+
+struct cpdma_ctlr *cpdma_ctlr_create(struct cpdma_params *params)
+{
+	struct cpdma_ctlr *ctlr;
+
+	ctlr = kzalloc(sizeof(*ctlr), GFP_KERNEL);
+	if (!ctlr)
+		return NULL;
+
+	ctlr->state = CPDMA_STATE_IDLE;
+	ctlr->params = *params;
+	ctlr->dev = params->dev;
+	raw_spin_lock_init(&ctlr->lock);
+
+	ctlr->pool = cpdma_desc_pool_create(ctlr->dev,
+					    ctlr->params.desc_mem_phys,
+					    ctlr->params.desc_hw_addr,
+					    ctlr->params.desc_mem_size,
+					    ctlr->params.desc_align);
+	if (!ctlr->pool) {
+		kfree(ctlr);
+		return NULL;
+	}
+
+	if (WARN_ON(ctlr->num_chan > CPDMA_MAX_CHANNELS))
+		ctlr->num_chan = CPDMA_MAX_CHANNELS;
+	return ctlr;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_create);
+
+int cpdma_ctlr_start(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_IDLE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EBUSY;
+	}
+
+	if (ctlr->params.has_soft_reset) {
+		unsigned long timeout = jiffies + HZ/10;
+
+		dma_reg_write(ctlr, CPDMA_SOFTRESET, 1);
+		while (time_before(jiffies, timeout)) {
+			if (dma_reg_read(ctlr, CPDMA_SOFTRESET) == 0)
+				break;
+		}
+		WARN_ON(!time_before(jiffies, timeout));
+	}
+
+	for (i = 0; i < ctlr->num_chan; i++) {
+		__raw_writel(0, ctlr->params.txhdp + 4 * i);
+		__raw_writel(0, ctlr->params.rxhdp + 4 * i);
+		__raw_writel(0, ctlr->params.txcp + 4 * i);
+		__raw_writel(0, ctlr->params.rxcp + 4 * i);
+	}
+
+	dma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);
+	dma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);
+
+	dma_reg_write(ctlr, CPDMA_TXCONTROL, 1);
+	dma_reg_write(ctlr, CPDMA_RXCONTROL, 1);
+
+	ctlr->state = CPDMA_STATE_ACTIVE;
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_start(ctlr->channels[i]);
+	}
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_start);
+
+int cpdma_ctlr_stop(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EINVAL;
+	}
+
+	ctlr->state = CPDMA_STATE_TEARDOWN;
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_stop(ctlr->channels[i]);
+	}
+
+	dma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);
+	dma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);
+
+	dma_reg_write(ctlr, CPDMA_TXCONTROL, 0);
+	dma_reg_write(ctlr, CPDMA_RXCONTROL, 0);
+
+	ctlr->state = CPDMA_STATE_IDLE;
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_stop);
+
+int cpdma_ctlr_dump(struct cpdma_ctlr *ctlr)
+{
+	struct device *dev = ctlr->dev;
+	unsigned long context;
+	int i;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	dev_info(dev, "CPDMA: state: %s", cpdma_state_str[ctlr->state]);
+
+	dev_info(dev, "CPDMA: txidver: %x",
+		 dma_reg_read(ctlr, CPDMA_TXIDVER));
+	dev_info(dev, "CPDMA: txcontrol: %x",
+		 dma_reg_read(ctlr, CPDMA_TXCONTROL));
+	dev_info(dev, "CPDMA: txteardown: %x",
+		 dma_reg_read(ctlr, CPDMA_TXTEARDOWN));
+	dev_info(dev, "CPDMA: rxidver: %x",
+		 dma_reg_read(ctlr, CPDMA_RXIDVER));
+	dev_info(dev, "CPDMA: rxcontrol: %x",
+		 dma_reg_read(ctlr, CPDMA_RXCONTROL));
+	dev_info(dev, "CPDMA: softreset: %x",
+		 dma_reg_read(ctlr, CPDMA_SOFTRESET));
+	dev_info(dev, "CPDMA: rxteardown: %x",
+		 dma_reg_read(ctlr, CPDMA_RXTEARDOWN));
+	dev_info(dev, "CPDMA: txintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTSTATRAW));
+	dev_info(dev, "CPDMA: txintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTSTATMASKED));
+	dev_info(dev, "CPDMA: txintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTMASKSET));
+	dev_info(dev, "CPDMA: txintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_TXINTMASKCLEAR));
+	dev_info(dev, "CPDMA: macinvector: %x",
+		 dma_reg_read(ctlr, CPDMA_MACINVECTOR));
+	dev_info(dev, "CPDMA: maceoivector: %x",
+		 dma_reg_read(ctlr, CPDMA_MACEOIVECTOR));
+	dev_info(dev, "CPDMA: rxintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTSTATRAW));
+	dev_info(dev, "CPDMA: rxintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTSTATMASKED));
+	dev_info(dev, "CPDMA: rxintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTMASKSET));
+	dev_info(dev, "CPDMA: rxintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_RXINTMASKCLEAR));
+	dev_info(dev, "CPDMA: dmaintstatraw: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTSTATRAW));
+	dev_info(dev, "CPDMA: dmaintstatmasked: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTSTATMASKED));
+	dev_info(dev, "CPDMA: dmaintmaskset: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTMASKSET));
+	dev_info(dev, "CPDMA: dmaintmaskclear: %x",
+		 dma_reg_read(ctlr, CPDMA_DMAINTMASKCLEAR));
+
+	if (!ctlr->params.has_ext_regs) {
+		dev_info(dev, "CPDMA: dmacontrol: %x",
+			 dma_reg_read(ctlr, CPDMA_DMACONTROL));
+		dev_info(dev, "CPDMA: dmastatus: %x",
+			 dma_reg_read(ctlr, CPDMA_DMASTATUS));
+		dev_info(dev, "CPDMA: rxbuffofs: %x",
+			 dma_reg_read(ctlr, CPDMA_RXBUFFOFS));
+	}
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++)
+		if (ctlr->channels[i])
+			cpdma_chan_dump(ctlr->channels[i]);
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_dump);
+
+int cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr)
+{
+	unsigned long context;
+	int ret = 0, i;
+
+	if (!ctlr)
+		return -EINVAL;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_IDLE)
+		cpdma_ctlr_stop(ctlr);
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_destroy(ctlr->channels[i]);
+	}
+
+	cpdma_desc_pool_destroy(ctlr->pool);
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(ctlr);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_destroy);
+
+int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable)
+{
+	unsigned long context;
+	int i, reg;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&ctlr->lock, context);
+		return -EINVAL;
+	}
+
+	reg = enable ? CPDMA_DMAINTMASKSET : CPDMA_DMAINTMASKCLEAR;
+	dma_reg_write(ctlr, reg, CPDMA_DMAINT_HOSTERR);
+
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_int_ctrl(ctlr->channels[i], enable);
+	}
+
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_int_ctrl);
+
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value)
+{
+	dma_reg_write(ctlr, CPDMA_MACEOIVECTOR, value);
+}
+EXPORT_SYMBOL_GPL(cpdma_ctlr_eoi);
+
+struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
+				     cpdma_handler_fn handler)
+{
+	struct cpdma_chan *chan;
+	int ret, offset = (chan_num % CPDMA_MAX_CHANNELS) * 4;
+	unsigned long context;
+
+	if (__chan_linear(chan_num) >= ctlr->num_chan)
+		return NULL;
+
+	ret = -ENOMEM;
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan)
+		goto err_chan_alloc;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	ret = -EBUSY;
+	if (ctlr->channels[chan_num])
+		goto err_chan_busy;
+
+	chan->ctlr	= ctlr;
+	chan->state	= CPDMA_STATE_IDLE;
+	chan->chan_num	= chan_num;
+	chan->handler	= handler;
+
+	if (is_rx_chan(chan)) {
+		chan->hdp	= ctlr->params.rxhdp + offset;
+		chan->cp	= ctlr->params.rxcp + offset;
+		chan->rxfree	= ctlr->params.rxfree + offset;
+		chan->int_set	= CPDMA_RXINTMASKSET;
+		chan->int_clear	= CPDMA_RXINTMASKCLEAR;
+		chan->td	= CPDMA_RXTEARDOWN;
+		chan->dir	= DMA_FROM_DEVICE;
+	} else {
+		chan->hdp	= ctlr->params.txhdp + offset;
+		chan->cp	= ctlr->params.txcp + offset;
+		chan->int_set	= CPDMA_TXINTMASKSET;
+		chan->int_clear	= CPDMA_TXINTMASKCLEAR;
+		chan->td	= CPDMA_TXTEARDOWN;
+		chan->dir	= DMA_TO_DEVICE;
+	}
+	chan->mask = BIT(chan_linear(chan));
+
+	raw_spin_lock_init(&chan->lock);
+
+	ctlr->channels[chan_num] = chan;
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return chan;
+
+err_chan_busy:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(chan);
+err_chan_alloc:
+	return ERR_PTR(ret);
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_create);
+
+int cpdma_chan_destroy(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr *ctlr;
+	unsigned long context;
+
+	if (!chan)
+		return -EINVAL;
+	ctlr = chan->ctlr;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+	if (chan->state != CPDMA_STATE_IDLE)
+		cpdma_chan_stop(chan);
+	ctlr->channels[chan->chan_num] = NULL;
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	kfree(chan);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_destroy);
+
+int cpdma_chan_get_stats(struct cpdma_chan *chan,
+			 struct cpdma_chan_stats *stats)
+{
+	unsigned long context;
+	if (!chan)
+		return -EINVAL;
+	raw_spin_lock_irqsave(&chan->lock, context);
+	memcpy(stats, &chan->stats, sizeof(*stats));
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+
+int cpdma_chan_dump(struct cpdma_chan *chan)
+{
+	unsigned long context;
+	struct device *dev = chan->ctlr->dev;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	dev_info(dev, "channel %d (%s %d) state %s",
+		 chan->chan_num, is_rx_chan(chan) ? "rx" : "tx",
+		 chan_linear(chan), cpdma_state_str[chan->state]);
+	dev_info(dev, "\thdp: %x\n", chan_read(chan, hdp));
+	dev_info(dev, "\tcp: %x\n", chan_read(chan, cp));
+	if (chan->rxfree) {
+		dev_info(dev, "\trxfree: %x\n",
+			 chan_read(chan, rxfree));
+	}
+
+	dev_info(dev, "\tstats head_enqueue: %d\n",
+		 chan->stats.head_enqueue);
+	dev_info(dev, "\tstats tail_enqueue: %d\n",
+		 chan->stats.tail_enqueue);
+	dev_info(dev, "\tstats pad_enqueue: %d\n",
+		 chan->stats.pad_enqueue);
+	dev_info(dev, "\tstats misqueued: %d\n",
+		 chan->stats.misqueued);
+	dev_info(dev, "\tstats desc_alloc_fail: %d\n",
+		 chan->stats.desc_alloc_fail);
+	dev_info(dev, "\tstats pad_alloc_fail: %d\n",
+		 chan->stats.pad_alloc_fail);
+	dev_info(dev, "\tstats runt_receive_buff: %d\n",
+		 chan->stats.runt_receive_buff);
+	dev_info(dev, "\tstats runt_transmit_buff: %d\n",
+		 chan->stats.runt_transmit_buff);
+	dev_info(dev, "\tstats empty_dequeue: %d\n",
+		 chan->stats.empty_dequeue);
+	dev_info(dev, "\tstats busy_dequeue: %d\n",
+		 chan->stats.busy_dequeue);
+	dev_info(dev, "\tstats good_dequeue: %d\n",
+		 chan->stats.good_dequeue);
+	dev_info(dev, "\tstats requeue: %d\n",
+		 chan->stats.requeue);
+	dev_info(dev, "\tstats teardown_dequeue: %d\n",
+		 chan->stats.teardown_dequeue);
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+
+static void __cpdma_chan_submit(struct cpdma_chan *chan,
+				struct cpdma_desc __iomem *desc)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*prev = chan->tail;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			desc_dma;
+	u32				mode;
+
+	desc_dma = desc_phys(pool, desc);
+
+	/* simple case - idle channel */
+	if (!chan->head) {
+		chan->stats.head_enqueue++;
+		chan->head = desc;
+		chan->tail = desc;
+		if (chan->state == CPDMA_STATE_ACTIVE)
+			chan_write(chan, hdp, desc_dma);
+		return;
+	}
+
+	/* first chain the descriptor at the tail of the list */
+	desc_write(prev, hw_next, desc_dma);
+	chan->tail = desc;
+	chan->stats.tail_enqueue++;
+
+	/* next check if EOQ has been triggered already */
+	mode = desc_read(prev, hw_mode);
+	if (((mode & (CPDMA_DESC_EOQ | CPDMA_DESC_OWNER)) == CPDMA_DESC_EOQ) &&
+	    (chan->state == CPDMA_STATE_ACTIVE)) {
+		desc_write(prev, hw_mode, mode & ~CPDMA_DESC_EOQ);
+		chan_write(chan, hdp, desc_dma);
+		chan->stats.misqueued++;
+	}
+}
+
+int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
+		      int len, gfp_t gfp_mask)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*desc;
+	dma_addr_t			buffer;
+	unsigned long			context;
+	u32				mode;
+	int				ret = 0;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	if (chan->state == CPDMA_STATE_TEARDOWN) {
+		ret = -EINVAL;
+		goto unlock_ret;
+	}
+
+	desc = cpdma_desc_alloc(ctlr->pool, 1);
+	if (!desc) {
+		chan->stats.desc_alloc_fail++;
+		ret = -ENOMEM;
+		goto unlock_ret;
+	}
+
+	if (len < ctlr->params.min_packet_size) {
+		len = ctlr->params.min_packet_size;
+		chan->stats.runt_transmit_buff++;
+	}
+
+	buffer = dma_map_single(ctlr->dev, data, len, chan->dir);
+	mode = CPDMA_DESC_OWNER | CPDMA_DESC_SOP | CPDMA_DESC_EOP;
+
+	desc_write(desc, hw_next,   0);
+	desc_write(desc, hw_buffer, buffer);
+	desc_write(desc, hw_len,    len);
+	desc_write(desc, hw_mode,   mode | len);
+	desc_write(desc, sw_token,  token);
+	desc_write(desc, sw_buffer, buffer);
+	desc_write(desc, sw_len,    len);
+
+	__cpdma_chan_submit(chan, desc);
+
+	if (chan->state == CPDMA_STATE_ACTIVE && chan->rxfree)
+		chan_write(chan, rxfree, 1);
+
+	chan->count++;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_submit);
+
+static void __cpdma_chan_free(struct cpdma_chan *chan,
+			      struct cpdma_desc __iomem *desc,
+			      int outlen, int status)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			buff_dma;
+	int				origlen;
+	void				*token;
+
+	//rtdm_printk("__cpdma_chan_free(%x, %x, %d, %d)\n", chan, desc, outlen, status);
+
+	token      = (void *)desc_read(desc, sw_token);
+	buff_dma   = desc_read(desc, sw_buffer);
+	origlen    = desc_read(desc, sw_len);
+
+
+	dma_unmap_single(ctlr->dev, buff_dma, origlen, chan->dir);
+	cpdma_desc_free(pool, desc, 1);
+	(*chan->handler)(token, outlen, status);
+}
+
+static int __cpdma_chan_process(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr		*ctlr = chan->ctlr;
+	struct cpdma_desc __iomem	*desc;
+	int				status, outlen;
+	struct cpdma_desc_pool		*pool = ctlr->pool;
+	dma_addr_t			desc_dma;
+	unsigned long			context;
+
+	//rtdm_printk("__cpdma_chan_process(%x)\n", chan);
+	
+	raw_spin_lock_irqsave(&chan->lock, context);
+	desc = chan->head;
+	if (!desc) {
+		chan->stats.empty_dequeue++;
+		status = -ENOENT;
+		goto unlock_ret;
+	}
+	desc_dma = desc_phys(pool, desc);
+
+	status	= __raw_readl(&desc->hw_mode);
+	outlen	= status & 0x7ff;
+	if (status & CPDMA_DESC_OWNER) {
+		chan->stats.busy_dequeue++;
+		status = -EBUSY;
+		goto unlock_ret;
+	}
+	status	= status & (CPDMA_DESC_EOQ | CPDMA_DESC_TD_COMPLETE);
+
+	chan->head = desc_from_phys(pool, desc_read(desc, hw_next));
+	chan_write(chan, cp, desc_dma);
+	chan->count--;
+	chan->stats.good_dequeue++;
+
+	if (status & CPDMA_DESC_EOQ) {
+		chan->stats.requeue++;
+		chan_write(chan, hdp, desc_phys(pool, chan->head));
+	}
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	__cpdma_chan_free(chan, desc, outlen, status);
+	return status;
+unlock_ret:
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	return status;
+}
+
+int cpdma_chan_process(struct cpdma_chan *chan, int quota)
+{
+	int used = 0, ret = 0;
+
+	//rtdm_printk("cpdma_chan_process(%x, %d)\n", chan, quota);
+	
+	if (chan->state != CPDMA_STATE_ACTIVE)
+		return -EINVAL;
+
+	while (used < quota) {
+		ret = __cpdma_chan_process(chan);
+		if (ret < 0)
+			break;
+		used++;
+	}
+	return used;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_process);
+
+int cpdma_chan_start(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr	*ctlr = chan->ctlr;
+	struct cpdma_desc_pool	*pool = ctlr->pool;
+	unsigned long		context;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_IDLE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EBUSY;
+	}
+	if (ctlr->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+	dma_reg_write(ctlr, chan->int_set, chan->mask);
+	chan->state = CPDMA_STATE_ACTIVE;
+	if (chan->head) {
+		chan_write(chan, hdp, desc_phys(pool, chan->head));
+		if (chan->rxfree)
+			chan_write(chan, rxfree, chan->count);
+	}
+
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_start);
+
+int cpdma_chan_stop(struct cpdma_chan *chan)
+{
+	struct cpdma_ctlr	*ctlr = chan->ctlr;
+	struct cpdma_desc_pool	*pool = ctlr->pool;
+	unsigned long		context;
+	int			ret;
+	unsigned long		timeout;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+
+	chan->state = CPDMA_STATE_TEARDOWN;
+	dma_reg_write(ctlr, chan->int_clear, chan->mask);
+
+	/* trigger teardown */
+	dma_reg_write(ctlr, chan->td, chan_linear(chan));
+
+	/* wait for teardown complete */
+	timeout = jiffies + HZ/10;	/* 100 msec */
+	while (time_before(jiffies, timeout)) {
+		u32 cp = chan_read(chan, cp);
+		if ((cp & CPDMA_TEARDOWN_VALUE) == CPDMA_TEARDOWN_VALUE)
+			break;
+		cpu_relax();
+	}
+	WARN_ON(!time_before(jiffies, timeout));
+	chan_write(chan, cp, CPDMA_TEARDOWN_VALUE);
+
+	/* handle completed packets */
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	do {
+		ret = __cpdma_chan_process(chan);
+		if (ret < 0)
+			break;
+	} while ((ret & CPDMA_DESC_TD_COMPLETE) == 0);
+	raw_spin_lock_irqsave(&chan->lock, context);
+
+	/* remaining packets haven't been tx/rx'ed, clean them up */
+	while (chan->head) {
+		struct cpdma_desc __iomem *desc = chan->head;
+		dma_addr_t next_dma;
+
+		next_dma = desc_read(desc, hw_next);
+		chan->head = desc_from_phys(pool, next_dma);
+		chan->count--;
+		chan->stats.teardown_dequeue++;
+
+		/* issue callback without locks held */
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		__cpdma_chan_free(chan, desc, 0, -ENOSYS);
+		raw_spin_lock_irqsave(&chan->lock, context);
+	}
+
+	chan->state = CPDMA_STATE_IDLE;
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cpdma_chan_stop);
+
+int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable)
+{
+	unsigned long context;
+
+	raw_spin_lock_irqsave(&chan->lock, context);
+	if (chan->state != CPDMA_STATE_ACTIVE) {
+		raw_spin_unlock_irqrestore(&chan->lock, context);
+		return -EINVAL;
+	}
+
+	dma_reg_write(chan->ctlr, enable ? chan->int_set : chan->int_clear,
+		      chan->mask);
+	raw_spin_unlock_irqrestore(&chan->lock, context);
+
+	return 0;
+}
+
+struct cpdma_control_info {
+	u32		reg;
+	u32		shift, mask;
+	int		access;
+#define ACCESS_RO	BIT(0)
+#define ACCESS_WO	BIT(1)
+#define ACCESS_RW	(ACCESS_RO | ACCESS_WO)
+};
+
+struct cpdma_control_info controls[] = {
+	[CPDMA_CMD_IDLE]	  = {CPDMA_DMACONTROL,	3,  1,      ACCESS_WO},
+	[CPDMA_COPY_ERROR_FRAMES] = {CPDMA_DMACONTROL,	4,  1,      ACCESS_RW},
+	[CPDMA_RX_OFF_LEN_UPDATE] = {CPDMA_DMACONTROL,	2,  1,      ACCESS_RW},
+	[CPDMA_RX_OWNERSHIP_FLIP] = {CPDMA_DMACONTROL,	1,  1,      ACCESS_RW},
+	[CPDMA_TX_PRIO_FIXED]	  = {CPDMA_DMACONTROL,	0,  1,      ACCESS_RW},
+	[CPDMA_STAT_IDLE]	  = {CPDMA_DMASTATUS,	31, 1,      ACCESS_RO},
+	[CPDMA_STAT_TX_ERR_CODE]  = {CPDMA_DMASTATUS,	20, 0xf,    ACCESS_RW},
+	[CPDMA_STAT_TX_ERR_CHAN]  = {CPDMA_DMASTATUS,	16, 0x7,    ACCESS_RW},
+	[CPDMA_STAT_RX_ERR_CODE]  = {CPDMA_DMASTATUS,	12, 0xf,    ACCESS_RW},
+	[CPDMA_STAT_RX_ERR_CHAN]  = {CPDMA_DMASTATUS,	8,  0x7,    ACCESS_RW},
+	[CPDMA_RX_BUFFER_OFFSET]  = {CPDMA_RXBUFFOFS,	0,  0xffff, ACCESS_RW},
+};
+
+int cpdma_control_get(struct cpdma_ctlr *ctlr, int control)
+{
+	unsigned long context;
+	struct cpdma_control_info *info = &controls[control];
+	int ret;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	ret = -ENOTSUPP;
+	if (!ctlr->params.has_ext_regs)
+		goto unlock_ret;
+
+	ret = -EINVAL;
+	if (ctlr->state != CPDMA_STATE_ACTIVE)
+		goto unlock_ret;
+
+	ret = -ENOENT;
+	if (control < 0 || control >= ARRAY_SIZE(controls))
+		goto unlock_ret;
+
+	ret = -EPERM;
+	if ((info->access & ACCESS_RO) != ACCESS_RO)
+		goto unlock_ret;
+
+	ret = (dma_reg_read(ctlr, info->reg) >> info->shift) & info->mask;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return ret;
+}
+
+int cpdma_control_set(struct cpdma_ctlr *ctlr, int control, int value)
+{
+	unsigned long context;
+	struct cpdma_control_info *info = &controls[control];
+	int ret;
+	u32 val;
+
+	raw_spin_lock_irqsave(&ctlr->lock, context);
+
+	ret = -ENOTSUPP;
+	if (!ctlr->params.has_ext_regs)
+		goto unlock_ret;
+
+	ret = -EINVAL;
+	if (ctlr->state != CPDMA_STATE_ACTIVE)
+		goto unlock_ret;
+
+	ret = -ENOENT;
+	if (control < 0 || control >= ARRAY_SIZE(controls))
+		goto unlock_ret;
+
+	ret = -EPERM;
+	if ((info->access & ACCESS_WO) != ACCESS_WO)
+		goto unlock_ret;
+
+	val  = dma_reg_read(ctlr, info->reg);
+	val &= ~(info->mask << info->shift);
+	val |= (value & info->mask) << info->shift;
+	dma_reg_write(ctlr, info->reg, val);
+	ret = 0;
+
+unlock_ret:
+	raw_spin_unlock_irqrestore(&ctlr->lock, context);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cpdma_control_set);
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/davinci_cpdma.h linux-5.4.5-new/net/rtnet/drivers/ticpsw/davinci_cpdma.h
--- linux-5.4.5/net/rtnet/drivers/ticpsw/davinci_cpdma.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/davinci_cpdma.h	2020-06-15 16:12:31.467695589 +0300
@@ -0,0 +1,109 @@
+/*
+ * Texas Instruments CPDMA Driver
+ *
+ * Copyright (C) 2010 Texas Instruments
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __DAVINCI_CPDMA_H__
+#define __DAVINCI_CPDMA_H__
+
+#define CPDMA_MAX_CHANNELS	BITS_PER_LONG
+
+#define tx_chan_num(chan)	(chan)
+#define rx_chan_num(chan)	((chan) + CPDMA_MAX_CHANNELS)
+#define is_rx_chan(chan)	((chan)->chan_num >= CPDMA_MAX_CHANNELS)
+#define is_tx_chan(chan)	(!is_rx_chan(chan))
+#define __chan_linear(chan_num)	((chan_num) & (CPDMA_MAX_CHANNELS - 1))
+#define chan_linear(chan)	__chan_linear((chan)->chan_num)
+
+struct cpdma_params {
+	struct device		*dev;
+	void __iomem		*dmaregs;
+	void __iomem		*txhdp, *rxhdp, *txcp, *rxcp;
+	void __iomem		*rxthresh, *rxfree;
+	int			num_chan;
+	bool			has_soft_reset;
+	int			min_packet_size;
+	u32			desc_mem_phys;
+	u32			desc_hw_addr;
+	int			desc_mem_size;
+	int			desc_align;
+
+	/*
+	 * Some instances of embedded cpdma controllers have extra control and
+	 * status registers.  The following flag enables access to these
+	 * "extended" registers.
+	 */
+	bool			has_ext_regs;
+};
+
+struct cpdma_chan_stats {
+	u32			head_enqueue;
+	u32			tail_enqueue;
+	u32			pad_enqueue;
+	u32			misqueued;
+	u32			desc_alloc_fail;
+	u32			pad_alloc_fail;
+	u32			runt_receive_buff;
+	u32			runt_transmit_buff;
+	u32			empty_dequeue;
+	u32			busy_dequeue;
+	u32			good_dequeue;
+	u32			requeue;
+	u32			teardown_dequeue;
+};
+
+struct cpdma_ctlr;
+struct cpdma_chan;
+
+typedef void (*cpdma_handler_fn)(void *token, int len, int status);
+
+struct cpdma_ctlr *cpdma_ctlr_create(struct cpdma_params *params);
+int cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_start(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_stop(struct cpdma_ctlr *ctlr);
+int cpdma_ctlr_dump(struct cpdma_ctlr *ctlr);
+
+struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
+				     cpdma_handler_fn handler);
+int cpdma_chan_destroy(struct cpdma_chan *chan);
+int cpdma_chan_start(struct cpdma_chan *chan);
+int cpdma_chan_stop(struct cpdma_chan *chan);
+int cpdma_chan_dump(struct cpdma_chan *chan);
+
+int cpdma_chan_get_stats(struct cpdma_chan *chan,
+			 struct cpdma_chan_stats *stats);
+int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
+		      int len, gfp_t gfp_mask);
+int cpdma_chan_process(struct cpdma_chan *chan, int quota);
+
+int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable);
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value);
+int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable);
+
+enum cpdma_control {
+	CPDMA_CMD_IDLE,			/* write-only */
+	CPDMA_COPY_ERROR_FRAMES,	/* read-write */
+	CPDMA_RX_OFF_LEN_UPDATE,	/* read-write */
+	CPDMA_RX_OWNERSHIP_FLIP,	/* read-write */
+	CPDMA_TX_PRIO_FIXED,		/* read-write */
+	CPDMA_STAT_IDLE,		/* read-only */
+	CPDMA_STAT_TX_ERR_CHAN,		/* read-only */
+	CPDMA_STAT_TX_ERR_CODE,		/* read-only */
+	CPDMA_STAT_RX_ERR_CHAN,		/* read-only */
+	CPDMA_STAT_RX_ERR_CODE,		/* read-only */
+	CPDMA_RX_BUFFER_OFFSET,		/* read-write */
+};
+
+int cpdma_control_get(struct cpdma_ctlr *ctlr, int control);
+int cpdma_control_set(struct cpdma_ctlr *ctlr, int control, int value);
+
+#endif
diff -Nur linux-5.4.5/net/rtnet/drivers/ticpsw/Makefile linux-5.4.5-new/net/rtnet/drivers/ticpsw/Makefile
--- linux-5.4.5/net/rtnet/drivers/ticpsw/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/drivers/ticpsw/Makefile	2020-06-15 16:12:31.471695575 +0300
@@ -0,0 +1,10 @@
+ccflags-y += -Inet/rtnet/stack/include -g -DDEBUG
+
+obj-$(CONFIG_XENO_DRIVERS_NET_DRV_TI_CPSW) += rt_ticpsw.o
+
+rt_ticpsw-y := \
+	cpsw.o \
+	cpsw_ale.o \
+	cpts.o \
+	davinci_cpdma.o
+
diff -Nur linux-5.4.5/net/rtnet/Kconfig linux-5.4.5-new/net/rtnet/Kconfig
--- linux-5.4.5/net/rtnet/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/Kconfig	2020-06-15 16:12:31.491695504 +0300
@@ -0,0 +1,24 @@
+menu "RTnet"
+
+config XENO_DRIVERS_NET
+    depends on NET
+    tristate "RTnet, TCP/IP socket interface"
+
+if XENO_DRIVERS_NET
+
+config XENO_DRIVERS_RTNET_CHECKED
+    bool "Internal Bug Checks"
+    default n
+    ---help---
+    Switch on if you face crashes when RTnet is running or if you suspect
+    any other RTnet-related issues. This feature will add a few sanity
+    checks at critical points that will produce warnings on the kernel
+    console in case certain internal bugs are detected.
+
+source "net/rtnet/stack/Kconfig"
+source "net/rtnet/drivers/Kconfig"
+source "net/rtnet/addons/Kconfig"
+
+endif
+
+endmenu
diff -Nur linux-5.4.5/net/rtnet/Makefile linux-5.4.5-new/net/rtnet/Makefile
--- linux-5.4.5/net/rtnet/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/Makefile	2020-06-15 16:12:31.491695504 +0300
@@ -0,0 +1 @@
+obj-$(CONFIG_XENO_DRIVERS_NET) += stack/ drivers/ addons/
diff -Nur linux-5.4.5/net/rtnet/stack/eth.c linux-5.4.5-new/net/rtnet/stack/eth.c
--- linux-5.4.5/net/rtnet/stack/eth.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/eth.c	2020-06-15 16:12:31.495695490 +0300
@@ -0,0 +1,141 @@
+/***
+ *
+ *  stack/eth.c - Ethernet-specific functions
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+
+
+/*
+ *  Create the Ethernet MAC header for an arbitrary protocol layer
+ *
+ *  saddr=NULL  means use device source address
+ *  daddr=NULL  means leave destination address (eg unresolved arp)
+ */
+int rt_eth_header(struct rtskb *skb, struct rtnet_device *rtdev,
+                  unsigned short type, void *daddr, void *saddr, unsigned len)
+{
+    struct ethhdr *eth = (struct ethhdr *)rtskb_push(skb,ETH_HLEN);
+
+    /*
+     *  Set rtskb mac field
+     */
+
+    skb->mac.ethernet = eth;
+
+    /*
+     *  Set the protocol type. For a packet of type ETH_P_802_3 we put the length
+     *  in here instead. It is up to the 802.2 layer to carry protocol information.
+     */
+
+    if (type!=ETH_P_802_3)
+        eth->h_proto = htons(type);
+    else
+        eth->h_proto = htons(len);
+
+    /*
+     *  Set the source hardware address.
+     */
+
+    if(saddr)
+        memcpy(eth->h_source,saddr,rtdev->addr_len);
+    else
+        memcpy(eth->h_source,rtdev->dev_addr,rtdev->addr_len);
+
+    if (rtdev->flags & (IFF_LOOPBACK|IFF_NOARP))
+    {
+        memset(eth->h_dest, 0, rtdev->addr_len);
+        return rtdev->hard_header_len;
+    }
+
+    if (daddr)
+    {
+        memcpy(eth->h_dest,daddr,rtdev->addr_len);
+        return rtdev->hard_header_len;
+    }
+
+    return -rtdev->hard_header_len;
+}
+
+
+
+
+unsigned short rt_eth_type_trans(struct rtskb *skb, struct rtnet_device *rtdev)
+{
+    struct ethhdr *eth;
+    unsigned char *rawp;
+
+
+    rtcap_mark_incoming(skb);
+
+    skb->mac.raw = skb->data;
+    rtskb_pull(skb,rtdev->hard_header_len);
+    eth = skb->mac.ethernet;
+
+    if (*eth->h_dest & 1)
+    {
+        if (memcmp(eth->h_dest,rtdev->broadcast, ETH_ALEN) == 0)
+            skb->pkt_type = PACKET_BROADCAST;
+        else
+            skb->pkt_type = PACKET_MULTICAST;
+    }
+
+    /*
+     *  This ALLMULTI check should be redundant by 1.4
+     *  so don't forget to remove it.
+     *
+     *  Seems, you forgot to remove it. All silly devices
+     *  seems to set IFF_PROMISC.
+     */
+
+    else if (1 /*rtdev->flags&IFF_PROMISC*/)
+    {
+        if (memcmp(eth->h_dest,rtdev->dev_addr, ETH_ALEN))
+            skb->pkt_type = PACKET_OTHERHOST;
+    }
+
+    if (ntohs(eth->h_proto) >= 1536)
+        return eth->h_proto;
+
+    rawp = skb->data;
+
+    /*
+     *  This is a magic hack to spot IPX packets. Older Novell breaks
+     *  the protocol design and runs IPX over 802.3 without an 802.2 LLC
+     *  layer. We look for FFFF which isn't a used 802.2 SSAP/DSAP. This
+     *  won't work for fault tolerant netware but does for the rest.
+     */
+    if (*(unsigned short *)rawp == 0xFFFF)
+        return htons(ETH_P_802_3);
+
+    /*
+     *  Real 802.2 LLC
+     */
+    return htons(ETH_P_802_2);
+}
+
+
+EXPORT_SYMBOL_GPL(rt_eth_header);
+EXPORT_SYMBOL_GPL(rt_eth_type_trans);
diff -Nur linux-5.4.5/net/rtnet/stack/include/ethernet/eth.h linux-5.4.5-new/net/rtnet/stack/include/ethernet/eth.h
--- linux-5.4.5/net/rtnet/stack/include/ethernet/eth.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ethernet/eth.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,32 @@
+/* ethernet/eth.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_ETH_H_
+#define __RTNET_ETH_H_
+
+
+#include <rtskb.h>
+#include <rtdev.h>
+
+extern int rt_eth_header(struct rtskb *skb,struct rtnet_device *rtdev, 
+			 unsigned short type,void *daddr,void *saddr,unsigned int len);
+extern unsigned short rt_eth_type_trans(struct rtskb *skb, struct rtnet_device *dev);
+
+
+#endif  /* __RTNET_ETH_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/INCLUDE.policy linux-5.4.5-new/net/rtnet/stack/include/INCLUDE.policy
--- linux-5.4.5/net/rtnet/stack/include/INCLUDE.policy	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/INCLUDE.policy	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,15 @@
+RTnet Include Policy
+
+1. Every source file (/<module>/<source>.c) shall have an associated
+   header file (/include/<module>/<source>.h). This header shall contain
+   all required #defines, types, and function prototypes (except they are
+   API related). 
+
+2. API functions, types, etc. shall be placed in header files located in
+   the main include directory (/include/<module>.h>). The header files
+   shall be named after the associated module.
+
+3. The main include directory shall only contain API header files. 
+
+4. All header files shall be includable without requiring further header
+   file to be included beforehand. 
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/af_inet.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/af_inet.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/af_inet.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/af_inet.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/ipv4/af_inet.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@wev.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_AF_INET_H_
+#define __RTNET_AF_INET_H_
+
+#include <rtnet_internal.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+extern struct xnvfile_directory ipv4_proc_root;
+#endif
+
+#endif  /* __RTNET_AF_INET_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/arp.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/arp.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/arp.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/arp.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,54 @@
+/***
+ *
+ *  include/ipv4/arp.h - Adress Resolution Protocol for RTnet
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ARP_H_
+#define __RTNET_ARP_H_
+
+#include <linux/if_arp.h>
+#include <linux/init.h>
+#include <linux/types.h>
+
+#include <ipv4/route.h>
+
+
+#define RT_ARP_SKB_PRIO     RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                             RTSKB_DEF_NRT_CHANNEL)
+
+void rt_arp_send(int type, int ptype, u32 dest_ip,
+                 struct rtnet_device *rtdev, u32 src_ip,
+                 unsigned char *dest_hw, unsigned char *src_hw,
+                 unsigned char *target_hw);
+
+static inline void rt_arp_solicit(struct rtnet_device *rtdev, u32 target)
+{
+    rt_arp_send(ARPOP_REQUEST, ETH_P_ARP, target, rtdev, rtdev->local_ip,
+                NULL, NULL, NULL);
+}
+
+void __init rt_arp_init(void);
+void rt_arp_release(void);
+
+
+#endif  /* __RTNET_ARP_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/icmp.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/icmp.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/icmp.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/icmp.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,56 @@
+/***
+ *
+ *  ipv4/icmp.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ICMP_H_
+#define __RTNET_ICMP_H_
+
+#include <linux/init.h>
+
+#include <rtskb.h>
+#include <rtnet_rtpc.h>
+#include <ipv4/protocol.h>
+
+
+#define RT_ICMP_PRIO            RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                                 RTSKB_DEF_NRT_CHANNEL)
+
+#define ICMP_REPLY_POOL_SIZE    8
+
+
+void rt_icmp_queue_echo_request(struct rt_proc_call *call);
+void rt_icmp_dequeue_echo_request(struct rt_proc_call *call);
+void rt_icmp_cleanup_echo_requests(void);
+int rt_icmp_send_echo(u32 daddr, u16 id, u16 sequence, size_t msg_size);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP
+void __init rt_icmp_init(void);
+void rt_icmp_release(void);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP */
+#define rt_icmp_init() do {} while (0)
+#define rt_icmp_release() do {} while (0)
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP */
+
+
+#endif  /* __RTNET_ICMP_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/ip_fragment.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_fragment.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/ip_fragment.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_fragment.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,39 @@
+/* ipv4/ip_fragment.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IP_FRAGMENT_H_
+#define __RTNET_IP_FRAGMENT_H_
+
+#include <linux/init.h>
+
+#include <rtskb.h>
+#include <ipv4/protocol.h>
+
+
+extern struct rtskb *rt_ip_defrag(struct rtskb *skb,
+                                  struct rtinet_protocol *ipprot);
+
+extern void rt_ip_frag_invalidate_socket(struct rtsocket *sock);
+
+extern int __init rt_ip_fragment_init(void);
+extern void rt_ip_fragment_cleanup(void);
+
+
+#endif  /* __RTNET_IP_FRAGMENT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/ip_input.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_input.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/ip_input.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_input.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,47 @@
+/* ipv4/ip_input.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IP_INPUT_H_
+#define __RTNET_IP_INPUT_H_
+
+#include <rtskb.h>
+#include <stack_mgr.h>
+
+
+extern int rt_ip_rcv(struct rtskb *skb, struct rtpacket_type *pt);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+typedef void (*rt_ip_fallback_handler_t)(struct rtskb *skb);
+
+/*
+ * This hook can be used to register a fallback handler for incoming
+ * IP packets. Typically this is done to move over to the standard Linux
+ * IP protocol (e.g. for handling TCP).
+ * Manipulating the fallback handler is expected to happen only when the
+ * RTnetinterfaces are shut down (avoiding race conditions).
+ *
+ * Note that merging RT and non-RT traffic this way most likely breaks hard
+ * real-time constraints!
+ */
+extern rt_ip_fallback_handler_t rt_ip_fallback_handler;
+#endif
+
+
+#endif  /* __RTNET_IP_INPUT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/ip_output.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_output.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/ip_output.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_output.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,42 @@
+/***
+ *
+ *  include/ipv4/ip_output.h - prepare outgoing IP packets
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_IP_OUTPUT_H_
+#define __RTNET_IP_OUTPUT_H_
+
+#include <linux/init.h>
+
+#include <rtdev.h>
+#include <ipv4/route.h>
+
+
+extern int rt_ip_build_xmit(struct rtsocket *sk,
+    int getfrag (const void *, unsigned char *, unsigned int, unsigned int),
+    const void *frag, unsigned length, struct dest_route *rt, int flags);
+
+extern void __init rt_ip_init(void);
+extern void rt_ip_release(void);
+
+
+#endif  /* __RTNET_IP_OUTPUT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/ip_sock.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_sock.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/ip_sock.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/ip_sock.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,32 @@
+/***
+ *
+ *  include/ipv4/ip_sock.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_IP_SOCK_H_
+#define __RTNET_IP_SOCK_H_
+
+#include <rtnet_socket.h>
+
+
+extern int rt_ip_ioctl(struct rtsocket *sock, int request, void *arg);
+
+#endif  /* __RTNET_IP_SOCK_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/protocol.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/protocol.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/protocol.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/protocol.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,64 @@
+/***
+ *
+ *  include/ipv4/protocol.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_PROTOCOL_H_
+#define __RTNET_PROTOCOL_H_
+
+#include <rtnet_socket.h>
+#include <rtskb.h>
+
+
+#define MAX_RT_INET_PROTOCOLS   32
+
+/***
+ * transport layer protocol
+ */
+struct rtinet_protocol {
+    char                *name;
+    unsigned short      protocol;
+
+    struct rtsocket     *(*dest_socket)(struct rtskb *);
+    void                (*rcv_handler)(struct rtskb *);
+    void                (*err_handler)(struct rtskb *);
+    int                 (*init_socket)(struct rtsocket *);
+};
+
+
+extern struct rtinet_protocol *rt_inet_protocols[];
+
+#define rt_inet_hashkey(id)  (id & (MAX_RT_INET_PROTOCOLS-1))
+extern void rt_inet_add_protocol(struct rtinet_protocol *prot);
+extern void rt_inet_del_protocol(struct rtinet_protocol *prot);
+extern int rt_inet_socket(struct rtsocket *sock, int type, int protocol);
+
+extern int rt_udp_bind(struct rtsocket *sock,
+                const struct sockaddr __user *addr, socklen_t addrlen);
+extern int rt_udp_ioctl(struct rtsocket *sock, unsigned int request, void __user *arg);
+extern ssize_t rt_udp_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg, int msg_flags, int msg_in_userspace);
+extern ssize_t rt_udp_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg, int msg_flags, int msg_in_userspace);
+extern void rt_udp_close(struct rtsocket *sock);
+/* TODO: add rt_socket_select_bind() */
+
+#endif  /* __RTNET_PROTOCOL_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/route.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/route.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/route.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/route.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,62 @@
+/***
+ *
+ *  include/ipv4/route.h - real-time routing
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  Rewritten version of the original route by David Schleef and Ulrich Marx
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_ROUTE_H_
+#define __RTNET_ROUTE_H_
+
+#include <linux/init.h>
+#include <linux/types.h>
+
+#include <rtdev.h>
+
+
+struct dest_route {
+    u32                 ip;
+    unsigned char       dev_addr[MAX_ADDR_LEN];
+    struct rtnet_device *rtdev;
+};
+
+
+int rt_ip_route_add_host(u32 addr, unsigned char *dev_addr,
+                         struct rtnet_device *rtdev);
+void rt_ip_route_del_all(struct rtnet_device *rtdev);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+int rt_ip_route_add_net(u32 addr, u32 mask, u32 gw_addr);
+int rt_ip_route_del_net(u32 addr, u32 mask);
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER
+int rt_ip_route_forward(struct rtskb *rtskb, u32 daddr);
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER */
+
+int rt_ip_route_del_host(u32 addr, struct rtnet_device *rtdev);
+int rt_ip_route_get_host(u32 addr, char* if_name, unsigned char *dev_addr,
+                         struct rtnet_device *rtdev);
+int rt_ip_route_output(struct dest_route *rt_buf, u32 daddr, u32 saddr);
+
+int __init rt_ip_routing_init(void);
+void rt_ip_routing_release(void);
+
+#endif  /* __RTNET_ROUTE_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/tcp.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/tcp.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/tcp.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/tcp.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,50 @@
+/***
+ *
+ *  include/ipv4/tcp.h
+ *
+ *  Copyright (C) 2009 Vladimir Zapolskiy <vladimir.zapolskiy@siemens.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License, version 2, as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+ 
+#ifndef __RTNET_TCP_H_
+#define __RTNET_TCP_H_
+
+#include <rtskb.h>
+#include <ipv4/protocol.h>
+
+/* Maximum number of active tcp sockets, must be power of 2 */
+#define RT_TCP_SOCKETS      32
+
+/*Maximum number of active tcp connections, must be power of 2 */
+#define RT_TCP_CONNECTIONS  64
+
+/* Maximum size of TCP input window */
+#define RT_TCP_WINDOW       4096
+
+/* Maximum number of retransmissions of invalid segments */
+#define RT_TCP_RETRANSMIT   3
+
+/* Number of milliseconds to wait for ACK */
+#define RT_TCP_WAIT_TIME    10
+
+/* Priority of RST|ACK replies (error condition => non-RT prio) */
+#define RT_TCP_RST_PRIO     RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, \
+                                             RTSKB_DEF_NRT_CHANNEL)
+
+/* rtskb pool for sending socket-less RST|ACK */
+#define RT_TCP_RST_POOL_SIZE 8
+
+#endif  /* __RTNET_TCP_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4/udp.h linux-5.4.5-new/net/rtnet/stack/include/ipv4/udp.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4/udp.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4/udp.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,33 @@
+/***
+ *
+ *  include/ipv4/udp.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_UDP_H_
+#define __RTNET_UDP_H_
+
+/* Maximum number of active udp sockets
+   Only increase with care (look-up delays!), must be power of 2 */
+#define RT_UDP_SOCKETS      64
+
+#endif  /* __RTNET_UDP_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/ipv4_chrdev.h linux-5.4.5-new/net/rtnet/stack/include/ipv4_chrdev.h
--- linux-5.4.5/net/rtnet/stack/include/ipv4_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/ipv4_chrdev.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,104 @@
+/***
+ *
+ *  include/ipv4.h
+ *
+ *  Real-Time IP/UDP/ICMP stack
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __IPV4_H_
+#define __RTCFG_H_
+
+#include <rtnet_chrdev.h>
+
+
+struct ipv4_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        /*** rtroute ***/
+        struct {
+            __u32       ip_addr;
+        } solicit;
+
+        struct {
+            __u8        dev_addr[DEV_ADDR_LEN];
+            __u32       ip_addr;
+        } gethost;
+
+        struct {
+            __u8        dev_addr[DEV_ADDR_LEN];
+            __u32       ip_addr;
+        } addhost;
+
+        struct {
+            __u32       ip_addr;
+        } delhost;
+
+        struct {
+            __u32       net_addr;
+            __u32       net_mask;
+            __u32       gw_addr;
+        } addnet;
+
+        struct {
+            __u32       net_addr;
+            __u32       net_mask;
+        } delnet;
+
+        /*** rtping ***/
+        struct {
+            __u32       ip_addr;
+            __u16       id;
+            __u16       sequence;
+            __u32       msg_size;
+            __u32       timeout;
+            __s64       rtt;
+        } ping;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define IOC_RT_HOST_ROUTE_ADD           _IOW(RTNET_IOC_TYPE_IPV4, 0,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_SOLICIT       _IOW(RTNET_IOC_TYPE_IPV4, 1,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_DELETE        _IOW(RTNET_IOC_TYPE_IPV4, 2 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_NET_ROUTE_ADD            _IOW(RTNET_IOC_TYPE_IPV4, 3 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_NET_ROUTE_DELETE         _IOW(RTNET_IOC_TYPE_IPV4, 4 |   \
+                                             RTNET_IOC_NODEV_PARAM,     \
+                                             struct ipv4_cmd)
+#define IOC_RT_PING                     _IOWR(RTNET_IOC_TYPE_IPV4, 5 |  \
+                                              RTNET_IOC_NODEV_PARAM,    \
+                                              struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_DELETE_DEV    _IOW(RTNET_IOC_TYPE_IPV4, 6,    \
+                                             struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_GET           _IOWR(RTNET_IOC_TYPE_IPV4, 7 |  \
+					      RTNET_IOC_NODEV_PARAM,    \
+					      struct ipv4_cmd)
+#define IOC_RT_HOST_ROUTE_GET_DEV       _IOWR(RTNET_IOC_TYPE_IPV4, 8,   \
+					      struct ipv4_cmd)
+
+#endif  /* __IPV4_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/nomac_chrdev.h linux-5.4.5-new/net/rtnet/stack/include/nomac_chrdev.h
--- linux-5.4.5/net/rtnet/stack/include/nomac_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/nomac_chrdev.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,41 @@
+/***
+ *
+ *  include/nomac_chrdev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_CHRDEV_H_
+#define __NOMAC_CHRDEV_H_
+
+#include <rtnet_chrdev.h>
+
+
+struct nomac_config {
+    struct rtnet_ioctl_head head;
+};
+
+
+#define NOMAC_IOC_ATTACH                _IOW(RTNET_IOC_TYPE_RTMAC_NOMAC, 0, \
+                                             struct nomac_config)
+#define NOMAC_IOC_DETACH                _IOW(RTNET_IOC_TYPE_RTMAC_NOMAC, 1, \
+                                             struct nomac_config)
+
+#endif /* __NOMAC_CHRDEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_client_event.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,46 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_client_event.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_CLIENT_EVENT_H_
+#define __RTCFG_CLIENT_EVENT_H_
+
+#include <rtcfg_chrdev.h>
+
+
+int rtcfg_main_state_client_0(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_1(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_announced(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data);
+int rtcfg_main_state_client_all_known(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data);
+int rtcfg_main_state_client_all_frames(int ifindex, RTCFG_EVENT event_id,
+                                       void* event_data);
+int rtcfg_main_state_client_2(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data);
+int rtcfg_main_state_client_ready(int ifindex, RTCFG_EVENT event_id,
+                                  void* event_data);
+
+#endif /* __RTCFG_CLIENT_EVENT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_conn_event.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,71 @@
+/***
+ *
+ *	include/rtcfg/rtcfg_conn_event.h
+ *
+ *	Real-Time Configuration Distribution Protocol
+ *
+ *	Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License as published by
+ *	the Free Software Foundation; either version 2 of the License, or
+ *	(at your option) any later version.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	You should have received a copy of the GNU General Public License
+ *	along with this program; if not, write to the Free Software
+ *	Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_CONN_EVENT_H_
+#define __RTCFG_CONN_EVENT_H_
+
+#include <linux/netdevice.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtcfg/rtcfg_file.h>
+#include <rtnet_internal.h>
+
+
+typedef enum {
+	RTCFG_CONN_SEARCHING,
+	RTCFG_CONN_STAGE_1,
+	RTCFG_CONN_STAGE_2,
+	RTCFG_CONN_READY,
+	RTCFG_CONN_DEAD
+} RTCFG_CONN_STATE;
+
+struct rtcfg_connection {
+	struct list_head		entry;
+	int						ifindex;
+	RTCFG_CONN_STATE		state;
+	u8						mac_addr[MAX_ADDR_LEN];
+	unsigned int			addr_type;
+	union {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+		u32					ip_addr;
+#endif
+	} addr;
+	void					*stage1_data;
+	size_t					stage1_size;
+	struct rtcfg_file		*stage2_file;
+	u32						cfg_offs;
+	unsigned int			flags;
+	unsigned int			burstrate;
+	nanosecs_abs_t			last_frame;
+	u64						cfg_timeout;
+#ifdef CONFIG_XENO_OPT_VFILE
+	struct xnvfile_regular	proc_entry;
+#endif
+};
+
+
+int rtcfg_do_conn_event(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+						void* event_data);
+
+#endif /* __RTCFG_CONN_EVENT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_event.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_event.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_event.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_event.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,124 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_event.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_EVENT_H_
+#define __RTCFG_EVENT_H_
+
+#include <linux/if_ether.h>
+#include <linux/netdevice.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_rtpc.h>
+
+
+#define FLAG_TIMER_STARTED          16
+#define FLAG_TIMER_SHUTDOWN         17
+#define FLAG_TIMER_PENDING          18
+
+#define _FLAG_TIMER_STARTED         (1 << FLAG_TIMER_STARTED)
+#define _FLAG_TIMER_SHUTDOWN        (1 << FLAG_TIMER_SHUTDOWN)
+#define _FLAG_TIMER_PENDING         (1 << FLAG_TIMER_PENDING)
+
+typedef enum {
+    RTCFG_MAIN_OFF,
+    RTCFG_MAIN_SERVER_RUNNING,
+    RTCFG_MAIN_CLIENT_0,
+    RTCFG_MAIN_CLIENT_1,
+    RTCFG_MAIN_CLIENT_ANNOUNCED,
+    RTCFG_MAIN_CLIENT_ALL_KNOWN,
+    RTCFG_MAIN_CLIENT_ALL_FRAMES,
+    RTCFG_MAIN_CLIENT_2,
+    RTCFG_MAIN_CLIENT_READY
+} RTCFG_MAIN_STATE;
+
+struct rtcfg_station {
+    u8 mac_addr[ETH_ALEN]; /* Ethernet-specific! */
+    u8 flags;
+};
+
+struct rtcfg_device {
+    RTCFG_MAIN_STATE                state;
+    u32                             other_stations;
+    u32                             stations_found;
+    u32                             stations_ready;
+    struct rt_mutex                    dev_mutex;
+    struct list_head                event_calls;
+    raw_spinlock_t                     event_calls_lock;
+    rtdm_timer_t                    timer;
+    unsigned long                   flags;
+    unsigned int                    burstrate;
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct xnvfile_directory        proc_entry;
+    struct xnvfile_regular          proc_state_vfile;
+    struct xnvfile_regular	    proc_stations_vfile;
+#endif
+
+    union {
+	struct {
+	    unsigned int            addr_type;
+	    union {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+		u32                 ip_addr;
+#endif
+	    } srv_addr;
+	    u8                      srv_mac_addr[MAX_ADDR_LEN];
+	    u8                      *stage2_buffer;
+	    u32                     cfg_len;
+	    u32                     cfg_offs;
+	    unsigned int            packet_counter;
+	    u32                     chain_len;
+	    struct rtskb            *stage2_chain;
+	    u32                     max_stations;
+	    struct rtcfg_station    *station_addr_list;
+	} clt;
+
+	struct {
+	    u32                     clients_configured;
+	    struct list_head        conn_list;
+	    u16                     heartbeat;
+	    u64                     heartbeat_timeout;
+	} srv;
+    } spec;
+};
+
+
+extern struct rtcfg_device device[MAX_RT_DEVICES];
+extern const char *rtcfg_event[];
+extern const char *rtcfg_main_state[];
+
+
+int rtcfg_do_main_event(int ifindex, RTCFG_EVENT event_id, void* event_data);
+void rtcfg_next_main_state(int ifindex, RTCFG_MAIN_STATE state);
+
+void rtcfg_queue_blocking_call(int ifindex, struct rt_proc_call *call);
+struct rt_proc_call *rtcfg_dequeue_blocking_call(int ifindex);
+void rtcfg_complete_cmd(int ifindex, RTCFG_EVENT event_id, int result);
+void rtcfg_reset_device(int ifindex);
+
+void rtcfg_init_state_machines(void);
+void rtcfg_cleanup_state_machines(void);
+
+#endif /* __RTCFG_EVENT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_file.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_file.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_file.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_file.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,45 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_file.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_FILE_H_
+#define __RTCFG_FILE_H_
+
+#include <linux/list.h>
+#include <linux/types.h>
+
+
+struct rtcfg_file {
+    struct list_head entry;
+    int              ref_count;
+    const char*      name;
+    size_t           size;
+    void*            buffer;
+};
+
+
+struct rtcfg_file *rtcfg_get_file(const char *filename);
+void rtcfg_add_file(struct rtcfg_file *file);
+int rtcfg_release_file(struct rtcfg_file *file);
+
+#endif /* __RTCFG_FILE_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_frame.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_frame.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_frame.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_frame.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,141 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_frame.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_FRAME_H_
+#define __RTCFG_FRAME_H_
+
+#include <linux/init.h>
+#include <linux/if_packet.h>
+#include <asm/byteorder.h>
+
+#include <rtcfg/rtcfg_event.h>
+
+
+#define ETH_RTCFG                   0x9022
+
+#define RTCFG_SKB_PRIO \
+    RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO-1, RTSKB_DEF_NRT_CHANNEL)
+
+#define RTCFG_ID_STAGE_1_CFG        0
+#define RTCFG_ID_ANNOUNCE_NEW       1
+#define RTCFG_ID_ANNOUNCE_REPLY     2
+#define RTCFG_ID_STAGE_2_CFG        3
+#define RTCFG_ID_STAGE_2_CFG_FRAG   4
+#define RTCFG_ID_ACK_CFG            5
+#define RTCFG_ID_READY              6
+#define RTCFG_ID_HEARTBEAT          7
+#define RTCFG_ID_DEAD_STATION       8
+
+#define RTCFG_ADDRSIZE_MAC          0
+#define RTCFG_ADDRSIZE_IP           4
+#define RTCFG_MAX_ADDRSIZE          RTCFG_ADDRSIZE_IP
+
+#define RTCFG_FLAG_STAGE_2_DATA 0
+#define RTCFG_FLAG_READY        1
+
+#define _RTCFG_FLAG_STAGE_2_DATA (1 << RTCFG_FLAG_STAGE_2_DATA)
+#define _RTCFG_FLAG_READY        (1 << RTCFG_FLAG_READY)
+
+struct rtcfg_frm_head {
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+    u8 id:5;
+    u8 version:3;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+    u8 version:3;
+    u8 id:5;
+#else
+    #error unsupported byte order
+#endif
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_1_cfg {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    client_addr[0];
+    u8                    server_addr[0];
+    u8                    burstrate;
+    u16                   cfg_len;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_announce {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    addr[0];
+    u8                    flags;
+    u8                    burstrate;
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_2_cfg {
+    struct rtcfg_frm_head head;
+    u8                    flags;
+    u32                   stations;
+    u16                   heartbeat_period;
+    u32                   cfg_len;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_stage_2_cfg_frag {
+    struct rtcfg_frm_head head;
+    u32                   frag_offs;
+    u8                    cfg_data[0];
+} __attribute__((packed));
+
+struct rtcfg_frm_ack_cfg {
+    struct rtcfg_frm_head head;
+    u32                   ack_len;
+} __attribute__((packed));
+
+struct rtcfg_frm_simple {
+    struct rtcfg_frm_head head;
+} __attribute__((packed));
+
+struct rtcfg_frm_dead_station {
+    struct rtcfg_frm_head head;
+    u8                    addr_type;
+    u8                    logical_addr[0];
+    u8                    physical_addr[32];
+} __attribute__((packed));
+
+
+int rtcfg_send_stage_1(struct rtcfg_connection *conn);
+int rtcfg_send_stage_2(struct rtcfg_connection *conn, int send_data);
+int rtcfg_send_stage_2_frag(struct rtcfg_connection *conn);
+int rtcfg_send_announce_new(int ifindex);
+int rtcfg_send_announce_reply(int ifindex, u8 *dest_mac_addr);
+int rtcfg_send_ack(int ifindex);
+int rtcfg_send_dead_station(struct rtcfg_connection *conn);
+
+int rtcfg_send_simple_frame(int ifindex, int frame_id, u8 *dest_addr);
+
+#define rtcfg_send_ready(ifindex)                                   \
+    rtcfg_send_simple_frame(ifindex, RTCFG_ID_READY, NULL)
+#define rtcfg_send_heartbeat(ifindex)                               \
+    rtcfg_send_simple_frame(ifindex, RTCFG_ID_HEARTBEAT,            \
+                            device[ifindex].spec.clt.srv_mac_addr)
+
+int __init rtcfg_init_frames(void);
+void rtcfg_cleanup_frames(void);
+
+#endif /* __RTCFG_FRAME_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,49 @@
+/***
+ *
+ *  include/rtcfg/rtcfg.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_H_INTERNAL_
+#define __RTCFG_H_INTERNAL_
+
+#include <rtdm/driver.h>
+
+
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+
+
+/***
+ * RTcfg debugging
+ */
+#ifdef CONFIG_XENO_DRIVERS_NET_RTCFG_DEBUG
+
+extern int rtcfg_debug;
+
+/* use 0 for production, 1 for verification, >2 for debug */
+#define RTCFG_DEFAULT_DEBUG_LEVEL    10
+
+#define RTCFG_DEBUG(n, args...) (rtcfg_debug >= (n)) ? (rtdm_printk(args)) : 0
+#else
+#define RTCFG_DEBUG(n, args...)
+#endif /* CONFIG_RTCFG_DEBUG */
+
+#endif /* __RTCFG_H_INTERNAL_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_ioctl.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,34 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_ioctl.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_IOCTL_H_
+#define __RTCFG_IOCTL_H_
+
+
+extern struct rtnet_ioctls rtcfg_ioctls;
+
+#define rtcfg_init_ioctls()     rtnet_register_ioctls(&rtcfg_ioctls)
+#define rtcfg_cleanup_ioctls()  rtnet_unregister_ioctls(&rtcfg_ioctls)
+
+#endif /* __RTCFG_IOCTL_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_proc.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_proc.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_proc.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_proc.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,61 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_proc.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_PROC_H_
+#define __RTCFG_PROC_H_
+
+#include <rtnet_internal.h>
+
+#ifdef CONFIG_XENO_OPT_VFILE
+
+extern struct mutex nrt_proc_lock;
+
+
+void rtcfg_update_conn_proc_entries(int ifindex);
+void rtcfg_remove_conn_proc_entries(int ifindex);
+
+int rtcfg_init_proc(void);
+void rtcfg_cleanup_proc(void);
+
+
+static inline void rtcfg_lockwr_proc(int ifindex)
+{
+    mutex_lock(&nrt_proc_lock);
+    rtcfg_remove_conn_proc_entries(ifindex);
+}
+
+static inline void rtcfg_unlockwr_proc(int ifindex)
+{
+    rtcfg_update_conn_proc_entries(ifindex);
+    mutex_unlock(&nrt_proc_lock);
+}
+
+#else
+
+#define rtcfg_lockwr_proc(x)    do {} while (0)
+#define rtcfg_unlockwr_proc(x)  do {} while (0)
+
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+#endif /* __RTCFG_PROC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_timer.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_timer.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg/rtcfg_timer.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg/rtcfg_timer.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,34 @@
+/***
+ *
+ *  include/rtcfg/rtcfg_timer.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_TIMER_H_
+#define __RTCFG_TIMER_H_
+
+void rtcfg_timer(rtdm_timer_t *t);
+
+void rtcfg_timer_run(void);
+
+void rtcfg_thread_signal(void);
+
+#endif /* __RTCFG_TIMER_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtcfg_chrdev.h linux-5.4.5-new/net/rtnet/stack/include/rtcfg_chrdev.h
--- linux-5.4.5/net/rtnet/stack/include/rtcfg_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtcfg_chrdev.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,179 @@
+/***
+ *
+ *  include/rtcfg.h
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTCFG_H_
+#define __RTCFG_H_
+
+#include <rtnet_chrdev.h>
+
+
+#define ERTCFG_START            0x0F00
+#define ESTAGE1SIZE             ERTCFG_START
+
+#define FLAG_STAGE_2_DATA       0x0001
+#define FLAG_READY              0x0002
+#define FLAG_ASSIGN_ADDR_BY_MAC 0x0100
+
+#define RTCFG_ADDR_MAC          0x00
+#define RTCFG_ADDR_IP           0x01
+#define RTCFG_ADDR_MASK         0xFF
+
+
+typedef enum {
+    RTCFG_CMD_SERVER,
+    RTCFG_CMD_ADD,
+    RTCFG_CMD_DEL,
+    RTCFG_CMD_WAIT,
+    RTCFG_CMD_CLIENT,
+    RTCFG_CMD_ANNOUNCE,
+    RTCFG_CMD_READY,
+    RTCFG_CMD_DETACH,
+
+    /* internal usage only */
+    RTCFG_TIMER,
+    RTCFG_FRM_STAGE_1_CFG,
+    RTCFG_FRM_ANNOUNCE_NEW,
+    RTCFG_FRM_ANNOUNCE_REPLY,
+    RTCFG_FRM_STAGE_2_CFG,
+    RTCFG_FRM_STAGE_2_CFG_FRAG,
+    RTCFG_FRM_ACK_CFG,
+    RTCFG_FRM_READY,
+    RTCFG_FRM_HEARTBEAT,
+    RTCFG_FRM_DEAD_STATION
+} RTCFG_EVENT;
+
+struct rtskb;
+struct rtcfg_station;
+struct rtcfg_connection;
+struct rtcfg_file;
+
+struct rtcfg_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        struct {
+            __u32                   period;
+            __u32                   burstrate;
+            __u32                   heartbeat;
+            __u32                   threshold;
+            __u32                   flags;
+        } server;
+
+        struct {
+            __u32                   addr_type;
+            __u32                   ip_addr;
+            __u8                    mac_addr[DEV_ADDR_LEN];
+            __u32                   timeout;
+            __u16                   stage1_size;
+            __u16                   __padding;
+            void                    *stage1_data;
+            const char              *stage2_filename;
+
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+        } add;
+
+        struct {
+            __u32                   addr_type;
+            __u32                   ip_addr;
+            __u8                    mac_addr[DEV_ADDR_LEN];
+
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+        } del;
+
+        struct {
+            __u32                   timeout;
+        } wait;
+
+        struct {
+            __u32                   timeout;
+            __u32                   max_stations;
+            __u64                   buffer_size;
+            void                    *buffer;
+
+            /* internal usage only */
+            struct rtcfg_station    *station_buf;
+            struct rtskb            *rtskb;
+        } client;
+
+        struct {
+            __u32                   timeout;
+            __u32                   flags;
+            __u32                   burstrate;
+            __u32                   __padding;
+            __u64                   buffer_size;
+            void                    *buffer;
+
+            /* internal usage only */
+            struct rtskb            *rtskb;
+        } announce;
+
+        struct {
+            __u32                   timeout;
+        } ready;
+
+        struct {
+            /* internal usage only */
+            struct rtcfg_connection *conn_buf;
+            struct rtcfg_file       *stage2_file;
+            struct rtcfg_station    *station_addr_list;
+            struct rtskb            *stage2_chain;
+        } detach;
+
+        __u64 __padding[16];
+    } args;
+
+    /* internal usage only */
+    union {
+        struct {
+            int         ifindex;
+            RTCFG_EVENT event_id;
+        } data;
+
+        __u64 __padding[2];
+    } internal;
+};
+
+
+#define RTCFG_IOC_SERVER        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_SERVER,  \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_ADD           _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_ADD,     \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_DEL           _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_DEL,     \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_WAIT          _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_WAIT,    \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_CLIENT        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_CLIENT,  \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_ANNOUNCE      _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_ANNOUNCE,\
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_READY         _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_READY,   \
+                                     struct rtcfg_cmd)
+#define RTCFG_IOC_DETACH        _IOW(RTNET_IOC_TYPE_RTCFG, RTCFG_CMD_DETACH,  \
+                                     struct rtcfg_cmd)
+
+#endif /* __RTCFG_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtdev.h linux-5.4.5-new/net/rtnet/stack/include/rtdev.h
--- linux-5.4.5/net/rtnet/stack/include/rtdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtdev.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,270 @@
+/***
+ *
+ *  rtdev.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTDEV_H_
+#define __RTDEV_H_
+
+#define MAX_RT_DEVICES                  8
+
+
+#ifdef __KERNEL__
+
+#include <asm/atomic.h>
+#include <linux/netdevice.h>
+
+#include <rtskb.h>
+#include <rtnet_internal.h>
+
+#define RTDEV_VERS_2_0                  0x0200
+
+#define PRIV_FLAG_UP                    0
+#define PRIV_FLAG_ADDING_ROUTE          1
+
+#ifndef NETIF_F_LLTX
+#define NETIF_F_LLTX                    4096
+#endif
+
+#define RTDEV_TX_OK		0
+#define RTDEV_TX_BUSY	1
+
+enum rtnet_link_state {
+	__RTNET_LINK_STATE_XOFF = 0,
+	__RTNET_LINK_STATE_START,
+	__RTNET_LINK_STATE_PRESENT,
+	__RTNET_LINK_STATE_NOCARRIER,
+};
+#define RTNET_LINK_STATE_XOFF (1 << __RTNET_LINK_STATE_XOFF)
+#define RTNET_LINK_STATE_START (1 << __RTNET_LINK_STATE_START)
+#define RTNET_LINK_STATE_PRESENT (1 << __RTNET_LINK_STATE_PRESENT)
+#define RTNET_LINK_STATE_NOCARRIER (1 << __RTNET_LINK_STATE_NOCARRIER)
+
+/***
+ *  rtnet_device
+ */
+struct rtnet_device {
+    /* Many field are borrowed from struct net_device in
+     * <linux/netdevice.h> - WY
+     */
+    unsigned int        vers;
+
+    char                name[IFNAMSIZ];
+
+    unsigned long       rmem_end;   /* shmem "recv" end     */
+    unsigned long       rmem_start; /* shmem "recv" start   */
+    unsigned long       mem_end;    /* shared mem end       */
+    unsigned long       mem_start;  /* shared mem start     */
+    unsigned long       base_addr;  /* device I/O address   */
+    unsigned int        irq;        /* device IRQ number    */
+
+    /*
+     *  Some hardware also needs these fields, but they are not
+     *  part of the usual set specified in Space.c.
+     */
+    unsigned char       if_port;    /* Selectable AUI, TP,..*/
+    unsigned char       dma;        /* DMA channel          */
+    __u16               __padding;
+
+    unsigned long       link_state;
+    int                 ifindex;
+    atomic_t            refcount;
+
+    struct module       *rt_owner;  /* like classic owner, but      *
+				     * forces correct macro usage   */
+
+    unsigned int        flags;      /* interface flags (a la BSD)   */
+    unsigned long       priv_flags; /* internal flags               */
+    unsigned short      type;       /* interface hardware type      */
+    unsigned short      hard_header_len;    /* hardware hdr length  */
+    unsigned int        mtu;        /* eth = 1536, tr = 4...        */
+    void                *priv;      /* pointer to private data      */
+    netdev_features_t   features;   /* [RT]NETIF_F_*                */
+
+    /* Interface address info. */
+    unsigned char       broadcast[MAX_ADDR_LEN];    /* hw bcast add */
+    unsigned char       dev_addr[MAX_ADDR_LEN];     /* hw address   */
+    unsigned char       addr_len;   /* hardware address length      */
+
+    int                 promiscuity;
+    int                 allmulti;
+
+    __u32               local_ip;   /* IP address in network order  */
+    __u32               broadcast_ip; /* broadcast IP in network order */
+
+    rtdm_event_t        *stack_event;
+
+    struct rt_mutex        xmit_mutex; /* protects xmit routine        */
+    raw_spinlock_t         rtdev_lock; /* management lock              */
+    struct mutex        nrt_lock;   /* non-real-time locking        */
+
+    unsigned int        add_rtskbs; /* additionally allocated global rtskbs */
+
+    struct rtskb_pool   dev_pool;
+
+    /* RTmac related fields */
+    struct rtmac_disc   *mac_disc;
+    struct rtmac_priv   *mac_priv;
+    int                 (*mac_detach)(struct rtnet_device *rtdev);
+
+    /* Device operations */
+    int                 (*open)(struct rtnet_device *rtdev);
+    int                 (*stop)(struct rtnet_device *rtdev);
+    int                 (*hard_header)(struct rtskb *, struct rtnet_device *,
+				       unsigned short type, void *daddr,
+				       void *saddr, unsigned int len);
+    int                 (*rebuild_header)(struct rtskb *);
+    int                 (*hard_start_xmit)(struct rtskb *skb,
+					   struct rtnet_device *dev);
+    int                 (*hw_reset)(struct rtnet_device *rtdev);
+
+    /* Transmission hook, managed by the stack core, RTcap, and RTmac
+     *
+     * If xmit_lock is used, start_xmit points either to rtdev_locked_xmit or
+     * the RTmac discipline handler. If xmit_lock is not required, start_xmit
+     * points to hard_start_xmit or the discipline handler.
+     */
+    int                 (*start_xmit)(struct rtskb *skb,
+				      struct rtnet_device *dev);
+
+    /* MTU hook, managed by the stack core and RTmac */
+    unsigned int        (*get_mtu)(struct rtnet_device *rtdev,
+				   unsigned int priority);
+
+    int                 (*do_ioctl)(struct rtnet_device *rtdev,
+				    struct ifreq *ifr, int cmd);
+    struct net_device_stats *(*get_stats)(struct rtnet_device *rtdev);
+
+    /* DMA pre-mapping hooks */
+    dma_addr_t          (*map_rtskb)(struct rtnet_device *rtdev,
+				     struct rtskb *skb);
+    void                (*unmap_rtskb)(struct rtnet_device *rtdev,
+				       struct rtskb *skb);
+};
+
+struct rtnet_core_cmd;
+
+struct rtdev_event_hook {
+    struct list_head    entry;
+    void                (*register_device)(struct rtnet_device *rtdev);
+    void                (*unregister_device)(struct rtnet_device *rtdev);
+    void                (*ifup)(struct rtnet_device *rtdev,
+				struct rtnet_core_cmd *up_cmd);
+    void                (*ifdown)(struct rtnet_device *rtdev);
+};
+
+extern struct list_head event_hook_list;
+extern struct mutex rtnet_devices_nrt_lock;
+extern struct rtnet_device *rtnet_devices[];
+
+
+struct rtnet_device *__rt_alloc_etherdev(unsigned sizeof_priv,
+					unsigned dev_pool_size,
+					struct module *module);
+#define rt_alloc_etherdev(priv_size, rx_size) \
+    __rt_alloc_etherdev(priv_size, rx_size, THIS_MODULE)
+
+void rtdev_free(struct rtnet_device *rtdev);
+
+int rt_register_rtnetdev(struct rtnet_device *rtdev);
+int rt_unregister_rtnetdev(struct rtnet_device *rtdev);
+
+void rtdev_add_event_hook(struct rtdev_event_hook *hook);
+void rtdev_del_event_hook(struct rtdev_event_hook *hook);
+
+void rtdev_alloc_name (struct rtnet_device *rtdev, const char *name_mask);
+
+/**
+ *  __rtdev_get_by_index - find a rtnet_device by its ifindex
+ *  @ifindex: index of device
+ *  @note: caller must hold rtnet_devices_nrt_lock
+ */
+static inline struct rtnet_device *__rtdev_get_by_index(int ifindex)
+{
+    return rtnet_devices[ifindex-1];
+}
+
+struct rtnet_device *rtdev_get_by_name(const char *if_name);
+struct rtnet_device *rtdev_get_by_index(int ifindex);
+struct rtnet_device *rtdev_get_by_hwaddr(unsigned short type,char *ha);
+struct rtnet_device *rtdev_get_loopback(void);
+
+int rtdev_reference(struct rtnet_device *rtdev);
+
+static inline void rtdev_dereference(struct rtnet_device *rtdev)
+{
+    smp_mb__before_atomic();
+    if (rtdev->rt_owner && atomic_dec_and_test(&rtdev->refcount))
+	module_put(rtdev->rt_owner);
+}
+
+int rtdev_xmit(struct rtskb *skb);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+int rtdev_xmit_proxy(struct rtskb *skb);
+#endif
+
+unsigned int rt_hard_mtu(struct rtnet_device *rtdev, unsigned int priority);
+
+int rtdev_open(struct rtnet_device *rtdev);
+int rtdev_close(struct rtnet_device *rtdev);
+
+int rtdev_map_rtskb(struct rtskb *skb);
+void rtdev_unmap_rtskb(struct rtskb *skb);
+
+struct rtskb *rtnetdev_alloc_rtskb(struct rtnet_device *dev, unsigned int size);
+
+#define rtnetdev_priv(dev) ((dev)->priv)
+
+#define rtdev_emerg(__dev, format, args...) \
+	pr_emerg("%s: " format, (__dev)->name, ##args)
+#define rtdev_alert(__dev, format, args...) \
+	pr_alert("%s: " format, (__dev)->name, ##args)
+#define rtdev_crit(__dev, format, args...) \
+	pr_crit("%s: " format, (__dev)->name, ##args)
+#define rtdev_err(__dev, format, args...) \
+	pr_err("%s: " format, (__dev)->name, ##args)
+#define rtdev_warn(__dev, format, args...) \
+	pr_warn("%s: " format, (__dev)->name, ##args)
+#define rtdev_notice(__dev, format, args...) \
+	pr_notice("%s: " format, (__dev)->name, ##args)
+#define rtdev_info(__dev, format, args...) \
+	pr_info("%s: " format, (__dev)->name, ##args)
+#define rtdev_dbg(__dev, format, args...) \
+	pr_debug("%s: " format, (__dev)->name, ##args)
+
+#ifdef VERBOSE_DEBUG
+#define rtdev_vdbg rtdev_dbg
+#else
+#define rtdev_vdbg(__dev, format, args...)			\
+({								\
+	if (0)							\
+		pr_debug("%s: " format, (__dev)->name, ##args);	\
+								\
+	0;							\
+})
+#endif
+
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTDEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtdev_mgr.h linux-5.4.5-new/net/rtnet/stack/include/rtdev_mgr.h
--- linux-5.4.5/net/rtnet/stack/include/rtdev_mgr.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtdev_mgr.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,41 @@
+/* rtdev_mgr.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTDEV_MGR_H_
+#define __RTDEV_MGR_H_
+
+#ifdef __KERNEL__
+
+#include <rtnet_internal.h>
+
+
+extern void rtnetif_err_rx(struct rtnet_device *rtdev);
+extern void rtnetif_err_tx(struct rtnet_device *rtdev);
+
+extern void rt_rtdev_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr);
+extern void rt_rtdev_disconnect (struct rtnet_device *rtdev);
+extern int rt_rtdev_mgr_init (struct rtnet_mgr *mgr);
+extern void rt_rtdev_mgr_delete (struct rtnet_mgr *mgr);
+extern int rt_rtdev_mgr_start (struct rtnet_mgr *mgr);
+extern int rt_rtdev_mgr_stop (struct rtnet_mgr *mgr);
+
+
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTDEV_MGR_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtdm_net.h linux-5.4.5-new/net/rtnet/stack/include/rtdm_net.h
--- linux-5.4.5/net/rtnet/stack/include/rtdm_net.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtdm_net.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,47 @@
+/*
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2005-2011 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#ifndef _COBALT_RTDM_NET_H
+#define _COBALT_RTDM_NET_H
+
+#include <rtdm_uapi_net.h>
+
+struct rtnet_callback {
+    void    (*func)(void *, void *);
+    void    *arg;
+};
+
+#define RTNET_RTIOC_CALLBACK    _IOW(RTIOC_TYPE_NETWORK, 0x12, \
+				     struct rtnet_callback)
+
+/* utility functions */
+
+/* provided by rt_ipv4 */
+unsigned long rt_inet_aton(const char *ip);
+
+/* provided by rt_packet */
+int rt_eth_aton(unsigned char *addr_buf, const char *mac);
+
+#define RTNET_RTDM_VER 914
+
+#endif  /* _COBALT_RTDM_NET_H */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtdm_uapi_net.h linux-5.4.5-new/net/rtnet/stack/include/rtdm_uapi_net.h
--- linux-5.4.5/net/rtnet/stack/include/rtdm_uapi_net.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtdm_uapi_net.h	2020-06-15 16:12:31.579695194 +0300
@@ -0,0 +1,82 @@
+/***
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2005-2011 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *  As a special exception to the GNU General Public license, the RTnet
+ *  project allows you to use this header file in unmodified form to produce
+ *  application programs executing in user-space which use RTnet services by
+ *  normal system calls. The resulting executable will not be covered by the
+ *  GNU General Public License merely as a result of this header file use.
+ *  Instead, this header file use will be considered normal use of RTnet and
+ *  not a "derived work" in the sense of the GNU General Public License.
+ *
+ *  This exception does not apply when the application code is built as a
+ *  static or dynamically loadable portion of the Linux kernel nor does the
+ *  exception override other reasons justifying application of the GNU General
+ *  Public License.
+ *
+ *  This exception applies only to the code released by the RTnet project
+ *  under the name RTnet and bearing this exception notice. If you copy code
+ *  from other sources into a copy of RTnet, the exception does not apply to
+ *  the code that you add in this way.
+ *
+ */
+
+#ifndef _RTDM_UAPI_NET_H
+#define _RTDM_UAPI_NET_H
+
+#include <rtnet_rtdm.h>
+#include <rtdm_uapi_rtdm.h>
+
+/* sub-classes: RTDM_CLASS_NETWORK */
+#define RTDM_SUBCLASS_RTNET     0
+
+#define RTIOC_TYPE_NETWORK      RTDM_CLASS_NETWORK
+
+/* RTnet-specific IOCTLs */
+#define RTNET_RTIOC_XMITPARAMS  _IOW(RTIOC_TYPE_NETWORK, 0x10, unsigned int)
+#define RTNET_RTIOC_PRIORITY    RTNET_RTIOC_XMITPARAMS  /* legacy */
+#define RTNET_RTIOC_TIMEOUT     _IOW(RTIOC_TYPE_NETWORK, 0x11, int64_t)
+/* RTNET_RTIOC_CALLBACK         _IOW(RTIOC_TYPE_NETWORK, 0x12, ...
+ * IOCTL only usable inside the kernel. */
+/* RTNET_RTIOC_NONBLOCK         _IOW(RTIOC_TYPE_NETWORK, 0x13, unsigned int)
+ * This IOCTL is no longer supported (and it was buggy anyway).
+ * Use RTNET_RTIOC_TIMEOUT with any negative timeout value instead. */
+#define RTNET_RTIOC_EXTPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x14, unsigned int)
+#define RTNET_RTIOC_SHRPOOL     _IOW(RTIOC_TYPE_NETWORK, 0x15, unsigned int)
+
+/* socket transmission priorities */
+#define SOCK_MAX_PRIO           0
+#define SOCK_DEF_PRIO           SOCK_MAX_PRIO + \
+				    (SOCK_MIN_PRIO-SOCK_MAX_PRIO+1)/2
+#define SOCK_MIN_PRIO           SOCK_NRT_PRIO - 1
+#define SOCK_NRT_PRIO           31
+
+/* socket transmission channels */
+#define SOCK_DEF_RT_CHANNEL     0           /* default rt xmit channel     */
+#define SOCK_DEF_NRT_CHANNEL    1           /* default non-rt xmit channel */
+#define SOCK_USER_CHANNEL       2           /* first user-defined channel  */
+
+/* argument construction for RTNET_RTIOC_XMITPARAMS */
+#define SOCK_XMIT_PARAMS(priority, channel) ((priority) | ((channel) << 16))
+
+#endif  /* !_RTDM_UAPI_NET_H */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtdm_uapi_rtdm.h linux-5.4.5-new/net/rtnet/stack/include/rtdm_uapi_rtdm.h
--- linux-5.4.5/net/rtnet/stack/include/rtdm_uapi_rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtdm_uapi_rtdm.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,206 @@
+/**
+ * @file
+ * Real-Time Driver Model for Xenomai, user API header.
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * @note Copyright (C) 2005, 2006 Jan Kiszka <jan.kiszka@web.de>
+ * @note Copyright (C) 2005 Joerg Langenberg <joerg.langenberg@gmx.net>
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
+ * @ingroup rtdm_user_api
+ */
+#ifndef _RTDM_UAPI_RTDM_H
+#define _RTDM_UAPI_RTDM_H
+
+/*!
+ * @addtogroup rtdm
+ * @{
+ */
+
+/*!
+ * @anchor rtdm_api_versioning @name API Versioning
+ * @{ */
+/** Common user and driver API version */
+#define RTDM_API_VER			9
+
+/** Minimum API revision compatible with the current release */
+#define RTDM_API_MIN_COMPAT_VER		9
+/** @} API Versioning */
+
+/** RTDM type for representing absolute dates. Its base type is a 64 bit
+ *  unsigned integer. The unit is 1 nanosecond. */
+typedef uint64_t nanosecs_abs_t;
+
+/** RTDM type for representing relative intervals. Its base type is a 64 bit
+ *  signed integer. The unit is 1 nanosecond. Relative intervals can also
+ *  encode the special timeouts "infinite" and "non-blocking", see
+ *  @ref RTDM_TIMEOUT_xxx. */
+typedef int64_t nanosecs_rel_t;
+
+/*!
+ * @anchor RTDM_TIMEOUT_xxx @name RTDM_TIMEOUT_xxx
+ * Special timeout values
+ * @{ */
+/** Block forever. */
+#define RTDM_TIMEOUT_INFINITE		0
+
+/** Any negative timeout means non-blocking. */
+#define RTDM_TIMEOUT_NONE		(-1)
+/** @} RTDM_TIMEOUT_xxx */
+/** @} rtdm */
+
+/*!
+ * @addtogroup rtdm_profiles
+ * @{
+ */
+
+/*!
+ * @anchor RTDM_CLASS_xxx   @name RTDM_CLASS_xxx
+ * Device classes
+ * @{ */
+#define RTDM_CLASS_PARPORT		1
+#define RTDM_CLASS_SERIAL		2
+#define RTDM_CLASS_CAN			3
+#define RTDM_CLASS_NETWORK		4
+#define RTDM_CLASS_RTMAC		5
+#define RTDM_CLASS_TESTING		6
+#define RTDM_CLASS_RTIPC		7
+#define RTDM_CLASS_COBALT		8
+#define RTDM_CLASS_UDD			9
+#define RTDM_CLASS_MEMORY		10
+#define RTDM_CLASS_GPIO			11
+#define RTDM_CLASS_SPI			12
+
+#define RTDM_CLASS_MISC			223
+#define RTDM_CLASS_EXPERIMENTAL		224
+#define RTDM_CLASS_MAX			255
+/** @} RTDM_CLASS_xxx */
+
+#define RTDM_SUBCLASS_GENERIC		(-1)
+
+#define RTIOC_TYPE_COMMON		0
+
+/*!
+ * @anchor device_naming    @name Device Naming
+ * Maximum length of device names (excluding the final null character)
+ * @{
+ */
+#define RTDM_MAX_DEVNAME_LEN		31
+/** @} Device Naming */
+
+/**
+ * Device information
+ */
+typedef struct rtdm_device_info {
+	/** Device flags, see @ref dev_flags "Device Flags" for details */
+	int device_flags;
+
+	/** Device class ID, see @ref RTDM_CLASS_xxx */
+	int device_class;
+
+	/** Device sub-class, either RTDM_SUBCLASS_GENERIC or a
+	 *  RTDM_SUBCLASS_xxx definition of the related @ref rtdm_profiles
+	 *  "Device Profile" */
+	int device_sub_class;
+
+	/** Supported device profile version */
+	int profile_version;
+} rtdm_device_info_t;
+
+/*!
+ * @anchor RTDM_PURGE_xxx_BUFFER    @name RTDM_PURGE_xxx_BUFFER
+ * Flags selecting buffers to be purged
+ * @{ */
+#define RTDM_PURGE_RX_BUFFER		0x0001
+#define RTDM_PURGE_TX_BUFFER		0x0002
+/** @} RTDM_PURGE_xxx_BUFFER*/
+
+/*!
+ * @anchor common_IOCTLs    @name Common IOCTLs
+ * The following IOCTLs are common to all device rtdm_profiles.
+ * @{
+ */
+
+/**
+ * Retrieve information about a device or socket.
+ * @param[out] arg Pointer to information buffer (struct rtdm_device_info)
+ */
+#define RTIOC_DEVICE_INFO \
+	_IOR(RTIOC_TYPE_COMMON, 0x00, struct rtdm_device_info)
+
+/**
+ * Purge internal device or socket buffers.
+ * @param[in] arg Purge mask, see @ref RTDM_PURGE_xxx_BUFFER
+ */
+#define RTIOC_PURGE		_IOW(RTIOC_TYPE_COMMON, 0x10, int)
+/** @} Common IOCTLs */
+/** @} rtdm */
+
+/* Internally used for mapping socket functions on IOCTLs */
+struct _rtdm_getsockopt_args {
+	int level;
+	int optname;
+	void *optval;
+	socklen_t *optlen;
+};
+
+struct _rtdm_setsockopt_args {
+	int level;
+	int optname;
+	const void *optval;
+	socklen_t optlen;
+};
+
+struct _rtdm_getsockaddr_args {
+	struct sockaddr *addr;
+	socklen_t *addrlen;
+};
+
+struct _rtdm_setsockaddr_args {
+	const struct sockaddr *addr;
+	socklen_t addrlen;
+};
+
+#define _RTIOC_GETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x20,		\
+				     struct _rtdm_getsockopt_args)
+#define _RTIOC_SETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x21,		\
+				     struct _rtdm_setsockopt_args)
+#define _RTIOC_BIND		_IOW(RTIOC_TYPE_COMMON, 0x22,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_CONNECT		_IOW(RTIOC_TYPE_COMMON, 0x23,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_LISTEN		_IOW(RTIOC_TYPE_COMMON, 0x24,		\
+				     int)
+#define _RTIOC_ACCEPT		_IOW(RTIOC_TYPE_COMMON, 0x25,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETSOCKNAME	_IOW(RTIOC_TYPE_COMMON, 0x26,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETPEERNAME	_IOW(RTIOC_TYPE_COMMON, 0x27,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_SHUTDOWN		_IOW(RTIOC_TYPE_COMMON, 0x28,		\
+				     int)
+
+/* Internally used for mmap() */
+struct _rtdm_mmap_request {
+	__u64 offset;
+	size_t length;
+	int prot;
+	int flags;
+};
+
+#endif /* !_RTDM_UAPI_RTDM_H */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_dev.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_dev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_DEV_H_
+#define __NOMAC_DEV_H_
+
+#include <rtmac/nomac/nomac.h>
+
+
+int nomac_dev_init(struct rtnet_device *rtdev, struct nomac_priv *nomac);
+
+
+static inline void nomac_dev_release(struct nomac_priv *nomac)
+{
+    rtdm_dev_unregister(&nomac->api_device);
+}
+
+#endif /* __NOMAC_DEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,54 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_H_
+#define __NOMAC_H_
+
+#include <rtdm/driver.h>
+
+#include <rtmac/rtmac_disc.h>
+
+
+#define RTMAC_TYPE_NOMAC        0
+
+#define NOMAC_MAGIC             0x004D0A0C
+
+
+struct nomac_priv {
+    unsigned int                magic;
+    struct rtnet_device         *rtdev;
+    char                        device_name[32];
+    struct rtdm_driver          api_driver;
+    struct rtdm_device          api_device;
+    /* ... */
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct list_head            list_entry;
+#endif
+};
+
+
+extern struct rtmac_disc        nomac_disc;
+
+#endif /* __NOMAC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_ioctl.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,32 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_ioctl.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_IOCTL_H_
+#define __NOMAC_IOCTL_H_
+
+
+int nomac_ioctl(struct rtnet_device *rtdev, unsigned int request,
+                unsigned long arg);
+
+#endif /* __NOMAC_IOCTL_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/nomac/nomac_proto.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/nomac/nomac_proto.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __NOMAC_PROTO_H_
+#define __NOMAC_PROTO_H_
+
+#include <rtdev.h>
+
+
+int nomac_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev);
+int nomac_nrt_packet_tx(struct rtskb *rtskb);
+
+int nomac_packet_rx(struct rtskb *rtskb);
+
+int nomac_proto_init(void);
+void nomac_proto_cleanup(void);
+
+#endif /* __NOMAC_PROTO_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_disc.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_disc.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_disc.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_disc.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,105 @@
+/***
+ *
+ *  include/rtmac/rtmac_disc.h
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_DISC_H_
+#define __RTMAC_DISC_H_
+
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/proc_fs.h>
+
+#include <rtdev.h>
+#include <rtnet_chrdev.h>
+
+
+#define RTMAC_NO_VNIC       NULL
+#define RTMAC_DEFAULT_VNIC  rtmac_vnic_xmit
+
+typedef int (*vnic_xmit_handler)(struct sk_buff *skb, struct net_device *dev);
+
+struct rtmac_priv {
+    int (*orig_start_xmit)(struct rtskb *skb, struct rtnet_device *dev);
+    struct net_device       *vnic;
+    struct net_device_stats vnic_stats;
+    struct rtskb_pool       vnic_skb_pool;
+    unsigned int            vnic_max_mtu;
+
+    u8                      disc_priv[0] __attribute__ ((aligned(16)));
+};
+
+struct rtmac_proc_entry {
+    const char *name;
+    //int (*handler)(struct xnvfile_regular_iterator *it, void *data);
+    //struct xnvfile_regular vfile;
+    struct proc_dir_entry *pde;
+};
+
+struct rtmac_disc {
+    struct list_head    list;
+
+    const char          *name;
+    unsigned int        priv_size;      /* size of rtmac_priv.disc_priv */
+    u16                 disc_type;
+
+    int                 (*packet_rx)(struct rtskb *skb);
+    /* rt_packet_tx prototype must be compatible with hard_start_xmit */
+    int                 (*rt_packet_tx)(struct rtskb *skb,
+					struct rtnet_device *dev);
+    int                 (*nrt_packet_tx)(struct rtskb *skb);
+
+    unsigned int        (*get_mtu)(struct rtnet_device *rtdev,
+				   unsigned int priority);
+
+    vnic_xmit_handler   vnic_xmit;
+
+    int                 (*attach)(struct rtnet_device *rtdev, void *disc_priv);
+    int                 (*detach)(struct rtnet_device *rtdev, void *disc_priv);
+
+    struct rtnet_ioctls ioctls;
+
+    struct rtmac_proc_entry *proc_entries;
+    unsigned nr_proc_entries;
+
+    struct module *owner;
+};
+
+
+int rtmac_disc_attach(struct rtnet_device *rtdev, struct rtmac_disc *disc);
+int rtmac_disc_detach(struct rtnet_device *rtdev);
+
+int __rtmac_disc_register(struct rtmac_disc *disc, struct module *module);
+#define rtmac_disc_register(disc) __rtmac_disc_register(disc, THIS_MODULE)
+
+void rtmac_disc_deregister(struct rtmac_disc *disc);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_disciplines_show(struct xnvfile_regular_iterator *it, void *d);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+#endif /* __RTMAC_DISC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_proc.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_proc.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_proc.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_proc.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/rtmac/rtmac_proc.h
+ *
+ *  rtmac - real-time networking medium access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_PROC_H_
+#define __RTMAC_PROC_H_
+
+
+int rtmac_disc_proc_register(struct rtmac_disc *disc);
+void rtmac_disc_proc_unregister(struct rtmac_disc *disc);
+
+int rtmac_proc_register(void);
+void rtmac_proc_release(void);
+
+
+#endif /* __RTMAC_PROC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_proto.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_proto.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_proto.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  include/rtmac/rtmac_proto.h
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTMAC_PROTO_H_
+#define __RTMAC_PROTO_H_
+
+#include <stack_mgr.h>
+
+
+#define RTMAC_VERSION           0x02
+#define ETH_RTMAC               0x9021
+
+#define RTMAC_FLAG_TUNNEL       0x01
+
+
+struct rtmac_hdr {
+    u16 type;
+    u8  ver;
+    u8  flags;
+} __attribute__ ((packed));
+
+
+
+static inline int rtmac_add_header(struct rtnet_device *rtdev, void *daddr,
+                                   struct rtskb *skb, u16 type, u8 flags)
+{
+    struct rtmac_hdr *hdr =
+        (struct rtmac_hdr *)rtskb_push(skb, sizeof(struct rtmac_hdr));
+
+
+    hdr->type  = htons(type);
+    hdr->ver   = RTMAC_VERSION;
+    hdr->flags = flags;
+
+    skb->rtdev = rtdev;
+
+    if (rtdev->hard_header &&
+        (rtdev->hard_header(skb, rtdev, ETH_RTMAC, daddr,
+                            rtdev->dev_addr, skb->len) < 0))
+        return -1;
+
+    return 0;
+}
+
+
+
+static inline int rtmac_xmit(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev = skb->rtdev;
+    int ret;
+
+
+    ret = rtdev->hard_start_xmit(skb, rtdev);
+    if (ret != 0)
+        kfree_rtskb(skb);
+
+    return ret;
+}
+
+
+extern struct rtpacket_type rtmac_packet_type;
+
+#define rtmac_proto_init()  rtdev_add_pack(&rtmac_packet_type)
+void rtmac_proto_release(void);
+
+#endif /* __RTMAC_PROTO_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_vnic.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_vnic.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/rtmac_vnic.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/rtmac_vnic.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,61 @@
+/* include/rtmac/rtmac_vnic.h
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#ifndef __RTMAC_VNIC_H_
+#define __RTMAC_VNIC_H_
+
+#ifdef __KERNEL__
+
+#include <linux/init.h>
+#include <linux/netdevice.h>
+
+#include <rtmac/rtmac_disc.h>
+
+#define DEFAULT_VNIC_RTSKBS     32
+
+
+int rtmac_vnic_rx(struct rtskb *skb, u16 type);
+
+int rtmac_vnic_xmit(struct sk_buff *skb, struct net_device *dev);
+
+void rtmac_vnic_set_max_mtu(struct rtnet_device *rtdev, unsigned int max_mtu);
+
+int rtmac_vnic_add(struct rtnet_device *rtdev, vnic_xmit_handler vnic_xmit);
+int rtmac_vnic_unregister(struct rtnet_device *rtdev);
+
+static inline void rtmac_vnic_cleanup(struct rtnet_device *rtdev)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+
+    rtskb_pool_release(&mac_priv->vnic_skb_pool);
+}
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_vnics_show(struct xnvfile_regular_iterator *it, void *data);
+#endif
+
+int __init rtmac_vnic_module_init(void);
+void rtmac_vnic_module_cleanup(void);
+
+
+#endif /* __KERNEL__ */
+
+#endif /* __RTMAC_VNIC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_dev.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,39 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_dev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_DEV_H_
+#define __TDMA_DEV_H_
+
+#include <rtmac/tdma/tdma.h>
+
+
+int tdma_dev_init(struct rtnet_device *rtdev, struct tdma_priv *tdma);
+
+
+static inline void tdma_dev_release(struct tdma_priv *tdma)
+{
+    rtdm_dev_unregister(&tdma->api_device);
+}
+
+#endif /* __TDMA_DEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,168 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_H_
+#define __TDMA_H_
+
+#include <rtdm/driver.h>
+
+#include <rtnet_rtpc.h>
+#include <rtmac/rtmac_disc.h>
+
+
+#define RTMAC_TYPE_TDMA         0x0001
+
+#define TDMA_MAGIC              0x3A0D4D0A
+
+#define TDMA_FLAG_CALIBRATED    1
+#define TDMA_FLAG_RECEIVED_SYNC 2
+#define TDMA_FLAG_MASTER        3   /* also set for backup masters */
+#define TDMA_FLAG_BACKUP_MASTER 4
+#define TDMA_FLAG_ATTACHED      5
+#define TDMA_FLAG_BACKUP_ACTIVE 6
+
+#define DEFAULT_SLOT            0
+#define DEFAULT_NRT_SLOT        1
+
+/* job IDs */
+#define WAIT_ON_SYNC            -1
+#define XMIT_SYNC               -2
+#define BACKUP_SYNC             -3
+#define XMIT_REQ_CAL            -4
+#define XMIT_RPL_CAL            -5
+
+
+struct tdma_priv;
+
+
+struct tdma_job {
+    struct list_head            entry;
+    int                         id;
+    unsigned int                ref_count;
+};
+
+
+#define SLOT_JOB(job)           ((struct tdma_slot *)(job))
+
+struct tdma_slot {
+    struct tdma_job             head;
+
+    u64                         offset;
+    unsigned int                period;
+    unsigned int                phasing;
+    unsigned int                mtu;
+    unsigned int                size;
+    struct rtskb_prio_queue     *queue;
+    struct rtskb_prio_queue     local_queue;
+};
+
+
+#define REQUEST_CAL_JOB(job)    ((struct tdma_request_cal *)(job))
+
+struct tdma_request_cal {
+    struct tdma_job             head;
+
+    struct tdma_priv            *tdma;
+    u64                         offset;
+    unsigned int                period;
+    unsigned int                phasing;
+    unsigned int                cal_rounds;
+    u64                         *cal_results;
+    u64                         *result_buffer;
+};
+
+
+#define REPLY_CAL_JOB(job)      ((struct tdma_reply_cal *)(job))
+
+struct tdma_reply_cal {
+    struct tdma_job             head;
+
+    u32                         reply_cycle;
+    u64                         reply_offset;
+    struct rtskb                *reply_rtskb;
+};
+
+struct tdma_priv {
+    unsigned int                magic;
+    struct rtnet_device         *rtdev;
+    char                        device_name[32];
+    struct rtdm_driver          api_driver;
+    struct rtdm_device          api_device;
+
+#ifdef ALIGN_RTOS_TASK
+    __u8                        __align[(ALIGN_RTOS_TASK -
+                                         ((sizeof(unsigned int) +
+                                           sizeof(struct rtnet_device *) +
+                                           sizeof(struct rtdm_device)
+                                          ) & (ALIGN_RTOS_TASK-1))
+                                         ) & (ALIGN_RTOS_TASK-1)];
+#endif
+    rtdm_task_t                 worker_task;
+    rtdm_event_t                worker_wakeup;
+    rtdm_event_t                xmit_event;
+    rtdm_event_t                sync_event;
+
+    unsigned long               flags;
+    unsigned int                cal_rounds;
+    u32                         current_cycle;
+    u64                         current_cycle_start;
+    u64                         master_packet_delay_ns;
+    nanosecs_rel_t              clock_offset;
+
+    struct tdma_job             sync_job;
+    struct tdma_job             *first_job;
+    struct tdma_job             *current_job;
+    volatile unsigned int       job_list_revision;
+
+    unsigned int                max_slot_id;
+    struct tdma_slot            **slot_table;
+
+    struct rt_proc_call         *calibration_call;
+    unsigned char               master_hw_addr[MAX_ADDR_LEN];
+
+    rtdm_lock_t                 lock;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+    struct rtskb_pool           cal_rtskb_pool;
+    u64                         cycle_period;
+    u64                         backup_sync_inc;
+#endif
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    struct list_head            list_entry;
+#endif
+};
+
+
+extern struct rtmac_disc        tdma_disc;
+
+#define print_jobs()            do { \
+    struct tdma_job *entry; \
+    rtdm_printk("%s:%d - ", __FUNCTION__, __LINE__); \
+    list_for_each_entry(entry, &tdma->first_job->entry, entry) \
+        rtdm_printk("%d ", entry->id); \
+    rtdm_printk("\n"); \
+} while (0)
+
+#endif /* __TDMA_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_ioctl.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,36 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_ioctl.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_IOCTL_H_
+#define __TDMA_IOCTL_H_
+
+#include <rtmac/tdma/tdma.h>
+
+
+int tdma_cleanup_slot(struct tdma_priv *tdma, struct tdma_slot *slot);
+
+int tdma_ioctl(struct rtnet_device *rtdev, unsigned int request,
+               unsigned long arg);
+
+#endif /* __TDMA_IOCTL_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_proto.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_proto.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_PROTO_H_
+#define __TDMA_PROTO_H_
+
+#include <rtdev.h>
+
+#include <rtmac/tdma/tdma.h>
+
+
+#define TDMA_FRM_VERSION    0x0201
+
+#define TDMA_FRM_SYNC       0x0000
+#define TDMA_FRM_REQ_CAL    0x0010
+#define TDMA_FRM_RPL_CAL    0x0011
+
+
+struct tdma_frm_head {
+    u16                     version;
+    u16                     id;
+} __attribute__((packed));
+
+
+#define SYNC_FRM(head)      ((struct tdma_frm_sync *)(head))
+
+struct tdma_frm_sync {
+    struct tdma_frm_head    head;
+    u32                     cycle_no;
+    u64                     xmit_stamp;
+    u64                     sched_xmit_stamp;
+} __attribute__((packed));
+
+
+#define REQ_CAL_FRM(head)   ((struct tdma_frm_req_cal *)(head))
+
+struct tdma_frm_req_cal {
+    struct tdma_frm_head    head;
+    u64                     xmit_stamp;
+    u32                     reply_cycle;
+    u64                     reply_slot_offset;
+} __attribute__((packed));
+
+
+#define RPL_CAL_FRM(head)   ((struct tdma_frm_rpl_cal *)(head))
+
+struct tdma_frm_rpl_cal {
+    struct tdma_frm_head    head;
+    u64                     request_xmit_stamp;
+    u64                     reception_stamp;
+    u64                     xmit_stamp;
+} __attribute__((packed));
+
+
+void tdma_xmit_sync_frame(struct tdma_priv *tdma);
+int tdma_xmit_request_cal_frame(struct tdma_priv *tdma, u32 reply_cycle,
+                                u64 reply_slot_offset);
+
+int tdma_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev);
+int tdma_nrt_packet_tx(struct rtskb *rtskb);
+
+int tdma_packet_rx(struct rtskb *rtskb);
+
+unsigned int tdma_get_mtu(struct rtnet_device *rtdev, unsigned int priority);
+
+#endif /* __TDMA_PROTO_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac/tdma/tdma_worker.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,35 @@
+/***
+ *
+ *  include/rtmac/tdma/tdma_worker.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_WORKER_H_
+#define __TDMA_WORKER_H_
+
+#include <rtdm/driver.h>
+
+
+#define DEF_WORKER_PRIO         RTDM_TASK_HIGHEST_PRIORITY
+
+void tdma_worker(void *arg);
+
+#endif /* __TDMA_WORKER_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtmac.h linux-5.4.5-new/net/rtnet/stack/include/rtmac.h
--- linux-5.4.5/net/rtnet/stack/include/rtmac.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtmac.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,96 @@
+/***
+ *
+ *  include/rtmac.h
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2004-2006 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *  As a special exception to the GNU General Public license, the RTnet
+ *  project allows you to use this header file in unmodified form to produce
+ *  application programs executing in user-space which use RTnet services by
+ *  normal system calls. The resulting executable will not be covered by the
+ *  GNU General Public License merely as a result of this header file use.
+ *  Instead, this header file use will be considered normal use of RTnet and
+ *  not a "derived work" in the sense of the GNU General Public License.
+ *
+ *  This exception does not apply when the application code is built as a
+ *  static or dynamically loadable portion of the Linux kernel nor does the
+ *  exception override other reasons justifying application of the GNU General
+ *  Public License.
+ *
+ *  This exception applies only to the code released by the RTnet project
+ *  under the name RTnet and bearing this exception notice. If you copy code
+ *  from other sources into a copy of RTnet, the exception does not apply to
+ *  the code that you add in this way.
+ *
+ */
+
+#ifndef __RTMAC_H_
+#define __RTMAC_H_
+
+#include <rtdm/rtdm.h>
+
+
+/* sub-classes: RTDM_CLASS_RTMAC */
+#define RTDM_SUBCLASS_TDMA          0
+#define RTDM_SUBCLASS_UNMANAGED     1
+
+#define RTIOC_TYPE_RTMAC            RTDM_CLASS_RTMAC
+
+
+/* ** Common Cycle Event Types ** */
+/* standard event, wake up once per cycle */
+#define RTMAC_WAIT_ON_DEFAULT       0x00
+/* wake up on media access of the station, may trigger multiple times per
+   cycle */
+#define RTMAC_WAIT_ON_XMIT          0x01
+
+/* ** TDMA-specific Cycle Event Types ** */
+/* tigger on on SYNC frame reception/transmission */
+#define TDMA_WAIT_ON_SYNC           RTMAC_WAIT_ON_DEFAULT
+#define TDMA_WAIT_ON_SOF            TDMA_WAIT_ON_SYNC /* legacy support */
+
+
+/* RTMAC_RTIOC_WAITONCYCLE_EX control and status data */
+struct rtmac_waitinfo {
+    /** Set to wait type before invoking the service */
+    unsigned int    type;
+
+    /** Set to sizeof(struct rtmac_waitinfo) before invoking the service */
+    size_t          size;
+
+    /** Counter of elementary cycles of the underlying RTmac discipline
+        (if applicable) */
+    unsigned long   cycle_no;
+
+    /** Date (in local time) of the last elementary cycle start of the RTmac
+        discipline (if applicable) */
+    nanosecs_abs_t  cycle_start;
+
+    /** Offset of the local clock to the global clock provided by the RTmac
+        discipline (if applicable): t_global = t_local + clock_offset */
+    nanosecs_rel_t  clock_offset;
+};
+
+
+/* RTmac Discipline IOCTLs */
+#define RTMAC_RTIOC_TIMEOFFSET      _IOR(RTIOC_TYPE_RTMAC, 0x00, int64_t)
+#define RTMAC_RTIOC_WAITONCYCLE     _IOW(RTIOC_TYPE_RTMAC, 0x01, unsigned int)
+#define RTMAC_RTIOC_WAITONCYCLE_EX  _IOWR(RTIOC_TYPE_RTMAC, 0x02, \
+                                          struct rtmac_waitinfo)
+
+#endif /* __RTMAC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_chrdev.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_chrdev.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_chrdev.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,123 @@
+/***
+ *
+ *  include/rtnet_chrdev.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999    Lineo, Inc
+ *                1999,2002 David A. Schleef <ds@schleef.org>
+ *                2002 Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003,2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_CHRDEV_H_
+#define __RTNET_CHRDEV_H_
+
+#include <rtdev.h>
+
+
+#ifdef __KERNEL__
+
+#include <linux/list.h>
+#include <linux/init.h>
+#include <linux/ioctl.h>
+#include <linux/netdevice.h>
+#include <linux/types.h>
+
+
+/* new extensible interface */
+struct rtnet_ioctls {
+    /* internal usage only */
+    struct list_head entry;
+    atomic_t         ref_count;
+
+    /* provider specification */
+    const char       *service_name;
+    unsigned int     ioctl_type;
+    int              (*handler)(struct rtnet_device *rtdev,
+                                unsigned int request, unsigned long arg);
+};
+
+extern int rtnet_register_ioctls(struct rtnet_ioctls *ioctls);
+extern void rtnet_unregister_ioctls(struct rtnet_ioctls *ioctls);
+
+extern int __init rtnet_chrdev_init(void);
+extern void rtnet_chrdev_release(void);
+
+#else   /* ifndef __KERNEL__ */
+
+#include <net/if.h>             /* IFNAMSIZ */
+#include <linux/types.h>
+
+#endif  /* __KERNEL__ */
+
+
+#define RTNET_MINOR             240 /* user interface for /dev/rtnet */
+#define DEV_ADDR_LEN            32  /* avoids inconsistent MAX_ADDR_LEN */
+
+
+struct rtnet_ioctl_head {
+    char if_name[IFNAMSIZ];
+};
+
+struct rtnet_core_cmd {
+    struct rtnet_ioctl_head head;
+
+    union {
+        /*** rtifconfig **/
+        struct {
+            __u32       ip_addr;
+            __u32       broadcast_ip;
+            __u32       set_dev_flags;
+            __u32       clear_dev_flags;
+            __u32       dev_addr_type;
+            __u32       __padding;
+            __u8        dev_addr[DEV_ADDR_LEN];
+        } up;
+
+        struct {
+            __u32       ifindex;
+            __u32       type;
+            __u32       ip_addr;
+            __u32       broadcast_ip;
+            __u32       mtu;
+            __u32       flags;
+            __u8        dev_addr[DEV_ADDR_LEN];
+        } info;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define RTNET_IOC_NODEV_PARAM           0x80
+
+#define RTNET_IOC_TYPE_CORE             0
+#define RTNET_IOC_TYPE_RTCFG            1
+#define RTNET_IOC_TYPE_IPV4             2
+#define RTNET_IOC_TYPE_RTMAC_NOMAC      100
+#define RTNET_IOC_TYPE_RTMAC_TDMA       110
+
+#define IOC_RT_IFUP                     _IOW(RTNET_IOC_TYPE_CORE, 0,    \
+                                             struct rtnet_core_cmd)
+#define IOC_RT_IFDOWN                   _IOW(RTNET_IOC_TYPE_CORE, 1,    \
+                                             struct rtnet_core_cmd)
+#define IOC_RT_IFINFO                   _IOWR(RTNET_IOC_TYPE_CORE, 2 |  \
+                                              RTNET_IOC_NODEV_PARAM,    \
+                                              struct rtnet_core_cmd)
+
+#endif  /* __RTNET_CHRDEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_internal.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_internal.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_internal.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_internal.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  rtnet_internal.h - internal declarations
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_INTERNAL_H_
+#define __RTNET_INTERNAL_H_
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <rtnet_rtdm.h>
+
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+#define RTNET_ASSERT(expr, func) \
+    if (!(expr)) \
+    { \
+	printk(KERN_ERR	"Assertion failed! %s:%s:%d %s\n", \
+	__FILE__, __FUNCTION__, __LINE__, (#expr)); \
+	func \
+    }
+#else
+#define RTNET_ASSERT(expr, func)
+#endif /* CONFIG_XENO_DRIVERS_NET_CHECKED */
+
+/* some configurables */
+
+#define RTNET_DEF_STACK_PRIORITY \
+    RTDM_TASK_HIGHEST_PRIORITY + RTDM_TASK_LOWER_PRIORITY
+/*#define RTNET_RTDEV_PRIORITY        5*/
+
+
+struct rtnet_device;
+
+struct rtnet_mgr {
+    struct task_struct     *task;
+    rtdm_event_t    event;
+};
+
+
+extern struct rtnet_mgr STACK_manager;
+extern struct rtnet_mgr RTDEV_manager;
+
+extern const char rtnet_rtdm_provider_name[];
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+extern struct xnvfile_directory rtnet_proc_root;
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+#include <linux/mutex.h>
+
+#endif /* __RTNET_INTERNAL_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_iovec.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_iovec.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_iovec.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_iovec.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,45 @@
+/* rtnet_iovec.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *               2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_IOVEC_H_
+#define __RTNET_IOVEC_H_
+
+#ifdef __KERNEL__
+
+#include <linux/uio.h>
+#include <rtnet_socket.h>
+
+struct user_msghdr;
+struct rtdm_fd;
+
+ssize_t rtnet_write_to_iov(struct rtsocket *sock,
+			   struct iovec *iov, int iovlen,
+			   const void *data, size_t len, int msg_in_userspace);
+
+ssize_t rtnet_read_from_iov(struct rtsocket *sock,
+			    struct iovec *iov, int iovlen,
+			    void *data, size_t len, int msg_in_userspace);
+#endif  /* __KERNEL__ */
+
+#endif  /* __RTNET_IOVEC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_port.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_port.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_port.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_port.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,119 @@
+/* include/rtnet_port.h
+ *
+ * RTnet - real-time networking subsystem
+ * Copyright (C) 2003      Wittawat Yamwong
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef __RTNET_PORT_H_
+#define __RTNET_PORT_H_
+
+#ifdef __KERNEL__
+
+#include <linux/bitops.h>
+#include <linux/moduleparam.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/vmalloc.h>
+#include <linux/bitops.h>
+
+#include <rtdev.h>
+#include <rtdev_mgr.h>
+#include <stack_mgr.h>
+#include <ethernet/eth.h>
+
+static inline void rtnetif_start_queue(struct rtnet_device *rtdev)
+{
+    clear_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline void rtnetif_wake_queue(struct rtnet_device *rtdev)
+{
+    if (test_and_clear_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state))
+    /*TODO __netif_schedule(dev); */ ;
+}
+
+static inline void rtnetif_stop_queue(struct rtnet_device *rtdev)
+{
+    set_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline int rtnetif_queue_stopped(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_XOFF, &rtdev->link_state);
+}
+
+static inline int rtnetif_running(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+}
+
+static inline int rtnetif_device_present(struct rtnet_device *rtdev)
+{
+    return test_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+}
+
+static inline void rtnetif_device_detach(struct rtnet_device *rtdev)
+{
+	if (test_and_clear_bit(__RTNET_LINK_STATE_PRESENT,
+			       &rtdev->link_state) &&
+	    rtnetif_running(rtdev)) {
+		rtnetif_stop_queue(rtdev);
+	}
+}
+
+static inline void rtnetif_device_attach(struct rtnet_device *rtdev)
+{
+	if (!test_and_set_bit(__RTNET_LINK_STATE_PRESENT,
+			      &rtdev->link_state) &&
+	    rtnetif_running(rtdev)) {
+		rtnetif_wake_queue(rtdev);
+		/* __netdev_watchdog_up(rtdev); */
+	}
+}
+
+static inline void rtnetif_carrier_on(struct rtnet_device *rtdev)
+{
+    clear_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+    /*
+    if (netif_running(dev))
+	__netdev_watchdog_up(dev);
+    */
+}
+
+static inline void rtnetif_carrier_off(struct rtnet_device *rtdev)
+{
+    set_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+}
+
+static inline int rtnetif_carrier_ok(struct rtnet_device *rtdev)
+{
+    return !test_bit(__RTNET_LINK_STATE_NOCARRIER, &rtdev->link_state);
+}
+
+#define NIPQUAD(addr) \
+	((unsigned char *)&addr)[0],	\
+	((unsigned char *)&addr)[1],	\
+	((unsigned char *)&addr)[2],	\
+	((unsigned char *)&addr)[3]
+#define NIPQUAD_FMT "%u.%u.%u.%u"
+
+#endif /* __KERNEL__ */
+
+#endif /* __RTNET_PORT_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_rtdm.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_rtdm.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_rtdm.h	2020-06-15 16:12:31.575695208 +0300
@@ -0,0 +1,60 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#ifndef __RTNET_RTDM_H_
+#define __RTNET_RTDM_H_
+
+#include <linux/swait.h>
+#include <linux/socket.h>
+
+//rtdm_task_t     task;
+
+typedef struct {
+	struct swait_queue_head head_swait;
+	unsigned char condition;
+} rtdm_event_t;
+
+#define rtdm_event_init(event, flags)	\
+	do { \
+		__rtdm_event_init(event); \
+	} while (0) 
+
+/* TODO: rtdm_event_pulse should not be 100% rtdm_event_signal */
+#define rtdm_event_pulse(event) \
+	rtdm_event_signal(event)
+
+#define rtdm_event_destroy(event) \
+	do { \
+	} while (0)
+
+void __rtdm_event_init(rtdm_event_t *event);
+void rtdm_event_signal(rtdm_event_t *event);
+int rtdm_event_wait(rtdm_event_t *event);
+
+#define rtdm_fd_to_private(fd) NULL
+//#define rtdm_fd_is_user(fd) 1
+#define rtdm_in_rt_context(x) \
+	((current->rt_priority > (MAX_RT_PRIO / 2)) && \
+	((current->policy == SCHED_FIFO) || \
+	 (current->policy == SCHED_RR)   || \
+	 (current->policy == SCHED_DEADLINE)))
+
+typedef __u32 socklen_t;
+
+#define RTDM_TASK_LOWEST_PRIORITY (1 + MAX_RT_PRIO / 2)
+#define RTDM_TASK_HIGHEST_PRIORITY (MAX_RT_PRIO - 1)
+#define RTDM_TASK_LOWER_PRIORITY	(-1)
+
+#define RTDM_IOV_FASTMAX  16
+int rtdm_get_iovec(int fd, struct iovec **iovp,
+                   const struct user_msghdr *msg,
+                   struct iovec *iov_fast, int msg_in_userspace);
+ssize_t rtdm_get_iov_flatlen(struct iovec *iov, int iovlen);
+static inline
+void rtdm_drop_iovec(struct iovec *iov, struct iovec *iov_fast)
+{
+        if (iov != iov_fast)
+                kfree(iov);
+}
+
+#endif  /* __RTNET_RTDM_H_ */
+
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_rtpc.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_rtpc.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_rtpc.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_rtpc.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  include/rtnet_rtpc.h
+ *
+ *  RTnet - real-time networking subsystem
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_RTPC_H_
+#define __RTNET_RTPC_H_
+
+#include <linux/init.h>
+
+#include <rtnet_internal.h>
+
+
+struct rt_proc_call;
+
+typedef int (*rtpc_proc)(struct rt_proc_call *call);
+typedef void (*rtpc_copy_back_proc)(struct rt_proc_call *call,
+                                    void *priv_data);
+typedef void (*rtpc_cleanup_proc)(void *priv_data);
+
+struct rt_proc_call {
+    struct list_head    list_entry;
+    int                 processed;
+    rtpc_proc           proc;
+    int                 result;
+    atomic_t            ref_count;
+    wait_queue_head_t   call_wq;
+    rtpc_cleanup_proc   cleanup_handler;
+    char                priv_data[0] __attribute__ ((aligned(8)));
+};
+
+#define CALL_PENDING    1000 /* result value for blocked calls */
+
+
+int rtnet_rtpc_dispatch_call(rtpc_proc rt_proc, unsigned int timeout,
+                             void *priv_data, size_t priv_data_size,
+                             rtpc_copy_back_proc copy_back_handler,
+                             rtpc_cleanup_proc cleanup_handler);
+
+
+void rtnet_rtpc_complete_call(struct rt_proc_call *call, int result);
+void rtnet_rtpc_complete_call_nrt(struct rt_proc_call *call, int result);
+
+#define rtpc_dispatch_call                  rtnet_rtpc_dispatch_call
+#define rtpc_complete_call                  rtnet_rtpc_complete_call
+#define rtpc_complete_call_nrt              rtnet_rtpc_complete_call_nrt
+
+#define rtpc_get_priv(call, type)           (type *)(call->priv_data)
+#define rtpc_get_result(call)               call->result
+#define rtpc_set_result(call, new_result)   call->result = new_result
+#define rtpc_set_cleanup_handler(call, handler) \
+    call->cleanup_handler = handler;
+
+
+int __init rtpc_init(void);
+void rtpc_cleanup(void);
+
+#endif /* __RTNET_RTPC_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtnet_socket.h linux-5.4.5-new/net/rtnet/stack/include/rtnet_socket.h
--- linux-5.4.5/net/rtnet/stack/include/rtnet_socket.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtnet_socket.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,144 @@
+/***
+ *
+ *  include/rtnet_socket.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTNET_SOCKET_H_
+#define __RTNET_SOCKET_H_
+
+#include <asm/atomic.h>
+#include <linux/list.h>
+#include <linux/semaphore.h>
+#include <linux/file.h>
+
+#include <rtdev.h>
+#include <stack_mgr.h>
+
+
+struct rtsocket {
+    int fd;
+    int fd_refs;
+    struct file *file;
+
+    int family;
+    int type;
+    unsigned short          protocol;
+
+    struct rtskb_pool       skb_pool;
+    unsigned int            pool_size;
+    struct mutex            pool_nrt_lock;
+
+    struct rtskb_queue      incoming;
+
+    raw_spinlock_t             param_lock;
+
+    unsigned int            priority;
+    ktime_t                 timeout;      /* receive timeout, 0 for infinite */
+
+    struct semaphore        pending_sem;
+
+    void                    (*callback_func)(void *, void *);
+    void                    *callback_arg;
+
+    unsigned long           flags;
+
+    union {
+	/* IP specific */
+	struct {
+	    u32             saddr;      /* source ip-addr (bind) */
+	    u32             daddr;      /* destination ip-addr */
+	    u16             sport;      /* source port */
+	    u16             dport;      /* destination port */
+
+	    int             reg_index;  /* index in port registry */
+	    u8              tos;
+	    u8              state;
+	} inet;
+
+	/* packet socket specific */
+	struct {
+	    struct rtpacket_type packet_type;
+	    int                  ifindex;
+	} packet;
+    } prot;
+
+    struct module *owner;
+};
+
+
+static inline int rt_socket_fd(struct rtsocket *sock)
+{
+    return sock->fd;
+}
+
+void *rtnet_get_arg(struct rtsocket *sock, void *tmp,
+		    const void *src, size_t len, int msg_in_userspace);
+
+int rtnet_put_arg(struct rtsocket *sock, void *dst,
+		  const void *src, size_t len, int msg_in_userspace);
+
+int rtdm_fd_lock(struct rtsocket *sock);
+int rtdm_fd_unlock(struct rtsocket *sock);
+
+#define rt_socket_reference(sock)   \
+    rtdm_fd_lock(sock)
+#define rt_socket_dereference(sock) \
+    rtdm_fd_unlock(sock)
+
+int __rt_socket_init(struct rtsocket *sock, unsigned short protocol,
+		struct module *module);
+#define rt_socket_init(sock, proto) \
+    __rt_socket_init(sock, proto, THIS_MODULE)
+
+void rt_socket_cleanup(struct rtsocket *sock);
+int rt_socket_common_ioctl(struct rtsocket *sock, int request, void __user *arg);
+int rt_socket_if_ioctl(struct rtsocket *sock, int request, void __user *arg);
+#if 0
+int rt_socket_select_bind(int fd,
+			  rtdm_selector_t *selector,
+			  enum rtdm_selecttype type,
+			  unsigned fd_index);
+#endif
+int __rt_bare_socket_init_icmp(struct rtsocket *sock, unsigned short protocol,
+                        unsigned int priority, unsigned int pool_size,
+                        struct module *module);
+#define rt_bare_socket_init_icmp(sock, proto, prio, pool_sz) \
+    __rt_bare_socket_init_icmp(sock, proto, prio, pool_sz, THIS_MODULE)
+int __rt_bare_socket_init(struct rtsocket *sock, unsigned short protocol,
+			unsigned int priority, unsigned int pool_size,
+			struct module *module);
+#define rt_bare_socket_init(sock, proto, prio, pool_sz) \
+    __rt_bare_socket_init(sock, proto, prio, pool_sz, THIS_MODULE)
+
+static inline void rt_bare_socket_cleanup(struct rtsocket *sock)
+{
+    rtskb_pool_release(&sock->skb_pool);
+    module_put(sock->owner);
+}
+
+#endif  /* __RTNET_SOCKET_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtskb_fifo.h linux-5.4.5-new/net/rtnet/stack/include/rtskb_fifo.h
--- linux-5.4.5/net/rtnet/stack/include/rtskb_fifo.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtskb_fifo.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,151 @@
+/***
+ *
+ *  include/rtskb_fifo.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTSKB_FIFO_H_
+#define __RTSKB_FIFO_H_
+
+#include <rtskb.h>
+
+
+struct rtskb_fifo {
+    unsigned long       read_pos ____cacheline_aligned_in_smp;
+    raw_spinlock_t         read_lock;
+    unsigned long       size_mask;
+    unsigned long       write_pos ____cacheline_aligned_in_smp;
+    raw_spinlock_t         write_lock;
+    struct rtskb        *buffer[0];
+};
+
+#define DECLARE_RTSKB_FIFO(name_prefix, size)   \
+struct {                                        \
+    struct rtskb_fifo   fifo;                   \
+    struct rtskb        *__buffer[(size)];      \
+} name_prefix                                   \
+
+
+static inline int __rtskb_fifo_insert(struct rtskb_fifo *fifo,
+                                      struct rtskb *rtskb)
+{
+    unsigned long pos = fifo->write_pos;
+    unsigned long new_pos = (pos + 1) & fifo->size_mask;
+
+    if (unlikely(new_pos == fifo->read_pos))
+        return -EAGAIN;
+
+    fifo->buffer[pos] = rtskb;
+
+    /* rtskb must have been written before write_pos update */
+    smp_wmb();
+
+    fifo->write_pos = new_pos;
+
+    return 0;
+}
+
+static inline int rtskb_fifo_insert(struct rtskb_fifo *fifo,
+                                    struct rtskb *rtskb)
+{
+    unsigned long context;
+    int result;
+
+    raw_spin_lock_irqsave(&fifo->write_lock, context);
+    result = __rtskb_fifo_insert(fifo, rtskb);
+    raw_spin_unlock_irqrestore(&fifo->write_lock, context);
+
+    return result;
+}
+
+static inline int rtskb_fifo_insert_inirq(struct rtskb_fifo *fifo,
+                                          struct rtskb *rtskb)
+{
+    int result;
+
+    raw_spin_lock(&fifo->write_lock);
+    result = __rtskb_fifo_insert(fifo, rtskb);
+    raw_spin_unlock(&fifo->write_lock);
+
+    return result;
+}
+
+static inline struct rtskb *__rtskb_fifo_remove(struct rtskb_fifo *fifo)
+{
+    unsigned long pos = fifo->read_pos;
+    struct rtskb *result;
+
+    /* check FIFO status first */
+    if (unlikely(pos == fifo->write_pos))
+        return NULL;
+
+    /* at least one rtskb is enqueued, so get the next one */
+    result = fifo->buffer[pos];
+
+    /* result must have been read before read_pos update */
+    smp_rmb();
+
+    fifo->read_pos = (pos + 1) & fifo->size_mask;
+
+    /* read_pos must have been written for a consitent fifo state on exit */
+    smp_wmb();
+
+    return result;
+}
+
+static inline struct rtskb *rtskb_fifo_remove(struct rtskb_fifo *fifo)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&fifo->read_lock, context);
+    result = __rtskb_fifo_remove(fifo);
+    raw_spin_unlock_irqrestore(&fifo->read_lock, context);
+
+    return result;
+}
+
+static inline struct rtskb *rtskb_fifo_remove_inirq(struct rtskb_fifo *fifo)
+{
+    struct rtskb *result;
+
+    raw_spin_lock(&fifo->read_lock);
+    result = __rtskb_fifo_remove(fifo);
+    raw_spin_unlock(&fifo->read_lock);
+
+    return result;
+}
+
+/* for now inlined... */
+static inline void rtskb_fifo_init(struct rtskb_fifo *fifo,
+                                   unsigned long size)
+{
+    fifo->read_pos  = 0;
+    fifo->write_pos = 0;
+    fifo->size_mask = size - 1;
+    raw_spin_lock_init(&fifo->read_lock);
+    raw_spin_lock_init(&fifo->write_lock);
+}
+
+#endif  /* __RTSKB_FIFO_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtskb.h linux-5.4.5-new/net/rtnet/stack/include/rtskb.h
--- linux-5.4.5/net/rtnet/stack/include/rtskb.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtskb.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,802 @@
+/***
+ *
+ *  include/rtskb.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>,
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __RTSKB_H_
+#define __RTSKB_H_
+
+#ifdef __KERNEL__
+
+#include <linux/skbuff.h>
+
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+
+
+/***
+
+rtskb Management - A Short Introduction
+---------------------------------------
+
+1. rtskbs (Real-Time Socket Buffers)
+
+A rtskb consists of a management structure (struct rtskb) and a fixed-sized
+(RTSKB_SIZE) data buffer. It is used to store network packets on their way from
+the API routines through the stack to the NICs or vice versa. rtskbs are
+allocated as one chunk of memory which contains both the managment structure
+and the buffer memory itself.
+
+
+2. rtskb Queues
+
+A rtskb queue is described by struct rtskb_queue. A queue can contain an
+unlimited number of rtskbs in an ordered way. A rtskb can either be added to
+the head (rtskb_queue_head()) or the tail of a queue (rtskb_queue_tail()). When
+a rtskb is removed from a queue (rtskb_dequeue()), it is always taken from the
+head. Queues are normally spin lock protected unless the __variants of the
+queuing functions are used.
+
+
+3. Prioritized rtskb Queues
+
+A prioritized queue contains a number of normal rtskb queues within an array.
+The array index of a sub-queue correspond to the priority of the rtskbs within
+this queue. For enqueuing a rtskb (rtskb_prio_queue_head()), its priority field
+is evaluated and the rtskb is then placed into the appropriate sub-queue. When
+dequeuing a rtskb, the first rtskb of the first non-empty sub-queue with the
+highest priority is returned. The current implementation supports 32 different
+priority levels, the lowest if defined by QUEUE_MIN_PRIO, the highest by
+QUEUE_MAX_PRIO.
+
+
+4. rtskb Pools
+
+As rtskbs must not be allocated by a normal memory manager during runtime,
+preallocated rtskbs are kept ready in several pools. Most packet producers
+(NICs, sockets, etc.) have their own pools in order to be independent of the
+load situation of other parts of the stack.
+
+When a pool is created (rtskb_pool_init()), the required rtskbs are allocated
+from a Linux slab cache. Pools can be extended (rtskb_pool_extend()) or
+shrinked (rtskb_pool_shrink()) during runtime. When shutting down the
+program/module, every pool has to be released (rtskb_pool_release()). All these
+commands demand to be executed within a non real-time context.
+
+Pools are organized as normal rtskb queues (struct rtskb_queue). When a rtskb
+is allocated (alloc_rtskb()), it is actually dequeued from the pool's queue.
+When freeing a rtskb (kfree_rtskb()), the rtskb is enqueued to its owning pool.
+rtskbs can be exchanged between pools (rtskb_acquire()). In this case, the
+passed rtskb switches over to from its owning pool to a given pool, but only if
+this pool can pass an empty rtskb from its own queue back.
+
+
+5. rtskb Chains
+
+To ease the defragmentation of larger IP packets, several rtskbs can form a
+chain. For these purposes, the first rtskb (and only the first!) provides a
+pointer to the last rtskb in the chain. When enqueuing the first rtskb of a
+chain, the whole chain is automatically placed into the destined queue. But,
+to dequeue a complete chain specialized calls are required (postfix: _chain).
+While chains also get freed en bloc (kfree_rtskb()) when passing the first
+rtskbs, it is not possible to allocate a chain from a pool (alloc_rtskb()); a
+newly allocated rtskb is always reset to a "single rtskb chain". Furthermore,
+the acquisition of complete chains is NOT supported (rtskb_acquire()).
+
+
+6. Capturing Support (Optional)
+
+When incoming or outgoing packets are captured, the assigned rtskb needs to be
+shared between the stack, the driver, and the capturing service. In contrast to
+many other network stacks, RTnet does not create a new rtskb head and
+re-references the payload. Instead, additional fields at the end of the rtskb
+structure are use for sharing a rtskb with a capturing service. If the sharing
+bit (RTSKB_CAP_SHARED) in cap_flags is set, the rtskb will not be returned to
+the owning pool upon the call of kfree_rtskb. Instead this bit will be reset,
+and a compensation rtskb stored in cap_comp_skb will be returned to the owning
+pool. cap_start and cap_len can be used to mirror the dimension of the full
+packet. This is required because the data and len fields will be modified while
+walking through the stack. cap_next allows to add a rtskb to a separate queue
+which is independent of any queue described in 2.
+
+Certain setup tasks for capturing packets can not become part of a capturing
+module, they have to be embedded into the stack. For this purpose, several
+inline functions are provided. rtcap_mark_incoming() is used to save the packet
+dimension right before it is modifed by the stack. rtcap_report_incoming()
+calls the capturing handler, if present, in order to let it process the
+received rtskb (e.g. allocate compensation rtskb, mark original rtskb as
+shared, and enqueue it).
+
+Outgoing rtskb have to be captured by adding a hook function to the chain of
+hard_start_xmit functions of a device. To measure the delay caused by RTmac
+between the request and the actual transmission, a time stamp can be taken using
+rtcap_mark_rtmac_enqueue(). This function is typically called by RTmac
+disciplines when they add a rtskb to their internal transmission queue. In such
+a case, the RTSKB_CAP_RTMAC_STAMP bit is set in cap_flags to indicate that the
+cap_rtmac_stamp field now contains valid data.
+
+ ***/
+
+
+#ifndef CHECKSUM_PARTIAL
+#define CHECKSUM_PARTIAL        CHECKSUM_HW
+#endif
+
+#define RTSKB_CAP_SHARED        1   /* rtskb shared between stack and RTcap */
+#define RTSKB_CAP_RTMAC_STAMP   2   /* cap_rtmac_stamp is valid             */
+
+#define RTSKB_UNMAPPED          0
+
+struct rtskb_queue;
+struct rtsocket;
+struct rtnet_device;
+
+/***
+ *  rtskb - realtime socket buffer
+ */
+struct rtskb {
+    struct rtskb        *next;      /* used for queuing rtskbs */
+    struct rtskb        *chain_end; /* marks the end of a rtskb chain starting
+				       with this very rtskb */
+
+    struct rtskb_pool   *pool;      /* owning pool */
+
+    unsigned int        priority;   /* bit 0..15: prio, 16..31: user-defined */
+
+    struct rtsocket     *sk;        /* assigned socket */
+    struct rtnet_device *rtdev;     /* source or destination device */
+
+    ktime_t      time_stamp; /* arrival or transmission (RTcap) time */
+
+    /* patch address of the transmission time stamp, can be NULL
+     * calculation: *xmit_stamp = cpu_to_be64(time_in_ns + *xmit_stamp)
+     */
+    ktime_t      *xmit_stamp;
+
+    /* transport layer */
+    union
+    {
+	struct tcphdr   *th;
+	struct udphdr   *uh;
+	struct icmphdr  *icmph;
+	struct iphdr    *ipihdr;
+	unsigned char   *raw;
+    } h;
+
+    /* network layer */
+    union
+    {
+	struct iphdr    *iph;
+	struct arphdr   *arph;
+	unsigned char   *raw;
+    } nh;
+
+    /* link layer */
+    union
+    {
+	struct ethhdr   *ethernet;
+	unsigned char   *raw;
+    } mac;
+
+    unsigned short      protocol;
+    unsigned char       pkt_type;
+
+    unsigned char       ip_summed;
+    unsigned int        csum;
+
+    unsigned char       *data;
+    unsigned char       *tail;
+    unsigned char       *end;
+    unsigned int        len;
+
+    dma_addr_t          buf_dma_addr;
+
+    unsigned char       *buf_start;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+    unsigned char       *buf_end;
+#endif
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+    int                 cap_flags;  /* see RTSKB_CAP_xxx                    */
+    struct rtskb        *cap_comp_skb; /* compensation rtskb                */
+    struct rtskb        *cap_next;  /* used for capture queue               */
+    unsigned char       *cap_start; /* start offset for capturing           */
+    unsigned int        cap_len;    /* capture length of this rtskb         */
+    nanosecs_abs_t      cap_rtmac_stamp; /* RTmac enqueuing time            */
+#endif
+
+    struct list_head    entry; /* for global rtskb list */
+};
+
+struct rtskb_queue {
+    struct rtskb        *first;
+    struct rtskb        *last;
+    raw_spinlock_t         lock;
+};
+
+struct rtskb_pool_lock_ops {
+    int (*trylock)(void *cookie);
+    void (*unlock)(void *cookie);
+};
+
+struct rtskb_pool {
+    struct rtskb_queue queue;
+    const struct rtskb_pool_lock_ops *lock_ops;
+    void *lock_cookie;
+};
+
+#define QUEUE_MAX_PRIO          0
+#define QUEUE_MIN_PRIO          31
+
+struct rtskb_prio_queue {
+    raw_spinlock_t lock;
+    unsigned long       usage;  /* bit array encoding non-empty sub-queues */
+    struct rtskb_queue  queue[QUEUE_MIN_PRIO+1];
+};
+
+#define RTSKB_PRIO_MASK         0x0000FFFF  /* bits  0..15: xmit prio    */
+#define RTSKB_CHANNEL_MASK      0xFFFF0000  /* bits 16..31: xmit channel */
+#define RTSKB_CHANNEL_SHIFT     16
+
+#define RTSKB_DEF_RT_CHANNEL    SOCK_DEF_RT_CHANNEL
+#define RTSKB_DEF_NRT_CHANNEL   SOCK_DEF_NRT_CHANNEL
+#define RTSKB_USER_CHANNEL      SOCK_USER_CHANNEL
+
+/* Note: always keep SOCK_XMIT_PARAMS consistent with definitions above! */
+#define RTSKB_PRIO_VALUE        SOCK_XMIT_PARAMS
+
+
+/* default values for the module parameter */
+#define DEFAULT_GLOBAL_RTSKBS       0       /* default number of rtskb's in global pool */
+#define DEFAULT_DEVICE_RTSKBS       16      /* default additional rtskbs per network adapter */
+#define DEFAULT_SOCKET_RTSKBS       16      /* default number of rtskb's in socket pools */
+
+#define ALIGN_RTSKB_STRUCT_LEN      SKB_DATA_ALIGN(sizeof(struct rtskb))
+#define RTSKB_SIZE                  1544    /* maximum needed by pcnet32-rt */
+
+extern unsigned int rtskb_pools;        /* current number of rtskb pools      */
+extern unsigned int rtskb_pools_max;    /* maximum number of rtskb pools      */
+extern unsigned int rtskb_amount;       /* current number of allocated rtskbs */
+extern unsigned int rtskb_amount_max;   /* maximum number of allocated rtskbs */
+
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+extern void rtskb_over_panic(struct rtskb *skb, int len, void *here);
+extern void rtskb_under_panic(struct rtskb *skb, int len, void *here);
+#endif
+
+extern struct rtskb *rtskb_pool_dequeue(struct rtskb_pool *pool);
+
+extern void rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb);
+
+extern struct rtskb *alloc_rtskb(unsigned int size, struct rtskb_pool *pool);
+
+extern void kfree_rtskb(struct rtskb *skb);
+#define dev_kfree_rtskb(a)  kfree_rtskb(a)
+
+
+#define rtskb_checksum_none_assert(skb) (skb->ip_summed = CHECKSUM_NONE)
+
+static inline void rtskb_tx_timestamp(struct rtskb *skb)
+{
+	ktime_t *ts = skb->xmit_stamp;
+
+	if (!ts)
+		return;
+
+	*ts = cpu_to_be64(ktime_get() + *ts);
+}
+
+/***
+ *  rtskb_queue_init - initialize the queue
+ *  @queue
+ */
+static inline void rtskb_queue_init(struct rtskb_queue *queue)
+{
+    raw_spin_lock_init(&queue->lock);
+    queue->first = NULL;
+    queue->last  = NULL;
+}
+
+/***
+ *  rtskb_prio_queue_init - initialize the prioritized queue
+ *  @prioqueue
+ */
+static inline void rtskb_prio_queue_init(struct rtskb_prio_queue *prioqueue)
+{
+    memset(prioqueue, 0, sizeof(struct rtskb_prio_queue));
+    raw_spin_lock_init(&prioqueue->lock);
+}
+
+/***
+ *  rtskb_queue_empty
+ *  @queue
+ */
+static inline int rtskb_queue_empty(struct rtskb_queue *queue)
+{
+    return (queue->first == NULL);
+}
+
+/***
+ *  rtskb__prio_queue_empty
+ *  @queue
+ */
+static inline int rtskb_prio_queue_empty(struct rtskb_prio_queue *prioqueue)
+{
+    return (prioqueue->usage == 0);
+}
+
+/***
+ *  __rtskb_queue_head - insert a buffer at the queue head (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_queue_head(struct rtskb_queue *queue,
+				      struct rtskb *skb)
+{
+    struct rtskb *chain_end = skb->chain_end;
+
+    chain_end->next = queue->first;
+
+    if (queue->first == NULL)
+	queue->last = chain_end;
+    queue->first = skb;
+}
+
+/***
+ *  rtskb_queue_head - insert a buffer at the queue head (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_queue_head(struct rtskb_queue *queue, struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_queue_head(queue, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+
+/***
+ *  __rtskb_prio_queue_head - insert a buffer at the prioritized queue head
+ *                            (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_prio_queue_head(struct rtskb_prio_queue *prioqueue,
+					   struct rtskb *skb)
+{
+    unsigned int prio = skb->priority & RTSKB_PRIO_MASK;
+
+    RTNET_ASSERT(prio <= 31, prio = 31;);
+
+    __rtskb_queue_head(&prioqueue->queue[prio], skb);
+    __set_bit(prio, &prioqueue->usage);
+}
+
+/***
+ *  rtskb_prio_queue_head - insert a buffer at the prioritized queue head
+ *                          (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_prio_queue_head(struct rtskb_prio_queue *prioqueue,
+					 struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    __rtskb_prio_queue_head(prioqueue, skb);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+}
+
+/***
+ *  __rtskb_queue_tail - insert a buffer at the queue tail (w/o locks)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_queue_tail(struct rtskb_queue *queue,
+				      struct rtskb *skb)
+{
+    struct rtskb *chain_end = skb->chain_end;
+
+    chain_end->next = NULL;
+
+    if (queue->first == NULL)
+	queue->first = skb;
+    else
+	queue->last->next = skb;
+    queue->last = chain_end;
+}
+
+/***
+ *  rtskb_queue_tail - insert a buffer at the queue tail (lock protected)
+ *  @queue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_queue_tail(struct rtskb_queue *queue,
+				    struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_queue_tail(queue, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+
+/***
+ *  __rtskb_prio_queue_tail - insert a buffer at the prioritized queue tail
+ *                            (w/o locks)
+ *  @prioqueue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void __rtskb_prio_queue_tail(struct rtskb_prio_queue *prioqueue,
+					   struct rtskb *skb)
+{
+    unsigned int prio = skb->priority & RTSKB_PRIO_MASK;
+
+    RTNET_ASSERT(prio <= 31, prio = 31;);
+
+    __rtskb_queue_tail(&prioqueue->queue[prio], skb);
+    __set_bit(prio, &prioqueue->usage);
+}
+
+/***
+ *  rtskb_prio_queue_tail - insert a buffer at the prioritized queue tail
+ *                          (lock protected)
+ *  @prioqueue: queue to use
+ *  @skb: buffer to queue
+ */
+static inline void rtskb_prio_queue_tail(struct rtskb_prio_queue *prioqueue,
+					 struct rtskb *skb)
+{
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    __rtskb_prio_queue_tail(prioqueue, skb);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+}
+
+/***
+ *  __rtskb_dequeue - remove from the head of the queue (w/o locks)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *__rtskb_dequeue(struct rtskb_queue *queue)
+{
+    struct rtskb *result;
+
+    if ((result = queue->first) != NULL) {
+	queue->first = result->next;
+	result->next = NULL;
+    }
+
+    return result;
+}
+
+/***
+ *  rtskb_dequeue - remove from the head of the queue (lock protected)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *rtskb_dequeue(struct rtskb_queue *queue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    result = __rtskb_dequeue(queue);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return result;
+}
+
+/***
+ *  __rtskb_prio_dequeue - remove from the head of the prioritized queue
+ *                         (w/o locks)
+ *  @prioqueue: queue to remove from
+ */
+static inline struct rtskb *
+    __rtskb_prio_dequeue(struct rtskb_prio_queue *prioqueue)
+{
+    int prio;
+    struct rtskb *result = NULL;
+    struct rtskb_queue *sub_queue;
+
+    if (prioqueue->usage) {
+	prio      = ffz(~prioqueue->usage);
+	sub_queue = &prioqueue->queue[prio];
+	result    = __rtskb_dequeue(sub_queue);
+	if (rtskb_queue_empty(sub_queue))
+	    __change_bit(prio, &prioqueue->usage);
+    }
+
+    return result;
+}
+
+/***
+ *  rtskb_prio_dequeue - remove from the head of the prioritized queue
+ *                       (lock protected)
+ *  @prioqueue: queue to remove from
+ */
+static inline struct rtskb *
+    rtskb_prio_dequeue(struct rtskb_prio_queue *prioqueue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    result = __rtskb_prio_dequeue(prioqueue);
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+
+    return result;
+}
+
+/***
+ *  __rtskb_dequeue_chain - remove a chain from the head of the queue
+ *                          (w/o locks)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *__rtskb_dequeue_chain(struct rtskb_queue *queue)
+{
+    struct rtskb *result;
+    struct rtskb *chain_end;
+
+    if ((result = queue->first) != NULL) {
+	chain_end = result->chain_end;
+	queue->first = chain_end->next;
+	chain_end->next = NULL;
+    }
+
+    return result;
+}
+
+/***
+ *  rtskb_dequeue_chain - remove a chain from the head of the queue
+ *                        (lock protected)
+ *  @queue: queue to remove from
+ */
+static inline struct rtskb *rtskb_dequeue_chain(struct rtskb_queue *queue)
+{
+    unsigned long context;
+    struct rtskb *result;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    result = __rtskb_dequeue_chain(queue);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return result;
+}
+
+/***
+ *  rtskb_prio_dequeue_chain - remove a chain from the head of the
+ *                             prioritized queue
+ *  @prioqueue: queue to remove from
+ */
+static inline
+    struct rtskb *rtskb_prio_dequeue_chain(struct rtskb_prio_queue *prioqueue)
+{
+    unsigned long context;
+    int prio;
+    struct rtskb *result = NULL;
+    struct rtskb_queue *sub_queue;
+
+    raw_spin_lock_irqsave(&prioqueue->lock, context);
+    if (prioqueue->usage) {
+	prio      = ffz(~prioqueue->usage);
+	sub_queue = &prioqueue->queue[prio];
+	result    = __rtskb_dequeue_chain(sub_queue);
+	if (rtskb_queue_empty(sub_queue))
+	    __change_bit(prio, &prioqueue->usage);
+    }
+    raw_spin_unlock_irqrestore(&prioqueue->lock, context);
+
+    return result;
+}
+
+/***
+ *  rtskb_queue_purge - clean the queue
+ *  @queue
+ */
+static inline void rtskb_queue_purge(struct rtskb_queue *queue)
+{
+    struct rtskb *skb;
+    while ( (skb=rtskb_dequeue(queue))!=NULL )
+	kfree_rtskb(skb);
+}
+
+static inline int rtskb_headlen(const struct rtskb *skb)
+{
+    return skb->len;
+}
+
+static inline void rtskb_reserve(struct rtskb *skb, unsigned int len)
+{
+    skb->data+=len;
+    skb->tail+=len;
+}
+
+static inline unsigned char *__rtskb_put(struct rtskb *skb, unsigned int len)
+{
+    unsigned char *tmp=skb->tail;
+
+    skb->tail+=len;
+    skb->len+=len;
+    return tmp;
+}
+
+#define rtskb_put(skb, length) \
+({ \
+    struct rtskb *__rtskb = (skb); \
+    unsigned int __len = (length); \
+    unsigned char *tmp=__rtskb->tail; \
+\
+    __rtskb->tail += __len; \
+    __rtskb->len  += __len; \
+\
+    RTNET_ASSERT(__rtskb->tail <= __rtskb->buf_end, \
+	rtskb_over_panic(__rtskb, __len, current_text_addr());); \
+\
+    tmp; \
+})
+
+static inline unsigned char *__rtskb_push(struct rtskb *skb, unsigned int len)
+{
+    skb->data-=len;
+    skb->len+=len;
+    return skb->data;
+}
+
+#define rtskb_push(skb, length) \
+({ \
+    struct rtskb *__rtskb = (skb); \
+    unsigned int __len = (length); \
+\
+    __rtskb->data -= __len; \
+    __rtskb->len  += __len; \
+\
+    RTNET_ASSERT(__rtskb->data >= __rtskb->buf_start, \
+	rtskb_under_panic(__rtskb, __len, current_text_addr());); \
+\
+    __rtskb->data; \
+})
+
+static inline unsigned char *__rtskb_pull(struct rtskb *skb, unsigned int len)
+{
+    RTNET_ASSERT(len <= skb->len, return NULL;);
+
+    skb->len -= len;
+
+    return skb->data += len;
+}
+
+static inline unsigned char *rtskb_pull(struct rtskb *skb, unsigned int len)
+{
+    if (len > skb->len)
+	return NULL;
+
+    skb->len -= len;
+
+    return skb->data += len;
+}
+
+static inline void rtskb_trim(struct rtskb *skb, unsigned int len)
+{
+    if (skb->len>len) {
+	skb->len = len;
+	skb->tail = skb->data+len;
+    }
+}
+
+static inline struct rtskb *rtskb_padto(struct rtskb *rtskb, unsigned int len)
+{
+    RTNET_ASSERT(len <= (unsigned int)(rtskb->buf_end + 1 - rtskb->data),
+		 return NULL;);
+
+    memset(rtskb->data + rtskb->len, 0, len - rtskb->len);
+
+    return rtskb;
+}
+
+static inline dma_addr_t rtskb_data_dma_addr(struct rtskb *rtskb,
+					     unsigned int offset)
+{
+    return rtskb->buf_dma_addr + rtskb->data - rtskb->buf_start + offset;
+}
+
+extern struct rtskb_pool global_pool;
+
+extern unsigned int rtskb_pool_init(struct rtskb_pool *pool,
+				    unsigned int initial_size,
+				    const struct rtskb_pool_lock_ops *lock_ops,
+				    void *lock_cookie);
+
+extern unsigned int __rtskb_module_pool_init(struct rtskb_pool *pool,
+					    unsigned int initial_size,
+					    struct module *module);
+
+#define rtskb_module_pool_init(pool, size) \
+    __rtskb_module_pool_init(pool, size, THIS_MODULE)
+
+extern void rtskb_pool_release(struct rtskb_pool *pool);
+
+extern unsigned int rtskb_pool_extend(struct rtskb_pool *pool,
+				      unsigned int add_rtskbs);
+extern unsigned int rtskb_pool_shrink(struct rtskb_pool *pool,
+				      unsigned int rem_rtskbs);
+extern int rtskb_acquire(struct rtskb *rtskb, struct rtskb_pool *comp_pool);
+extern struct rtskb* rtskb_clone(struct rtskb *rtskb,
+				 struct rtskb_pool *pool);
+
+extern int rtskb_pools_init(void);
+extern void rtskb_pools_release(void);
+
+extern unsigned int rtskb_copy_and_csum_bits(const struct rtskb *skb,
+					     int offset, u8 *to, int len,
+					     unsigned int csum);
+extern void rtskb_copy_and_csum_dev(const struct rtskb *skb, u8 *to);
+
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+
+extern rtdm_lock_t rtcap_lock;
+extern void (*rtcap_handler)(struct rtskb *skb);
+
+static inline void rtcap_mark_incoming(struct rtskb *skb)
+{
+    skb->cap_start = skb->data;
+    skb->cap_len   = skb->len;
+}
+
+static inline void rtcap_report_incoming(struct rtskb *skb)
+{
+    unsigned long context;
+
+
+    raw_spin_lock_irqsave(&rtcap_lock, context);
+    if (rtcap_handler != NULL)
+	rtcap_handler(skb);
+
+    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+}
+
+static inline void rtcap_mark_rtmac_enqueue(struct rtskb *skb)
+{
+    /* rtskb start and length are probably not valid yet */
+    skb->cap_flags |= RTSKB_CAP_RTMAC_STAMP;
+    skb->cap_rtmac_stamp = rtdm_clock_read();
+}
+
+#else /* ifndef CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP */
+
+#define rtcap_mark_incoming(skb)
+#define rtcap_report_incoming(skb)
+#define rtcap_mark_rtmac_enqueue(skb)
+
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP */
+
+
+#endif /* __KERNEL__ */
+
+#endif  /* __RTSKB_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtwlan.h linux-5.4.5-new/net/rtnet/stack/include/rtwlan.h
--- linux-5.4.5/net/rtnet/stack/include/rtwlan.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtwlan.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,256 @@
+/* rtwlan.h
+ *
+ * This file is a rtnet adaption from ieee80211/ieee80211.h used by the
+ * rt2x00-2.0.0-b3 sourceforge project
+ *
+ * Merged with mainline ieee80211.h in Aug 2004.  Original ieee802_11
+ * remains copyright by the original authors
+ *
+ * Portions of the merged code are based on Host AP (software wireless
+ * LAN access point) driver for Intersil Prism2/2.5/3.
+ *
+ * Copyright (c) 2001-2002, SSH Communications Security Corp and Jouni Malinen
+ * <jkmaline@cc.hut.fi>
+ * Copyright (c) 2002-2003, Jouni Malinen <jkmaline@cc.hut.fi>
+ *
+ * Adaption to a generic IEEE 802.11 stack by James Ketrenos
+ * <jketreno@linux.intel.com>
+ * Copyright (c) 2004-2005, Intel Corporation
+ *
+ * Adaption to rtnet
+ * Copyright (c) 2006, Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef RTWLAN_H
+#define RTWLAN_H
+
+#include <linux/if_ether.h>	/* ETH_ALEN */
+#include <linux/kernel.h>	/* ARRAY_SIZE */
+
+#include <rtskb.h>
+#include <rtwlan_io.h>
+
+#define IEEE80211_1ADDR_LEN 10
+#define IEEE80211_2ADDR_LEN 16
+#define IEEE80211_3ADDR_LEN 24
+#define IEEE80211_4ADDR_LEN 30
+#define IEEE80211_FCS_LEN    4
+#define IEEE80211_HLEN			(IEEE80211_4ADDR_LEN)
+#define IEEE80211_FRAME_LEN		(IEEE80211_DATA_LEN + IEEE80211_HLEN)
+
+#define MIN_FRAG_THRESHOLD     256U
+#define	MAX_FRAG_THRESHOLD     2346U
+
+/* Frame control field constants */
+#define IEEE80211_FCTL_VERS		0x0003
+#define IEEE80211_FCTL_FTYPE		0x000c
+#define IEEE80211_FCTL_STYPE		0x00f0
+#define IEEE80211_FCTL_TODS		0x0100
+#define IEEE80211_FCTL_FROMDS		0x0200
+#define IEEE80211_FCTL_MOREFRAGS	0x0400
+#define IEEE80211_FCTL_RETRY		0x0800
+#define IEEE80211_FCTL_PM		0x1000
+#define IEEE80211_FCTL_MOREDATA		0x2000
+#define IEEE80211_FCTL_PROTECTED	0x4000
+#define IEEE80211_FCTL_ORDER		0x8000
+
+#define IEEE80211_FTYPE_MGMT		0x0000
+#define IEEE80211_FTYPE_CTL		0x0004
+#define IEEE80211_FTYPE_DATA		0x0008
+
+/* management */
+#define IEEE80211_STYPE_ASSOC_REQ	0x0000
+#define IEEE80211_STYPE_ASSOC_RESP      0x0010
+#define IEEE80211_STYPE_REASSOC_REQ	0x0020
+#define IEEE80211_STYPE_REASSOC_RESP	0x0030
+#define IEEE80211_STYPE_PROBE_REQ	0x0040
+#define IEEE80211_STYPE_PROBE_RESP	0x0050
+#define IEEE80211_STYPE_BEACON		0x0080
+#define IEEE80211_STYPE_ATIM		0x0090
+#define IEEE80211_STYPE_DISASSOC	0x00A0
+#define IEEE80211_STYPE_AUTH		0x00B0
+#define IEEE80211_STYPE_DEAUTH		0x00C0
+#define IEEE80211_STYPE_ACTION		0x00D0
+
+/* control */
+#define IEEE80211_STYPE_PSPOLL		0x00A0
+#define IEEE80211_STYPE_RTS		0x00B0
+#define IEEE80211_STYPE_CTS		0x00C0
+#define IEEE80211_STYPE_ACK		0x00D0
+#define IEEE80211_STYPE_CFEND		0x00E0
+#define IEEE80211_STYPE_CFENDACK	0x00F0
+
+/* data */
+#define IEEE80211_STYPE_DATA		0x0000
+#define IEEE80211_STYPE_DATA_CFACK	0x0010
+#define IEEE80211_STYPE_DATA_CFPOLL	0x0020
+#define IEEE80211_STYPE_DATA_CFACKPOLL	0x0030
+#define IEEE80211_STYPE_NULLFUNC	0x0040
+#define IEEE80211_STYPE_CFACK		0x0050
+#define IEEE80211_STYPE_CFPOLL		0x0060
+#define IEEE80211_STYPE_CFACKPOLL	0x0070
+#define IEEE80211_STYPE_QOS_DATA        0x0080
+
+#define RTWLAN_SCTL_SEQ		0xFFF0
+
+#define WLAN_FC_GET_VERS(fc) ((fc) & IEEE80211_FCTL_VERS)
+#define WLAN_FC_GET_TYPE(fc) ((fc) & IEEE80211_FCTL_FTYPE)
+#define WLAN_FC_GET_STYPE(fc) ((fc) & IEEE80211_FCTL_STYPE)
+
+#define IEEE80211_DSSS_RATE_1MB                 0x02
+#define IEEE80211_DSSS_RATE_2MB                 0x04
+#define IEEE80211_DSSS_RATE_5MB                 0x0B
+#define IEEE80211_DSSS_RATE_11MB                0x16
+#define IEEE80211_OFDM_RATE_6MB                 0x0C
+#define IEEE80211_OFDM_RATE_9MB                 0x12
+#define IEEE80211_OFDM_RATE_12MB		0x18
+#define IEEE80211_OFDM_RATE_18MB		0x24
+#define IEEE80211_OFDM_RATE_24MB		0x30
+#define IEEE80211_OFDM_RATE_36MB		0x48
+#define IEEE80211_OFDM_RATE_48MB		0x60
+#define IEEE80211_OFDM_RATE_54MB		0x6C
+#define IEEE80211_BASIC_RATE_MASK		0x80
+
+#define MAC_FMT "%02x:%02x:%02x:%02x:%02x:%02x"
+#define MAC_ARG(x) ((u8*)(x))[0],((u8*)(x))[1],((u8*)(x))[2],((u8*)(x))[3],((u8*)(x))[4],((u8*)(x))[5]
+
+#ifdef CONFIG_RTWLAN_DEBUG
+#define RTWLAN_DEBUG_PRINTK(__message...)	do{ rtdm_printk(__message); }while(0)
+#define RTWLAN_DEBUG(__message,__args...)	RTWLAN_DEBUG_PRINTK(KERN_DEBUG  "rtwlan->%s: Debug - " __message,__FUNCTION__,##__args);
+#else
+#define RTWLAN_DEBUG(__message...)	do{  }while(0)
+#endif
+
+struct rtwlan_stats {
+    unsigned long	rx_packets;		/* total packets received	*/
+    unsigned long	tx_packets;		/* total packets transmitted	*/
+    unsigned long       tx_retry;               /* total packets transmitted with retry */
+};
+
+struct rtwlan_device {
+
+    struct rtwlan_stats stats;
+
+    struct rtskb_pool skb_pool;
+
+    int mode;
+
+    int (*hard_start_xmit)(struct rtskb *rtskb, struct rtnet_device * rtnet_dev);
+
+    /* This must be the last item */
+    u8 priv[0];
+};
+
+/* Minimal header; can be used for passing 802.11 frames with sufficient
+ * information to determine what type of underlying data type is actually
+ * stored in the data. */
+struct ieee80211_hdr {
+    u16 frame_ctl;
+    u16 duration_id;
+    u8 payload[0];
+} __attribute__ ((packed));
+
+struct ieee80211_hdr_3addr {
+    u16 frame_ctl;
+    u16 duration_id;
+    u8 addr1[ETH_ALEN];
+    u8 addr2[ETH_ALEN];
+    u8 addr3[ETH_ALEN];
+    u16 seq_ctl;
+    u8 payload[0];
+} __attribute__ ((packed));
+
+
+static inline int ieee80211_get_hdrlen(u16 fc)
+{
+    int hdrlen = IEEE80211_3ADDR_LEN;
+    u16 stype = WLAN_FC_GET_STYPE(fc);
+
+    switch (WLAN_FC_GET_TYPE(fc)) {
+	case IEEE80211_FTYPE_DATA:
+	    if ((fc & IEEE80211_FCTL_FROMDS) && (fc & IEEE80211_FCTL_TODS))
+		hdrlen = IEEE80211_4ADDR_LEN;
+	    if (stype & IEEE80211_STYPE_QOS_DATA)
+		hdrlen += 2;
+	    break;
+
+	case IEEE80211_FTYPE_CTL:
+	    switch (WLAN_FC_GET_STYPE(fc)) {
+		case IEEE80211_STYPE_CTS:
+		case IEEE80211_STYPE_ACK:
+		    hdrlen = IEEE80211_1ADDR_LEN;
+		    break;
+
+		default:
+		    hdrlen = IEEE80211_2ADDR_LEN;
+		    break;
+	    }
+	    break;
+    }
+
+    return hdrlen;
+}
+
+
+static inline int ieee80211_is_ofdm_rate(u8 rate)
+{
+    switch (rate & ~IEEE80211_BASIC_RATE_MASK) {
+	case IEEE80211_OFDM_RATE_6MB:
+	case IEEE80211_OFDM_RATE_9MB:
+	case IEEE80211_OFDM_RATE_12MB:
+	case IEEE80211_OFDM_RATE_18MB:
+	case IEEE80211_OFDM_RATE_24MB:
+	case IEEE80211_OFDM_RATE_36MB:
+	case IEEE80211_OFDM_RATE_48MB:
+	case IEEE80211_OFDM_RATE_54MB:
+	    return 1;
+    }
+    return 0;
+}
+
+static inline int ieee80211_is_dsss_rate(u8 rate)
+{
+    switch (rate & ~IEEE80211_BASIC_RATE_MASK) {
+	case IEEE80211_DSSS_RATE_1MB:
+	case IEEE80211_DSSS_RATE_2MB:
+	case IEEE80211_DSSS_RATE_5MB:
+	case IEEE80211_DSSS_RATE_11MB:
+	    return 1;
+    }
+    return 0;
+}
+
+
+static inline void * rtwlan_priv(struct rtwlan_device *rtwlan_dev)
+{
+    return (void *)rtwlan_dev + sizeof(struct rtwlan_device);
+}
+
+struct rtnet_device * rtwlan_alloc_dev(unsigned sizeof_priv, unsigned dev_pool_size);
+int rtwlan_rx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev);
+int rtwlan_tx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTWLAN
+int __init rtwlan_init(void);
+void rtwlan_exit(void);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTWLAN */
+#define rtwlan_init()   0
+#define rtwlan_exit()
+#endif /* CONFIG_XENO_DRIVERS_NET_RTWLAN */
+
+#endif
diff -Nur linux-5.4.5/net/rtnet/stack/include/rtwlan_io.h linux-5.4.5-new/net/rtnet/stack/include/rtwlan_io.h
--- linux-5.4.5/net/rtnet/stack/include/rtwlan_io.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/rtwlan_io.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,120 @@
+/* rtwlan_io.h
+ *
+ * Copyright (C) 2006      Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef RTWLAN_IO
+#define RTWLAN_IO
+
+#include <rtnet_chrdev.h>
+
+#define RTWLAN_TXMODE_RAW 0
+#define RTWLAN_TXMODE_ACK 1
+#define RTWLAN_TXMODE_MCAST 2
+
+#define ENORTWLANDEV 0xff08
+
+struct rtwlan_cmd {
+
+    struct rtnet_ioctl_head head;
+
+    union {
+
+        struct {
+            unsigned int    bitrate;
+            unsigned int    channel;
+            unsigned int    retry;
+            unsigned int    txpower;
+            unsigned int    mode;
+            unsigned int    autoresponder;
+            unsigned int    dropbcast;
+            unsigned int    dropmcast;
+            unsigned int    bbpsens;
+        }set;
+
+        struct {
+            unsigned int    address;
+            unsigned int    value;
+        }reg;
+
+        struct {
+            int            ifindex;
+            unsigned int   flags;
+            unsigned int   bitrate;
+            unsigned int   channel;
+            unsigned int   retry;
+            unsigned int   txpower;
+            unsigned int   bbpsens; 
+            unsigned int   mode;
+            unsigned int   autoresponder;
+            unsigned int   dropbcast;
+            unsigned int   dropmcast;
+            unsigned int   rx_packets;
+            unsigned int   tx_packets;
+            unsigned int   tx_retry;
+        }info;
+    }args;
+};
+
+#define RTNET_IOC_TYPE_RTWLAN 8
+
+#define IOC_RTWLAN_IFINFO    _IOWR(RTNET_IOC_TYPE_RTWLAN,	\
+                                   0 | RTNET_IOC_NODEV_PARAM,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BITRATE   _IOWR(RTNET_IOC_TYPE_RTWLAN, 1,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_CHANNEL   _IOWR(RTNET_IOC_TYPE_RTWLAN, 2,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_TXPOWER   _IOWR(RTNET_IOC_TYPE_RTWLAN, 3,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_RETRY     _IOWR(RTNET_IOC_TYPE_RTWLAN, 4,    \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_TXMODE      _IOWR(RTNET_IOC_TYPE_RTWLAN, 5,    \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_DROPBCAST _IOWR(RTNET_IOC_TYPE_RTWLAN, 6,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_DROPMCAST _IOWR(RTNET_IOC_TYPE_RTWLAN, 7,    \
+				   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_REGREAD   _IOWR(RTNET_IOC_TYPE_RTWLAN, 8,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_REGWRITE  _IOWR(RTNET_IOC_TYPE_RTWLAN, 9,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPWRITE  _IOWR(RTNET_IOC_TYPE_RTWLAN, 10,	\
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPREAD   _IOWR(RTNET_IOC_TYPE_RTWLAN, 11,   \
+				   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_BBPSENS   _IOWR(RTNET_IOC_TYPE_RTWLAN, 12,   \
+                                   struct rtwlan_cmd)
+
+#define IOC_RTWLAN_AUTORESP  _IOWR(RTNET_IOC_TYPE_RTWLAN, 13,   \
+                                   struct rtwlan_cmd)
+
+
+#endif
diff -Nur linux-5.4.5/net/rtnet/stack/include/stack_mgr.h linux-5.4.5-new/net/rtnet/stack/include/stack_mgr.h
--- linux-5.4.5/net/rtnet/stack/include/stack_mgr.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/stack_mgr.h	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,106 @@
+/***
+ *
+ *  stack_mgr.h
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2002      Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __STACK_MGR_H_
+#define __STACK_MGR_H_
+
+#ifdef __KERNEL__
+
+#include <linux/list.h>
+
+#include <rtnet_internal.h>
+#include <rtdev.h>
+
+
+/***
+ * network layer protocol (layer 3)
+ */
+
+#define RTPACKET_HASH_TBL_SIZE  64
+#define RTPACKET_HASH_KEY_MASK  (RTPACKET_HASH_TBL_SIZE-1)
+
+struct rtpacket_type {
+    struct list_head    list_entry;
+
+    unsigned short      type;
+    short               refcount;
+
+    int                 (*handler)(struct rtskb *, struct rtpacket_type *);
+    int                 (*err_handler)(struct rtskb *, struct rtnet_device *,
+                                       struct rtpacket_type *);
+    bool                (*trylock)(struct rtpacket_type *);
+    void                (*unlock)(struct rtpacket_type *);
+
+    struct module	*owner;
+};
+
+
+int __rtdev_add_pack(struct rtpacket_type *pt, struct module *module);
+#define rtdev_add_pack(pt) \
+    __rtdev_add_pack(pt, THIS_MODULE)
+
+void rtdev_remove_pack(struct rtpacket_type *pt);
+
+static inline bool rtdev_lock_pack(struct rtpacket_type *pt)
+{
+    return try_module_get(pt->owner);
+}
+
+static inline void rtdev_unlock_pack(struct rtpacket_type *pt)
+{
+    module_put(pt->owner);
+}
+
+void rt_stack_connect(struct rtnet_device *rtdev, struct rtnet_mgr *mgr);
+void rt_stack_disconnect(struct rtnet_device *rtdev);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK)
+void rt_stack_deliver(struct rtskb *rtskb);
+#endif /* CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK */
+
+int rt_stack_mgr_init(struct rtnet_mgr *mgr);
+void rt_stack_mgr_delete(struct rtnet_mgr *mgr);
+
+void rtnetif_rx(struct rtskb *skb);
+
+static inline void rtnetif_tx(struct rtnet_device *rtdev)
+{
+}
+
+static inline void rt_mark_stack_mgr(struct rtnet_device *rtdev)
+{
+    if (rtdev->stack_event->condition == 1) {
+	/* warn */
+	trace_printk("rtdev->stack_event->condition already true\n");
+    }
+    rtdm_event_signal(rtdev->stack_event);
+}
+
+#endif /* __KERNEL__ */
+
+#endif  /* __STACK_MGR_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/tdma_chrdev.h linux-5.4.5-new/net/rtnet/stack/include/tdma_chrdev.h
--- linux-5.4.5/net/rtnet/stack/include/tdma_chrdev.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/tdma_chrdev.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,88 @@
+/***
+ *
+ *  include/tdma_chrdev.h
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __TDMA_CHRDEV_H_
+#define __TDMA_CHRDEV_H_
+
+#ifndef __KERNEL__
+# include <inttypes.h>
+#endif
+
+#include <rtnet_chrdev.h>
+
+
+#define MIN_SLOT_SIZE       60
+
+
+struct tdma_config {
+    struct rtnet_ioctl_head head;
+
+    union {
+        struct {
+            __u64       cycle_period;
+            __u64       backup_sync_offset;
+            __u32       cal_rounds;
+            __u32       max_cal_requests;
+            __u32       max_slot_id;
+        } master;
+
+        struct {
+            __u32       cal_rounds;
+            __u32       max_slot_id;
+        } slave;
+
+        struct {
+            __s32       id;
+            __u32       period;
+            __u64       offset;
+            __u32       phasing;
+            __u32       size;
+            __s32       joint_slot;
+            __u32       cal_timeout;
+            __u64       *cal_results;
+        } set_slot;
+
+        struct {
+            __s32       id;
+        } remove_slot;
+
+        __u64 __padding[8];
+    } args;
+};
+
+
+#define TDMA_IOC_MASTER                 _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 0, \
+                                             struct tdma_config)
+#define TDMA_IOC_SLAVE                  _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 1, \
+                                             struct tdma_config)
+#define TDMA_IOC_CAL_RESULT_SIZE        _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 2, \
+                                             struct tdma_config)
+#define TDMA_IOC_SET_SLOT               _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 3, \
+                                             struct tdma_config)
+#define TDMA_IOC_REMOVE_SLOT            _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 4, \
+                                             struct tdma_config)
+#define TDMA_IOC_DETACH                 _IOW(RTNET_IOC_TYPE_RTMAC_TDMA, 5, \
+                                             struct tdma_config)
+
+#endif /* __TDMA_CHRDEV_H_ */
diff -Nur linux-5.4.5/net/rtnet/stack/include/uapi_rtdm.h linux-5.4.5-new/net/rtnet/stack/include/uapi_rtdm.h
--- linux-5.4.5/net/rtnet/stack/include/uapi_rtdm.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/include/uapi_rtdm.h	2020-06-15 16:12:31.571695222 +0300
@@ -0,0 +1,206 @@
+/**
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * @file
+ * Real-Time Driver Model for Xenomai, user API header.
+ *
+ * @note Copyright (C) 2005, 2006 Jan Kiszka <jan.kiszka@web.de>
+ * @note Copyright (C) 2005 Joerg Langenberg <joerg.langenberg@gmx.net>
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
+ * @ingroup rtdm_user_api
+ */
+#ifndef _RTDM_UAPI_RTDM_H
+#define _RTDM_UAPI_RTDM_H
+
+/*!
+ * @addtogroup rtdm
+ * @{
+ */
+
+/*!
+ * @anchor rtdm_api_versioning @name API Versioning
+ * @{ */
+/** Common user and driver API version */
+#define RTDM_API_VER			9
+
+/** Minimum API revision compatible with the current release */
+#define RTDM_API_MIN_COMPAT_VER		9
+/** @} API Versioning */
+
+/** RTDM type for representing absolute dates. Its base type is a 64 bit
+ *  unsigned integer. The unit is 1 nanosecond. */
+typedef uint64_t nanosecs_abs_t;
+
+/** RTDM type for representing relative intervals. Its base type is a 64 bit
+ *  signed integer. The unit is 1 nanosecond. Relative intervals can also
+ *  encode the special timeouts "infinite" and "non-blocking", see
+ *  @ref RTDM_TIMEOUT_xxx. */
+typedef int64_t nanosecs_rel_t;
+
+/*!
+ * @anchor RTDM_TIMEOUT_xxx @name RTDM_TIMEOUT_xxx
+ * Special timeout values
+ * @{ */
+/** Block forever. */
+#define RTDM_TIMEOUT_INFINITE		0
+
+/** Any negative timeout means non-blocking. */
+#define RTDM_TIMEOUT_NONE		(-1)
+/** @} RTDM_TIMEOUT_xxx */
+/** @} rtdm */
+
+/*!
+ * @addtogroup rtdm_profiles
+ * @{
+ */
+
+/*!
+ * @anchor RTDM_CLASS_xxx   @name RTDM_CLASS_xxx
+ * Device classes
+ * @{ */
+#define RTDM_CLASS_PARPORT		1
+#define RTDM_CLASS_SERIAL		2
+#define RTDM_CLASS_CAN			3
+#define RTDM_CLASS_NETWORK		4
+#define RTDM_CLASS_RTMAC		5
+#define RTDM_CLASS_TESTING		6
+#define RTDM_CLASS_RTIPC		7
+#define RTDM_CLASS_COBALT		8
+#define RTDM_CLASS_UDD			9
+#define RTDM_CLASS_MEMORY		10
+#define RTDM_CLASS_GPIO			11
+#define RTDM_CLASS_SPI			12
+
+#define RTDM_CLASS_MISC			223
+#define RTDM_CLASS_EXPERIMENTAL		224
+#define RTDM_CLASS_MAX			255
+/** @} RTDM_CLASS_xxx */
+
+#define RTDM_SUBCLASS_GENERIC		(-1)
+
+#define RTIOC_TYPE_COMMON		0
+
+/*!
+ * @anchor device_naming    @name Device Naming
+ * Maximum length of device names (excluding the final null character)
+ * @{
+ */
+#define RTDM_MAX_DEVNAME_LEN		31
+/** @} Device Naming */
+
+/**
+ * Device information
+ */
+typedef struct rtdm_device_info {
+	/** Device flags, see @ref dev_flags "Device Flags" for details */
+	int device_flags;
+
+	/** Device class ID, see @ref RTDM_CLASS_xxx */
+	int device_class;
+
+	/** Device sub-class, either RTDM_SUBCLASS_GENERIC or a
+	 *  RTDM_SUBCLASS_xxx definition of the related @ref rtdm_profiles
+	 *  "Device Profile" */
+	int device_sub_class;
+
+	/** Supported device profile version */
+	int profile_version;
+} rtdm_device_info_t;
+
+/*!
+ * @anchor RTDM_PURGE_xxx_BUFFER    @name RTDM_PURGE_xxx_BUFFER
+ * Flags selecting buffers to be purged
+ * @{ */
+#define RTDM_PURGE_RX_BUFFER		0x0001
+#define RTDM_PURGE_TX_BUFFER		0x0002
+/** @} RTDM_PURGE_xxx_BUFFER*/
+
+/*!
+ * @anchor common_IOCTLs    @name Common IOCTLs
+ * The following IOCTLs are common to all device rtdm_profiles.
+ * @{
+ */
+
+/**
+ * Retrieve information about a device or socket.
+ * @param[out] arg Pointer to information buffer (struct rtdm_device_info)
+ */
+#define RTIOC_DEVICE_INFO \
+	_IOR(RTIOC_TYPE_COMMON, 0x00, struct rtdm_device_info)
+
+/**
+ * Purge internal device or socket buffers.
+ * @param[in] arg Purge mask, see @ref RTDM_PURGE_xxx_BUFFER
+ */
+#define RTIOC_PURGE		_IOW(RTIOC_TYPE_COMMON, 0x10, int)
+/** @} Common IOCTLs */
+/** @} rtdm */
+
+/* Internally used for mapping socket functions on IOCTLs */
+struct _rtdm_getsockopt_args {
+	int level;
+	int optname;
+	void *optval;
+	socklen_t *optlen;
+};
+
+struct _rtdm_setsockopt_args {
+	int level;
+	int optname;
+	const void *optval;
+	socklen_t optlen;
+};
+
+struct _rtdm_getsockaddr_args {
+	struct sockaddr *addr;
+	socklen_t *addrlen;
+};
+
+struct _rtdm_setsockaddr_args {
+	const struct sockaddr *addr;
+	socklen_t addrlen;
+};
+
+#define _RTIOC_GETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x20,		\
+				     struct _rtdm_getsockopt_args)
+#define _RTIOC_SETSOCKOPT	_IOW(RTIOC_TYPE_COMMON, 0x21,		\
+				     struct _rtdm_setsockopt_args)
+#define _RTIOC_BIND		_IOW(RTIOC_TYPE_COMMON, 0x22,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_CONNECT		_IOW(RTIOC_TYPE_COMMON, 0x23,		\
+				     struct _rtdm_setsockaddr_args)
+#define _RTIOC_LISTEN		_IOW(RTIOC_TYPE_COMMON, 0x24,		\
+				     int)
+#define _RTIOC_ACCEPT		_IOW(RTIOC_TYPE_COMMON, 0x25,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETSOCKNAME	_IOW(RTIOC_TYPE_COMMON, 0x26,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_GETPEERNAME	_IOW(RTIOC_TYPE_COMMON, 0x27,		\
+				     struct _rtdm_getsockaddr_args)
+#define _RTIOC_SHUTDOWN		_IOW(RTIOC_TYPE_COMMON, 0x28,		\
+				     int)
+
+/* Internally used for mmap() */
+struct _rtdm_mmap_request {
+	__u64 offset;
+	size_t length;
+	int prot;
+	int flags;
+};
+
+#endif /* !_RTDM_UAPI_RTDM_H */
diff -Nur linux-5.4.5/net/rtnet/stack/iovec.c linux-5.4.5-new/net/rtnet/stack/iovec.c
--- linux-5.4.5/net/rtnet/stack/iovec.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/iovec.c	2020-06-15 16:12:31.495695490 +0300
@@ -0,0 +1,109 @@
+/***
+ *
+ *  stack/iovec.c
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 1999,2000 Zentropic Computing, LLC
+ *                2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *  
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+
+#include <rtnet_rtdm.h>
+
+ssize_t rtnet_write_to_iov(struct rtsocket *sock,
+			   struct iovec *iov, int iovlen,
+			   const void *data, size_t len, int msg_in_userspace)
+{
+	ssize_t ret = 0;
+	size_t nbytes;
+	int n;
+
+	for (n = 0; len > 0 && n < iovlen; n++, iov++) {
+		if (iov->iov_len == 0)
+			continue;
+
+		nbytes = iov->iov_len;
+		if (nbytes > len)
+			nbytes = len;
+
+		ret = rtnet_put_arg(sock, iov->iov_base, data, nbytes, msg_in_userspace);
+		if (ret)
+			break;
+	
+		len -= nbytes;
+		data += nbytes;
+		iov->iov_len -= nbytes;
+		iov->iov_base += nbytes;
+		ret += nbytes;
+		if (ret < 0) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rtnet_write_to_iov);
+
+ssize_t rtnet_read_from_iov(struct rtsocket *sock,
+			    struct iovec *iov, int iovlen,
+			    void *data, size_t len, int msg_in_userspace)
+{
+	ssize_t ret = 0;
+	size_t nbytes;
+	int n;
+
+	for (n = 0; len > 0 && n < iovlen; n++, iov++) {
+		if (iov->iov_len == 0)
+			continue;
+
+		nbytes = iov->iov_len;
+		if (nbytes > len)
+			nbytes = len;
+
+		if (!msg_in_userspace)
+			memcpy(data, iov->iov_base, nbytes);
+		else {
+			ret = copy_from_user(data, iov->iov_base, nbytes);
+			if (ret)
+				break;
+		}
+	
+		len -= nbytes;
+		data += nbytes;
+		iov->iov_len -= nbytes;
+		iov->iov_base += nbytes;
+		ret += nbytes;
+		if (ret < 0) {
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rtnet_read_from_iov);
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/af_inet.c linux-5.4.5-new/net/rtnet/stack/ipv4/af_inet.c
--- linux-5.4.5/net/rtnet/stack/ipv4/af_inet.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/af_inet.c	2020-06-15 16:12:31.535695349 +0300
@@ -0,0 +1,367 @@
+/***
+ *
+ *  ipv4/af_inet.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/uaccess.h>
+
+#include <ipv4_chrdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_rtpc.h>
+#include <ipv4/arp.h>
+#include <ipv4/icmp.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+
+MODULE_LICENSE("GPL");
+
+struct route_solicit_params {
+    struct rtnet_device *rtdev;
+    __u32               ip_addr;
+};
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct xnvfile_directory ipv4_proc_root;
+EXPORT_SYMBOL_GPL(ipv4_proc_root);
+#endif
+
+
+static int route_solicit_handler(struct rt_proc_call *call)
+{
+    struct route_solicit_params *param;
+    struct rtnet_device         *rtdev;
+
+
+    param = rtpc_get_priv(call, struct route_solicit_params);
+    rtdev = param->rtdev;
+
+    if ((rtdev->flags & IFF_UP) == 0)
+	return -ENODEV;
+
+    rt_arp_solicit(rtdev, param->ip_addr);
+
+    return 0;
+}
+
+
+
+static void cleanup_route_solicit(void *priv_data)
+{
+    struct route_solicit_params *param;
+
+
+    param = (struct route_solicit_params *)priv_data;
+    rtdev_dereference(param->rtdev);
+}
+
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP
+static int ping_handler(struct rt_proc_call *call)
+{
+    struct ipv4_cmd *cmd;
+    int             err;
+
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+
+    rt_icmp_queue_echo_request(call);
+
+    err = rt_icmp_send_echo(cmd->args.ping.ip_addr, cmd->args.ping.id,
+			    cmd->args.ping.sequence, cmd->args.ping.msg_size);
+    if (err < 0) {
+	rt_icmp_dequeue_echo_request(call);
+	return err;
+    }
+
+    return -CALL_PENDING;
+}
+
+
+
+static void ping_complete_handler(struct rt_proc_call *call, void *priv_data)
+{
+    struct ipv4_cmd *cmd;
+    struct ipv4_cmd *usr_cmd = (struct ipv4_cmd *)priv_data;
+
+
+    if (rtpc_get_result(call) < 0)
+	return;
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+    usr_cmd->args.ping.ip_addr = cmd->args.ping.ip_addr;
+    usr_cmd->args.ping.rtt     = cmd->args.ping.rtt;
+}
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP */
+
+
+
+static int ipv4_ioctl(struct rtnet_device *rtdev, unsigned int request,
+		      unsigned long arg)
+{
+    struct ipv4_cmd             cmd;
+    struct route_solicit_params params;
+    int                         ret;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+	return -EFAULT;
+
+    switch (request) {
+	case IOC_RT_HOST_ROUTE_ADD:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    ret = rt_ip_route_add_host(cmd.args.addhost.ip_addr,
+				       cmd.args.addhost.dev_addr, rtdev);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_SOLICIT:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    if (!rtdev_reference(rtdev)) {
+                mutex_unlock(&rtdev->nrt_lock);
+                return -EIDRM;
+            }
+
+	    params.rtdev   = rtdev;
+	    params.ip_addr = cmd.args.solicit.ip_addr;
+
+	    /* We need the rtpc wrapping because rt_arp_solicit can block on a
+	     * real-time lock in the NIC's xmit routine. */
+	    ret = rtpc_dispatch_call(route_solicit_handler, 0, &params,
+				     sizeof(params), NULL,
+				     cleanup_route_solicit);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_DELETE:
+	case IOC_RT_HOST_ROUTE_DELETE_DEV:
+	    ret = rt_ip_route_del_host(cmd.args.delhost.ip_addr, rtdev);
+	    break;
+
+	case IOC_RT_HOST_ROUTE_GET:
+	case IOC_RT_HOST_ROUTE_GET_DEV:
+	    ret = rt_ip_route_get_host(cmd.args.gethost.ip_addr,
+				       cmd.head.if_name,
+				       cmd.args.gethost.dev_addr, rtdev);
+	    if (ret >= 0) {
+		if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		    ret = -EFAULT;
+	    }
+	    break;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+	case IOC_RT_NET_ROUTE_ADD:
+	    ret = rt_ip_route_add_net(cmd.args.addnet.net_addr,
+				      cmd.args.addnet.net_mask,
+				      cmd.args.addnet.gw_addr);
+	    break;
+
+	case IOC_RT_NET_ROUTE_DELETE:
+	    ret = rt_ip_route_del_net(cmd.args.delnet.net_addr,
+				      cmd.args.delnet.net_mask);
+	    break;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP
+	case IOC_RT_PING:
+	    ret = rtpc_dispatch_call(ping_handler, cmd.args.ping.timeout, &cmd,
+				     sizeof(cmd), ping_complete_handler, NULL);
+	    if (ret >= 0) {
+		if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		    ret = -EFAULT;
+	    }
+	    if (ret < 0)
+		rt_icmp_cleanup_echo_requests();
+	    break;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP */
+
+	default:
+	    ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+unsigned long rt_inet_aton(const char *ip)
+{
+    int p, n, c;
+    union { unsigned long l; char c[4]; } u;
+    p = n = 0;
+    while ((c = *ip++)) {
+	if (c != '.') {
+	    n = n*10 + c-'0';
+	} else {
+	    if (n > 0xFF) {
+		return 0;
+	    }
+	    u.c[p++] = n;
+	    n = 0;
+	}
+    }
+    u.c[3] = n;
+    return u.l;
+}
+
+
+
+static void rt_ip_ifup(struct rtnet_device *rtdev,
+		       struct rtnet_core_cmd *up_cmd)
+{
+    struct rtnet_device *tmp;
+    int                 i;
+
+
+    rt_ip_route_del_all(rtdev); /* cleanup routing table */
+
+    if (up_cmd->args.up.ip_addr != 0xFFFFFFFF) {
+	rtdev->local_ip     = up_cmd->args.up.ip_addr;
+	rtdev->broadcast_ip = up_cmd->args.up.broadcast_ip;
+    }
+
+    if (rtdev->local_ip != 0) {
+	if (rtdev->flags & IFF_LOOPBACK) {
+	    for (i = 0; i < MAX_RT_DEVICES; i++)
+		if ((tmp = rtdev_get_by_index(i)) != NULL) {
+		    rt_ip_route_add_host(tmp->local_ip,
+					 rtdev->dev_addr, rtdev);
+		    rtdev_dereference(tmp);
+		}
+	} else if ((tmp = rtdev_get_loopback()) != NULL) {
+	    rt_ip_route_add_host(rtdev->local_ip,
+				 tmp->dev_addr, tmp);
+	    rtdev_dereference(tmp);
+	}
+
+	if (rtdev->flags & IFF_BROADCAST)
+	    rt_ip_route_add_host(up_cmd->args.up.broadcast_ip,
+				 rtdev->broadcast, rtdev);
+    }
+}
+
+
+
+static void rt_ip_ifdown(struct rtnet_device *rtdev)
+{
+    rt_ip_route_del_all(rtdev);
+}
+
+
+
+static struct rtdev_event_hook  rtdev_hook = {
+    .unregister_device = rt_ip_ifdown,
+    .ifup =              rt_ip_ifup,
+    .ifdown =            rt_ip_ifdown
+};
+
+static struct rtnet_ioctls ipv4_ioctls = {
+    .service_name =     "IPv4",
+    .ioctl_type =       RTNET_IOC_TYPE_IPV4,
+    .handler =          ipv4_ioctl
+};
+
+
+static int __init rt_ipv4_proto_init(void)
+{
+    int i;
+    int result;
+
+
+    /* Network-Layer */
+    rt_ip_init();
+    rt_arp_init();
+
+    /* Transport-Layer */
+    for (i=0; i<MAX_RT_INET_PROTOCOLS; i++)
+	rt_inet_protocols[i]=NULL;
+
+    rt_icmp_init();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    result = xnvfile_init_dir("ipv4", &ipv4_proc_root, &rtnet_proc_root);
+    if (result < 0)
+	goto err1;
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    if ((result = rt_ip_routing_init()) < 0)
+	goto err2;
+    if ((result = rtnet_register_ioctls(&ipv4_ioctls)) < 0)
+	goto err3;
+
+    rtdev_add_event_hook(&rtdev_hook);
+
+    return 0;
+
+  err3:
+    rt_ip_routing_release();
+
+  err2:
+#ifdef CONFIG_XENO_OPT_VFILE
+    xnvfile_destroy_dir(&ipv4_proc_root);
+  err1:
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    rt_icmp_release();
+    rt_arp_release();
+    rt_ip_release();
+
+    return result;
+}
+
+
+static void __exit rt_ipv4_proto_release(void)
+{
+    rt_ip_release();
+
+    rtdev_del_event_hook(&rtdev_hook);
+    rtnet_unregister_ioctls(&ipv4_ioctls);
+    rt_ip_routing_release();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    xnvfile_destroy_dir(&ipv4_proc_root);
+#endif
+
+    /* Transport-Layer */
+    rt_icmp_release();
+
+    /* Network-Layer */
+    rt_arp_release();
+}
+
+
+module_init(rt_ipv4_proto_init);
+module_exit(rt_ipv4_proto_release);
+
+
+EXPORT_SYMBOL_GPL(rt_inet_aton);
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/arp.c linux-5.4.5-new/net/rtnet/stack/ipv4/arp.c
--- linux-5.4.5/net/rtnet/stack/ipv4/arp.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/arp.c	2020-06-15 16:12:31.535695349 +0300
@@ -0,0 +1,220 @@
+/***
+ *
+ *  ipv4/arp.h - Adress Resolution Protocol for RTnet
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtdev.h>
+#include <stack_mgr.h>
+#include <ipv4/arp.h>
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+#include <ipv4/ip_input.h>
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP */
+
+/***
+ *  arp_send:   Create and send an arp packet. If (dest_hw == NULL),
+ *              we create a broadcast message.
+ */
+void rt_arp_send(int type, int ptype, u32 dest_ip, struct rtnet_device *rtdev,
+		 u32 src_ip, unsigned char *dest_hw, unsigned char *src_hw,
+		 unsigned char *target_hw)
+{
+    struct rtskb *skb;
+    struct arphdr *arp;
+    unsigned char *arp_ptr;
+
+    if (rtdev->flags & IFF_NOARP)
+	return;
+
+    if (!(skb=alloc_rtskb(sizeof(struct arphdr) + 2*(rtdev->addr_len+4) +
+			   rtdev->hard_header_len+15, &global_pool)))
+	return;
+
+    rtskb_reserve(skb, (rtdev->hard_header_len+15)&~15);
+
+    skb->nh.raw = skb->data;
+    arp = (struct arphdr *)rtskb_put(skb, sizeof(struct arphdr) +
+				     2*(rtdev->addr_len+4));
+
+    skb->rtdev = rtdev;
+    skb->protocol = __constant_htons (ETH_P_ARP);
+    skb->priority = RT_ARP_SKB_PRIO;
+    if (src_hw == NULL)
+	src_hw = rtdev->dev_addr;
+    if (dest_hw == NULL)
+	dest_hw = rtdev->broadcast;
+
+    /*
+     *  Fill the device header for the ARP frame
+     */
+    if (rtdev->hard_header &&
+	(rtdev->hard_header(skb,rtdev,ptype,dest_hw,src_hw,skb->len) < 0))
+	goto out;
+
+    arp->ar_hrd = htons(rtdev->type);
+    arp->ar_pro = __constant_htons(ETH_P_IP);
+    arp->ar_hln = rtdev->addr_len;
+    arp->ar_pln = 4;
+    arp->ar_op = htons(type);
+
+    arp_ptr=(unsigned char *)(arp+1);
+
+    memcpy(arp_ptr, src_hw, rtdev->addr_len);
+    arp_ptr+=rtdev->addr_len;
+
+    memcpy(arp_ptr, &src_ip,4);
+    arp_ptr+=4;
+
+    if (target_hw != NULL)
+	memcpy(arp_ptr, target_hw, rtdev->addr_len);
+    else
+	memset(arp_ptr, 0, rtdev->addr_len);
+    arp_ptr+=rtdev->addr_len;
+
+    memcpy(arp_ptr, &dest_ip, 4);
+
+
+    /* send the frame */
+    rtdev_xmit(skb);
+
+    return;
+
+  out:
+    kfree_rtskb(skb);
+}
+
+
+
+/***
+ *  arp_rcv:    Receive an arp request by the device layer.
+ */
+int rt_arp_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct rtnet_device *rtdev = skb->rtdev;
+    struct arphdr       *arp = skb->nh.arph;
+    unsigned char       *arp_ptr= (unsigned char *)(arp+1);
+    unsigned char       *sha;
+    u32                 sip, tip;
+    u16                 dev_type = rtdev->type;
+
+    /*
+     *  The hardware length of the packet should match the hardware length
+     *  of the device.  Similarly, the hardware types should match.  The
+     *  device should be ARP-able.  Also, if pln is not 4, then the lookup
+     *  is not from an IP number.  We can't currently handle this, so toss
+     *  it.
+     */
+    if ((arp->ar_hln != rtdev->addr_len) ||
+	(rtdev->flags & IFF_NOARP) ||
+	(skb->pkt_type == PACKET_OTHERHOST) ||
+	(skb->pkt_type == PACKET_LOOPBACK) ||
+	(arp->ar_pln != 4))
+	goto out;
+
+    switch (dev_type) {
+	default:
+	    if ((arp->ar_pro != __constant_htons(ETH_P_IP)) &&
+		(htons(dev_type) != arp->ar_hrd))
+		goto out;
+	    break;
+	case ARPHRD_ETHER:
+	    /*
+	     * ETHERNET devices will accept ARP hardware types of either
+	     * 1 (Ethernet) or 6 (IEEE 802.2).
+	     */
+	    if ((arp->ar_hrd != __constant_htons(ARPHRD_ETHER)) &&
+		(arp->ar_hrd != __constant_htons(ARPHRD_IEEE802))) {
+		goto out;
+	    }
+	    if (arp->ar_pro != __constant_htons(ETH_P_IP)) {
+		goto out;
+	    }
+	    break;
+    }
+
+    /* Understand only these message types */
+    if ((arp->ar_op != __constant_htons(ARPOP_REPLY)) &&
+	(arp->ar_op != __constant_htons(ARPOP_REQUEST)))
+	goto out;
+
+    /*
+     *  Extract fields
+     */
+    sha=arp_ptr;
+    arp_ptr += rtdev->addr_len;
+    memcpy(&sip, arp_ptr, 4);
+
+    arp_ptr += 4;
+    arp_ptr += rtdev->addr_len;
+    memcpy(&tip, arp_ptr, 4);
+
+    /* process only requests/replies directed to us */
+    if (tip == rtdev->local_ip) {
+	rt_ip_route_add_host(sip, sha, rtdev);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+	if (!rt_ip_fallback_handler)
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP */
+		if (arp->ar_op == __constant_htons(ARPOP_REQUEST)) {
+			rt_arp_send(ARPOP_REPLY, ETH_P_ARP, sip, rtdev, tip, sha,
+				rtdev->dev_addr, sha);
+			goto out1;
+		}
+    }
+
+out:
+#ifdef CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP
+    if (rt_ip_fallback_handler) {
+	    rt_ip_fallback_handler(skb);
+	    return 0;
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY_ARP */
+out1:
+    kfree_rtskb(skb);
+    return 0;
+}
+
+
+
+static struct rtpacket_type arp_packet_type = {
+    type:       __constant_htons(ETH_P_ARP),
+    handler:    &rt_arp_rcv
+};
+
+
+
+/***
+ *  rt_arp_init
+ */
+void __init rt_arp_init(void)
+{
+    rtdev_add_pack(&arp_packet_type);
+}
+
+
+
+/***
+ *  rt_arp_release
+ */
+void rt_arp_release(void)
+{
+    rtdev_remove_pack(&arp_packet_type);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/icmp.c linux-5.4.5-new/net/rtnet/stack/ipv4/icmp.c
--- linux-5.4.5/net/rtnet/stack/ipv4/icmp.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/icmp.c	2020-06-15 16:12:31.551695293 +0300
@@ -0,0 +1,562 @@
+/***
+ *
+ *  ipv4/icmp.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2002       Vinay Sridhara <vinaysridhara@yahoo.com>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/icmp.h>
+#include <net/checksum.h>
+
+#include <rtskb.h>
+#include <rtnet_socket.h>
+#include <ipv4_chrdev.h>
+#include <ipv4/icmp.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+
+/***
+ * Structure for sending the icmp packets
+ */
+struct icmp_bxm
+{
+    unsigned int        csum;
+    size_t              head_len;
+    size_t              data_len;
+    off_t               offset;
+    struct {
+	struct icmphdr  icmph;
+	ktime_t  timestamp;
+    } head;
+    union {
+	struct rtskb    *skb;
+	void            *buf;
+    } data;
+};
+
+struct rt_icmp_control
+{
+    void    (*handler)(struct rtskb *skb);
+    short   error;      /* This ICMP is classed as an error message */
+};
+
+
+raw_spinlock_t echo_calls_lock;
+LIST_HEAD(echo_calls);
+
+#if 0
+static struct {
+    /*
+     * Scratch pad, provided so that rt_socket_dereference(&icmp_socket);
+     * remains legal.
+     */
+    struct rtdm_dev_context dummy;
+
+    /*
+     *  Socket for icmp replies
+     *  It is not part of the socket pool. It may furthermore be used
+     *  concurrently by multiple tasks because all fields are static excect
+     *  skb_pool, but that one is spinlock protected.
+     */
+    struct rtsocket socket;
+} icmp_socket_container;
+#endif
+
+struct rtsocket icmp_rtsocket;
+
+//#define icmp_fd		(&icmp_socket_container.dummy.fd)
+//#define icmp_socket     ((struct rtsocket *)rtdm_fd_to_private(icmp_fd))
+#define icmp_socket (&icmp_rtsocket)
+
+
+void rt_icmp_queue_echo_request(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    list_add_tail(&call->list_entry, &echo_calls);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+}
+
+
+
+void rt_icmp_dequeue_echo_request(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    list_del(&call->list_entry);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+}
+
+
+
+void rt_icmp_cleanup_echo_requests(void)
+{
+    unsigned long      context;
+    struct list_head    *entry;
+    struct list_head    *next;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+    entry = echo_calls.next;
+    INIT_LIST_HEAD(&echo_calls);
+    raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+
+    while (entry != &echo_calls) {
+	next = entry->next;
+	rtpc_complete_call_nrt((struct rt_proc_call *)entry, -EINTR);
+	entry = next;
+    }
+
+    /* purge any pending ICMP fragments */
+    rt_ip_frag_invalidate_socket(icmp_socket);
+}
+
+
+
+/***
+ *  rt_icmp_discard - dummy function
+ */
+static void rt_icmp_discard(struct rtskb *skb)
+{
+}
+
+
+
+static int rt_icmp_glue_reply_bits(const void *p, unsigned char *to,
+				   unsigned int offset, unsigned int fraglen)
+{
+    struct icmp_bxm *icmp_param = (struct icmp_bxm *)p;
+    struct icmphdr  *icmph;
+    unsigned long   csum;
+
+
+    /* TODO: add support for fragmented ICMP packets */
+    if (offset != 0)
+	return -EMSGSIZE;
+
+    csum = csum_partial_copy_nocheck((void *)&icmp_param->head, to,
+				     icmp_param->head_len, icmp_param->csum);
+
+    csum = rtskb_copy_and_csum_bits(icmp_param->data.skb,
+				    icmp_param->offset,
+				    to + icmp_param->head_len,
+				    fraglen - icmp_param->head_len,
+				    csum);
+
+    icmph = (struct icmphdr *)to;
+
+    icmph->checksum = csum_fold(csum);
+
+    return 0;
+}
+
+
+
+/***
+ *  common reply function
+ */
+static void rt_icmp_send_reply(struct icmp_bxm *icmp_param, struct rtskb *skb)
+{
+    struct dest_route   rt;
+    int                 err;
+
+
+    icmp_param->head.icmph.checksum = 0;
+    icmp_param->csum = 0;
+
+    /* route back to the source address via the incoming device */
+    if (rt_ip_route_output(&rt, skb->nh.iph->saddr,
+			   skb->rtdev->local_ip) != 0)
+	return;
+
+    rt_socket_reference(icmp_socket);
+    err = rt_ip_build_xmit(icmp_socket, rt_icmp_glue_reply_bits, icmp_param,
+			   sizeof(struct icmphdr) + icmp_param->data_len,
+			   &rt, MSG_DONTWAIT);
+    if (err)
+	    rt_socket_dereference(icmp_socket);
+
+    rtdev_dereference(rt.rtdev);
+
+    RTNET_ASSERT(err == 0,
+		 printk("RTnet: %s() error in xmit\n", __FUNCTION__););
+    (void)err;
+}
+
+
+
+/***
+ *  rt_icmp_echo - handles echo replies on our previously sent requests
+ */
+static void rt_icmp_echo_reply(struct rtskb *skb)
+{
+    unsigned long      context;
+    struct rt_proc_call *call;
+    struct ipv4_cmd     *cmd;
+
+
+    raw_spin_lock_irqsave(&echo_calls_lock, context);
+
+    if (!list_empty(&echo_calls)) {
+	call = (struct rt_proc_call *)echo_calls.next;
+	list_del(&call->list_entry);
+
+	raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+    } else {
+	raw_spin_unlock_irqrestore(&echo_calls_lock, context);
+	return;
+    }
+
+    cmd = rtpc_get_priv(call, struct ipv4_cmd);
+
+    cmd->args.ping.ip_addr = skb->nh.iph->saddr;
+    cmd->args.ping.rtt     = 0;
+
+    if ((skb->h.icmph->un.echo.id == cmd->args.ping.id) &&
+	(ntohs(skb->h.icmph->un.echo.sequence) == cmd->args.ping.sequence) &&
+	skb->len == cmd->args.ping.msg_size) {
+	if (skb->len >= sizeof(ktime_t))
+	    cmd->args.ping.rtt =
+		ktime_get() - *((ktime_t *)skb->data);
+	rtpc_complete_call(call, sizeof(struct icmphdr) + skb->len);
+    } else
+	rtpc_complete_call(call, 0);
+}
+
+
+
+/***
+ *  rt_icmp_echo_request - handles echo requests sent by other stations
+ */
+static void rt_icmp_echo_request(struct rtskb *skb)
+{
+    struct icmp_bxm icmp_param;
+
+
+    icmp_param.head.icmph = *skb->h.icmph;
+    icmp_param.head.icmph.type = ICMP_ECHOREPLY;
+    icmp_param.data.skb = skb;
+    icmp_param.offset = 0;
+    icmp_param.data_len = skb->len;
+    icmp_param.head_len = sizeof(struct icmphdr);
+
+    rt_icmp_send_reply(&icmp_param, skb);
+
+    return;
+}
+
+
+
+static int rt_icmp_glue_request_bits(const void *p, unsigned char *to,
+				     unsigned int offset, unsigned int fraglen)
+{
+    struct icmp_bxm *icmp_param = (struct icmp_bxm *)p;
+    struct icmphdr  *icmph;
+    unsigned long   csum;
+
+
+    /* TODO: add support for fragmented ICMP packets */
+    RTNET_ASSERT(offset == 0,
+		 printk("RTnet: %s() does not support fragmentation.\n",
+			     __FUNCTION__);
+		 return -1;);
+
+    csum = csum_partial_copy_nocheck((void *)&icmp_param->head, to,
+				     icmp_param->head_len, icmp_param->csum);
+
+    csum = csum_partial_copy_nocheck(icmp_param->data.buf,
+				     to + icmp_param->head_len,
+				     fraglen - icmp_param->head_len,
+				     csum);
+
+    icmph = (struct icmphdr *)to;
+
+    icmph->checksum = csum_fold(csum);
+
+    return 0;
+}
+
+
+
+/***
+ *  common request function
+ */
+static int rt_icmp_send_request(u32 daddr, struct icmp_bxm *icmp_param)
+{
+    struct dest_route   rt;
+    unsigned int        size;
+    int                 err;
+
+
+    icmp_param->head.icmph.checksum = 0;
+    icmp_param->csum = 0;
+
+    if ((err = rt_ip_route_output(&rt, daddr, INADDR_ANY)) < 0)
+	return err;
+
+    /* TODO: add support for fragmented ICMP packets */
+    size = icmp_param->head_len + icmp_param->data_len;
+    if (size + 20 /* ip header */ > rt.rtdev->get_mtu(rt.rtdev, RT_ICMP_PRIO))
+	err = -EMSGSIZE;
+    else {
+	rt_socket_reference(icmp_socket);
+	err = rt_ip_build_xmit(icmp_socket, rt_icmp_glue_request_bits,
+			       icmp_param, size, &rt, MSG_DONTWAIT);
+	if (err)
+	    rt_socket_dereference(icmp_socket);
+    }
+
+    rtdev_dereference(rt.rtdev);
+
+    return err;
+}
+
+
+
+/***
+ *  rt_icmp_echo_request - sends an echo request to the specified address
+ */
+int rt_icmp_send_echo(u32 daddr, u16 id, u16 sequence, size_t msg_size)
+{
+    struct icmp_bxm icmp_param;
+    unsigned char   pattern_buf[msg_size];
+    off_t           pos;
+
+
+    /* first purge any potentially pending ICMP fragments */
+    rt_ip_frag_invalidate_socket(icmp_socket);
+
+    icmp_param.head.icmph.type = ICMP_ECHO;
+    icmp_param.head.icmph.code = 0;
+    icmp_param.head.icmph.un.echo.id       = id;
+    icmp_param.head.icmph.un.echo.sequence = htons(sequence);
+    icmp_param.offset = 0;
+
+    if (msg_size >= sizeof(ktime_t)) {
+	icmp_param.head_len = sizeof(struct icmphdr) + sizeof(ktime_t);
+	icmp_param.data_len = msg_size - sizeof(ktime_t);
+
+	for (pos = 0; pos < icmp_param.data_len; pos++)
+	    pattern_buf[pos] = pos & 0xFF;
+
+	icmp_param.head.timestamp = ktime_get();
+    } else {
+	icmp_param.head_len = sizeof(struct icmphdr) + msg_size;
+	icmp_param.data_len = 0;
+
+	for (pos = 0; pos < msg_size; pos++)
+	    pattern_buf[pos] = pos & 0xFF;
+    }
+    icmp_param.data.buf = pattern_buf;
+
+    return rt_icmp_send_request(daddr, &icmp_param);
+}
+
+
+
+/***
+ *  rt_icmp_socket
+ */
+int rt_icmp_socket(struct rtsocket *sock)
+{
+    /* we don't support user-created ICMP sockets */
+    return -ENOPROTOOPT;
+}
+
+
+
+static struct rt_icmp_control rt_icmp_pointers[NR_ICMP_TYPES+1] =
+{
+    /* ECHO REPLY (0) */
+    { rt_icmp_echo_reply,       0 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* DEST UNREACH (3) */
+    { rt_icmp_discard,          1 },
+
+    /* SOURCE QUENCH (4) */
+    { rt_icmp_discard,          1 },
+
+    /* REDIRECT (5) */
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* ECHO (8) */
+    { rt_icmp_echo_request,     0 },
+    { rt_icmp_discard,          1 },
+    { rt_icmp_discard,          1 },
+
+    /* TIME EXCEEDED (11) */
+    { rt_icmp_discard,          1 },
+
+    /* PARAMETER PROBLEM (12) */
+    { rt_icmp_discard,          1 },
+
+    /* TIMESTAMP (13) */
+    { rt_icmp_discard,          0 },
+
+    /* TIMESTAMP REPLY (14) */
+    { rt_icmp_discard,          0 },
+
+    /* INFO (15) */
+    { rt_icmp_discard,          0 },
+
+    /* INFO REPLY (16) */
+    { rt_icmp_discard,          0 },
+
+    /* ADDR MASK (17) */
+    { rt_icmp_discard,          0 },
+
+    /* ADDR MASK REPLY (18) */
+    { rt_icmp_discard,          0 }
+};
+
+
+
+/***
+ *  rt_icmp_dest_pool
+ */
+struct rtsocket *rt_icmp_dest_socket(struct rtskb *skb)
+{
+    rt_socket_reference(icmp_socket);
+    return icmp_socket;
+}
+
+
+
+/***
+ *  rt_icmp_rcv
+ */
+void rt_icmp_rcv(struct rtskb *skb)
+{
+    struct icmphdr  *icmpHdr = skb->h.icmph;
+    unsigned int    length   = skb->len;
+
+
+    /* check header sanity and don't accept fragmented packets */
+    if ((length < sizeof(struct icmphdr)) || (skb->next != NULL))
+    {
+	printk("RTnet: improper length in icmp packet\n");
+	goto cleanup;
+    }
+
+    if (ip_compute_csum((unsigned char *)icmpHdr, length))
+    {
+	printk("RTnet: invalid checksum in icmp packet %d\n", length);
+	goto cleanup;
+    }
+
+    if (!rtskb_pull(skb, sizeof(struct icmphdr)))
+    {
+	printk("RTnet: pull failed %p\n", (skb->sk));
+	goto cleanup;
+    }
+
+
+    if (icmpHdr->type > NR_ICMP_TYPES)
+    {
+	printk("RTnet: invalid icmp type\n");
+	goto cleanup;
+    }
+
+    /* sane packet, process it */
+    rt_icmp_pointers[icmpHdr->type].handler(skb);
+
+  cleanup:
+    kfree_rtskb(skb);
+}
+
+
+
+/***
+ *  rt_icmp_rcv_err
+ */
+void rt_icmp_rcv_err(struct rtskb *skb)
+{
+    printk("RTnet: rt_icmp_rcv err\n");
+}
+
+
+
+/***
+ *  ICMP-Initialisation
+ */
+static struct rtinet_protocol icmp_protocol = {
+    .protocol =     IPPROTO_ICMP,
+    .dest_socket =  &rt_icmp_dest_socket,
+    .rcv_handler =  &rt_icmp_rcv,
+    .err_handler =  &rt_icmp_rcv_err,
+    .init_socket =  &rt_icmp_socket
+};
+
+
+
+/***
+ *  rt_icmp_init
+ */
+void __init rt_icmp_init(void)
+{
+    int skbs;
+
+    raw_spin_lock_init(&echo_calls_lock);
+
+    icmp_socket->fd = 1;
+    skbs = rt_bare_socket_init_icmp(icmp_socket, IPPROTO_ICMP, RT_ICMP_PRIO,
+			    ICMP_REPLY_POOL_SIZE);
+    BUG_ON(skbs < 0);
+    if (skbs < ICMP_REPLY_POOL_SIZE)
+	printk("RTnet: allocated only %d icmp rtskbs\n", skbs);
+
+    icmp_socket->prot.inet.tos = 0;
+    icmp_socket->fd_refs = 1;
+
+    rt_inet_add_protocol(&icmp_protocol);
+}
+
+
+
+/***
+ *  rt_icmp_release
+ */
+void rt_icmp_release(void)
+{
+    rt_icmp_cleanup_echo_requests();
+    rt_inet_del_protocol(&icmp_protocol);
+    rt_bare_socket_cleanup(icmp_socket);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/ip_fragment.c linux-5.4.5-new/net/rtnet/stack/ipv4/ip_fragment.c
--- linux-5.4.5/net/rtnet/stack/ipv4/ip_fragment.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/ip_fragment.c	2020-06-15 16:12:31.551695293 +0300
@@ -0,0 +1,356 @@
+/* ip_fragment.c
+ *
+ * Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *               2003      Mathias Koehrer <mathias_koehrer@yahoo.de>
+ *               2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca,
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#include <linux/module.h>
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_socket.h>
+
+#include <linux/ip.h>
+#include <linux/in.h>
+
+#include <ipv4/ip_fragment.h>
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+#include <ipv4/ip_input.h>
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY */
+
+/*
+ * This defined sets the number of incoming fragmented IP messages that
+ * can be handled in parallel.
+ */
+#define COLLECTOR_COUNT 10
+
+struct ip_collector
+{
+    int   in_use;
+    __u32 saddr;
+    __u32 daddr;
+    __u16 id;
+    __u8  protocol;
+
+    struct rtskb_queue frags;
+    struct rtsocket *sock;
+    unsigned int buf_size;
+};
+
+static struct ip_collector collector[COLLECTOR_COUNT];
+
+
+static void alloc_collector(struct rtskb *skb, struct rtsocket *sock)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+    struct iphdr        *iph = skb->nh.iph;
+
+
+    /*
+     * Find a free collector
+     *
+     * Note: We once used to clean up probably outdated chains, but the
+     * algorithm was not stable enough and could cause incorrect drops even
+     * under medium load. If we run in overload, we will loose data anyhow.
+     * What we should do in the future is to account collectors per socket or
+     * socket owner and set quotations.
+     * Garbage collection is now performed only on socket close.
+     */
+    for (i = 0; i < COLLECTOR_COUNT; i++) {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (!p_coll->in_use) {
+            p_coll->in_use        = 1;
+            p_coll->buf_size      = skb->len;
+            p_coll->frags.first   = skb;
+            p_coll->frags.last    = skb;
+            p_coll->saddr         = iph->saddr;
+            p_coll->daddr         = iph->daddr;
+            p_coll->id            = iph->id;
+            p_coll->protocol      = iph->protocol;
+            p_coll->sock          = sock;
+
+            raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+            return;
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+
+    printk("RTnet: IP fragmentation - no collector available\n");
+    kfree_rtskb(skb);
+}
+
+
+
+/*
+ * Return a pointer to the collector that holds the message which
+ * fits to the iphdr of the passed rtskb.
+ * */
+static struct rtskb *add_to_collector(struct rtskb *skb, unsigned int offset, int more_frags)
+{
+	int                 i, err;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+    struct iphdr        *iph = skb->nh.iph;
+    struct rtskb        *first_skb;
+
+
+    /* Search in existing collectors */
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (p_coll->in_use  &&
+            (iph->saddr    == p_coll->saddr) &&
+            (iph->daddr    == p_coll->daddr) &&
+            (iph->id       == p_coll->id) &&
+            (iph->protocol == p_coll->protocol))
+        {
+            first_skb = p_coll->frags.first;
+
+            /* Acquire the rtskb at the expense of the protocol pool */
+            if (rtskb_acquire(skb, &p_coll->sock->skb_pool) != 0) {
+                /* We have to drop this fragment => clean up the whole chain */
+                p_coll->in_use = 0;
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+#ifdef FRAG_DBG
+                printk("RTnet: Compensation pool empty - IP fragments "
+                            "dropped (saddr:%x, daddr:%x)\n",
+                            iph->saddr, iph->daddr);
+#endif
+
+                kfree_rtskb(first_skb);
+                kfree_rtskb(skb);
+                return NULL;
+            }
+
+            /* Optimized version of __rtskb_queue_tail */
+            skb->next = NULL;
+            p_coll->frags.last->next = skb;
+            p_coll->frags.last = skb;
+
+            /* Extend the chain */
+            first_skb->chain_end = skb;
+
+            /* Sanity check: unordered fragments are not allowed! */
+            if (offset != p_coll->buf_size) {
+                /* We have to drop this fragment => clean up the whole chain */
+                p_coll->in_use = 0;
+                skb = first_skb;
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+                break; /* leave the for loop */
+            }
+
+            p_coll->buf_size += skb->len;
+
+            if (!more_frags) {
+                p_coll->in_use = 0;
+
+		err = rt_socket_reference(p_coll->sock);
+
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+
+		if (err < 0) {
+			kfree_rtskb(first_skb);
+			return NULL;
+		}
+
+                return first_skb;
+            } else {
+                raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+                return NULL;
+            }
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+    if (rt_ip_fallback_handler) {
+            __rtskb_push(skb, iph->ihl*4);
+            rt_ip_fallback_handler(skb);
+            return NULL;
+    }
+#endif
+
+#ifdef FRAG_DBG
+    printk("RTnet: Unordered IP fragment (saddr:%x, daddr:%x)"
+                " - dropped\n", iph->saddr, iph->daddr);
+#endif
+
+    kfree_rtskb(skb);
+    return NULL;
+}
+
+
+
+/*
+ * Cleans up all collectors referring to the specified socket.
+ * This is now the only kind of garbage collection we do.
+ */
+void rt_ip_frag_invalidate_socket(struct rtsocket *sock)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if ((p_coll->in_use) && (p_coll->sock == sock))
+        {
+            p_coll->in_use = 0;
+            kfree_rtskb(p_coll->frags.first);
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+}
+EXPORT_SYMBOL_GPL(rt_ip_frag_invalidate_socket);
+
+
+
+/*
+ * Cleans up all existing collectors
+ */
+static void cleanup_all_collectors(void)
+{
+    int                 i;
+    unsigned long      context;
+    struct ip_collector *p_coll;
+
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+    {
+        p_coll = &collector[i];
+        raw_spin_lock_irqsave(&p_coll->frags.lock, context);
+
+        if (p_coll->in_use)
+        {
+            p_coll->in_use = 0;
+            kfree_rtskb(p_coll->frags.first);
+        }
+
+        raw_spin_unlock_irqrestore(&p_coll->frags.lock, context);
+    }
+}
+
+
+
+/*
+ * This function returns an rtskb that contains the complete, accumulated IP message.
+ * If not all fragments of the IP message have been received yet, it returns NULL
+ * Note: the IP header must have already been pulled from the rtskb!
+ * */
+struct rtskb *rt_ip_defrag(struct rtskb *skb, struct rtinet_protocol *ipprot)
+{
+    unsigned int    more_frags;
+    unsigned int    offset;
+    struct rtsocket *sock;
+    struct iphdr    *iph = skb->nh.iph;
+    int             ret;
+
+
+    /* Parse the IP header */
+    offset = ntohs(iph->frag_off);
+    more_frags = offset & IP_MF;
+    offset &= IP_OFFSET;
+    offset <<= 3;   /* offset is in 8-byte chunks */
+
+    /* First fragment? */
+    if (offset == 0)
+    {
+        /* Get the destination socket */
+        if ((sock = ipprot->dest_socket(skb)) == NULL) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+                if (rt_ip_fallback_handler) {
+                    __rtskb_push(skb, iph->ihl*4);
+                    rt_ip_fallback_handler(skb);
+                    return NULL;
+                }
+#endif
+            /* Drop the rtskb */
+            kfree_rtskb(skb);
+            return NULL;
+        }
+
+	/* Acquire the rtskb, to unlock the device skb pool */
+        ret = rtskb_acquire(skb, &sock->skb_pool);
+
+        if (ret != 0) {
+            /* Drop the rtskb */
+            kfree_rtskb(skb);
+        } else {
+            /* Allocates a new collector */
+            alloc_collector(skb, sock);
+        }
+
+        /* Packet is queued or freed, socket can be released */
+        rt_socket_dereference(sock);
+
+        return NULL;
+    }
+    else
+    {
+        /* Add to an existing collector */
+        return add_to_collector(skb, offset, more_frags);
+    }
+}
+
+
+
+int __init rt_ip_fragment_init(void)
+{
+    int i;
+
+
+    /* Probably not needed (static variable...) */
+    memset(collector, 0, sizeof(collector));
+
+    for (i = 0; i < COLLECTOR_COUNT; i++)
+        raw_spin_lock_init(&collector[i].frags.lock);
+
+    return 0;
+}
+
+
+
+void rt_ip_fragment_cleanup(void)
+{
+    cleanup_all_collectors();
+}
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/ip_input.c linux-5.4.5-new/net/rtnet/stack/ipv4/ip_input.c
--- linux-5.4.5/net/rtnet/stack/ipv4/ip_input.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/ip_input.c	2020-06-15 16:12:31.551695293 +0300
@@ -0,0 +1,169 @@
+/***
+ *
+ *  ipv4/ip_input.c - process incoming IP packets
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtskb.h>
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+#include <ipv4/ip_input.h>
+
+rt_ip_fallback_handler_t rt_ip_fallback_handler = NULL;
+EXPORT_SYMBOL_GPL(rt_ip_fallback_handler);
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY */
+
+
+
+/***
+ *  rt_ip_local_deliver
+ */
+static inline void rt_ip_local_deliver(struct rtskb *skb)
+{
+    struct iphdr *iph       = skb->nh.iph;
+    unsigned short protocol = iph->protocol;
+    struct rtinet_protocol *ipprot;
+    struct rtsocket *sock;
+    int err;
+
+
+    ipprot = rt_inet_protocols[rt_inet_hashkey(protocol)];
+
+    /* Check if we are supporting the protocol */
+    if ((ipprot != NULL) && (ipprot->protocol == protocol))
+    {
+        __rtskb_pull(skb, iph->ihl*4);
+
+        /* Point into the IP datagram, just past the header. */
+        skb->h.raw = skb->data;
+
+        /* Reassemble IP fragments */
+        if (iph->frag_off & htons(IP_MF|IP_OFFSET)) {
+            skb = rt_ip_defrag(skb, ipprot);
+            if (!skb)
+                return;
+
+	    sock = skb->sk;
+        } else {
+            /* Get the destination socket */
+            if ((sock = ipprot->dest_socket(skb)) == NULL) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+                if (rt_ip_fallback_handler) {
+                    __rtskb_push(skb, iph->ihl*4);
+                    rt_ip_fallback_handler(skb);
+                    return;
+                }
+#endif
+                kfree_rtskb(skb);
+                return;
+            }
+
+            /* Acquire the rtskb, to unlock the device skb pool */
+            err = rtskb_acquire(skb, &sock->skb_pool);
+
+            if (err) {
+                kfree_rtskb(skb);
+		rt_socket_dereference(sock);
+                return;
+            }
+        }
+
+        /* Deliver the packet to the next layer */
+        ipprot->rcv_handler(skb);
+
+	/* Packet is queued, socket can be released */
+	rt_socket_dereference(sock);
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+    } else if (rt_ip_fallback_handler) {
+        /* If a fallback handler for IP protocol has been installed,
+         * call it. */
+        rt_ip_fallback_handler(skb);
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY */
+    } else {
+	if (IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4_DEBUG))
+		printk("RTnet: no protocol found\n");
+        kfree_rtskb(skb);
+    }
+}
+
+
+
+/***
+ *  rt_ip_rcv
+ */
+int rt_ip_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct iphdr *iph;
+    __u32 len;
+
+    /* When the interface is in promisc. mode, drop all the crap
+     * that it receives, do not try to analyse it.
+     */
+    if (skb->pkt_type == PACKET_OTHERHOST)
+        goto drop;
+
+    iph = skb->nh.iph;
+
+    /*
+     *  RFC1122: 3.1.2.2 MUST silently discard any IP frame that fails the checksum.
+     *
+     *  Is the datagram acceptable?
+     *
+     *  1.  Length at least the size of an ip header
+     *  2.  Version of 4
+     *  3.  Checksums correctly. [Speed optimisation for later, skip loopback checksums]
+     *  4.  Doesn't have a bogus length
+     */
+    if (iph->ihl < 5 || iph->version != 4)
+        goto drop;
+
+    if ( ip_fast_csum((u8 *)iph, iph->ihl)!=0 )
+        goto drop;
+
+    len = ntohs(iph->tot_len);
+    if ( (skb->len<len) || (len<((__u32)iph->ihl<<2)) )
+        goto drop;
+
+    rtskb_trim(skb, len);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER
+    if (rt_ip_route_forward(skb, iph->daddr))
+        return 0;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER */
+
+    rt_ip_local_deliver(skb);
+    return 0;
+
+  drop:
+    kfree_rtskb(skb);
+    return 0;
+}
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/ip_output.c linux-5.4.5-new/net/rtnet/stack/ipv4/ip_output.c
--- linux-5.4.5/net/rtnet/stack/ipv4/ip_output.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/ip_output.c	2020-06-15 16:12:31.551695293 +0300
@@ -0,0 +1,285 @@
+/***
+ *
+ *  ipv4/ip_output.c - prepare outgoing IP packets
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/ip.h>
+#include <net/checksum.h>
+#include <net/ip.h>
+
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_input.h>
+#include <ipv4/route.h>
+
+
+raw_spinlock_t rt_ip_id_lock;
+static u16          rt_ip_id_count = 0;
+
+/***
+ *  Slow path for fragmented packets
+ */
+int rt_ip_build_xmit_slow(struct rtsocket *sk,
+	int getfrag(const void *, char *, unsigned int, unsigned int),
+	const void *frag, unsigned length, struct dest_route *rt,
+	int msg_flags, unsigned int mtu, unsigned int prio)
+{
+    int             err, next_err;
+    struct rtskb    *skb;
+    struct rtskb    *next_skb;
+    struct          iphdr *iph;
+    struct          rtnet_device *rtdev = rt->rtdev;
+    unsigned int    fragdatalen;
+    unsigned int    offset = 0;
+    u16             msg_rt_ip_id;
+    unsigned long  context;
+    unsigned int    rtskb_size;
+    int             hh_len = (rtdev->hard_header_len + 15) & ~15;
+
+
+    #define FRAGHEADERLEN sizeof(struct iphdr)
+
+    fragdatalen  = ((mtu - FRAGHEADERLEN) & ~7);
+
+    /* Store id in local variable */
+    raw_spin_lock_irqsave(&rt_ip_id_lock, context);
+    msg_rt_ip_id = rt_ip_id_count++;
+    raw_spin_unlock_irqrestore(&rt_ip_id_lock, context);
+
+    rtskb_size = mtu + hh_len + 15;
+
+    /* TODO: delay previous skb until ALL errors are catched which may occure
+	     during next skb setup */
+
+    /* Preallocate first rtskb */
+    skb = alloc_rtskb(rtskb_size, &sk->skb_pool);
+    if (skb == NULL)
+	return -ENOBUFS;
+
+    for (offset = 0; offset < length; offset += fragdatalen)
+    {
+	int fraglen; /* The length (IP, including ip-header) of this
+			very fragment */
+	__u16 frag_off = offset >> 3 ;
+
+
+	next_err = 0;
+	if (offset >= length - fragdatalen)
+	{
+	    /* last fragment */
+	    fraglen  = FRAGHEADERLEN + length - offset ;
+	    next_skb = NULL;
+	}
+	else
+	{
+	    fraglen = FRAGHEADERLEN + fragdatalen;
+	    frag_off |= IP_MF;
+
+	    next_skb = alloc_rtskb(rtskb_size, &sk->skb_pool);
+	    if (next_skb == NULL) {
+		frag_off &= ~IP_MF; /* cut the chain */
+		next_err = -ENOBUFS;
+	    }
+	}
+
+	rtskb_reserve(skb, hh_len);
+
+	skb->rtdev    = rtdev;
+	skb->nh.iph   = iph = (struct iphdr *)rtskb_put(skb, fraglen);
+	skb->priority = prio;
+
+	iph->version  = 4;
+	iph->ihl      = 5;    /* 20 byte header - no options */
+	iph->tos      = sk->prot.inet.tos;
+	iph->tot_len  = htons(fraglen);
+	iph->id       = htons(msg_rt_ip_id);
+	iph->frag_off = htons(frag_off);
+	iph->ttl      = 255;
+	iph->protocol = sk->protocol;
+	iph->saddr    = rtdev->local_ip;
+	iph->daddr    = rt->ip;
+	iph->check    = 0; /* required! */
+	iph->check    = ip_fast_csum((unsigned char *)iph, 5 /*iph->ihl*/);
+
+	if ( (err=getfrag(frag, ((char *)iph) + 5 /*iph->ihl*/ * 4, offset,
+			  fraglen - FRAGHEADERLEN)) )
+	    goto error;
+
+	if (rtdev->hard_header) {
+	    err = rtdev->hard_header(skb, rtdev, ETH_P_IP, rt->dev_addr,
+				     rtdev->dev_addr, skb->len);
+	    if (err < 0)
+		goto error;
+	}
+
+	err = rtdev_xmit(skb);
+
+	skb = next_skb;
+
+	if (err != 0) {
+	    err = -EAGAIN;
+	    goto error;
+	}
+
+	if (next_err != 0)
+	    return next_err;
+    }
+    return 0;
+
+  error:
+    if (skb != NULL) {
+	kfree_rtskb(skb);
+
+	if (next_skb != NULL)
+	    kfree_rtskb(next_skb);
+    }
+    return err;
+}
+
+
+
+/***
+ *  Fast path for unfragmented packets.
+ */
+int rt_ip_build_xmit(struct rtsocket *sk,
+	int getfrag(const void *, char *, unsigned int, unsigned int),
+	const void *frag, unsigned length, struct dest_route *rt,
+	int msg_flags)
+{
+    int                     err = 0;
+    struct rtskb            *skb;
+    struct iphdr            *iph;
+    int                     hh_len;
+    u16                     msg_rt_ip_id;
+    unsigned long          context;
+    struct  rtnet_device    *rtdev = rt->rtdev;
+    unsigned int            prio;
+    unsigned int            mtu;
+
+
+    /* sk->priority may encode both priority and output channel. Make sure
+       we use a consitent value, also for the MTU which is derived from the
+       channel. */
+    prio = (volatile unsigned int)sk->priority;
+    mtu = rtdev->get_mtu(rtdev, prio);
+
+    /*
+     *  Try the simple case first. This leaves fragmented frames, and by choice
+     *  RAW frames within 20 bytes of maximum size(rare) to the long path
+     */
+    length += sizeof(struct iphdr);
+
+    if (length > mtu)
+	return rt_ip_build_xmit_slow(sk, getfrag, frag,
+				     length - sizeof(struct iphdr),
+				     rt, msg_flags, mtu, prio);
+
+    /* Store id in local variable */
+    raw_spin_lock_irqsave(&rt_ip_id_lock, context);
+    msg_rt_ip_id = rt_ip_id_count++;
+    raw_spin_unlock_irqrestore(&rt_ip_id_lock, context);
+
+    hh_len = (rtdev->hard_header_len+15)&~15;
+
+    skb = alloc_rtskb(length+hh_len+15, &sk->skb_pool);
+    if (skb==NULL)
+	return -ENOBUFS;
+
+    rtskb_reserve(skb, hh_len);
+
+    skb->rtdev    = rtdev;
+    skb->nh.iph   = iph = (struct iphdr *) rtskb_put(skb, length);
+    skb->priority = prio;
+
+    iph->version  = 4;
+    iph->ihl      = 5;
+    iph->tos      = sk->prot.inet.tos;
+    iph->tot_len  = htons(length);
+    iph->id       = htons(msg_rt_ip_id);
+    iph->frag_off = htons(IP_DF);
+    iph->ttl      = 255;
+    iph->protocol = sk->protocol;
+    iph->saddr    = rtdev->local_ip;
+    iph->daddr    = rt->ip;
+    iph->check    = 0; /* required! */
+    iph->check    = ip_fast_csum((unsigned char *)iph, 5 /*iph->ihl*/);
+
+    if ( (err=getfrag(frag, ((char *)iph) + 5 /*iph->ihl*/ * 4, 0,
+		      length - 5 /*iph->ihl*/ * 4)) )
+	goto error;
+
+    if (rtdev->hard_header) {
+	err = rtdev->hard_header(skb, rtdev, ETH_P_IP, rt->dev_addr,
+				 rtdev->dev_addr, skb->len);
+	if (err < 0)
+	    goto error;
+    }
+
+    err = rtdev_xmit(skb);
+
+    if (err)
+	return -EAGAIN;
+    else
+	return 0;
+
+  error:
+    kfree_rtskb(skb);
+    return err;
+}
+EXPORT_SYMBOL_GPL(rt_ip_build_xmit);
+
+
+
+/***
+ *  IP protocol layer initialiser
+ */
+static struct rtpacket_type ip_packet_type = {
+    .type =     __constant_htons(ETH_P_IP),
+    .handler =  &rt_ip_rcv
+};
+
+
+
+/***
+ *  ip_init
+ */
+void __init rt_ip_init(void)
+{
+    raw_spin_lock_init(&rt_ip_id_lock);
+    rtdev_add_pack(&ip_packet_type);
+    rt_ip_fragment_init();
+}
+
+
+
+/***
+ *  ip_release
+ */
+void rt_ip_release(void)
+{
+    rtdev_remove_pack(&ip_packet_type);
+    rt_ip_fragment_cleanup();
+}
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/ip_sock.c linux-5.4.5-new/net/rtnet/stack/ipv4/ip_sock.c
--- linux-5.4.5/net/rtnet/stack/ipv4/ip_sock.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/ip_sock.c	2020-06-15 16:12:31.551695293 +0300
@@ -0,0 +1,206 @@
+/***
+ *
+ *  ipv4/ip_sock.c
+ *
+ *  Copyright (C) 2003       Hans-Peter Bock <hpbock@avaapgh.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *                2019       Sebastian Smolorz <sebastian.smolorz@gmx.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/errno.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+
+#include <rtnet_socket.h>
+#include <uapi_rtdm.h>
+
+int rt_ip_setsockopt(int fd, struct rtsocket *s, int level,
+		     int optname, const void __user *optval, socklen_t optlen)
+{
+    int err = 0;
+    unsigned int _tos, *tos;
+
+    if (level != SOL_IP)
+	return -ENOPROTOOPT;
+
+    if (optlen < sizeof(unsigned int))
+	return -EINVAL;
+
+    switch (optname) {
+	case IP_TOS:
+	    tos = rtnet_get_arg(s, &_tos, optval, sizeof(_tos), 1);
+	    if (IS_ERR(tos))
+		return PTR_ERR(tos);
+	    else
+		s->prot.inet.tos = *tos;
+	    break;
+
+	default:
+	    err = -ENOPROTOOPT;
+	    break;
+    }
+
+    return err;
+}
+
+
+
+int rt_ip_getsockopt(int fd, struct rtsocket *s, int level,
+		     int optname, void __user *optval,
+		     socklen_t __user *optlen)
+{
+    int err = 0;
+    unsigned int tos;
+    socklen_t _len, *len;
+
+    len = rtnet_get_arg(s, &_len, optlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(unsigned int))
+	return -EINVAL;
+
+    switch (optname) {
+	case IP_TOS:
+	    tos = s->prot.inet.tos;
+	    err = rtnet_put_arg(s, optval, &tos, sizeof(tos), 1);
+	    if (!err) {
+		*len = sizeof(unsigned int);
+		err = rtnet_put_arg(s, optlen, len, sizeof(socklen_t), 1);
+	    }
+	    break;
+
+	default:
+	    err = -ENOPROTOOPT;
+	    break;
+    }
+
+    return err;
+}
+
+
+
+int rt_ip_getsockname(int fd, struct rtsocket *s,
+		      struct sockaddr __user *addr,
+		      socklen_t __user *addrlen)
+{
+    struct sockaddr_in _sin;
+    socklen_t *len, _len;
+    int ret;
+
+    len = rtnet_get_arg(s, &_len, addrlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(struct sockaddr_in))
+	return -EINVAL;
+
+    _sin.sin_family      = AF_INET;
+    _sin.sin_addr.s_addr = s->prot.inet.saddr;
+    _sin.sin_port        = s->prot.inet.sport;
+    memset(&_sin.sin_zero, 0, sizeof(_sin.sin_zero));
+    ret = rtnet_put_arg(s, addr, &_sin, sizeof(_sin), 1);
+    if (ret)
+	return ret;
+
+    *len = sizeof(struct sockaddr_in);
+    ret = rtnet_put_arg(s, addrlen, len, sizeof(socklen_t), 1);
+
+    return ret;
+}
+
+
+
+int rt_ip_getpeername(int fd, struct rtsocket *s,
+		      struct sockaddr __user *addr,
+		      socklen_t __user *addrlen)
+{
+    struct sockaddr_in _sin;
+    socklen_t *len, _len;
+    int ret;
+
+    len = rtnet_get_arg(s, &_len, addrlen, sizeof(_len), 1);
+    if (IS_ERR(len))
+	return PTR_ERR(len);
+
+    if (*len < sizeof(struct sockaddr_in))
+	return -EINVAL;
+
+    _sin.sin_family      = AF_INET;
+    _sin.sin_addr.s_addr = s->prot.inet.daddr;
+    _sin.sin_port        = s->prot.inet.dport;
+    memset(&_sin.sin_zero, 0, sizeof(_sin.sin_zero));
+    ret = rtnet_put_arg(s, addr, &_sin, sizeof(_sin), 1);
+    if (ret)
+	return ret;
+
+    *len = sizeof(struct sockaddr_in);
+    ret = rtnet_put_arg(s, addrlen, len, sizeof(socklen_t), 1);
+
+    return ret;
+}
+
+
+
+int rt_ip_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    struct _rtdm_getsockaddr_args   _getaddr, *getaddr;
+    struct _rtdm_getsockopt_args    _getopt, *getopt;
+    struct _rtdm_setsockopt_args    _setopt, *setopt;
+
+
+    switch (request) {
+	case _RTIOC_SETSOCKOPT:
+	    setopt = rtnet_get_arg(sock, &_setopt, arg, sizeof(_setopt), 1);
+	    if (IS_ERR(setopt))
+		return PTR_ERR(setopt);
+
+	    return rt_ip_setsockopt(sock->fd, sock, setopt->level, setopt->optname,
+				    setopt->optval, setopt->optlen);
+
+	case _RTIOC_GETSOCKOPT:
+	    getopt = rtnet_get_arg(sock, &_getopt, arg, sizeof(_getopt), 1);
+	    if (IS_ERR(getopt))
+		return PTR_ERR(getopt);
+
+	    return rt_ip_getsockopt(sock->fd, sock, getopt->level, getopt->optname,
+				    getopt->optval, getopt->optlen);
+
+	case _RTIOC_GETSOCKNAME:
+	    getaddr = rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+	    if (IS_ERR(getaddr))
+		return PTR_ERR(getaddr);
+
+	    return rt_ip_getsockname(sock->fd, sock, getaddr->addr, getaddr->addrlen);
+
+	case _RTIOC_GETPEERNAME:
+	    getaddr = rtnet_get_arg(sock, &_getaddr, arg, sizeof(_getaddr), 1);
+	    if (IS_ERR(getaddr))
+		return PTR_ERR(getaddr);
+
+	    return rt_ip_getpeername(sock->fd, sock, getaddr->addr, getaddr->addrlen);
+
+	default:
+	    return rt_socket_if_ioctl(sock, request, arg);
+    }
+}
+EXPORT_SYMBOL_GPL(rt_ip_ioctl);
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/Kconfig linux-5.4.5-new/net/rtnet/stack/ipv4/Kconfig
--- linux-5.4.5/net/rtnet/stack/ipv4/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/Kconfig	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,75 @@
+config XENO_DRIVERS_NET_RTIPV4
+    depends on XENO_DRIVERS_NET
+    tristate "Real-Time IPv4"
+    default y
+    ---help---
+    Enables the real-time capable IPv4 support of RTnet. The protocol is
+    implemented as a separate module. Supplementing tools (rtroute,
+    rtping) and examples are provided as well. Moreover, RTcfg will
+    include IPv4 support when this option is switched on.
+
+    For further information see also Documentation/README.routing and
+    Documentation/README.ipfragmentation.
+
+config XENO_DRIVERS_NET_RTIPV4_ICMP
+    bool "ICMP support"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    default y
+    ---help---
+    Enables ICMP support of the RTnet Real-Time IPv4 protocol.
+
+    When the RTnet-Proxy is enabled while this feature is disabled, ICMP
+    will be forwarded to the Linux network stack.
+
+config XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES
+    int "Maximum host routing table entries"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    default 32
+    ---help---
+    Each IPv4 supporting interface and each remote host that is directly
+    reachable via via some output interface requires a host routing table
+    entry. If you run larger networks with may hosts per subnet, you may
+    have to increase this limit. Must be power of 2!
+
+config XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    bool "IP Network Routing"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    ---help---
+    Enables routing across IPv4 real-time networks. You will only require
+    this feature in complex networks, while switching it off for flat,
+    single-segment networks improves code size and the worst-case routing
+    decision delay.
+
+    See Documentation/README.routing for further information.
+
+config XENO_DRIVERS_NET_RTIPV4_NET_ROUTES
+    int "Maximum network routing table entries"
+    depends on XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    default 16
+    ---help---
+    Each route describing a target network reachable via a router
+    requires an entry in the network routing table. If you run very
+    complex realtime networks, you may have to increase this limit. Must
+    be power of 2!
+
+config XENO_DRIVERS_NET_RTIPV4_ROUTER
+    bool "IP Router"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    ---help---
+    When switched on, the RTnet station will be able to forward IPv4
+    packets that are not directed to the station itself. Typically used in
+    combination with CONFIG_RTNET_RTIPV4_NETROUTING.
+
+    See Documentation/README.routing for further information.
+
+config XENO_DRIVERS_NET_RTIPV4_DEBUG
+    bool "RTipv4 Debugging"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    default n
+    
+    ---help---
+    Enables debug message output of the RTipv4 layer. Typically, you
+    may want to turn this on for tracing issues in packet delivery.
+
+source "net/rtnet/stack/ipv4/udp/Kconfig"
+# source "drivers/xenomai/net/stack/ipv4/tcp/Kconfig"
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/Makefile linux-5.4.5-new/net/rtnet/stack/ipv4/Makefile
--- linux-5.4.5/net/rtnet/stack/ipv4/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/Makefile	2020-06-15 16:12:31.563695250 +0300
@@ -0,0 +1,19 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTIPV4) += rtipv4.o
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTIPV4_UDP) += udp/
+
+#obj-$(CONFIG_XENO_DRIVERS_NET_RTIPV4_TCP) += tcp/
+
+rtipv4-y := \
+	route.o \
+	protocol.o \
+	arp.o \
+	af_inet.o \
+	ip_input.o \
+	ip_sock.o \
+	ip_output.o \
+	ip_fragment.o
+
+rtipv4-$(CONFIG_XENO_DRIVERS_NET_RTIPV4_ICMP) += icmp.o
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/protocol.c linux-5.4.5-new/net/rtnet/stack/ipv4/protocol.c
--- linux-5.4.5/net/rtnet/stack/ipv4/protocol.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/protocol.c	2020-06-15 16:12:31.555695279 +0300
@@ -0,0 +1,100 @@
+/***
+ *
+ *  ipv4/protocol.c
+ *
+ *  rtnet - real-time networking subsystem
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/socket.h>
+#include <linux/in.h>
+
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
+
+
+
+struct rtinet_protocol *rt_inet_protocols[MAX_RT_INET_PROTOCOLS];
+
+/***
+ * rt_inet_add_protocol
+ */
+void rt_inet_add_protocol(struct rtinet_protocol *prot)
+{
+    unsigned char hash = rt_inet_hashkey(prot->protocol);
+
+
+    if ( rt_inet_protocols[hash]==NULL )
+	rt_inet_protocols[hash] = prot;
+}
+EXPORT_SYMBOL_GPL(rt_inet_add_protocol);
+
+
+/***
+ * rt_inet_del_protocol
+ */
+void rt_inet_del_protocol(struct rtinet_protocol *prot)
+{
+    unsigned char hash = rt_inet_hashkey(prot->protocol);
+
+
+    if ( prot==rt_inet_protocols[hash] )
+	rt_inet_protocols[hash] = NULL;
+}
+EXPORT_SYMBOL_GPL(rt_inet_del_protocol);
+
+
+
+/***
+ * rt_inet_socket - initialize an Internet socket
+ * @sock: socket structure
+ * @protocol: protocol id
+ */
+int rt_inet_socket(struct rtsocket *sock, int type, int protocol)
+{
+    struct rtinet_protocol  *prot;
+
+    if (protocol == 0)
+	switch (type) {
+	case SOCK_DGRAM:
+	    protocol = IPPROTO_UDP;
+	    break;
+	case SOCK_STREAM:
+	    protocol = IPPROTO_TCP;
+	    break;
+	}
+
+    prot = rt_inet_protocols[rt_inet_hashkey(protocol)];
+
+    /* create the socket (call the socket creator) */
+    if ((prot != NULL) && (prot->protocol == protocol))
+	return prot->init_socket(sock);
+    else {
+	printk("RTnet: protocol with id %d not found\n", protocol);
+
+	return -ENOPROTOOPT;
+    }
+}
+EXPORT_SYMBOL_GPL(rt_inet_socket);
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/route.c linux-5.4.5-new/net/rtnet/stack/ipv4/route.c
--- linux-5.4.5/net/rtnet/stack/ipv4/route.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/route.c	2020-06-15 16:12:31.547695306 +0300
@@ -0,0 +1,1085 @@
+/***
+ *
+ *  ipv4/route.c - real-time routing
+ *
+ *  Copyright (C) 2004, 2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  Rewritten version of the original route by David Schleef and Ulrich Marx
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <net/ip.h>
+
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtnet_chrdev.h>
+#include <ipv4/af_inet.h>
+#include <ipv4/route.h>
+
+
+struct xnvfile_rev_tag {
+        int rev;
+};
+struct xnvfile_rev_tag host_route_tag;
+static inline void xnvfile_touch_tag(struct xnvfile_rev_tag *tag)
+{
+         tag->rev++;
+}
+
+/* FIXME: should also become some tunable parameter */
+#define ROUTER_FORWARD_PRIO \
+    RTSKB_PRIO_VALUE(QUEUE_MAX_PRIO+(QUEUE_MIN_PRIO-QUEUE_MAX_PRIO+1)/2, \
+		     RTSKB_DEF_RT_CHANNEL)
+
+
+/* First-level routing: explicite host routes */
+struct host_route {
+    struct host_route       *next;
+    struct dest_route       dest_host;
+};
+
+/* Second-level routing: routes to other networks */
+struct net_route {
+    struct net_route        *next;
+    u32                     dest_net_ip;
+    u32                     dest_net_mask;
+    u32                     gw_ip;
+};
+
+#if (CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES & (CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES - 1))
+# error CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES must be power of 2
+#endif
+#if CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES < 256
+# define HOST_HASH_TBL_SIZE 64
+#else
+# define HOST_HASH_TBL_SIZE ((CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES / 256) * 64)
+#endif
+#define HOST_HASH_KEY_MASK  (HOST_HASH_TBL_SIZE-1)
+
+static struct host_route    host_routes[CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES];
+static struct host_route    *free_host_route;
+static int                  allocated_host_routes;
+static struct host_route    *host_hash_tbl[HOST_HASH_TBL_SIZE];
+static raw_spinlock_t host_table_lock;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+#if (CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES & (CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES - 1))
+# error CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES must be power of 2
+#endif
+#if CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES < 256
+# define NET_HASH_TBL_SIZE  64
+#else
+# define NET_HASH_TBL_SIZE  ((CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES / 256) * 64)
+#endif
+#define NET_HASH_KEY_MASK   (NET_HASH_TBL_SIZE-1)
+#define NET_HASH_KEY_SHIFT  8
+
+static struct net_route     net_routes[CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES];
+static struct net_route     *free_net_route;
+static int                  allocated_net_routes;
+static struct net_route     *net_hash_tbl[NET_HASH_TBL_SIZE + 1];
+static unsigned int         net_hash_key_shift = NET_HASH_KEY_SHIFT;
+static DEFINE_RTDM_LOCK(net_table_lock);
+
+module_param(net_hash_key_shift, uint, 0444);
+MODULE_PARM_DESC(net_hash_key_shift, "destination right shift for "
+		 "network hash key (default: 8)");
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+
+
+/***
+ *  proc filesystem section
+ */
+#ifdef CONFIG_XENO_OPT_VFILE
+static int rtnet_ipv4_route_show(struct xnvfile_regular_iterator *it, void *d)
+{
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    u32 mask;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+    xnvfile_printf(it, "Host routes allocated/total:\t%d/%d\n"
+	    "Host hash table size:\t\t%d\n",
+	    allocated_host_routes,
+	    CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES,
+	    HOST_HASH_TBL_SIZE);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    xnvfile_printf(it, "Network routes allocated/total:\t%d/%d\n"
+	    "Network hash table size:\t%d\n"
+	    "Network hash key shift/mask:\t%d/%08X\n",
+	    allocated_net_routes,
+	    CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES, NET_HASH_TBL_SIZE,
+	    net_hash_key_shift, mask);
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER
+    xnvfile_printf(it, "IP Router:\t\t\tyes\n");
+#else
+    xnvfile_printf(it, "IP Router:\t\t\tno\n");
+#endif
+
+    return 0;
+}
+
+static int rtnet_ipv4_module_lock(struct xnvfile *vfile)
+{
+    bool res = try_module_get(THIS_MODULE);
+    if (!res)
+	return -EIDRM;
+
+    return 0;
+}
+
+static void rtnet_ipv4_module_unlock(struct xnvfile *vfile)
+{
+    module_put(THIS_MODULE);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_module_lock_ops = {
+    .get = rtnet_ipv4_module_lock,
+    .put = rtnet_ipv4_module_unlock,
+};
+
+static struct xnvfile_regular_ops rtnet_ipv4_route_vfile_ops = {
+    .show = rtnet_ipv4_route_show,
+};
+
+static struct xnvfile_regular rtnet_ipv4_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_module_lock_ops,
+    },
+    .ops = &rtnet_ipv4_route_vfile_ops,
+};
+
+static unsigned long rtnet_ipv4_host_route_lock_ctx;
+
+static int rtnet_ipv4_host_route_lock(struct xnvfile *vfile)
+{
+    raw_spin_lock_irqsave(&host_table_lock, rtnet_ipv4_host_route_lock_ctx);
+    return 0;
+}
+
+static void rtnet_ipv4_host_route_unlock(struct xnvfile *vfile)
+{
+    raw_spin_unlock_irqrestore(&host_table_lock, rtnet_ipv4_host_route_lock_ctx);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_host_route_lock_ops = {
+    .get = rtnet_ipv4_host_route_lock,
+    .put = rtnet_ipv4_host_route_unlock,
+};
+
+struct rtnet_ipv4_host_route_priv {
+    unsigned key;
+    struct host_route *entry_ptr;
+};
+
+struct rtnet_ipv4_host_route_data {
+    int key;
+    char name[IFNAMSIZ];
+    struct dest_route dest_host;
+};
+
+static void *rtnet_ipv4_host_route_begin(struct xnvfile_snapshot_iterator *it)
+{
+    struct rtnet_ipv4_host_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_host_route_data *data;
+    unsigned routes;
+    int err;
+
+    routes = allocated_host_routes;
+    if (!routes)
+	return VFILE_SEQ_EMPTY;
+
+    data = kmalloc(sizeof(*data) * routes, GFP_KERNEL);
+    if (data == NULL)
+	return NULL;
+
+    err = rtnet_ipv4_module_lock(NULL);
+    if (err < 0) {
+	kfree(data);
+	return VFILE_SEQ_EMPTY;
+    }
+
+    priv->key = -1;
+    priv->entry_ptr = NULL;
+    return data;
+}
+
+static void rtnet_ipv4_host_route_end(struct xnvfile_snapshot_iterator *it,
+				    void *buf)
+{
+    rtnet_ipv4_module_unlock(NULL);
+    kfree(buf);
+}
+
+static int rtnet_ipv4_host_route_next(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_host_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_host_route_data *p = data;
+    struct rtnet_device *rtdev;
+
+    if (priv->entry_ptr == NULL) {
+	if (++priv->key >= HOST_HASH_TBL_SIZE)
+	    return 0;
+
+	priv->entry_ptr = host_hash_tbl[priv->key];
+	if (priv->entry_ptr == NULL)
+	    return VFILE_SEQ_SKIP;
+    }
+
+    rtdev = priv->entry_ptr->dest_host.rtdev;
+
+    if (!rtdev_reference(rtdev))
+	return -EIDRM;
+
+    memcpy(&p->name, rtdev->name, sizeof(p->name));
+
+    rtdev_dereference(rtdev);
+
+    p->key = priv->key;
+
+    memcpy(&p->dest_host, &priv->entry_ptr->dest_host, sizeof(p->dest_host));
+
+    priv->entry_ptr = priv->entry_ptr->next;
+
+    return 1;
+}
+
+static int rtnet_ipv4_host_route_show(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_host_route_data *p = data;
+
+    if (p == NULL) {
+	xnvfile_printf(it, "Hash\tDestination\tHW Address\t\tDevice\n");
+	return 0;
+    }
+
+    xnvfile_printf(it, "%02X\t%u.%u.%u.%-3u\t"
+		"%02X:%02X:%02X:%02X:%02X:%02X\t%s\n",
+		p->key, NIPQUAD(p->dest_host.ip),
+		p->dest_host.dev_addr[0], p->dest_host.dev_addr[1],
+		p->dest_host.dev_addr[2], p->dest_host.dev_addr[3],
+		p->dest_host.dev_addr[4], p->dest_host.dev_addr[5],
+		p->name);
+    return 0;
+}
+
+static struct xnvfile_snapshot_ops rtnet_ipv4_host_route_vfile_ops = {
+    .begin = rtnet_ipv4_host_route_begin,
+    .end = rtnet_ipv4_host_route_end,
+    .next = rtnet_ipv4_host_route_next,
+    .show = rtnet_ipv4_host_route_show,
+};
+
+static struct xnvfile_snapshot rtnet_ipv4_host_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_host_route_lock_ops,
+    },
+    .privsz = sizeof(struct rtnet_ipv4_host_route_priv),
+    .datasz = sizeof(struct rtnet_ipv4_host_route_data),
+    .tag = &host_route_tag,
+    .ops = &rtnet_ipv4_host_route_vfile_ops,
+};
+
+static struct xnvfile_link rtnet_ipv4_arp_vfile;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+static unsigned long rtnet_ipv4_net_route_lock_ctx;
+
+static int rtnet_ipv4_net_route_lock(struct xnvfile *vfile)
+{
+    raw_spin_lock_irqsave(&net_table_lock, rtnet_ipv4_net_route_lock_ctx);
+    return 0;
+}
+
+static void rtnet_ipv4_net_route_unlock(struct xnvfile *vfile)
+{
+    raw_spin_unlock_irqrestore(&net_table_lock, rtnet_ipv4_net_route_lock_ctx);
+}
+
+static struct xnvfile_lock_ops rtnet_ipv4_net_route_lock_ops = {
+    .get = rtnet_ipv4_net_route_lock,
+    .put = rtnet_ipv4_net_route_unlock,
+};
+
+struct rtnet_ipv4_net_route_priv {
+    unsigned key;
+    struct net_route *entry_ptr;
+};
+
+struct rtnet_ipv4_net_route_data {
+    int key;
+    u32 dest_net_ip;
+    u32 dest_net_mask;
+    u32 gw_ip;
+};
+
+struct xnvfile_rev_tag net_route_tag;
+
+static void *rtnet_ipv4_net_route_begin(struct xnvfile_snapshot_iterator *it)
+{
+    struct rtnet_ipv4_net_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_net_route_data *data;
+    unsigned routes;
+    int err;
+
+    routes = allocated_net_routes;
+    if (!routes)
+	return VFILE_SEQ_EMPTY;
+
+    data = kmalloc(sizeof(*data) * routes, GFP_KERNEL);
+    if (data == NULL)
+	return NULL;
+
+    err = rtnet_ipv4_module_lock(NULL);
+    if (err < 0) {
+	kfree(data);
+	return VFILE_SEQ_EMPTY;
+    }
+
+    priv->key = -1;
+    priv->entry_ptr = NULL;
+    return data;
+}
+
+static void rtnet_ipv4_net_route_end(struct xnvfile_snapshot_iterator *it,
+				    void *buf)
+{
+    rtnet_ipv4_module_unlock(NULL);
+    kfree(buf);
+}
+
+static int rtnet_ipv4_net_route_next(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_net_route_priv *priv = xnvfile_iterator_priv(it);
+    struct rtnet_ipv4_net_route_data *p = data;
+
+    if (priv->entry_ptr == NULL) {
+	if (++priv->key >= NET_HASH_TBL_SIZE + 1)
+	    return 0;
+
+	priv->entry_ptr = net_hash_tbl[priv->key];
+	if (priv->entry_ptr == NULL)
+	    return VFILE_SEQ_SKIP;
+    }
+
+    p->key = priv->key;
+    p->dest_net_ip = priv->entry_ptr->dest_net_ip;
+    p->dest_net_mask = priv->entry_ptr->dest_net_mask;
+    p->gw_ip = priv->entry_ptr->gw_ip;
+
+    priv->entry_ptr = priv->entry_ptr->next;
+
+    return 1;
+}
+
+static int rtnet_ipv4_net_route_show(struct xnvfile_snapshot_iterator *it,
+				    void *data)
+{
+    struct rtnet_ipv4_net_route_data *p = data;
+
+    if (p == NULL) {
+	xnvfile_printf(it, "Hash\tDestination\tMask\t\t\tGateway\n");
+	return 0;
+    }
+
+    if (p->key < NET_HASH_TBL_SIZE)
+	xnvfile_printf(it, "%02X\t%u.%u.%u.%-3u\t%u.%u.%u.%-3u"
+		    "\t\t%u.%u.%u.%-3u\n",
+		    p->key, NIPQUAD(p->dest_net_ip),
+		    NIPQUAD(p->dest_net_mask),
+		    NIPQUAD(p->gw_ip));
+    else
+	xnvfile_printf(it, "*\t%u.%u.%u.%-3u\t%u.%u.%u.%-3u\t\t"
+		    "%u.%u.%u.%-3u\n",
+		    NIPQUAD(p->dest_net_ip),
+		    NIPQUAD(p->dest_net_mask),
+		    NIPQUAD(p->gw_ip));
+
+    return 0;
+}
+
+static struct xnvfile_snapshot_ops rtnet_ipv4_net_route_vfile_ops = {
+    .begin = rtnet_ipv4_net_route_begin,
+    .end = rtnet_ipv4_net_route_end,
+    .next = rtnet_ipv4_net_route_next,
+    .show = rtnet_ipv4_net_route_show,
+};
+
+static struct xnvfile_snapshot rtnet_ipv4_net_route_vfile = {
+    .entry = {
+	.lockops = &rtnet_ipv4_net_route_lock_ops,
+    },
+    .privsz = sizeof(struct rtnet_ipv4_net_route_priv),
+    .datasz = sizeof(struct rtnet_ipv4_net_route_data),
+    .tag = &net_route_tag,
+    .ops = &rtnet_ipv4_net_route_vfile_ops,
+};
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+
+
+static int __init rt_route_proc_register(void)
+{
+    int err;
+
+    err = xnvfile_init_regular("route",
+			    &rtnet_ipv4_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err1;
+
+    err = xnvfile_init_snapshot("host_route",
+				&rtnet_ipv4_host_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err2;
+
+    /* create "arp" as an alias for "host_route" */
+    err = xnvfile_init_link("arp", "host_route",
+			    &rtnet_ipv4_arp_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err3;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    err = xnvfile_init_snapshot("net_route",
+				&rtnet_ipv4_net_route_vfile, &ipv4_proc_root);
+    if (err < 0)
+	goto err4;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+    return 0;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+  err4:
+    xnvfile_destroy_link(&rtnet_ipv4_arp_vfile);
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+  err3:
+    xnvfile_destroy_snapshot(&rtnet_ipv4_host_route_vfile);
+
+  err2:
+    xnvfile_destroy_regular(&rtnet_ipv4_route_vfile);
+
+  err1:
+    printk("RTnet: unable to initialize /proc entries (route)\n");
+    return err;
+}
+
+
+
+static void rt_route_proc_unregister(void)
+{
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    xnvfile_destroy_snapshot(&rtnet_ipv4_net_route_vfile);
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+    xnvfile_destroy_link(&rtnet_ipv4_arp_vfile);
+    xnvfile_destroy_snapshot(&rtnet_ipv4_host_route_vfile);
+    xnvfile_destroy_regular(&rtnet_ipv4_route_vfile);
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+/***
+ *  rt_alloc_host_route - allocates new host route
+ */
+static inline struct host_route *rt_alloc_host_route(void)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    if ((rt = free_host_route) != NULL) {
+	free_host_route = rt->next;
+	allocated_host_routes++;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return rt;
+}
+
+
+
+/***
+ *  rt_free_host_route - releases host route
+ *
+ *  Note: must be called with host_table_lock held
+ */
+static inline void rt_free_host_route(struct host_route *rt)
+{
+    rt->next        = free_host_route;
+    free_host_route = rt;
+    allocated_host_routes--;
+}
+
+
+
+/***
+ *  rt_ip_route_add_host: add or update host route
+ */
+int rt_ip_route_add_host(u32 addr, unsigned char *dev_addr,
+			 struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *new_route;
+    struct host_route   *rt;
+    unsigned int        key;
+    int                 ret = 0;
+
+
+    raw_spin_lock_irqsave(&rtdev->rtdev_lock, context);
+
+    if ((!test_bit(PRIV_FLAG_UP, &rtdev->priv_flags) ||
+	test_and_set_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags))) {
+	raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+	return -EBUSY;
+    }
+
+    raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+    if ((new_route = rt_alloc_host_route()) != NULL) {
+	new_route->dest_host.ip    = addr;
+	new_route->dest_host.rtdev = rtdev;
+	memcpy(new_route->dest_host.dev_addr, dev_addr, rtdev->addr_len);
+    }
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    xnvfile_touch_tag(&host_route_tag);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (rt->dest_host.rtdev->local_ip == rtdev->local_ip)) {
+	    rt->dest_host.rtdev = rtdev;
+	    memcpy(rt->dest_host.dev_addr, dev_addr, rtdev->addr_len);
+
+	    if (new_route)
+		rt_free_host_route(new_route);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	    goto out;
+	}
+
+	rt = rt->next;
+    }
+
+    if (new_route) {
+	new_route->next    = host_hash_tbl[key];
+	host_hash_tbl[key] = new_route;
+
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+    } else {
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	/*ERRMSG*/printk("RTnet: no more host routes available\n");
+	ret = -ENOBUFS;
+    }
+
+  out:
+    clear_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags);
+
+    return ret;
+}
+
+
+
+/***
+ *  rt_ip_route_del_host - deletes specified host route
+ */
+int rt_ip_route_del_host(u32 addr, struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+    struct host_route   **last_ptr;
+    unsigned int        key;
+
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+    last_ptr = &host_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (!rtdev || (rt->dest_host.rtdev->local_ip == rtdev->local_ip))) {
+	    *last_ptr = rt->next;
+
+	    rt_free_host_route(rt);
+
+	    xnvfile_touch_tag(&host_route_tag);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return -ENOENT;
+}
+
+
+
+/***
+ *  rt_ip_route_del_all - deletes all routes associated with a specified device
+ */
+void rt_ip_route_del_all(struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *host_rt;
+    struct host_route   **last_host_ptr;
+    unsigned int        key;
+    u32                 ip;
+
+
+    for (key = 0; key < HOST_HASH_TBL_SIZE; key++) {
+      host_start_over:
+	last_host_ptr = &host_hash_tbl[key];
+
+	raw_spin_lock_irqsave(&host_table_lock, context);
+
+	host_rt = host_hash_tbl[key];
+	while (host_rt != NULL) {
+	    if (host_rt->dest_host.rtdev == rtdev) {
+		*last_host_ptr = host_rt->next;
+
+		rt_free_host_route(host_rt);
+
+		raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+		goto host_start_over;
+	    }
+
+	    last_host_ptr = &host_rt->next;
+	    host_rt = host_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&host_table_lock, context);
+    }
+
+    if ((ip = rtdev->local_ip) != 0)
+	rt_ip_route_del_host(ip, rtdev);
+}
+
+
+/***
+ *  rt_ip_route_get_host - check if specified host route is resolved
+ */
+int rt_ip_route_get_host(u32 addr, char *if_name, unsigned char *dev_addr,
+			 struct rtnet_device *rtdev)
+{
+    unsigned long      context;
+    struct host_route   *rt;
+    unsigned int        key;
+
+
+    key = ntohl(addr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    rt = host_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_host.ip == addr) &&
+	    (!rtdev || rt->dest_host.rtdev->local_ip == rtdev->local_ip)) {
+	    memcpy(dev_addr, rt->dest_host.dev_addr,
+		   rt->dest_host.rtdev->addr_len);
+	    strncpy(if_name, rt->dest_host.rtdev->name, IFNAMSIZ);
+
+	    raw_spin_unlock_irqrestore(&host_table_lock, context);
+	    return 0;
+	}
+
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+    return -ENOENT;
+}
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+/***
+ *  rt_alloc_net_route - allocates new network route
+ */
+static inline struct net_route *rt_alloc_net_route(void)
+{
+    unsigned long      context;
+    struct net_route    *rt;
+
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    if ((rt = free_net_route) != NULL) {
+	free_net_route = rt->next;
+	allocated_net_routes++;
+    }
+
+    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+    return rt;
+}
+
+
+
+/***
+ *  rt_free_net_route - releases network route
+ *
+ *  Note: must be called with net_table_lock held
+ */
+static inline void rt_free_net_route(struct net_route *rt)
+{
+    rt->next       = free_net_route;
+    free_net_route = rt;
+    allocated_host_routes--;
+}
+
+
+
+/***
+ *  rt_ip_route_add_net: add or update network route
+ */
+int rt_ip_route_add_net(u32 addr, u32 mask, u32 gw_addr)
+{
+    unsigned long      context;
+    struct net_route    *new_route;
+    struct net_route    *rt;
+    struct net_route    **last_ptr;
+    unsigned int        key;
+    u32                 shifted_mask;
+
+
+    addr &= mask;
+
+    if ((new_route = rt_alloc_net_route()) != NULL) {
+	new_route->dest_net_ip   = addr;
+	new_route->dest_net_mask = mask;
+	new_route->gw_ip         = gw_addr;
+    }
+
+    shifted_mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    if ((mask & shifted_mask) == shifted_mask)
+	key = (ntohl(addr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+    else
+	key = NET_HASH_TBL_SIZE;
+    last_ptr = &net_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    xnvfile_touch_tag(&net_route_tag);
+
+    rt = net_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_net_ip == addr) && (rt->dest_net_mask == mask)) {
+	    rt->gw_ip = gw_addr;
+
+	    if (new_route)
+		rt_free_net_route(new_route);
+
+	    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    if (new_route) {
+	new_route->next = *last_ptr;
+	*last_ptr       = new_route;
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	return 0;
+    } else {
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	/*ERRMSG*/printk("RTnet: no more network routes available\n");
+	return -ENOBUFS;
+    }
+}
+
+
+
+/***
+ *  rt_ip_route_del_net - deletes specified network route
+ */
+int rt_ip_route_del_net(u32 addr, u32 mask)
+{
+    unsigned long      context;
+    struct net_route    *rt;
+    struct net_route    **last_ptr;
+    unsigned int        key;
+    u32                 shifted_mask;
+
+
+    addr &= mask;
+
+    shifted_mask = NET_HASH_KEY_MASK << net_hash_key_shift;
+    if ((mask & shifted_mask) == shifted_mask)
+	key = (ntohl(addr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+    else
+	key = NET_HASH_TBL_SIZE;
+    last_ptr = &net_hash_tbl[key];
+
+    raw_spin_lock_irqsave(&net_table_lock, context);
+
+    rt = net_hash_tbl[key];
+    while (rt != NULL) {
+	if ((rt->dest_net_ip == addr) && (rt->dest_net_mask == mask)) {
+	    *last_ptr = rt->next;
+
+	    rt_free_net_route(rt);
+
+	    xnvfile_touch_tag(&net_route_tag);
+
+	    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	    return 0;
+	}
+
+	last_ptr = &rt->next;
+	rt = rt->next;
+    }
+
+    raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+    return -ENOENT;
+}
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+
+
+/***
+ *  rt_ip_route_output - looks up output route
+ *
+ *  Note: increments refcount on returned rtdev in rt_buf
+ */
+int rt_ip_route_output(struct dest_route *rt_buf, u32 daddr, u32 saddr)
+{
+    unsigned long      context;
+    struct host_route   *host_rt;
+    unsigned int        key;
+
+#ifndef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    #define DADDR       daddr
+#else
+    #define DADDR       real_daddr
+
+    struct net_route    *net_rt;
+    int                 lookup_gw  = 1;
+    u32                 real_daddr = daddr;
+
+
+  restart:
+#endif /* !CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+    key = ntohl(daddr) & HOST_HASH_KEY_MASK;
+
+    raw_spin_lock_irqsave(&host_table_lock, context);
+
+    host_rt = host_hash_tbl[key];
+    if (likely(saddr == INADDR_ANY))
+	while (host_rt != NULL) {
+	    if (host_rt->dest_host.ip == daddr) {
+	      host_route_found:
+		if (!rtdev_reference(host_rt->dest_host.rtdev)) {
+		    raw_spin_unlock_irqrestore(&host_table_lock, context);
+		    goto next;
+		}
+
+		memcpy(rt_buf->dev_addr, &host_rt->dest_host.dev_addr,
+		       sizeof(rt_buf->dev_addr));
+		rt_buf->rtdev = host_rt->dest_host.rtdev;
+
+		raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+		rt_buf->ip = DADDR;
+
+		return 0;
+	    }
+	  next:
+	    host_rt = host_rt->next;
+	}
+    else
+	while (host_rt != NULL) {
+	    if ((host_rt->dest_host.ip == daddr) &&
+		(host_rt->dest_host.rtdev->local_ip == saddr))
+		goto host_route_found;
+	    host_rt = host_rt->next;
+	}
+
+    raw_spin_unlock_irqrestore(&host_table_lock, context);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    if (lookup_gw) {
+	lookup_gw = 0;
+	key = (ntohl(daddr) >> net_hash_key_shift) & NET_HASH_KEY_MASK;
+
+	raw_spin_lock_irqsave(&net_table_lock, context);
+
+	net_rt = net_hash_tbl[key];
+	while (net_rt != NULL) {
+	    if (net_rt->dest_net_ip == (daddr & net_rt->dest_net_mask)) {
+		daddr = net_rt->gw_ip;
+
+		raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+		/* start over, now using the gateway ip as destination */
+		goto restart;
+	    }
+
+	    net_rt = net_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+	/* last try: no hash key */
+	raw_spin_lock_irqsave(&net_table_lock, context);
+
+	net_rt = net_hash_tbl[NET_HASH_TBL_SIZE];
+	while (net_rt != NULL) {
+	    if (net_rt->dest_net_ip == (daddr & net_rt->dest_net_mask)) {
+		daddr = net_rt->gw_ip;
+
+		raw_spin_unlock_irqrestore(&net_table_lock, context);
+
+		/* start over, now using the gateway ip as destination */
+		goto restart;
+	    }
+
+	    net_rt = net_rt->next;
+	}
+
+	raw_spin_unlock_irqrestore(&net_table_lock, context);
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+    /*ERRMSG*/printk("RTnet: host %u.%u.%u.%u unreachable\n", NIPQUAD(daddr));
+    return -EHOSTUNREACH;
+}
+
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER
+int rt_ip_route_forward(struct rtskb *rtskb, u32 daddr)
+{
+    struct rtnet_device *rtdev = rtskb->rtdev;
+    struct dest_route   dest;
+
+
+    if (likely((daddr == rtdev->local_ip) || (daddr == rtdev->broadcast_ip) ||
+	(rtdev->flags & IFF_LOOPBACK)))
+	return 0;
+
+    if (rtskb_acquire(rtskb, &global_pool) != 0) {
+	/*ERRMSG*/printk("RTnet: router overloaded, dropping packet\n");
+	goto error;
+    }
+
+    if (rt_ip_route_output(&dest, daddr, INADDR_ANY) < 0) {
+	/*ERRMSG*/printk("RTnet: unable to forward packet from %u.%u.%u.%u\n",
+			      NIPQUAD(rtskb->nh.iph->saddr));
+	goto error;
+    }
+
+    rtskb->rtdev    = dest.rtdev;
+    rtskb->priority = ROUTER_FORWARD_PRIO;
+
+    if ((dest.rtdev->hard_header) &&
+	(dest.rtdev->hard_header(rtskb, dest.rtdev, ETH_P_IP, dest.dev_addr,
+				 dest.rtdev->dev_addr, rtskb->len) < 0))
+	goto error;
+
+    rtdev_xmit(rtskb);
+
+    return 1;
+
+  error:
+    kfree_rtskb(rtskb);
+    return 1;
+}
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_ROUTER */
+
+
+
+/***
+ *  rt_ip_routing_init: initialize
+ */
+int __init rt_ip_routing_init(void)
+{
+    int i;
+
+    raw_spin_lock_init(&host_table_lock);
+
+    for (i = 0; i < CONFIG_XENO_DRIVERS_NET_RTIPV4_HOST_ROUTES-2; i++)
+	host_routes[i].next = &host_routes[i+1];
+    free_host_route = &host_routes[0];
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING
+    for (i = 0; i < CONFIG_XENO_DRIVERS_NET_RTIPV4_NET_ROUTES-2; i++)
+	net_routes[i].next = &net_routes[i+1];
+    free_net_route = &net_routes[0];
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4_NETROUTING */
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    return rt_route_proc_register();
+#else /* !CONFIG_XENO_OPT_VFILE */
+    return 0;
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+
+/***
+ *  rt_ip_routing_realease
+ */
+void rt_ip_routing_release(void)
+{
+#ifdef CONFIG_XENO_OPT_VFILE
+    rt_route_proc_unregister();
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+EXPORT_SYMBOL_GPL(rt_ip_route_add_host);
+EXPORT_SYMBOL_GPL(rt_ip_route_del_host);
+EXPORT_SYMBOL_GPL(rt_ip_route_del_all);
+EXPORT_SYMBOL_GPL(rt_ip_route_output);
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/udp/Kconfig linux-5.4.5-new/net/rtnet/stack/ipv4/udp/Kconfig
--- linux-5.4.5/net/rtnet/stack/ipv4/udp/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/udp/Kconfig	2020-06-15 16:12:31.555695279 +0300
@@ -0,0 +1,6 @@
+config XENO_DRIVERS_NET_RTIPV4_UDP
+    tristate "UDP support"
+    depends on XENO_DRIVERS_NET_RTIPV4
+    default y
+    ---help---
+    Enables UDP support of the RTnet Real-Time IPv4 protocol.
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/udp/Makefile linux-5.4.5-new/net/rtnet/stack/ipv4/udp/Makefile
--- linux-5.4.5/net/rtnet/stack/ipv4/udp/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/udp/Makefile	2020-06-15 16:12:31.555695279 +0300
@@ -0,0 +1,5 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTIPV4_UDP) += rtudp.o
+
+rtudp-y := udp.o
diff -Nur linux-5.4.5/net/rtnet/stack/ipv4/udp/udp.c linux-5.4.5-new/net/rtnet/stack/ipv4/udp/udp.c
--- linux-5.4.5/net/rtnet/stack/ipv4/udp/udp.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/ipv4/udp/udp.c	2020-06-15 16:12:31.563695250 +0300
@@ -0,0 +1,869 @@
+/***
+ *
+ *  ipv4/udp.c - UDP implementation for RTnet
+ *
+ *  Copyright (C) 1999, 2000 Zentropic Computing, LLC
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/err.h>
+#include <linux/udp.h>
+#include <linux/tcp.h>
+#include <net/checksum.h>
+#include <linux/list.h>
+#include <linux/hrtimer.h>
+
+#include <rtskb.h>
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <ipv4/ip_fragment.h>
+#include <ipv4/ip_output.h>
+#include <ipv4/ip_sock.h>
+#include <ipv4/protocol.h>
+#include <ipv4/route.h>
+#include <ipv4/udp.h>
+
+
+/***
+ *  This structure is used to register a UDP socket for reception. All
+ +  structures are kept in the port_registry array to increase the cache
+ *  locality during the critical port lookup in rt_udp_v4_lookup().
+ */
+struct udp_socket {
+    u16             sport;      /* local port */
+    u32             saddr;      /* local ip-addr */
+    struct rtsocket *sock;
+    struct hlist_node link;
+};
+
+/***
+ *  Automatic port number assignment
+
+ *  The automatic assignment of port numbers to unbound sockets is realised as
+ *  a simple addition of two values:
+ *   - the socket ID (lower 8 bits of file descriptor) which is set during
+ *     initialisation and left unchanged afterwards
+ *   - the start value auto_port_start which is a module parameter
+
+ *  auto_port_mask, also a module parameter, is used to define the range of
+ *  port numbers which are used for automatic assignment. Any number within
+ *  this range will be rejected when passed to bind_rt().
+
+ */
+static unsigned int         auto_port_start = 1024;
+static unsigned int         auto_port_mask  = ~(RT_UDP_SOCKETS-1);
+static int                  free_ports      = RT_UDP_SOCKETS;
+#define RT_PORT_BITMAP_WORDS \
+    ((RT_UDP_SOCKETS + BITS_PER_LONG - 1) / BITS_PER_LONG)
+static unsigned long        port_bitmap[RT_PORT_BITMAP_WORDS];
+static struct udp_socket    port_registry[RT_UDP_SOCKETS];
+static raw_spinlock_t udp_socket_base_lock;
+
+static struct hlist_head port_hash[RT_UDP_SOCKETS * 2];
+#define port_hash_mask (RT_UDP_SOCKETS * 2 - 1)
+
+MODULE_LICENSE("GPL");
+
+module_param(auto_port_start, uint, 0444);
+module_param(auto_port_mask, uint, 0444);
+MODULE_PARM_DESC(auto_port_start, "Start of automatically assigned port range");
+MODULE_PARM_DESC(auto_port_mask,
+                 "Mask that defines port range for automatic assignment");
+
+static inline struct udp_socket *port_hash_search(u32 saddr, u16 sport)
+{
+        unsigned bucket = sport & port_hash_mask;
+        struct udp_socket *sock;
+
+        hlist_for_each_entry(sock, &port_hash[bucket], link)
+                if (sock->sport == sport &&
+                    (saddr == INADDR_ANY
+                     || sock->saddr == saddr
+                     || sock->saddr == INADDR_ANY))
+                        return sock;
+
+        return NULL;
+}
+
+static inline int port_hash_insert(struct udp_socket *sock, u32 saddr, u16 sport)
+{
+        unsigned bucket;
+
+        if (port_hash_search(saddr, sport))
+                return -EADDRINUSE;
+
+        bucket = sport & port_hash_mask;
+        sock->saddr = saddr;
+        sock->sport = sport;
+        hlist_add_head(&sock->link, &port_hash[bucket]);
+        return 0;
+}
+
+static inline void port_hash_del(struct udp_socket *sock)
+{
+        hlist_del(&sock->link);
+}
+
+/***
+ *  rt_udp_v4_lookup
+ */
+static inline struct rtsocket *rt_udp_v4_lookup(u32 daddr, u16 dport)
+{
+    unsigned long  context;
+    struct udp_socket *sock;
+
+    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+    sock = port_hash_search(daddr, dport);
+    if (sock && rt_socket_reference(sock->sock) == 0) {
+
+            raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+            return sock->sock;
+    }
+
+    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+    return NULL;
+}
+
+
+
+/***
+ *  rt_udp_bind - bind socket to local address
+ *  @s:     socket
+ *  @addr:  local address
+ */
+int rt_udp_bind(struct rtsocket *sock,
+		const struct sockaddr __user *addr, socklen_t addrlen)
+{
+    struct sockaddr_in  _sin, *sin;
+    unsigned long      context;
+    int                 index;
+    int                 err = 0;
+
+
+    if (addrlen < sizeof(struct sockaddr_in))
+	    return -EINVAL;
+    
+    sin = rtnet_get_arg(sock, &_sin, addr, sizeof(_sin), 1);
+    if (IS_ERR(sin))
+	    return PTR_ERR(sin);
+
+    if ((sin->sin_port & auto_port_mask) == auto_port_start)
+	    return -EINVAL;
+
+    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+    if ((index = sock->prot.inet.reg_index) < 0) {
+        /* socket is being closed */
+        err = -EBADF;
+        goto unlock_out;
+    }
+    if (sock->prot.inet.state != TCP_CLOSE) {
+        err = -EINVAL;
+        goto unlock_out;
+    }
+
+    port_hash_del(&port_registry[index]);
+    if (port_hash_insert(&port_registry[index],
+                         sin->sin_addr.s_addr,
+                         sin->sin_port ?: index + auto_port_start)) {
+            port_hash_insert(&port_registry[index],
+                             port_registry[index].saddr,
+                             port_registry[index].sport);
+            raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+            return -EADDRINUSE;
+    }
+
+    /* set the source-addr */
+    sock->prot.inet.saddr = port_registry[index].saddr;
+
+    /* set source port, if not set by user */
+    sock->prot.inet.sport = port_registry[index].sport;
+
+ unlock_out:
+    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+    return err;
+}
+EXPORT_SYMBOL_GPL(rt_udp_bind);
+
+
+/***
+ *  rt_udp_connect
+ */
+int rt_udp_connect(struct rtsocket *sock,
+		   const struct sockaddr __user *serv_addr, socklen_t addrlen)
+{
+	struct sockaddr _sa, *sa;
+	struct sockaddr_in _sin, *sin;
+	unsigned long      context;
+	int                 index;
+
+	if (addrlen < sizeof(struct sockaddr))
+		return -EINVAL;
+	
+	sa = rtnet_get_arg(sock, &_sa, serv_addr, sizeof(_sa), 1);
+	if (IS_ERR(sa))
+		return PTR_ERR(sa);
+
+	if (sa->sa_family == AF_UNSPEC) {
+		if ((index = sock->prot.inet.reg_index) < 0)
+			/* socket is being closed */
+			return -EBADF;
+
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+		sock->prot.inet.saddr = INADDR_ANY;
+		/* Note: The following line differs from standard
+		   stacks, and we also don't remove the socket from
+		   the port list. Might get fixed in the future... */
+		sock->prot.inet.sport = index + auto_port_start;
+		sock->prot.inet.daddr = INADDR_ANY;
+		sock->prot.inet.dport = 0;
+		sock->prot.inet.state = TCP_CLOSE;
+
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+	} else {
+		if (addrlen < sizeof(struct sockaddr_in))
+			return -EINVAL;
+
+		sin = rtnet_get_arg(sock, &_sin, serv_addr, sizeof(_sin), 1);
+		if (IS_ERR(sin))
+			return PTR_ERR(sin);
+
+		if (sin->sin_family != AF_INET)
+			return -EINVAL;
+		
+		raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+		if (sock->prot.inet.state != TCP_CLOSE) {
+			raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+			return -EINVAL;
+		}
+
+		sock->prot.inet.state = TCP_ESTABLISHED;
+		sock->prot.inet.daddr = sin->sin_addr.s_addr;
+		sock->prot.inet.dport = sin->sin_port;
+
+		raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+	}
+
+	return 0;
+}
+
+
+
+/***
+ *  rt_udp_socket - create a new UDP-Socket
+ *  @s: socket
+ */
+int rt_udp_socket(struct rtsocket *sock)
+{
+    int             ret;
+    int             i;
+    int             index;
+    unsigned long  context;
+
+
+    if ((ret = rt_socket_init(sock, IPPROTO_UDP)) != 0)
+        return ret;
+
+    sock->prot.inet.saddr = INADDR_ANY;
+    sock->prot.inet.state = TCP_CLOSE;
+    sock->prot.inet.tos   = 0;
+
+    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+    /* enforce maximum number of UDP sockets */
+    if (free_ports == 0) {
+        raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+        rt_socket_cleanup(sock);
+        return -EAGAIN;
+    }
+    free_ports--;
+
+    /* find free auto-port in bitmap */
+    for (i = 0; i < RT_PORT_BITMAP_WORDS; i++)
+        if (port_bitmap[i] != (unsigned long)-1)
+            break;
+    index = ffz(port_bitmap[i]);
+    set_bit(index, &port_bitmap[i]);
+    index += i*32;
+    sock->prot.inet.reg_index = index;
+    sock->prot.inet.sport     = index + auto_port_start;
+
+    /* register UDP socket */
+    port_hash_insert(&port_registry[index], INADDR_ANY, sock->prot.inet.sport);
+    port_registry[index].sock  = sock;
+
+    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+    return 0;
+}
+
+
+
+/***
+ *  rt_udp_close
+ */
+void rt_udp_close(struct rtsocket *sock)
+{
+    struct rtskb    *del;
+    int             port;
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+    sock->prot.inet.state = TCP_CLOSE;
+
+    if (sock->prot.inet.reg_index >= 0) {
+        port = sock->prot.inet.reg_index;
+        clear_bit(port % BITS_PER_LONG, &port_bitmap[port / BITS_PER_LONG]);
+        port_hash_del(&port_registry[port]);
+
+        free_ports++;
+
+        sock->prot.inet.reg_index = -1;
+    }
+
+    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+    /* cleanup already collected fragments */
+    rt_ip_frag_invalidate_socket(sock);
+
+    /* free packets in incoming queue */
+    while ((del = rtskb_dequeue(&sock->incoming)) != NULL)
+        kfree_rtskb(del);
+
+    rt_socket_cleanup(sock);
+}
+EXPORT_SYMBOL_GPL(rt_udp_close);
+
+
+int rt_udp_ioctl(struct rtsocket *sock, unsigned int request, void __user *arg)
+{
+	const struct _rtdm_setsockaddr_args *setaddr;
+	struct _rtdm_setsockaddr_args _setaddr;
+
+	/* fast path for common socket IOCTLs */
+	if (_IOC_TYPE(request) == RTIOC_TYPE_NETWORK)
+		return rt_socket_common_ioctl(sock, request, arg);
+
+	switch (request) {
+        case _RTIOC_BIND:
+        case _RTIOC_CONNECT:
+		setaddr = rtnet_get_arg(sock, &_setaddr, arg, sizeof(_setaddr), 1);
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		if (request == _RTIOC_BIND)
+			return rt_udp_bind(sock, setaddr->addr, setaddr->addrlen);
+
+		return rt_udp_connect(sock, setaddr->addr, setaddr->addrlen);
+
+        default:
+		return rt_ip_ioctl(sock, request, arg);
+	}
+}
+EXPORT_SYMBOL_GPL(rt_udp_ioctl);
+
+/*
+ *  rt_udp_recvmsg
+ */
+ssize_t rt_udp_recvmsg(struct rtsocket *sock, struct user_msghdr *u_msg, int msg_flags, int msg_in_userspace)
+{
+    size_t              len;
+    struct rtskb        *skb;
+    struct rtskb        *first_skb;
+    size_t              copied = 0;
+    size_t              block_size;
+    size_t              data_len;
+    struct udphdr       *uh;
+    struct sockaddr_in  sin;
+    struct hrtimer_sleeper     timeout, *to=NULL;
+    int                 ret, flags;
+    struct user_msghdr _msg, *msg;
+    socklen_t namelen;
+    struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+    msg = rtnet_get_arg(sock, &_msg, u_msg, sizeof(_msg), msg_in_userspace);
+
+    if (IS_ERR(msg))
+	    return PTR_ERR(msg);
+
+    if (msg->msg_iovlen < 0)
+	    return -EINVAL;
+
+    if (msg->msg_iovlen == 0)
+	    return 0;
+    
+    ret = rtdm_get_iovec(sock->fd, &iov, msg, iov_fast, msg_in_userspace);
+    if (ret)
+	    return ret;
+
+    /* non-blocking receive? */
+    if (msg_flags & MSG_DONTWAIT) {
+	/* down_trylock returns 0 (success) or 1 (fail). */
+        if((ret = down_trylock(&sock->pending_sem)))
+		ret = -EWOULDBLOCK;
+    } else {
+	to = NULL;
+    	if(sock->timeout) {
+		to = &timeout;
+		/* If the current task is in a real-time scheduling class,
+		 * the hrtimer will be marked in the mode for hard interrupt expiry.
+		 */
+		hrtimer_init_sleeper_on_stack(to, CLOCK_REALTIME,
+			      HRTIMER_MODE_REL);
+		hrtimer_set_expires(&to->timer, sock->timeout);
+    	}
+    	ret = down_hrtimeout(&sock->pending_sem, to);
+    }
+    if (unlikely(ret < 0))
+	switch (ret) {
+	    default:
+		ret = -EBADF;   /* socket has been closed */
+	    case -EWOULDBLOCK:
+	    case -ETIMEDOUT:
+	    case -EINTR:
+		rtdm_drop_iovec(iov, iov_fast);
+		return ret;
+	}
+
+    skb = rtskb_dequeue_chain(&sock->incoming);
+    RTNET_ASSERT(skb != NULL, return -EFAULT;);
+    uh = skb->h.uh;
+    first_skb = skb;
+
+    /* copy the address if required. */
+    if (msg->msg_name) {
+	    memset(&sin, 0, sizeof(sin));
+	    sin.sin_family      = AF_INET;
+	    sin.sin_port        = uh->source;
+	    sin.sin_addr.s_addr = skb->nh.iph->saddr;
+	    ret = rtnet_put_arg(sock, msg->msg_name, &sin, sizeof(sin), msg_in_userspace);
+	    if (ret)
+		    goto fail;
+
+	    namelen = sizeof(sin);
+	    ret = rtnet_put_arg(sock, &u_msg->msg_namelen, &namelen, sizeof(namelen), msg_in_userspace);
+	    if (ret)
+		    goto fail;
+       }
+
+    data_len = ntohs(uh->len) - sizeof(struct udphdr);
+
+    /* remove the UDP header */
+    __rtskb_pull(skb, sizeof(struct udphdr));
+
+    flags = msg->msg_flags & ~MSG_TRUNC;
+    len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+
+    /* iterate over all IP fragments */
+    do {
+        rtskb_trim(skb, data_len);
+
+        block_size = skb->len;
+        copied += block_size;
+        data_len -= block_size;
+
+        /* The data must not be longer than the available buffer size */
+        if (copied > len) {
+            block_size -= copied - len;
+            copied = len;
+	    flags |= MSG_TRUNC;
+        }
+
+        /* copy the data */
+	ret = rtnet_write_to_iov(sock, iov, msg->msg_iovlen, skb->data, block_size, msg_in_userspace);
+	if (ret)
+		goto fail;
+
+        /* next fragment */
+        skb = skb->next;
+    } while (skb && !(flags & MSG_TRUNC));
+
+    /* did we copied all bytes? */
+    if (data_len > 0)
+	    flags |= MSG_TRUNC;
+
+    if (flags != msg->msg_flags) {
+	    ret = rtnet_put_arg(sock, &u_msg->msg_flags, &flags, sizeof(flags), msg_in_userspace);
+	    if (ret)
+		    goto fail;
+    }
+out:
+    if ((msg_flags & MSG_PEEK) == 0)
+        kfree_rtskb(first_skb);
+    else {
+        __rtskb_push(first_skb, sizeof(struct udphdr));
+        rtskb_queue_head(&sock->incoming, first_skb);
+        up(&sock->pending_sem);
+    }
+    rtdm_drop_iovec(iov, iov_fast);
+
+    return copied;
+fail:
+    copied = ret;
+    goto out;
+}
+EXPORT_SYMBOL_GPL(rt_udp_recvmsg);
+
+
+/***
+ *  struct udpfakehdr
+ */
+struct udpfakehdr
+{
+    struct udphdr uh;
+    u32 daddr;
+    u32 saddr;
+    struct rtsocket *sock;
+    struct iovec *iov;
+    int iovlen;
+    u32 wcheck;
+};
+
+
+
+/***
+ *
+ */
+static int rt_udp_getfrag(const void *p, unsigned char *to,
+                          unsigned int offset, unsigned int fraglen)
+{
+    struct udpfakehdr *ufh = (struct udpfakehdr *)p;
+    int ret;
+
+
+    // We should optimize this function a bit (copy+csum...)!
+    if (offset) {
+	    ret = rtnet_read_from_iov(ufh->sock, ufh->iov, ufh->iovlen, to, fraglen, 0);
+	    return ret < 0 ? ret : 0;
+    }
+
+    ret = rtnet_read_from_iov(ufh->sock, ufh->iov, ufh->iovlen,
+			      to + sizeof(struct udphdr),
+			      fraglen - sizeof(struct udphdr), 0);
+    if (ret < 0)
+	    return ret;
+
+    /* Checksum of the complete data part of the UDP message: */
+    ufh->wcheck = csum_partial(to + sizeof(struct udphdr),
+			       fraglen - sizeof(struct udphdr),
+			       ufh->wcheck);
+
+    /* Checksum of the udp header: */
+    ufh->wcheck = csum_partial((unsigned char *)ufh,
+			       sizeof(struct udphdr), ufh->wcheck);
+    
+    ufh->uh.check = csum_tcpudp_magic(ufh->saddr, ufh->daddr, ntohs(ufh->uh.len),
+				      IPPROTO_UDP, ufh->wcheck);
+
+    if (ufh->uh.check == 0)
+            ufh->uh.check = -1;
+
+    memcpy(to, ufh, sizeof(struct udphdr));
+
+    return 0;
+}
+
+
+
+/***
+ *  rt_udp_sendmsg
+ */
+ssize_t rt_udp_sendmsg(struct rtsocket *sock, const struct user_msghdr *msg,
+		int msg_flags, int msg_in_userspace)
+{
+    size_t              len;
+    int                 ulen;
+    struct sockaddr_in  _sin, *sin;
+    struct udpfakehdr   ufh;
+    struct dest_route   rt;
+    u32                 saddr;
+    u32                 daddr;
+    u16                 dport;
+    int                 err;
+    unsigned long      context;
+    struct user_msghdr _msg;
+    struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+    if (msg_flags & MSG_OOB)   /* Mirror BSD error message compatibility */
+        return -EOPNOTSUPP;
+
+    if (msg_flags & ~(MSG_DONTROUTE|MSG_DONTWAIT) )
+        return -EINVAL;
+
+    msg = rtnet_get_arg(sock, &_msg, msg, sizeof(*msg), msg_in_userspace);
+    if (IS_ERR(msg))
+            return PTR_ERR(msg);
+
+    if (msg->msg_iovlen < 0)
+	    return -EINVAL;
+
+    if (msg->msg_iovlen == 0) {
+	    printk(KERN_INFO "%s %d\n", __func__, __LINE__);
+	    return 0;
+    }
+
+    err = rtdm_get_iovec(sock->fd, &iov, msg, iov_fast, msg_in_userspace);
+    if (err) {
+	    return err;
+    }
+
+    len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+    if ((len < 0) || (len > 0xFFFF-sizeof(struct iphdr)-sizeof(struct udphdr))) {
+	    err = -EMSGSIZE;
+	    goto out;
+    }
+
+    ulen = len + sizeof(struct udphdr);
+
+    if (msg->msg_name && msg->msg_namelen == sizeof(*sin)) {
+	    sin = rtnet_get_arg(sock, &_sin, msg->msg_name, sizeof(_sin), msg_in_userspace);
+	    if (IS_ERR(sin)) {
+		    err = PTR_ERR(sin);
+		    goto out;
+	    }
+
+	    if (sin->sin_family != AF_INET && sin->sin_family != AF_UNSPEC) {
+		    err = -EINVAL;
+		    goto out;
+	    }
+
+	    daddr = sin->sin_addr.s_addr;
+	    dport = sin->sin_port;
+	    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+    } else {
+	    raw_spin_lock_irqsave(&udp_socket_base_lock, context);
+
+	    if (sock->prot.inet.state != TCP_ESTABLISHED) {
+		    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+		    err = -ENOTCONN;
+		    goto out;
+	    }
+
+	    daddr = sock->prot.inet.daddr;
+	    dport = sock->prot.inet.dport;
+    }
+    
+    saddr         = sock->prot.inet.saddr;
+    ufh.uh.source = sock->prot.inet.sport;
+
+    raw_spin_unlock_irqrestore(&udp_socket_base_lock, context);
+
+    if ((daddr | dport) == 0) {
+	    err = -EINVAL;
+	    goto out;
+    }
+
+    /* get output route */
+    err = rt_ip_route_output(&rt, daddr, saddr);
+    if (err)
+	    goto out;
+
+    /* we found a route, remember the routing dest-addr could be the netmask */
+    ufh.saddr     = saddr != INADDR_ANY ? saddr : rt.rtdev->local_ip;
+    ufh.daddr     = daddr;
+    ufh.uh.dest   = dport;
+    ufh.uh.len    = htons(ulen);
+    ufh.uh.check  = 0;
+    ufh.sock      = sock;
+    ufh.iov       = iov;
+    ufh.iovlen    = msg->msg_iovlen;
+    ufh.wcheck    = 0;
+
+    err = rt_ip_build_xmit(sock, rt_udp_getfrag, &ufh, ulen, &rt, msg_flags);
+
+    /* Drop the reference obtained in rt_ip_route_output() */
+    rtdev_dereference(rt.rtdev);
+out:
+    rtdm_drop_iovec(iov, iov_fast);
+
+    return err ?: len;
+}
+EXPORT_SYMBOL_GPL(rt_udp_sendmsg);
+
+
+/***
+ *  rt_udp_check
+ */
+static inline unsigned short rt_udp_check(struct udphdr *uh, int len,
+                                          unsigned long saddr,
+                                          unsigned long daddr,
+                                          unsigned long base)
+{
+    return(csum_tcpudp_magic(saddr, daddr, len, IPPROTO_UDP, base));
+}
+
+
+
+struct rtsocket *rt_udp_dest_socket(struct rtskb *skb)
+{
+    struct udphdr           *uh   = skb->h.uh;
+    unsigned short          ulen  = ntohs(uh->len);
+    u32                     saddr = skb->nh.iph->saddr;
+    u32                     daddr = skb->nh.iph->daddr;
+    struct rtnet_device*    rtdev = skb->rtdev;
+
+
+    if (uh->check == 0)
+        skb->ip_summed = CHECKSUM_UNNECESSARY;
+/* ip_summed (yet) never equals CHECKSUM_PARTIAL
+    else
+        if (skb->ip_summed == CHECKSUM_PARTIAL) {
+            skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+            if ( !rt_udp_check(uh, ulen, saddr, daddr, skb->csum) )
+                return NULL;
+
+            skb->ip_summed = CHECKSUM_NONE;
+        }*/
+
+    if (skb->ip_summed != CHECKSUM_UNNECESSARY)
+        skb->csum = csum_tcpudp_nofold(saddr, daddr, ulen, IPPROTO_UDP, 0);
+
+    /* patch broadcast daddr */
+    if (daddr == rtdev->broadcast_ip)
+        daddr = rtdev->local_ip;
+
+    /* find the destination socket */
+    skb->sk = rt_udp_v4_lookup(daddr, uh->dest);
+
+    return skb->sk;
+}
+
+/***
+ *  rt_udp_rcv
+ */
+void rt_udp_rcv (struct rtskb *skb)
+{
+    struct rtsocket *sock = skb->sk;
+    void            (*callback_func)(void *, void *);
+    void            *callback_arg;
+    unsigned long  context;
+
+    rtskb_queue_tail(&sock->incoming, skb);
+    up(&sock->pending_sem);
+
+    raw_spin_lock_irqsave(&sock->param_lock, context);
+    callback_func = sock->callback_func;
+    callback_arg  = sock->callback_arg;
+    raw_spin_unlock_irqrestore(&sock->param_lock, context);
+
+    if (callback_func)
+        callback_func(sock, callback_arg);
+}
+
+
+
+/***
+ *  rt_udp_rcv_err
+ */
+void rt_udp_rcv_err (struct rtskb *skb)
+{
+    printk("RTnet: rt_udp_rcv err\n");
+}
+
+
+
+/***
+ *  UDP-Initialisation
+ */
+static struct rtinet_protocol udp_protocol = {
+    .protocol =     IPPROTO_UDP,
+    .dest_socket =  &rt_udp_dest_socket,
+    .rcv_handler =  &rt_udp_rcv,
+    .err_handler =  &rt_udp_rcv_err,
+    .init_socket =  &rt_udp_socket
+};
+
+#if 0
+/* kept for doc purposes */
+static struct rtdm_driver udp_driver = {
+    .profile_info =     RTDM_PROFILE_INFO(udp,
+                                        RTDM_CLASS_NETWORK,
+                                        RTDM_SUBCLASS_RTNET,
+                                        RTNET_RTDM_VER),
+    .device_flags =     RTDM_PROTOCOL_DEVICE,
+    .device_count =	1,
+    .context_size =     sizeof(struct rtsocket),
+
+    .protocol_family =  PF_INET,
+    .socket_type =      SOCK_DGRAM,
+
+    /* default is UDP */
+    .ops = {
+        .socket =       rt_inet_socket,
+        .close =        rt_udp_close,
+        .ioctl_rt =     rt_udp_ioctl,
+        .ioctl_nrt =    rt_udp_ioctl,
+        .recvmsg_rt =   rt_udp_recvmsg,
+        .sendmsg_rt =   rt_udp_sendmsg,
+        .select =       rt_socket_select_bind,
+    },
+};
+#endif
+
+/***
+ *  rt_udp_init
+ */
+static int __init rt_udp_init(void)
+{
+    int i;
+
+    raw_spin_lock_init(&udp_socket_base_lock);
+
+    if ((auto_port_start < 0) || (auto_port_start >= 0x10000 - RT_UDP_SOCKETS))
+        auto_port_start = 1024;
+    auto_port_start = htons(auto_port_start & (auto_port_mask & 0xFFFF));
+    auto_port_mask  = htons(auto_port_mask | 0xFFFF0000);
+
+    rt_inet_add_protocol(&udp_protocol);
+
+    for (i = 0; i < ARRAY_SIZE(port_hash); i++)
+            INIT_HLIST_HEAD(&port_hash[i]);
+
+    return 0;
+}
+
+
+
+/***
+ *  rt_udp_release
+ */
+static void __exit rt_udp_release(void)
+{
+    rt_inet_del_protocol(&udp_protocol);
+}
+
+module_init(rt_udp_init);
+module_exit(rt_udp_release);
diff -Nur linux-5.4.5/net/rtnet/stack/Kconfig linux-5.4.5-new/net/rtnet/stack/Kconfig
--- linux-5.4.5/net/rtnet/stack/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/Kconfig	2020-06-15 16:12:31.495695490 +0300
@@ -0,0 +1,41 @@
+menu "Protocol Stack"
+    depends on XENO_DRIVERS_NET
+
+comment "Stack parameters"
+
+config XENO_DRIVERS_NET_RX_FIFO_SIZE
+    int "Size of central RX-FIFO"
+    depends on XENO_DRIVERS_NET
+    default 32
+    ---help---
+    Size of FIFO between NICs and stack manager task. Must be power
+    of two! Effectively, only CONFIG_RTNET_RX_FIFO_SIZE-1 slots will
+    be usable.
+
+config XENO_DRIVERS_NET_ETH_P_ALL
+    depends on XENO_DRIVERS_NET
+    bool "Support for ETH_P_ALL"
+    ---help---
+    Enables core support for registering listeners on all layer 3
+    protocols (ETH_P_ALL). Internally this is currently realised by
+    clone-copying incoming frames for those listners, future versions
+    will implement buffer sharing for efficiency reasons. Use with
+    care, every ETH_P_ALL-listener adds noticable overhead to the
+    reception path.
+
+config XENO_DRIVERS_NET_RTWLAN
+    depends on XENO_DRIVERS_NET
+    bool "Real-Time WLAN"
+    ---help---
+    Enables core support for real-time wireless LAN. RT-WLAN is based
+    on low-level access to 802.11-compliant adapters and is currently
+    in an experimental stage.
+
+comment "Protocols"
+
+source "net/rtnet/stack/ipv4/Kconfig"
+source "net/rtnet/stack/packet/Kconfig"
+source "net/rtnet/stack/rtmac/Kconfig"
+source "net/rtnet/stack/rtcfg/Kconfig"
+
+endmenu
diff -Nur linux-5.4.5/net/rtnet/stack/Makefile linux-5.4.5-new/net/rtnet/stack/Makefile
--- linux-5.4.5/net/rtnet/stack/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/Makefile	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,26 @@
+ccflags-y += -Inet/rtnet/stack/include -Ikernel/
+
+obj-$(CONFIG_XENO_DRIVERS_NET) += rtnet.o
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTIPV4) += ipv4/
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTPACKET) += packet/
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTMAC) += rtmac/
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTCFG) += rtcfg/
+
+rtnet-y :=  \
+	rtnet_rtdm.o \
+	iovec.o \
+	rtdev.o \
+	rtdev_mgr.o \
+	rtnet_chrdev.o \
+	rtnet_module.o \
+	rtnet_rtpc.o \
+	rtskb.o \
+	socket.o \
+	stack_mgr.o \
+	eth.o
+
+rtnet-$(CONFIG_XENO_DRIVERS_NET_RTWLAN) += rtwlan.o
diff -Nur linux-5.4.5/net/rtnet/stack/packet/af_packet.c linux-5.4.5-new/net/rtnet/stack/packet/af_packet.c
--- linux-5.4.5/net/rtnet/stack/packet/af_packet.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/packet/af_packet.c	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,684 @@
+/***
+ *
+ *  packet/af_packet.c
+ *
+ *  RTnet - real-time networking subsystem
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <stack_mgr.h>
+
+MODULE_LICENSE("GPL");
+
+
+/***
+ *  rt_packet_rcv
+ */
+static int rt_packet_rcv(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct rtsocket *sock   = container_of(pt, struct rtsocket,
+					   prot.packet.packet_type);
+    int             ifindex = sock->prot.packet.ifindex;
+    void            (*callback_func)(struct rtdm_fd *, void *);
+    void            *callback_arg;
+    rtdm_lockctx_t  context;
+
+
+    if (unlikely((ifindex != 0) && (ifindex != skb->rtdev->ifindex)))
+	return -EUNATCH;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ETH_P_ALL
+    if (pt->type == htons(ETH_P_ALL)) {
+	struct rtskb *clone_skb = rtskb_clone(skb, &sock->skb_pool);
+	if (clone_skb == NULL)
+	    goto out;
+	skb = clone_skb;
+    } else
+#endif /* CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+	if (unlikely(rtskb_acquire(skb, &sock->skb_pool) < 0)) {
+	    kfree_rtskb(skb);
+	    goto out;
+	}
+
+    rtskb_queue_tail(&sock->incoming, skb);
+    rtdm_sem_up(&sock->pending_sem);
+
+    rtdm_lock_get_irqsave(&sock->param_lock, context);
+    callback_func = sock->callback_func;
+    callback_arg  = sock->callback_arg;
+    rtdm_lock_put_irqrestore(&sock->param_lock, context);
+
+    if (callback_func)
+	callback_func(rt_socket_fd(sock), callback_arg);
+
+  out:
+    return 0;
+}
+
+static bool rt_packet_trylock(struct rtpacket_type *pt)
+{
+    struct rtsocket *sock   = container_of(pt, struct rtsocket,
+					   prot.packet.packet_type);
+    struct rtdm_fd *fd = rtdm_private_to_fd(sock);
+
+    if (rtdm_fd_lock(fd) < 0)
+	return false;
+
+    return true;
+}
+
+static void rt_packet_unlock(struct rtpacket_type *pt)
+{
+    struct rtsocket *sock   = container_of(pt, struct rtsocket,
+					   prot.packet.packet_type);
+    struct rtdm_fd *fd = rtdm_private_to_fd(sock);
+
+    rtdm_fd_unlock(fd);
+}
+
+/***
+ *  rt_packet_bind
+ */
+static int rt_packet_bind(struct rtdm_fd *fd, struct rtsocket *sock,
+			  const struct sockaddr *addr, socklen_t addrlen)
+{
+	struct sockaddr_ll  _sll, *sll;
+	struct rtpacket_type    *pt  = &sock->prot.packet.packet_type;
+	int                     new_type;
+	int                     ret;
+	rtdm_lockctx_t          context;
+
+	if (addrlen < sizeof(struct sockaddr_ll))
+		return -EINVAL;
+	
+	sll = rtnet_get_arg(fd, &_sll, addr, sizeof(_sll));
+	if (IS_ERR(sll))
+		return PTR_ERR(sll);
+
+	if (sll->sll_family != AF_PACKET)
+		return -EINVAL;
+	
+	new_type = (sll->sll_protocol != 0) ? sll->sll_protocol : sock->protocol;
+
+	rtdm_lock_get_irqsave(&sock->param_lock, context);
+
+	/* release existing binding */
+	if (pt->type != 0)
+		rtdev_remove_pack(pt);
+
+	pt->type = new_type;
+	sock->prot.packet.ifindex = sll->sll_ifindex;
+
+	/* if protocol is non-zero, register the packet type */
+	if (new_type != 0) {
+		pt->handler     = rt_packet_rcv;
+		pt->err_handler = NULL;
+		pt->trylock     = rt_packet_trylock;
+		pt->unlock      = rt_packet_unlock;
+
+		ret = rtdev_add_pack(pt);
+	} else
+		ret = 0;
+
+	rtdm_lock_put_irqrestore(&sock->param_lock, context);
+
+	return ret;
+}
+
+
+
+/***
+ *  rt_packet_getsockname
+ */
+static int rt_packet_getsockname(struct rtdm_fd *fd, struct rtsocket *sock,
+				 struct sockaddr *addr, socklen_t *addrlen)
+{
+	struct sockaddr_ll  _sll, *sll;
+	struct rtnet_device *rtdev;
+	rtdm_lockctx_t      context;
+	socklen_t _namelen, *namelen;
+	int ret;
+
+	namelen = rtnet_get_arg(fd, &_namelen, addrlen, sizeof(_namelen));
+	if (IS_ERR(namelen))
+		return PTR_ERR(namelen);
+
+	if (*namelen < sizeof(struct sockaddr_ll))
+		return -EINVAL;
+
+	sll = rtnet_get_arg(fd, &_sll, addr, sizeof(_sll));
+	if (IS_ERR(sll))
+		return PTR_ERR(sll);
+   
+	rtdm_lock_get_irqsave(&sock->param_lock, context);
+
+	sll->sll_family   = AF_PACKET;
+	sll->sll_ifindex  = sock->prot.packet.ifindex;
+	sll->sll_protocol = sock->protocol;
+
+	rtdm_lock_put_irqrestore(&sock->param_lock, context);
+
+	rtdev = rtdev_get_by_index(sll->sll_ifindex);
+	if (rtdev != NULL) {
+		sll->sll_hatype = rtdev->type;
+		sll->sll_halen  = rtdev->addr_len;
+		memcpy(sll->sll_addr, rtdev->dev_addr, rtdev->addr_len);
+		rtdev_dereference(rtdev);
+	} else {
+		sll->sll_hatype = 0;
+		sll->sll_halen  = 0;
+	}
+
+	*namelen = sizeof(struct sockaddr_ll);
+
+	ret = rtnet_put_arg(fd, addr, sll, sizeof(*sll));
+	if (ret)
+		return ret;
+
+	return rtnet_put_arg(fd, addrlen, namelen, sizeof(*namelen));
+}
+
+
+
+/***
+ * rt_packet_socket - initialize a packet socket
+ */
+static int rt_packet_socket(struct rtdm_fd *fd, int protocol)
+{
+    struct rtsocket *sock = rtdm_fd_to_private(fd);
+    int             ret;
+
+
+    if ((ret = rt_socket_init(fd, protocol)) != 0)
+	return ret;
+
+    sock->prot.packet.packet_type.type		= protocol;
+    sock->prot.packet.ifindex			= 0;
+    sock->prot.packet.packet_type.trylock	= rt_packet_trylock;
+    sock->prot.packet.packet_type.unlock        = rt_packet_unlock;
+
+    /* if protocol is non-zero, register the packet type */
+    if (protocol != 0) {
+	sock->prot.packet.packet_type.handler     = rt_packet_rcv;
+	sock->prot.packet.packet_type.err_handler = NULL;
+
+	if ((ret = rtdev_add_pack(&sock->prot.packet.packet_type)) < 0) {
+	    rt_socket_cleanup(fd);
+	    return ret;
+	}
+    }
+
+    return 0;
+}
+
+
+
+/***
+ *  rt_packet_close
+ */
+static void rt_packet_close(struct rtdm_fd *fd)
+{
+    struct rtsocket         *sock = rtdm_fd_to_private(fd);
+    struct rtpacket_type    *pt = &sock->prot.packet.packet_type;
+    struct rtskb            *del;
+    rtdm_lockctx_t          context;
+
+
+    rtdm_lock_get_irqsave(&sock->param_lock, context);
+
+    if (pt->type != 0) {
+	rtdev_remove_pack(pt);
+	pt->type = 0;
+    }
+
+    rtdm_lock_put_irqrestore(&sock->param_lock, context);
+
+    /* free packets in incoming queue */
+    while ((del = rtskb_dequeue(&sock->incoming)) != NULL) {
+	kfree_rtskb(del);
+    }
+
+    rt_socket_cleanup(fd);
+}
+
+
+
+/***
+ *  rt_packet_ioctl
+ */
+static int rt_packet_ioctl(struct rtdm_fd *fd, unsigned int request, void __user *arg)
+{
+	struct rtsocket *sock = rtdm_fd_to_private(fd);
+	const struct _rtdm_setsockaddr_args *setaddr;
+	struct _rtdm_setsockaddr_args _setaddr;
+	const struct _rtdm_getsockaddr_args *getaddr;
+	struct _rtdm_getsockaddr_args _getaddr;
+
+	/* fast path for common socket IOCTLs */
+	if (_IOC_TYPE(request) == RTIOC_TYPE_NETWORK)
+		return rt_socket_common_ioctl(fd, request, arg);
+
+	switch (request) {
+	case _RTIOC_BIND:
+		setaddr = rtnet_get_arg(fd, &_setaddr, arg, sizeof(_setaddr));
+		if (IS_ERR(setaddr))
+			return PTR_ERR(setaddr);
+		return rt_packet_bind(fd, sock, setaddr->addr, setaddr->addrlen);
+
+	case _RTIOC_GETSOCKNAME:
+		getaddr = rtnet_get_arg(fd, &_getaddr, arg, sizeof(_getaddr));
+		if (IS_ERR(getaddr))
+			return PTR_ERR(getaddr);
+		return rt_packet_getsockname(fd, sock, getaddr->addr,
+					     getaddr->addrlen);
+
+	default:
+		return rt_socket_if_ioctl(fd, request, arg);
+	}
+}
+
+
+
+/***
+ *  rt_packet_recvmsg
+ */
+static ssize_t
+rt_packet_recvmsg(struct rtdm_fd *fd, struct user_msghdr *u_msg, int msg_flags)
+{
+    struct rtsocket     *sock = rtdm_fd_to_private(fd);
+    ssize_t             len;
+    size_t              copy_len;
+    struct rtskb        *rtskb;
+    struct sockaddr_ll  sll;
+    int			ret, flags;
+    nanosecs_rel_t      timeout = sock->timeout;
+    struct user_msghdr _msg, *msg;
+    socklen_t namelen;
+    struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+    msg = rtnet_get_arg(fd, &_msg, u_msg, sizeof(_msg));
+    if (IS_ERR(msg))
+	    return PTR_ERR(msg);
+   
+    if (msg->msg_iovlen < 0)
+	    return -EINVAL;
+
+    if (msg->msg_iovlen == 0)
+	    return 0;
+
+    ret = rtdm_get_iovec(fd, &iov, msg, iov_fast);
+    if (ret)
+	    return ret;
+
+    /* non-blocking receive? */
+    if (msg_flags & MSG_DONTWAIT)
+	timeout = -1;
+
+    ret = rtdm_sem_timeddown(&sock->pending_sem, timeout, NULL);
+    if (unlikely(ret < 0))
+	switch (ret) {
+	    default:
+		ret = -EBADF;   /* socket has been closed */
+	    case -EWOULDBLOCK:
+	    case -ETIMEDOUT:
+	    case -EINTR:
+		rtdm_drop_iovec(iov, iov_fast);
+		return ret;
+	}
+
+    rtskb = rtskb_dequeue_chain(&sock->incoming);
+    RTNET_ASSERT(rtskb != NULL, return -EFAULT;);
+
+    /* copy the address if required. */
+    if (msg->msg_name) {
+	struct rtnet_device *rtdev = rtskb->rtdev;
+	memset(&sll, 0, sizeof(sll));
+	sll.sll_family   = AF_PACKET;
+	sll.sll_hatype   = rtdev->type;
+	sll.sll_protocol = rtskb->protocol;
+	sll.sll_pkttype  = rtskb->pkt_type;
+	sll.sll_ifindex  = rtdev->ifindex;
+
+	/* Ethernet specific - we rather need some parse handler here */
+	memcpy(sll.sll_addr, rtskb->mac.ethernet->h_source, ETH_ALEN);
+	sll.sll_halen = ETH_ALEN;
+	ret = rtnet_put_arg(fd, msg->msg_name, &sll, sizeof(sll));
+	if (ret)
+		goto fail;
+
+	namelen = sizeof(sll);
+	ret = rtnet_put_arg(fd, &u_msg->msg_namelen, &namelen, sizeof(namelen));
+	if (ret)
+		goto fail;
+    }
+
+    /* Include the header in raw delivery */
+    if (rtdm_fd_to_context(fd)->device->driver->socket_type != SOCK_DGRAM)
+	rtskb_push(rtskb, rtskb->data - rtskb->mac.raw);
+
+    /* The data must not be longer than the available buffer size */
+    copy_len = rtskb->len;
+    len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+    if (len < 0) {
+	    copy_len = len;
+	    goto out;
+    }
+    
+    if (copy_len > len) {
+	copy_len = len;
+	flags = msg->msg_flags | MSG_TRUNC;
+	ret = rtnet_put_arg(fd, &u_msg->msg_flags, &flags, sizeof(flags));
+	if (ret)
+		goto fail;
+    }
+
+    copy_len = rtnet_write_to_iov(fd, iov, msg->msg_iovlen, rtskb->data, copy_len);
+out:
+    if ((msg_flags & MSG_PEEK) == 0) {
+	kfree_rtskb(rtskb);
+    } else {
+	rtskb_queue_head(&sock->incoming, rtskb);
+	rtdm_sem_up(&sock->pending_sem);
+    }
+
+    rtdm_drop_iovec(iov, iov_fast);
+
+    return copy_len;
+fail:
+    copy_len = ret;
+    goto out;
+}
+
+
+
+/***
+ *  rt_packet_sendmsg
+ */
+static ssize_t
+rt_packet_sendmsg(struct rtdm_fd *fd, const struct user_msghdr *msg, int msg_flags)
+{
+    struct rtsocket     *sock = rtdm_fd_to_private(fd);
+    size_t              len;
+    struct sockaddr_ll  _sll, *sll;
+    struct rtnet_device *rtdev;
+    struct rtskb        *rtskb;
+    unsigned short      proto;
+    unsigned char       *addr;
+    int                 ifindex;
+    ssize_t             ret;
+    struct user_msghdr _msg;
+    struct iovec iov_fast[RTDM_IOV_FASTMAX], *iov;
+
+    if (msg_flags & MSG_OOB)    /* Mirror BSD error message compatibility */
+	return -EOPNOTSUPP;
+    if (msg_flags & ~MSG_DONTWAIT)
+	return -EINVAL;
+
+    msg = rtnet_get_arg(fd, &_msg, msg, sizeof(*msg));
+    if (IS_ERR(msg))
+	    return PTR_ERR(msg);
+
+    if (msg->msg_iovlen < 0)
+	    return -EINVAL;
+
+    if (msg->msg_iovlen == 0)
+	    return 0;
+    
+    ret = rtdm_get_iovec(fd, &iov, msg, iov_fast);
+    if (ret)
+	    return ret;
+
+    if (msg->msg_name == NULL) {
+	/* Note: We do not care about races with rt_packet_bind here -
+	   the user has to do so. */
+	ifindex = sock->prot.packet.ifindex;
+	proto   = sock->prot.packet.packet_type.type;
+	addr    = NULL;
+	sll = NULL;
+    } else {
+	    sll = rtnet_get_arg(fd, &_sll, msg->msg_name, sizeof(_sll));
+	    if (IS_ERR(sll)) {
+		    ret = PTR_ERR(sll);
+		    goto abort;
+	    }
+
+	    if ((msg->msg_namelen < sizeof(struct sockaddr_ll)) ||
+		(msg->msg_namelen <
+		 (sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr))) ||
+		((sll->sll_family != AF_PACKET) &&
+		 (sll->sll_family != AF_UNSPEC))) {
+		    ret = -EINVAL;
+		    goto abort;
+	    }
+
+	    ifindex = sll->sll_ifindex;
+	    proto   = sll->sll_protocol;
+	    addr    = sll->sll_addr;
+    }
+
+    if ((rtdev = rtdev_get_by_index(ifindex)) == NULL) {
+	    ret = -ENODEV;
+	    goto abort;
+    }
+
+    len = rtdm_get_iov_flatlen(iov, msg->msg_iovlen);
+    rtskb = alloc_rtskb(rtdev->hard_header_len + len, &sock->skb_pool);
+    if (rtskb == NULL) {
+	ret = -ENOBUFS;
+	goto out;
+    }
+
+    /* If an RTmac discipline is active, this becomes a pure sanity check to
+       avoid writing beyond rtskb boundaries. The hard check is then performed
+       upon rtdev_xmit() by the discipline's xmit handler. */
+    if (len > rtdev->mtu +
+	((rtdm_fd_to_context(fd)->device->driver->socket_type == SOCK_RAW) ?
+	    rtdev->hard_header_len : 0)) {
+	ret = -EMSGSIZE;
+	goto err;
+    }
+
+    if ((sll != NULL) && (sll->sll_halen != rtdev->addr_len)) {
+	ret = -EINVAL;
+	goto err;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    rtskb->rtdev    = rtdev;
+    rtskb->priority = sock->priority;
+
+    if (rtdev->hard_header) {
+	int hdr_len;
+
+	ret = -EINVAL;
+	hdr_len = rtdev->hard_header(rtskb, rtdev, ntohs(proto),
+				     addr, NULL, len);
+	if (rtdm_fd_to_context(fd)->device->driver->socket_type != SOCK_DGRAM) {
+	    rtskb->tail = rtskb->data;
+	    rtskb->len = 0;
+	} else if (hdr_len < 0)
+	    goto err;
+    }
+
+    ret = rtnet_read_from_iov(fd, iov, msg->msg_iovlen, rtskb_put(rtskb, len), len);
+
+    if ((rtdev->flags & IFF_UP) != 0) {
+	if ((ret = rtdev_xmit(rtskb)) == 0)
+	    ret = len;
+    } else {
+	ret = -ENETDOWN;
+	goto err;
+    }
+
+ out:
+    rtdev_dereference(rtdev);
+ abort:
+    rtdm_drop_iovec(iov, iov_fast);
+
+    return ret;
+ err:
+    kfree_rtskb(rtskb);
+    goto out;
+}
+
+
+
+static struct rtdm_driver packet_proto_drv = {
+    .profile_info =     RTDM_PROFILE_INFO(packet,
+					RTDM_CLASS_NETWORK,
+					RTDM_SUBCLASS_RTNET,
+					RTNET_RTDM_VER),
+    .device_flags =     RTDM_PROTOCOL_DEVICE,
+    .device_count =     1,
+    .context_size =     sizeof(struct rtsocket),
+
+    .protocol_family =  PF_PACKET,
+    .socket_type =      SOCK_DGRAM,
+
+
+    .ops = {
+	.socket =       rt_packet_socket,
+	.close =        rt_packet_close,
+	.ioctl_rt =     rt_packet_ioctl,
+	.ioctl_nrt =    rt_packet_ioctl,
+	.recvmsg_rt =   rt_packet_recvmsg,
+	.sendmsg_rt =   rt_packet_sendmsg,
+	.select =       rt_socket_select_bind,
+    },
+};
+
+static struct rtdm_device packet_proto_dev = {
+    .driver = &packet_proto_drv,
+    .label = "packet",
+};
+
+
+static struct rtdm_driver raw_packet_proto_drv = {
+    .profile_info =     RTDM_PROFILE_INFO(raw_packet,
+					RTDM_CLASS_NETWORK,
+					RTDM_SUBCLASS_RTNET,
+					RTNET_RTDM_VER),
+    .device_flags =     RTDM_PROTOCOL_DEVICE,
+    .device_count =     1,
+    .context_size =     sizeof(struct rtsocket),
+
+    .protocol_family =  PF_PACKET,
+    .socket_type =      SOCK_RAW,
+
+    .ops = {
+	.socket =       rt_packet_socket,
+	.close =        rt_packet_close,
+	.ioctl_rt =     rt_packet_ioctl,
+	.ioctl_nrt =    rt_packet_ioctl,
+	.recvmsg_rt =   rt_packet_recvmsg,
+	.sendmsg_rt =   rt_packet_sendmsg,
+	.select =       rt_socket_select_bind,
+    },
+};
+
+static struct rtdm_device raw_packet_proto_dev = {
+    .driver = &raw_packet_proto_drv,
+    .label = "raw_packet",
+};
+
+static int __init rt_packet_proto_init(void)
+{
+    int err;
+
+    err = rtdm_dev_register(&packet_proto_dev);
+    if (err)
+	return err;
+
+    err = rtdm_dev_register(&raw_packet_proto_dev);
+    if (err)
+	rtdm_dev_unregister(&packet_proto_dev);
+
+    return err;
+}
+
+
+static void rt_packet_proto_release(void)
+{
+    rtdm_dev_unregister(&packet_proto_dev);
+    rtdm_dev_unregister(&raw_packet_proto_dev);
+}
+
+
+module_init(rt_packet_proto_init);
+module_exit(rt_packet_proto_release);
+
+
+
+/**********************************************************
+ * Utilities                                              *
+ **********************************************************/
+
+static int hex2int(unsigned char hex_char)
+{
+    if ((hex_char >= '0') && (hex_char <= '9'))
+	return hex_char - '0';
+    else if ((hex_char >= 'a') && (hex_char <= 'f'))
+	return hex_char - 'a' + 10;
+    else if ((hex_char >= 'A') && (hex_char <= 'F'))
+	return hex_char - 'A' + 10;
+    else
+	return -EINVAL;
+}
+
+
+
+int rt_eth_aton(unsigned char *addr_buf, const char *mac)
+{
+    int i = 0;
+    int nibble;
+
+
+    while (1) {
+	if (*mac == 0)
+	    return -EINVAL;
+
+	if ((nibble = hex2int(*mac++)) < 0)
+	    return nibble;
+	*addr_buf = nibble << 4;
+
+	if (*mac == 0)
+	    return -EINVAL;
+
+	if ((nibble = hex2int(*mac++)) < 0)
+	    return nibble;
+	*addr_buf++ |= nibble;
+
+	if (++i == 6)
+	    break;
+
+	if ((*mac == 0) || (*mac++ != ':'))
+	    return -EINVAL;
+
+    }
+    return 0;
+}
+
+EXPORT_SYMBOL_GPL(rt_eth_aton);
diff -Nur linux-5.4.5/net/rtnet/stack/packet/Kconfig linux-5.4.5-new/net/rtnet/stack/packet/Kconfig
--- linux-5.4.5/net/rtnet/stack/packet/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/packet/Kconfig	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,14 @@
+config XENO_DRIVERS_NET_RTPACKET
+    depends on XENO_DRIVERS_NET
+    tristate "Real-Time Packet Socket Support"
+    default y
+    ---help---
+    Enables real-time packet sockets for RTnet. This support is
+    implemented in a separate module. When loaded, application programs
+    can send and received so-called "cooked" packets directly at OSI layer
+    2 (device layer). This means that RTnet will still maintain the
+    device-dependent packet header but leave the full data segment to the
+    user.
+
+    Examples like raw-ethernet or netshm make use of this support. See
+    also Linux man page packet(7).
diff -Nur linux-5.4.5/net/rtnet/stack/packet/Makefile linux-5.4.5-new/net/rtnet/stack/packet/Makefile
--- linux-5.4.5/net/rtnet/stack/packet/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/packet/Makefile	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,5 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTPACKET) += rtpacket.o
+
+rtpacket-y := af_packet.o
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/Kconfig linux-5.4.5-new/net/rtnet/stack/rtcfg/Kconfig
--- linux-5.4.5/net/rtnet/stack/rtcfg/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/Kconfig	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,23 @@
+config XENO_DRIVERS_NET_RTCFG
+    depends on XENO_DRIVERS_NET
+    tristate "RTcfg Service"
+    default y
+    ---help---
+    The Real-Time Configuration service configures and monitors nodes in
+    a RTnet network. It works both with plain MAC as well as with IPv4
+    addresses (in case CONFIG_RTNET_RTIPV4 has been switched on). RTcfg
+    consists of a configuration server, which can run on the same station
+    as the TDMA master e.g., and one or more clients. Clients can join and
+    leave the network during runtime without interfering with other
+    stations. Besides network configuration, the RTcfg server can also
+    distribute custom data.
+
+    See Documentation/README.rtcfg for further information.
+
+config XENO_DRIVERS_NET_RTCFG_DEBUG
+    bool "RTcfg Debugging"
+    depends on XENO_DRIVERS_NET_RTCFG
+    default n
+    ---help---
+    Enables debug message output of the RTcfg state machines. Switch on if
+    you have to trace some problem related to RTcfg.
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/Makefile linux-5.4.5-new/net/rtnet/stack/rtcfg/Makefile
--- linux-5.4.5/net/rtnet/stack/rtcfg/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/Makefile	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,14 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTCFG) += rtcfg.o
+
+rtcfg-y := \
+	rtcfg_module.o \
+	rtcfg_event.o \
+	rtcfg_client_event.o \
+	rtcfg_conn_event.o \
+	rtcfg_ioctl.o \
+	rtcfg_frame.o \
+	rtcfg_timer.o \
+	rtcfg_file.o \
+	rtcfg_proc.o
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_client_event.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_client_event.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_client_event.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_client_event.c	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,1183 @@
+/***
+ *
+ *  rtcfg/rtcfg_client_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+static int rtcfg_client_get_frag(int ifindex, struct rt_proc_call *call);
+static void rtcfg_client_detach(int ifindex, struct rt_proc_call *call);
+static void rtcfg_client_recv_stage_1(int ifindex, struct rtskb *rtskb);
+static int rtcfg_client_recv_announce(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_stage_2_cfg(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_stage_2_frag(int ifindex, struct rtskb *rtskb);
+static int rtcfg_client_recv_ready(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_recv_dead_station(int ifindex, struct rtskb *rtskb);
+static void rtcfg_client_update_server(int ifindex, struct rtskb *rtskb);
+
+
+/*** Client States ***/
+
+int rtcfg_main_state_client_0(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            rtcfg_client_recv_stage_1(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_1(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_cmd    *cmd_event;
+    int                 ret;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_CLIENT:
+            /* second trial (buffer was probably too small) */
+            rtcfg_queue_blocking_call(ifindex,
+                (struct rt_proc_call *)event_data);
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_0);
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_ANNOUNCE:
+            cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+            if (cmd_event->args.announce.burstrate == 0) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                return -EINVAL;
+            }
+
+            rtcfg_queue_blocking_call(ifindex,
+                (struct rt_proc_call *)event_data);
+
+	    if (cmd_event->args.announce.flags & _RTCFG_FLAG_STAGE_2_DATA)
+		set_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags);
+	    if (cmd_event->args.announce.flags & _RTCFG_FLAG_READY)
+		set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags);
+            if (cmd_event->args.announce.burstrate < rtcfg_dev->burstrate)
+                rtcfg_dev->burstrate = cmd_event->args.announce.burstrate;
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ANNOUNCED);
+
+            ret = rtcfg_send_announce_new(ifindex);
+            if (ret < 0) {
+                rtcfg_dequeue_blocking_call(ifindex);
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                return ret;
+            }
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_announced(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_ANNOUNCE:
+            return rtcfg_client_get_frag(ifindex, call);
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG:
+            rtcfg_client_recv_stage_2_cfg(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG_FRAG:
+            rtcfg_client_recv_stage_2_frag(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+                    rtcfg_next_main_state(ifindex,
+                        RTCFG_MAIN_CLIENT_ALL_KNOWN);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+                    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_KNOWN);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_all_known(int ifindex, RTCFG_EVENT event_id,
+                                      void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_ANNOUNCE:
+            return rtcfg_client_get_frag(ifindex, call);
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_STAGE_2_CFG_FRAG:
+            rtcfg_client_recv_stage_2_frag(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_all_frames(int ifindex, RTCFG_EVENT event_id,
+                                       void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+                    rtcfg_next_main_state(ifindex,
+					test_bit(RTCFG_FLAG_READY,
+						&rtcfg_dev->flags) ?
+					RTCFG_MAIN_CLIENT_READY
+					: RTCFG_MAIN_CLIENT_2);
+                }
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+                    rtcfg_next_main_state(ifindex,
+					test_bit(RTCFG_FLAG_READY,
+						&rtcfg_dev->flags) ?
+					RTCFG_MAIN_CLIENT_READY
+					: RTCFG_MAIN_CLIENT_2);
+                }
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+int rtcfg_main_state_client_2(int ifindex, RTCFG_EVENT event_id,
+                              void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_READY:
+            rtcfg_dev = &device[ifindex];
+
+            if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                rtpc_complete_call(call, 0);
+            else
+                rtcfg_queue_blocking_call(ifindex, call);
+
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_READY);
+
+            if (!test_and_set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags))
+                rtcfg_send_ready(ifindex);
+
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+            return -CALL_PENDING;
+
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0)
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            /* ignore */
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            kfree_rtskb(rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+int rtcfg_main_state_client_ready(int ifindex, RTCFG_EVENT event_id,
+                                  void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rt_proc_call *call  = (struct rt_proc_call *)event_data;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    switch (event_id) {
+        case RTCFG_CMD_DETACH:
+            rtcfg_client_detach(ifindex, call);
+            break;
+
+        case RTCFG_FRM_READY:
+            if (rtcfg_client_recv_ready(ifindex, rtskb) == 0) {
+                rtcfg_dev = &device[ifindex];
+                if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                    rtcfg_complete_cmd(ifindex, RTCFG_CMD_READY, 0);
+
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            }
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            if (rtcfg_client_recv_announce(ifindex, rtskb) == 0) {
+                rtcfg_send_announce_reply(ifindex,
+                                          rtskb->mac.ethernet->h_source);
+                rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            }
+            kfree_rtskb(rtskb);
+            break;
+
+        case RTCFG_FRM_DEAD_STATION:
+            rtcfg_client_recv_dead_station(ifindex, rtskb);
+            break;
+
+        case RTCFG_FRM_STAGE_1_CFG:
+            rtcfg_client_update_server(ifindex, rtskb);
+            break;
+
+        default:
+            rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+                        rtcfg_event[event_id], ifindex, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Client Command Event Handlers ***/
+
+static int rtcfg_client_get_frag(int ifindex, struct rt_proc_call *call)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) == 0) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        return -EINVAL;
+    }
+
+    rtcfg_send_ack(ifindex);
+
+    if (rtcfg_dev->spec.clt.cfg_offs >= rtcfg_dev->spec.clt.cfg_len) {
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+            rtpc_complete_call(call, 0);
+
+            rtcfg_next_main_state(ifindex,
+				test_bit(RTCFG_FLAG_READY,
+					&rtcfg_dev->flags) ?
+				RTCFG_MAIN_CLIENT_READY : RTCFG_MAIN_CLIENT_2);
+        } else {
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_FRAMES);
+            rtcfg_queue_blocking_call(ifindex, call);
+        }
+    } else
+        rtcfg_queue_blocking_call(ifindex, call);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return -CALL_PENDING;
+}
+
+
+
+/* releases rtcfg_dev->dev_mutex on return */
+static void rtcfg_client_detach(int ifindex, struct rt_proc_call *call)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rtcfg_cmd    *cmd_event;
+
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    cmd_event->args.detach.station_addr_list =
+        rtcfg_dev->spec.clt.station_addr_list;
+    cmd_event->args.detach.stage2_chain = rtcfg_dev->spec.clt.stage2_chain;
+
+    while (1) {
+        call = rtcfg_dequeue_blocking_call(ifindex);
+        if (call == NULL)
+            break;
+
+        rtpc_complete_call(call, -ENODEV);
+    }
+
+    if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+        rtdm_timer_destroy(&rtcfg_dev->timer);
+    rtcfg_reset_device(ifindex);
+
+    rtcfg_next_main_state(cmd_event->internal.data.ifindex, RTCFG_MAIN_OFF);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+}
+
+
+
+/*** Client Frame Event Handlers ***/
+
+static void rtcfg_client_recv_stage_1(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_1_cfg *stage_1_cfg;
+    struct rt_proc_call          *call;
+    struct rtcfg_cmd             *cmd_event;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+    u8                           addr_type;
+    int                          ret;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    addr_type = stage_1_cfg->addr_type;
+
+    switch (stage_1_cfg->addr_type) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            struct rtnet_device *rtdev, *tmp;
+            u32                 daddr, saddr, mask, bcast;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg) +
+                    2*RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg "
+                            "frame\n");
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            rtdev = rtskb->rtdev;
+
+            memcpy(&daddr, stage_1_cfg->client_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            memcpy(&saddr, stage_1_cfg->server_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            __rtskb_pull(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+            /* Broadcast: IP is used to address client */
+            if (rtskb->pkt_type == PACKET_BROADCAST) {
+                /* directed to us? */
+                if (daddr != rtdev->local_ip) {
+                    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                    kfree_rtskb(rtskb);
+                    return;
+                }
+
+            /* Unicast: IP address is assigned by the server */
+            } else {
+                /* default netmask */
+                if (ntohl(daddr) <= 0x7FFFFFFF)         /* 127.255.255.255  */
+                    mask = 0x000000FF;                  /* 255.0.0.0        */
+                else if (ntohl(daddr) <= 0xBFFFFFFF)    /* 191.255.255.255  */
+                    mask = 0x0000FFFF;                  /* 255.255.0.0      */
+                else
+                    mask = 0x00FFFFFF;                  /* 255.255.255.0    */
+                bcast = daddr | (~mask);
+
+                rt_ip_route_del_all(rtdev); /* cleanup routing table */
+
+                rtdev->local_ip     = daddr;
+                rtdev->broadcast_ip = bcast;
+
+                if ((tmp = rtdev_get_loopback()) != NULL) {
+                    rt_ip_route_add_host(daddr, tmp->dev_addr, tmp);
+                    rtdev_dereference(tmp);
+                }
+
+                if (rtdev->flags & IFF_BROADCAST)
+                    rt_ip_route_add_host(bcast, rtdev->broadcast, rtdev);
+            }
+
+            /* update routing table */
+            rt_ip_route_add_host(saddr, rtskb->mac.ethernet->h_source, rtdev);
+
+            rtcfg_dev->spec.clt.srv_addr.ip_addr = saddr;
+            break;
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        stage_1_cfg->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    rtcfg_dev->spec.clt.addr_type = addr_type;
+
+    /* Ethernet-specific */
+    memcpy(rtcfg_dev->spec.clt.srv_mac_addr,
+        rtskb->mac.ethernet->h_source, ETH_ALEN);
+
+    rtcfg_dev->burstrate = stage_1_cfg->burstrate;
+
+    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_1);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    while (1) {
+        call = rtcfg_dequeue_blocking_call(ifindex);
+        if (call == NULL)
+            break;
+
+        cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+        if (cmd_event->internal.data.event_id == RTCFG_CMD_CLIENT) {
+            ret = 0;
+
+            /* note: only the first pending call gets data */
+            if ((rtskb != NULL) &&
+                (cmd_event->args.client.buffer_size > 0)) {
+                ret = ntohs(stage_1_cfg->cfg_len);
+
+                cmd_event->args.client.rtskb = rtskb;
+                rtskb = NULL;
+            }
+        } else
+            ret = -EINVAL;
+
+        rtpc_complete_call(call, ret);
+    }
+
+    if (rtskb)
+        kfree_rtskb(rtskb);
+}
+
+
+
+static int rtcfg_add_to_station_list(struct rtcfg_device *rtcfg_dev,
+                                     u8 *mac_addr, u8 flags)
+{
+   if (rtcfg_dev->stations_found == rtcfg_dev->spec.clt.max_stations) {
+        RTCFG_DEBUG(1, "RTcfg: insufficient memory for storing new station "
+                    "address\n");
+        return -ENOMEM;
+    }
+
+    /* Ethernet-specific! */
+    memcpy(&rtcfg_dev->spec.clt.
+           station_addr_list[rtcfg_dev->stations_found].mac_addr,
+           mac_addr, ETH_ALEN);
+
+    rtcfg_dev->spec.clt.station_addr_list[rtcfg_dev->stations_found].flags =
+        flags;
+
+    rtcfg_dev->stations_found++;
+    if ((flags & _RTCFG_FLAG_READY) != 0)
+        rtcfg_dev->stations_ready++;
+
+    return 0;
+}
+
+
+
+/* Notes:
+ *  o rtcfg_client_recv_announce does not release the passed rtskb.
+ *  o On success, rtcfg_client_recv_announce returns without releasing the
+ *    device lock.
+ */
+static int rtcfg_client_recv_announce(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_announce *announce_frm;
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    u32                       i;
+    u32                       announce_frm_addr;
+    int                       result;
+
+
+    announce_frm = (struct rtcfg_frm_announce *)rtskb->data;
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_announce)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid announce frame (id: %d)\n",
+                    announce_frm->head.id);
+        return -EINVAL;
+    }
+
+    switch (announce_frm->addr_type) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        case RTCFG_ADDR_IP:
+            if (rtskb->len < sizeof(struct rtcfg_frm_announce) +
+                    RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid announce frame "
+                            "(id: %d)\n", announce_frm->head.id);
+                return -EINVAL;
+            }
+
+            memcpy(&announce_frm_addr, announce_frm->addr, 4);
+
+            /* update routing table */
+            rt_ip_route_add_host(announce_frm_addr,
+                                 rtskb->mac.ethernet->h_source, rtskb->rtdev);
+
+            announce_frm = (struct rtcfg_frm_announce *)
+                (((u8 *)announce_frm) + RTCFG_ADDRSIZE_IP);
+
+            break;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        announce_frm->addr_type, __FUNCTION__);
+            return -EINVAL;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   rtskb->mac.ethernet->h_source, ETH_ALEN) == 0)
+            return 0;
+
+    result = rtcfg_add_to_station_list(rtcfg_dev,
+        rtskb->mac.ethernet->h_source, announce_frm->flags);
+    if (result < 0)
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return result;
+}
+
+
+
+static void rtcfg_client_queue_frag(int ifindex, struct rtskb *rtskb,
+                                    size_t data_len)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+    int                 result;
+
+
+    rtskb_trim(rtskb, data_len);
+
+    if (rtcfg_dev->spec.clt.stage2_chain == NULL)
+        rtcfg_dev->spec.clt.stage2_chain = rtskb;
+    else {
+        rtcfg_dev->spec.clt.stage2_chain->chain_end->next = rtskb;
+        rtcfg_dev->spec.clt.stage2_chain->chain_end = rtskb;
+    }
+
+    rtcfg_dev->spec.clt.cfg_offs  += data_len;
+    rtcfg_dev->spec.clt.chain_len += data_len;
+
+    if ((rtcfg_dev->spec.clt.cfg_offs >= rtcfg_dev->spec.clt.cfg_len) ||
+        (++rtcfg_dev->spec.clt.packet_counter == rtcfg_dev->burstrate)) {
+
+        while (1) {
+            call = rtcfg_dequeue_blocking_call(ifindex);
+            if (call == NULL)
+                break;
+
+            cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+            result = 0;
+
+            /* note: only the first pending call gets data */
+            if (rtcfg_dev->spec.clt.stage2_chain != NULL) {
+                result = rtcfg_dev->spec.clt.chain_len;
+                cmd_event->args.announce.rtskb =
+                    rtcfg_dev->spec.clt.stage2_chain;
+                rtcfg_dev->spec.clt.stage2_chain = NULL;
+            }
+
+            rtpc_complete_call(call,
+                (cmd_event->internal.data.event_id == RTCFG_CMD_ANNOUNCE) ?
+                result : -EINVAL);
+        }
+
+        rtcfg_dev->spec.clt.packet_counter = 0;
+        rtcfg_dev->spec.clt.chain_len      = 0;
+    }
+}
+
+
+
+static void rtcfg_client_recv_stage_2_cfg(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_2_cfg *stage_2_cfg;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+    size_t                       data_len;
+    int                          ret;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_2_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_2_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_2_cfg = (struct rtcfg_frm_stage_2_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg));
+
+    if (stage_2_cfg->heartbeat_period) {
+	ret = rtdm_timer_init(&rtcfg_dev->timer, rtcfg_timer, "rtcfg-timer");
+	if (ret == 0) {
+	    ret = rtdm_timer_start(&rtcfg_dev->timer,
+				XN_INFINITE,
+				(nanosecs_rel_t)ntohs(stage_2_cfg->heartbeat_period) *
+				1000000,
+				RTDM_TIMERMODE_RELATIVE);
+	    if (ret < 0)
+		rtdm_timer_destroy(&rtcfg_dev->timer);
+	}
+
+        if (ret < 0)
+            /*ERRMSG*/rtdm_printk("RTcfg: unable to create timer task\n");
+        else
+	    set_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags);
+    }
+
+    /* add server to station list */
+    if (rtcfg_add_to_station_list(rtcfg_dev,
+            rtskb->mac.ethernet->h_source, stage_2_cfg->flags) < 0) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: unable to process stage_2_cfg frage\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    rtcfg_dev->other_stations   = ntohl(stage_2_cfg->stations);
+    rtcfg_dev->spec.clt.cfg_len = ntohl(stage_2_cfg->cfg_len);
+    data_len = MIN(rtcfg_dev->spec.clt.cfg_len, rtskb->len);
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) &&
+        (data_len > 0)) {
+        rtcfg_client_queue_frag(ifindex, rtskb, data_len);
+        rtskb = NULL;
+
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations)
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_KNOWN);
+    } else {
+        if (rtcfg_dev->stations_found == rtcfg_dev->other_stations) {
+            rtcfg_complete_cmd(ifindex, RTCFG_CMD_ANNOUNCE, 0);
+
+            rtcfg_next_main_state(ifindex,
+				test_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags) ?
+				RTCFG_MAIN_CLIENT_READY : RTCFG_MAIN_CLIENT_2);
+        } else
+            rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ALL_FRAMES);
+
+        rtcfg_send_ack(ifindex);
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+static void rtcfg_client_recv_stage_2_frag(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_2_cfg_frag *stage_2_frag;
+    struct rtcfg_device               *rtcfg_dev = &device[ifindex];
+    size_t                            data_len;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_2_cfg_frag)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_2_cfg_frag frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_2_frag = (struct rtcfg_frm_stage_2_cfg_frag *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg_frag));
+
+    data_len = MIN(rtcfg_dev->spec.clt.cfg_len - rtcfg_dev->spec.clt.cfg_offs,
+                   rtskb->len);
+
+    if (test_bit(RTCFG_FLAG_STAGE_2_DATA, &rtcfg_dev->flags) == 0) {
+        RTCFG_DEBUG(1, "RTcfg: unexpected stage 2 fragment, we did not "
+                    "request any data!\n");
+
+    } else if (rtcfg_dev->spec.clt.cfg_offs !=
+               ntohl(stage_2_frag->frag_offs)) {
+        RTCFG_DEBUG(1, "RTcfg: unexpected stage 2 fragment (expected: %d, "
+                    "received: %d)\n", rtcfg_dev->spec.clt.cfg_offs,
+                    ntohl(stage_2_frag->frag_offs));
+
+        rtcfg_send_ack(ifindex);
+        rtcfg_dev->spec.clt.packet_counter = 0;
+    } else {
+        rtcfg_client_queue_frag(ifindex, rtskb, data_len);
+        rtskb = NULL;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+/* Notes:
+ *  o On success, rtcfg_client_recv_ready returns without releasing the
+ *    device lock.
+ */
+static int rtcfg_client_recv_ready(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    u32                     i;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_simple)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid ready frame\n");
+        kfree_rtskb(rtskb);
+        return -EINVAL;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   rtskb->mac.ethernet->h_source, ETH_ALEN) == 0) {
+            if ((rtcfg_dev->spec.clt.station_addr_list[i].flags &
+                 _RTCFG_FLAG_READY) == 0) {
+                rtcfg_dev->spec.clt.station_addr_list[i].flags |=
+                    _RTCFG_FLAG_READY;
+                rtcfg_dev->stations_ready++;
+            }
+            break;
+        }
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static void rtcfg_client_recv_dead_station(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_dead_station *dead_station_frm;
+    struct rtcfg_device           *rtcfg_dev = &device[ifindex];
+    u32                           i;
+
+
+    dead_station_frm = (struct rtcfg_frm_dead_station *)rtskb->data;
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_dead_station)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid dead station frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    switch (dead_station_frm->addr_type) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            u32 ip;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_dead_station) +
+                    RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid dead station frame\n");
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            memcpy(&ip, dead_station_frm->logical_addr, 4);
+
+            /* only delete remote IPs from routing table */
+            if (rtskb->rtdev->local_ip != ip)
+                rt_ip_route_del_host(ip, rtskb->rtdev);
+
+            dead_station_frm = (struct rtcfg_frm_dead_station *)
+                (((u8 *)dead_station_frm) + RTCFG_ADDRSIZE_IP);
+
+            break;
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        dead_station_frm->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    for (i = 0; i < rtcfg_dev->stations_found; i++)
+        /* Ethernet-specific! */
+        if (memcmp(rtcfg_dev->spec.clt.station_addr_list[i].mac_addr,
+                   dead_station_frm->physical_addr, ETH_ALEN) == 0) {
+            if ((rtcfg_dev->spec.clt.station_addr_list[i].flags &
+                 _RTCFG_FLAG_READY) != 0)
+                rtcfg_dev->stations_ready--;
+
+            rtcfg_dev->stations_found--;
+            memmove(&rtcfg_dev->spec.clt.station_addr_list[i],
+                    &rtcfg_dev->spec.clt.station_addr_list[i+1],
+                    sizeof(struct rtcfg_station) *
+                    (rtcfg_dev->stations_found - i));
+
+            if (rtcfg_dev->state == RTCFG_MAIN_CLIENT_ALL_KNOWN)
+                rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_ANNOUNCED);
+            break;
+        }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+}
+
+
+
+static void rtcfg_client_update_server(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_frm_stage_1_cfg *stage_1_cfg;
+    struct rtcfg_device          *rtcfg_dev = &device[ifindex];
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg)) {
+        rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+        RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg frame\n");
+        kfree_rtskb(rtskb);
+        return;
+    }
+
+    stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)rtskb->data;
+    __rtskb_pull(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    switch (stage_1_cfg->addr_type) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        case RTCFG_ADDR_IP: {
+            struct rtnet_device *rtdev;
+            u32                 daddr, saddr;
+
+            if (rtskb->len < sizeof(struct rtcfg_frm_stage_1_cfg) +
+                    2*RTCFG_ADDRSIZE_IP) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                RTCFG_DEBUG(1, "RTcfg: received invalid stage_1_cfg "
+                            "frame\n");
+                kfree_rtskb(rtskb);
+                break;
+            }
+
+            rtdev = rtskb->rtdev;
+
+            memcpy(&daddr, stage_1_cfg->client_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            memcpy(&saddr, stage_1_cfg->server_addr, 4);
+            stage_1_cfg = (struct rtcfg_frm_stage_1_cfg *)
+                (((u8 *)stage_1_cfg) + RTCFG_ADDRSIZE_IP);
+
+            __rtskb_pull(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+            /* directed to us? */
+            if ((rtskb->pkt_type == PACKET_BROADCAST) &&
+                (daddr != rtdev->local_ip)) {
+                rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+                kfree_rtskb(rtskb);
+                return;
+            }
+
+            /* update routing table */
+            rt_ip_route_add_host(saddr, rtskb->mac.ethernet->h_source, rtdev);
+
+            rtcfg_dev->spec.clt.srv_addr.ip_addr = saddr;
+            break;
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+        case RTCFG_ADDR_MAC:
+            /* nothing to do */
+            break;
+
+        default:
+            rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+            RTCFG_DEBUG(1, "RTcfg: unknown addr_type %d in %s()\n",
+                        stage_1_cfg->addr_type, __FUNCTION__);
+            kfree_rtskb(rtskb);
+            return;
+    }
+
+    /* Ethernet-specific */
+    memcpy(rtcfg_dev->spec.clt.srv_mac_addr,
+        rtskb->mac.ethernet->h_source, ETH_ALEN);
+
+    rtcfg_send_announce_reply(ifindex, rtskb->mac.ethernet->h_source);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_conn_event.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_conn_event.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_conn_event.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_conn_event.c	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,396 @@
+/***
+ *
+ *  rtcfg/rtcfg_conn_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+
+
+/****************************** states ***************************************/
+static int rtcfg_conn_state_searching(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_stage_1(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_stage_2(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_ready(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_conn_state_dead(
+    struct rtcfg_connection *conn, RTCFG_EVENT event_id, void* event_data);
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTCFG_DEBUG
+const char *rtcfg_conn_state[] = {
+    "RTCFG_CONN_SEARCHING",
+    "RTCFG_CONN_STAGE_1",
+    "RTCFG_CONN_STAGE_2",
+    "RTCFG_CONN_READY",
+    "RTCFG_CONN_DEAD"
+};
+#endif /* CONFIG_XENO_DRIVERS_NET_RTCFG_DEBUG */
+
+
+static void rtcfg_conn_recv_announce_new(struct rtcfg_connection *conn,
+                                         struct rtskb *rtskb);
+static void rtcfg_conn_check_cfg_timeout(struct rtcfg_connection *conn);
+static void rtcfg_conn_check_heartbeat(struct rtcfg_connection *conn);
+
+
+
+static int (*state[])(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+                      void* event_data) =
+{
+    rtcfg_conn_state_searching,
+    rtcfg_conn_state_stage_1,
+    rtcfg_conn_state_stage_2,
+    rtcfg_conn_state_ready,
+    rtcfg_conn_state_dead
+};
+
+
+
+int rtcfg_do_conn_event(struct rtcfg_connection *conn, RTCFG_EVENT event_id,
+                        void* event_data)
+{
+    int conn_state = conn->state;
+
+
+    RTCFG_DEBUG(3, "RTcfg: %s() conn=%p, event=%s, state=%s\n", __FUNCTION__,
+                conn, rtcfg_event[event_id], rtcfg_conn_state[conn_state]);
+
+    return (*state[conn_state])(conn, event_id, event_data);
+}
+
+
+
+static void rtcfg_next_conn_state(struct rtcfg_connection *conn,
+                                  RTCFG_CONN_STATE state)
+{
+    RTCFG_DEBUG(4, "RTcfg: next connection state=%s \n",
+                rtcfg_conn_state[state]);
+
+    conn->state = state;
+}
+
+
+
+static int rtcfg_conn_state_searching(struct rtcfg_connection *conn,
+                                      RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtcfg_device *rtcfg_dev = &device[conn->ifindex];
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            rtcfg_conn_recv_announce_new(conn, rtskb);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            conn->last_frame = rtskb->time_stamp;
+
+            rtcfg_next_conn_state(conn, RTCFG_CONN_READY);
+
+            rtcfg_dev->stations_found++;
+            rtcfg_dev->stations_ready++;
+            rtcfg_dev->spec.srv.clients_configured++;
+            if (rtcfg_dev->spec.srv.clients_configured ==
+                rtcfg_dev->other_stations)
+                rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_stage_1(struct rtcfg_connection *conn,
+                                    RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb             *rtskb     = (struct rtskb *)event_data;
+    struct rtcfg_device      *rtcfg_dev = &device[conn->ifindex];
+    struct rtcfg_frm_ack_cfg *ack_cfg;
+    int                      packets;
+
+
+    switch (event_id) {
+        case RTCFG_FRM_ACK_CFG:
+            conn->last_frame = rtskb->time_stamp;
+
+            ack_cfg = (struct rtcfg_frm_ack_cfg *)rtskb->data;
+            conn->cfg_offs = ntohl(ack_cfg->ack_len);
+
+            if ((conn->flags & _RTCFG_FLAG_STAGE_2_DATA) != 0) {
+                if (conn->cfg_offs >= conn->stage2_file->size) {
+                    rtcfg_dev->spec.srv.clients_configured++;
+                    if (rtcfg_dev->spec.srv.clients_configured ==
+                        rtcfg_dev->other_stations)
+                        rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+                    rtcfg_next_conn_state(conn,
+                        ((conn->flags & _RTCFG_FLAG_READY) != 0) ?
+                        RTCFG_CONN_READY : RTCFG_CONN_STAGE_2);
+                } else {
+                    packets = conn->burstrate;
+                    while ((conn->cfg_offs < conn->stage2_file->size) &&
+                        (packets > 0)) {
+                        rtcfg_send_stage_2_frag(conn);
+                        packets--;
+                    }
+                }
+            } else {
+                rtcfg_dev->spec.srv.clients_configured++;
+                if (rtcfg_dev->spec.srv.clients_configured ==
+                    rtcfg_dev->other_stations)
+                    rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_WAIT, 0);
+                rtcfg_next_conn_state(conn,
+                    ((conn->flags & _RTCFG_FLAG_READY) != 0) ?
+                    RTCFG_CONN_READY : RTCFG_CONN_STAGE_2);
+            }
+
+            break;
+
+        case RTCFG_TIMER:
+            rtcfg_conn_check_cfg_timeout(conn);
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_stage_2(struct rtcfg_connection *conn,
+                                    RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb        *rtskb = (struct rtskb *)event_data;
+    struct rtcfg_device *rtcfg_dev = &device[conn->ifindex];
+
+
+    switch (event_id) {
+        case RTCFG_FRM_READY:
+            conn->last_frame = rtskb->time_stamp;
+
+            rtcfg_next_conn_state(conn, RTCFG_CONN_READY);
+
+            conn->flags |= _RTCFG_FLAG_READY;
+            rtcfg_dev->stations_ready++;
+
+            if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+                rtcfg_complete_cmd(conn->ifindex, RTCFG_CMD_READY, 0);
+
+            break;
+
+        case RTCFG_TIMER:
+            rtcfg_conn_check_cfg_timeout(conn);
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_ready(struct rtcfg_connection *conn,
+                                  RTCFG_EVENT event_id, void* event_data)
+{
+    struct rtskb *rtskb = (struct rtskb *)event_data;
+
+
+    switch (event_id) {
+        case RTCFG_TIMER:
+            rtcfg_conn_check_heartbeat(conn);
+            break;
+
+        case RTCFG_FRM_HEARTBEAT:
+            conn->last_frame = rtskb->time_stamp;
+            break;
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static int rtcfg_conn_state_dead(struct rtcfg_connection *conn,
+                                 RTCFG_EVENT event_id, void* event_data)
+{
+    switch (event_id) {
+        case RTCFG_FRM_ANNOUNCE_NEW:
+            rtcfg_conn_recv_announce_new(conn, (struct rtskb *)event_data);
+            break;
+
+        case RTCFG_FRM_ANNOUNCE_REPLY:
+            /* Spec to-do: signal station that it is assumed to be dead
+               (=> reboot command?) */
+
+        default:
+            RTCFG_DEBUG(1, "RTcfg: unknown event %s for conn %p in %s()\n",
+                        rtcfg_event[event_id], conn, __FUNCTION__);
+            return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+static void rtcfg_conn_recv_announce_new(struct rtcfg_connection *conn,
+                                         struct rtskb *rtskb)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[conn->ifindex];
+    struct rtcfg_frm_announce *announce_new;
+    int                       packets;
+
+
+    conn->last_frame = rtskb->time_stamp;
+
+    announce_new = (struct rtcfg_frm_announce *)rtskb->data;
+
+    conn->flags = announce_new->flags;
+    if (announce_new->burstrate < conn->burstrate)
+        conn->burstrate = announce_new->burstrate;
+
+    rtcfg_next_conn_state(conn, RTCFG_CONN_STAGE_1);
+
+    rtcfg_dev->stations_found++;
+    if ((conn->flags & _RTCFG_FLAG_READY) != 0)
+        rtcfg_dev->stations_ready++;
+
+    if (((conn->flags & _RTCFG_FLAG_STAGE_2_DATA) != 0) &&
+        (conn->stage2_file != NULL)) {
+        packets = conn->burstrate - 1;
+
+        rtcfg_send_stage_2(conn, 1);
+
+        while ((conn->cfg_offs < conn->stage2_file->size) &&
+            (packets > 0)) {
+            rtcfg_send_stage_2_frag(conn);
+            packets--;
+        }
+    } else {
+        rtcfg_send_stage_2(conn, 0);
+        conn->flags &= ~_RTCFG_FLAG_STAGE_2_DATA;
+    }
+}
+
+
+
+static void rtcfg_conn_check_cfg_timeout(struct rtcfg_connection *conn)
+{
+    struct rtcfg_device *rtcfg_dev;
+
+
+    if (!conn->cfg_timeout)
+        return;
+
+    if (rtdm_clock_read() >= conn->last_frame + conn->cfg_timeout) {
+        rtcfg_dev = &device[conn->ifindex];
+
+        rtcfg_dev->stations_found--;
+        if (conn->state == RTCFG_CONN_STAGE_2)
+            rtcfg_dev->spec.srv.clients_configured--;
+
+        rtcfg_next_conn_state(conn, RTCFG_CONN_SEARCHING);
+        conn->cfg_offs = 0;
+        conn->flags    = 0;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        if (conn->addr_type == RTCFG_ADDR_IP) {
+            struct rtnet_device *rtdev;
+
+            /* MAC address yet unknown -> use broadcast address */
+            rtdev = rtdev_get_by_index(conn->ifindex);
+            if (rtdev == NULL)
+                return;
+            memcpy(conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+            rtdev_dereference(rtdev);
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+    }
+}
+
+
+
+static void rtcfg_conn_check_heartbeat(struct rtcfg_connection *conn)
+{
+    u64                 timeout;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    timeout = device[conn->ifindex].spec.srv.heartbeat_timeout;
+    if (!timeout)
+        return;
+
+    if (rtdm_clock_read() >= conn->last_frame + timeout) {
+        rtcfg_dev = &device[conn->ifindex];
+
+        rtcfg_dev->stations_found--;
+        rtcfg_dev->stations_ready--;
+        rtcfg_dev->spec.srv.clients_configured--;
+
+        rtcfg_send_dead_station(conn);
+
+        rtcfg_next_conn_state(conn, RTCFG_CONN_DEAD);
+        conn->cfg_offs = 0;
+        conn->flags    = 0;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+        if ((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) {
+            struct rtnet_device *rtdev = rtdev_get_by_index(conn->ifindex);
+
+            rt_ip_route_del_host(conn->addr.ip_addr, rtdev);
+
+            if (rtdev == NULL)
+                return;
+
+            if (!(conn->addr_type & FLAG_ASSIGN_ADDR_BY_MAC))
+                /* MAC address yet unknown -> use broadcast address */
+                memcpy(conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+
+            rtdev_dereference(rtdev);
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+    }
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_event.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_event.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_event.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_event.c	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,784 @@
+/***
+ *
+ *  rtcfg/rtcfg_event.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/vmalloc.h>
+
+#include <rtdev.h>
+#include <ipv4/route.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_client_event.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_file.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+/*** Common and Server States ***/
+static int rtcfg_main_state_off(
+    int ifindex, RTCFG_EVENT event_id, void* event_data);
+static int rtcfg_main_state_server_running(
+    int ifindex, RTCFG_EVENT event_id, void* event_data);
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_RTCFG_DEBUG
+const char *rtcfg_event[] = {
+    "RTCFG_CMD_SERVER",
+    "RTCFG_CMD_ADD",
+    "RTCFG_CMD_DEL",
+    "RTCFG_CMD_WAIT",
+    "RTCFG_CMD_CLIENT",
+    "RTCFG_CMD_ANNOUNCE",
+    "RTCFG_CMD_READY",
+    "RTCFG_CMD_DETACH",
+    "RTCFG_TIMER",
+    "RTCFG_FRM_STAGE_1_CFG",
+    "RTCFG_FRM_ANNOUNCE_NEW",
+    "RTCFG_FRM_ANNOUNCE_REPLY",
+    "RTCFG_FRM_STAGE_2_CFG",
+    "RTCFG_FRM_STAGE_2_CFG_FRAG",
+    "RTCFG_FRM_ACK_CFG",
+    "RTCFG_FRM_READY",
+    "RTCFG_FRM_HEARTBEAT",
+    "RTCFG_FRM_DEAD_STATION"
+};
+
+const char *rtcfg_main_state[] = {
+    "RTCFG_MAIN_OFF",
+    "RTCFG_MAIN_SERVER_RUNNING",
+    "RTCFG_MAIN_CLIENT_0",
+    "RTCFG_MAIN_CLIENT_1",
+    "RTCFG_MAIN_CLIENT_ANNOUNCED",
+    "RTCFG_MAIN_CLIENT_ALL_KNOWN",
+    "RTCFG_MAIN_CLIENT_ALL_FRAMES",
+    "RTCFG_MAIN_CLIENT_2",
+    "RTCFG_MAIN_CLIENT_READY"
+};
+
+int rtcfg_debug = RTCFG_DEFAULT_DEBUG_LEVEL;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTCFG_DEBUG */
+
+
+struct rtcfg_device device[MAX_RT_DEVICES];
+
+static int (*state[])(int ifindex, RTCFG_EVENT event_id, void* event_data) =
+{
+    rtcfg_main_state_off,
+    rtcfg_main_state_server_running,
+    rtcfg_main_state_client_0,
+    rtcfg_main_state_client_1,
+    rtcfg_main_state_client_announced,
+    rtcfg_main_state_client_all_known,
+    rtcfg_main_state_client_all_frames,
+    rtcfg_main_state_client_2,
+    rtcfg_main_state_client_ready
+};
+
+
+static int rtcfg_server_add(struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_del(struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_detach(int ifindex, struct rtcfg_cmd *cmd_event);
+static int rtcfg_server_recv_announce(int ifindex, RTCFG_EVENT event_id,
+				      struct rtskb *rtskb);
+static int rtcfg_server_recv_ack(int ifindex, struct rtskb *rtskb);
+static int rtcfg_server_recv_simple_frame(int ifindex, RTCFG_EVENT event_id,
+					  struct rtskb *rtskb);
+
+
+
+int rtcfg_do_main_event(int ifindex, RTCFG_EVENT event_id, void* event_data)
+{
+    int main_state;
+
+
+    rtdm_mutex_lock(&device[ifindex].dev_mutex);
+
+    main_state = device[ifindex].state;
+
+    RTCFG_DEBUG(3, "RTcfg: %s() rtdev=%d, event=%s, state=%s\n", __FUNCTION__,
+		ifindex, rtcfg_event[event_id], rtcfg_main_state[main_state]);
+
+    return (*state[main_state])(ifindex, event_id, event_data);
+}
+
+
+
+void rtcfg_next_main_state(int ifindex, RTCFG_MAIN_STATE state)
+{
+    RTCFG_DEBUG(4, "RTcfg: next main state=%s \n", rtcfg_main_state[state]);
+
+    device[ifindex].state = state;
+}
+
+
+
+static int rtcfg_main_state_off(int ifindex, RTCFG_EVENT event_id,
+				void* event_data)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct rt_proc_call     *call      = (struct rt_proc_call *)event_data;
+    struct rtcfg_cmd        *cmd_event;
+    int                     ret;
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+    switch (event_id) {
+	case RTCFG_CMD_SERVER:
+	    INIT_LIST_HEAD(&rtcfg_dev->spec.srv.conn_list);
+
+	    ret = rtdm_timer_init(&rtcfg_dev->timer, rtcfg_timer, "rtcfg-timer");
+	    if (ret == 0) {
+		    ret = rtdm_timer_start(&rtcfg_dev->timer,
+					    XN_INFINITE,
+					    (nanosecs_rel_t)
+					    cmd_event->args.server.period
+					    * 1000000,
+					    RTDM_TIMERMODE_RELATIVE);
+		    if (ret < 0)
+			    rtdm_timer_destroy(&rtcfg_dev->timer);
+	    }
+	    if (ret < 0) {
+		rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+		return ret;
+	    }
+
+	    if (cmd_event->args.server.flags & _RTCFG_FLAG_READY)
+		    set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags);
+	    set_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags);
+
+	    rtcfg_dev->burstrate = cmd_event->args.server.burstrate;
+
+	    rtcfg_dev->spec.srv.heartbeat = cmd_event->args.server.heartbeat;
+
+	    rtcfg_dev->spec.srv.heartbeat_timeout =
+		    ((u64)cmd_event->args.server.heartbeat) * 1000000 *
+		    cmd_event->args.server.threshold;
+
+	    rtcfg_next_main_state(ifindex, RTCFG_MAIN_SERVER_RUNNING);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    break;
+
+	case RTCFG_CMD_CLIENT:
+	    rtcfg_dev->spec.clt.station_addr_list =
+		cmd_event->args.client.station_buf;
+	    cmd_event->args.client.station_buf = NULL;
+
+	    rtcfg_dev->spec.clt.max_stations =
+		cmd_event->args.client.max_stations;
+	    rtcfg_dev->other_stations = -1;
+
+	    rtcfg_queue_blocking_call(ifindex, call);
+
+	    rtcfg_next_main_state(ifindex, RTCFG_MAIN_CLIENT_0);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	default:
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+			rtcfg_event[event_id], ifindex, __FUNCTION__);
+	    return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Server States ***/
+
+static int rtcfg_main_state_server_running(int ifindex, RTCFG_EVENT event_id,
+					   void* event_data)
+{
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+    struct rtcfg_device *rtcfg_dev;
+    struct rtskb        *rtskb;
+
+
+    switch (event_id) {
+	case RTCFG_CMD_ADD:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_add(cmd_event);
+
+	case RTCFG_CMD_DEL:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_del(cmd_event);
+
+	case RTCFG_CMD_WAIT:
+	    call = (struct rt_proc_call *)event_data;
+
+	    rtcfg_dev = &device[ifindex];
+
+	    if (rtcfg_dev->spec.srv.clients_configured ==
+		rtcfg_dev->other_stations)
+		rtpc_complete_call(call, 0);
+	    else
+		rtcfg_queue_blocking_call(ifindex, call);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	case RTCFG_CMD_READY:
+	    call = (struct rt_proc_call *)event_data;
+
+	    rtcfg_dev = &device[ifindex];
+
+	    if (rtcfg_dev->stations_ready == rtcfg_dev->other_stations)
+		rtpc_complete_call(call, 0);
+	    else
+		rtcfg_queue_blocking_call(ifindex, call);
+
+	    if (!test_and_set_bit(RTCFG_FLAG_READY, &rtcfg_dev->flags))
+		rtcfg_send_ready(ifindex);
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    return -CALL_PENDING;
+
+	case RTCFG_CMD_DETACH:
+	    call      = (struct rt_proc_call *)event_data;
+	    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	    return rtcfg_server_detach(ifindex, cmd_event);
+
+	case RTCFG_FRM_ANNOUNCE_NEW:
+	case RTCFG_FRM_ANNOUNCE_REPLY:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_announce(ifindex, event_id, rtskb);
+
+	case RTCFG_FRM_ACK_CFG:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_ack(ifindex, rtskb);
+
+	case RTCFG_FRM_READY:
+	case RTCFG_FRM_HEARTBEAT:
+	    rtskb = (struct rtskb *)event_data;
+	    return rtcfg_server_recv_simple_frame(ifindex, event_id, rtskb);
+
+	default:
+	    rtdm_mutex_unlock(&device[ifindex].dev_mutex);
+
+	    RTCFG_DEBUG(1, "RTcfg: unknown event %s for rtdev %d in %s()\n",
+			rtcfg_event[event_id], ifindex, __FUNCTION__);
+	    return -EINVAL;
+    }
+    return 0;
+}
+
+
+
+/*** Server Command Event Handlers ***/
+
+static int rtcfg_server_add(struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_device     *rtcfg_dev;
+    struct rtcfg_connection *conn;
+    struct rtcfg_connection *new_conn;
+    struct list_head        *entry;
+    unsigned int            addr_type;
+
+    rtcfg_dev = &device[cmd_event->internal.data.ifindex];
+    addr_type = cmd_event->args.add.addr_type & RTCFG_ADDR_MASK;
+
+    new_conn = cmd_event->args.add.conn_buf;
+    memset(new_conn, 0, sizeof(struct rtcfg_connection));
+
+    new_conn->ifindex      = cmd_event->internal.data.ifindex;
+    new_conn->state        = RTCFG_CONN_SEARCHING;
+    new_conn->addr_type    = cmd_event->args.add.addr_type;
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+    new_conn->addr.ip_addr = cmd_event->args.add.ip_addr;
+#endif
+    new_conn->stage1_data  = cmd_event->args.add.stage1_data;
+    new_conn->stage1_size  = cmd_event->args.add.stage1_size;
+    new_conn->burstrate    = rtcfg_dev->burstrate;
+    new_conn->cfg_timeout  = ((u64)cmd_event->args.add.timeout) * 1000000;
+
+    if (cmd_event->args.add.addr_type == RTCFG_ADDR_IP) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	struct rtnet_device *rtdev;
+
+	/* MAC address yet unknown -> use broadcast address */
+	rtdev = rtdev_get_by_index(cmd_event->internal.data.ifindex);
+	if (rtdev == NULL) {
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	    return -ENODEV;
+	}
+	memcpy(new_conn->mac_addr, rtdev->broadcast, MAX_ADDR_LEN);
+	rtdev_dereference(rtdev);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	return -EPROTONOSUPPORT;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+    } else
+	memcpy(new_conn->mac_addr, cmd_event->args.add.mac_addr, MAX_ADDR_LEN);
+
+    /* get stage 2 file */
+    if (cmd_event->args.add.stage2_file != NULL) {
+	if (cmd_event->args.add.stage2_file->buffer != NULL) {
+	    new_conn->stage2_file = cmd_event->args.add.stage2_file;
+	    rtcfg_add_file(new_conn->stage2_file);
+
+	    cmd_event->args.add.stage2_file = NULL;
+	} else {
+	    new_conn->stage2_file =
+		rtcfg_get_file(cmd_event->args.add.stage2_file->name);
+	    if (new_conn->stage2_file == NULL) {
+		rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+		return 1;
+	    }
+	}
+    }
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	if (
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	    ((addr_type == RTCFG_ADDR_IP) &&
+	     (conn->addr.ip_addr == cmd_event->args.add.ip_addr)) ||
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	    ((addr_type == RTCFG_ADDR_MAC) &&
+	     (memcmp(conn->mac_addr, new_conn->mac_addr,
+		     MAX_ADDR_LEN) == 0))
+	   ) {
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    if ((new_conn->stage2_file) &&
+		(rtcfg_release_file(new_conn->stage2_file) == 0)) {
+		/* Note: This assignment cannot overwrite a valid file pointer.
+		 * Effectively, it will only be executed when
+		 * new_conn->stage2_file is the pointer originally passed by
+		 * rtcfg_ioctl. But checking this assumptions does not cause
+		 * any harm :o)
+		 */
+		RTNET_ASSERT(cmd_event->args.add.stage2_file == NULL, ;);
+
+		cmd_event->args.add.stage2_file = new_conn->stage2_file;
+	    }
+
+	    return -EEXIST;
+	}
+    }
+
+    list_add_tail(&new_conn->entry, &rtcfg_dev->spec.srv.conn_list);
+    rtcfg_dev->other_stations++;
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    cmd_event->args.add.conn_buf    = NULL;
+    cmd_event->args.add.stage1_data = NULL;
+
+    return 0;
+}
+
+
+
+static int rtcfg_server_del(struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_connection *conn;
+    struct list_head        *entry;
+    unsigned int            addr_type;
+    struct rtcfg_device     *rtcfg_dev;
+
+
+    rtcfg_dev = &device[cmd_event->internal.data.ifindex];
+    addr_type = cmd_event->args.add.addr_type & RTCFG_ADDR_MASK;
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	if ((addr_type == conn->addr_type) && (
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	     ((addr_type == RTCFG_ADDR_IP) &&
+	      (conn->addr.ip_addr == cmd_event->args.add.ip_addr)) ||
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	     ((addr_type == RTCFG_ADDR_MAC) &&
+	      (memcmp(conn->mac_addr, cmd_event->args.add.mac_addr,
+		      MAX_ADDR_LEN) == 0)))) {
+	    list_del(&conn->entry);
+	    rtcfg_dev->other_stations--;
+
+	    if (conn->state > RTCFG_CONN_SEARCHING) {
+		rtcfg_dev->stations_found--;
+		if (conn->state >= RTCFG_CONN_STAGE_2)
+		    rtcfg_dev->spec.srv.clients_configured--;
+		if (conn->flags & _RTCFG_FLAG_READY)
+		    rtcfg_dev->stations_ready--;
+	    }
+
+	    if ((conn->stage2_file) &&
+		(rtcfg_release_file(conn->stage2_file) == 0))
+		cmd_event->args.del.stage2_file = conn->stage2_file;
+
+	    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	    cmd_event->args.del.conn_buf = conn;
+
+	    return 0;
+	}
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return -ENOENT;
+}
+
+
+
+static int rtcfg_server_detach(int ifindex, struct rtcfg_cmd *cmd_event)
+{
+    struct rtcfg_connection *conn;
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+
+
+    if (!list_empty(&rtcfg_dev->spec.srv.conn_list)) {
+	conn = list_entry(rtcfg_dev->spec.srv.conn_list.next,
+			  struct rtcfg_connection, entry);
+
+	list_del(&conn->entry);
+	rtcfg_dev->other_stations--;
+
+	if (conn->state > RTCFG_CONN_SEARCHING) {
+	    rtcfg_dev->stations_found--;
+	    if (conn->state >= RTCFG_CONN_STAGE_2)
+		rtcfg_dev->spec.srv.clients_configured--;
+	    if (conn->flags & _RTCFG_FLAG_READY)
+		rtcfg_dev->stations_ready--;
+	}
+
+	if ((conn->stage2_file) &&
+	    (rtcfg_release_file(conn->stage2_file) == 0))
+	    cmd_event->args.detach.stage2_file = conn->stage2_file;
+
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+	cmd_event->args.detach.conn_buf = conn;
+
+	return -EAGAIN;
+    }
+
+    if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+	rtdm_timer_destroy(&rtcfg_dev->timer);
+    rtcfg_reset_device(ifindex);
+
+    rtcfg_next_main_state(ifindex, RTCFG_MAIN_OFF);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    return 0;
+}
+
+
+
+/*** Server Frame Event Handlers ***/
+
+static int rtcfg_server_recv_announce(int ifindex, RTCFG_EVENT event_id,
+				      struct rtskb *rtskb)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct list_head          *entry;
+    struct rtcfg_frm_announce *announce;
+    struct rtcfg_connection   *conn;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_announce)) {
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	RTCFG_DEBUG(1, "RTcfg: received invalid announce frame\n");
+	return -EINVAL;
+    }
+
+    announce = (struct rtcfg_frm_announce *)rtskb->data;
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	switch (announce->addr_type) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	    u32 announce_addr;
+	    case RTCFG_ADDR_IP:
+		memcpy(&announce_addr, announce->addr, 4);
+
+		if (((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) &&
+		    (announce_addr == conn->addr.ip_addr)) {
+		    /* save MAC address - Ethernet-specific! */
+		    memcpy(conn->mac_addr, rtskb->mac.ethernet->h_source,
+			   ETH_ALEN);
+
+		    /* update routing table */
+		    rt_ip_route_add_host(conn->addr.ip_addr, conn->mac_addr,
+					 rtskb->rtdev);
+
+		    /* remove IP address */
+		    __rtskb_pull(rtskb, RTCFG_ADDRSIZE_IP);
+
+		    rtcfg_do_conn_event(conn, event_id, rtskb);
+
+		    goto out;
+		}
+		break;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+	    case RTCFG_ADDR_MAC:
+		/* Ethernet-specific! */
+		if (memcmp(conn->mac_addr, rtskb->mac.ethernet->h_source,
+			   ETH_ALEN) == 0) {
+		    rtcfg_do_conn_event(conn, event_id, rtskb);
+
+		    goto out;
+		}
+		break;
+	}
+    }
+
+out:
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static int rtcfg_server_recv_ack(int ifindex, struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+
+
+    if (rtskb->len < sizeof(struct rtcfg_frm_ack_cfg)) {
+	rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+	RTCFG_DEBUG(1, "RTcfg: received invalid ack_cfg frame\n");
+	return -EINVAL;
+    }
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	/* find the corresponding connection - Ethernet-specific! */
+	if (memcmp(conn->mac_addr,
+		   rtskb->mac.ethernet->h_source, ETH_ALEN) != 0)
+	    continue;
+
+	rtcfg_do_conn_event(conn, RTCFG_FRM_ACK_CFG, rtskb);
+
+	break;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+static int rtcfg_server_recv_simple_frame(int ifindex, RTCFG_EVENT event_id,
+					  struct rtskb *rtskb)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+
+
+    list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	/* find the corresponding connection - Ethernet-specific! */
+	if (memcmp(conn->mac_addr,
+		   rtskb->mac.ethernet->h_source, ETH_ALEN) != 0)
+	    continue;
+
+	rtcfg_do_conn_event(conn, event_id, rtskb);
+
+	break;
+    }
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+/*** Utility Functions ***/
+
+void rtcfg_queue_blocking_call(int ifindex, struct rt_proc_call *call)
+{
+    rtdm_lockctx_t      context;
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtdm_lock_get_irqsave(&rtcfg_dev->event_calls_lock, context);
+    list_add_tail(&call->list_entry, &rtcfg_dev->event_calls);
+    rtdm_lock_put_irqrestore(&rtcfg_dev->event_calls_lock, context);
+}
+
+
+
+struct rt_proc_call *rtcfg_dequeue_blocking_call(int ifindex)
+{
+    rtdm_lockctx_t      context;
+    struct rt_proc_call *call;
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtdm_lock_get_irqsave(&rtcfg_dev->event_calls_lock, context);
+    if (!list_empty(&rtcfg_dev->event_calls)) {
+	call = (struct rt_proc_call *)rtcfg_dev->event_calls.next;
+	list_del(&call->list_entry);
+    } else
+	call = NULL;
+    rtdm_lock_put_irqrestore(&rtcfg_dev->event_calls_lock, context);
+
+    return call;
+}
+
+
+
+void rtcfg_complete_cmd(int ifindex, RTCFG_EVENT event_id, int result)
+{
+    struct rt_proc_call *call;
+    struct rtcfg_cmd    *cmd_event;
+
+
+    while (1) {
+	call = rtcfg_dequeue_blocking_call(ifindex);
+	if (call == NULL)
+	    break;
+
+	cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+
+	rtpc_complete_call(call,
+	    (cmd_event->internal.data.event_id == event_id) ? result
+							    : -EINVAL);
+    }
+}
+
+
+
+void rtcfg_reset_device(int ifindex)
+{
+    struct rtcfg_device *rtcfg_dev = &device[ifindex];
+
+
+    rtcfg_dev->other_stations = 0;
+    rtcfg_dev->stations_found = 0;
+    rtcfg_dev->stations_ready = 0;
+    rtcfg_dev->flags          = 0;
+    rtcfg_dev->burstrate      = 0;
+
+    memset(&rtcfg_dev->spec, 0, sizeof(rtcfg_dev->spec));
+    INIT_LIST_HEAD(&rtcfg_dev->spec.srv.conn_list);
+}
+
+
+
+void rtcfg_init_state_machines(void)
+{
+    int                 i;
+    struct rtcfg_device *rtcfg_dev;
+
+
+    memset(device, 0, sizeof(device));
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtcfg_dev = &device[i];
+	rtcfg_dev->state = RTCFG_MAIN_OFF;
+
+	rtdm_mutex_init(&rtcfg_dev->dev_mutex);
+
+	INIT_LIST_HEAD(&rtcfg_dev->event_calls);
+	rtdm_lock_init(&rtcfg_dev->event_calls_lock);
+    }
+}
+
+
+
+void rtcfg_cleanup_state_machines(void)
+{
+    int                     i;
+    struct rtcfg_device     *rtcfg_dev;
+    struct rtcfg_connection *conn;
+    struct list_head        *entry;
+    struct list_head        *tmp;
+    struct rt_proc_call     *call;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtcfg_dev = &device[i];
+
+	if (test_and_clear_bit(FLAG_TIMER_STARTED, &rtcfg_dev->flags))
+		rtdm_timer_destroy(&rtcfg_dev->timer);
+
+	/*
+	 * No need to synchronize with rtcfg_timer here: the task running
+	 * rtcfg_timer is already dead.
+	 */
+
+	rtdm_mutex_destroy(&rtcfg_dev->dev_mutex);
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+	    list_for_each_safe(entry, tmp, &rtcfg_dev->spec.srv.conn_list) {
+		conn = list_entry(entry, struct rtcfg_connection, entry);
+
+		if (conn->stage1_data != NULL)
+		    kfree(conn->stage1_data);
+
+		if ((conn->stage2_file != NULL) &&
+		    (rtcfg_release_file(conn->stage2_file) == 0)){
+		    vfree(conn->stage2_file->buffer);
+		    kfree(conn->stage2_file);
+		}
+
+		kfree(entry);
+	    }
+	} else if (rtcfg_dev->state != RTCFG_MAIN_OFF) {
+	    if (rtcfg_dev->spec.clt.station_addr_list != NULL)
+		kfree(rtcfg_dev->spec.clt.station_addr_list);
+
+	    if (rtcfg_dev->spec.clt.stage2_chain != NULL)
+		kfree_rtskb(rtcfg_dev->spec.clt.stage2_chain);
+	}
+
+	while (1) {
+	    call = rtcfg_dequeue_blocking_call(i);
+	    if (call == NULL)
+		break;
+
+	    rtpc_complete_call_nrt(call, -ENODEV);
+	}
+    }
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_file.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_file.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_file.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_file.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,86 @@
+/***
+ *
+ *  rtcfg/rtcfg_file.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+
+#include <rtdm/driver.h>
+#include <rtcfg_chrdev.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_file.h>
+
+
+/* Note:
+ * We don't need any special lock protection while manipulating the
+ * rtcfg_files list. The list is only accessed through valid connections, and
+ * connections are already lock-protected.
+ */
+LIST_HEAD(rtcfg_files);
+
+
+struct rtcfg_file *rtcfg_get_file(const char *filename)
+{
+    struct list_head  *entry;
+    struct rtcfg_file *file;
+
+
+    RTCFG_DEBUG(4, "RTcfg: looking for file %s\n", filename);
+
+    list_for_each(entry, &rtcfg_files) {
+        file = list_entry(entry, struct rtcfg_file, entry);
+
+        if (strcmp(file->name, filename) == 0) {
+            file->ref_count++;
+
+            RTCFG_DEBUG(4, "RTcfg: reusing file entry, now %d users\n",
+                        file->ref_count);
+
+            return file;
+        }
+    }
+
+    return NULL;
+}
+
+
+
+void rtcfg_add_file(struct rtcfg_file *file)
+{
+    RTCFG_DEBUG(4, "RTcfg: adding file %s to list\n", file->name);
+
+    file->ref_count = 1;
+    list_add_tail(&file->entry, &rtcfg_files);
+}
+
+
+
+int rtcfg_release_file(struct rtcfg_file *file)
+{
+    if (--file->ref_count == 0) {
+        RTCFG_DEBUG(4, "RTcfg: removing file %s from list\n", file->name);
+
+        list_del(&file->entry);
+    }
+
+    return file->ref_count;
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_frame.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_frame.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_frame.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_frame.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,596 @@
+/***
+ *
+ *  rtcfg/rtcfg_frame.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/if_ether.h>
+
+#include <stack_mgr.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+
+static unsigned int num_rtskbs = 32;
+module_param(num_rtskbs, uint, 0444);
+MODULE_PARM_DESC(num_rtskbs, "Number of realtime socket buffers used by RTcfg");
+
+static struct rtskb_pool    rtcfg_pool;
+static rtdm_task_t          rx_task;
+static rtdm_event_t         rx_event;
+static struct rtskb_queue   rx_queue;
+
+
+void rtcfg_thread_signal(void)
+{
+    rtdm_event_signal(&rx_event);
+}
+
+static int rtcfg_rx_handler(struct rtskb *rtskb, struct rtpacket_type *pt)
+{
+    if (rtskb_acquire(rtskb, &rtcfg_pool) == 0) {
+	rtskb_queue_tail(&rx_queue, rtskb);
+	rtcfg_thread_signal();
+   } else
+	kfree_rtskb(rtskb);
+
+    return 0;
+}
+
+
+
+static void rtcfg_rx_task(void *arg)
+{
+    struct rtskb          *rtskb;
+    struct rtcfg_frm_head *frm_head;
+    struct rtnet_device   *rtdev;
+
+
+    while (!rtdm_task_should_stop()) {
+	if (rtdm_event_wait(&rx_event) < 0)
+	    break;
+
+	while ((rtskb = rtskb_dequeue(&rx_queue))) {
+	    rtdev = rtskb->rtdev;
+
+	    if (rtskb->pkt_type == PACKET_OTHERHOST) {
+		kfree_rtskb(rtskb);
+		continue;
+	    }
+
+	    if (rtskb->len < sizeof(struct rtcfg_frm_head)) {
+		RTCFG_DEBUG(1, "RTcfg: %s() received an invalid frame\n",
+			    __FUNCTION__);
+		kfree_rtskb(rtskb);
+		continue;
+	    }
+
+	    frm_head = (struct rtcfg_frm_head *)rtskb->data;
+
+	    if (rtcfg_do_main_event(rtskb->rtdev->ifindex,
+				    frm_head->id + RTCFG_FRM_STAGE_1_CFG,
+				    rtskb) < 0)
+		kfree_rtskb(rtskb);
+	}
+
+	rtcfg_timer_run();
+    }
+}
+
+
+
+int rtcfg_send_frame(struct rtskb *rtskb, struct rtnet_device *rtdev,
+		     u8 *dest_addr)
+{
+    int ret;
+
+
+    rtskb->rtdev    = rtdev;
+    rtskb->priority = RTCFG_SKB_PRIO;
+
+    if (rtdev->hard_header) {
+	ret = rtdev->hard_header(rtskb, rtdev, ETH_RTCFG, dest_addr,
+				 rtdev->dev_addr, rtskb->len);
+	if (ret < 0)
+	    goto err;
+    }
+
+    if ((rtdev->flags & IFF_UP) != 0) {
+	ret = 0;
+	if (rtdev_xmit(rtskb) != 0)
+	    ret = -EAGAIN;
+    } else {
+	ret = -ENETDOWN;
+	goto err;
+    }
+
+    rtdev_dereference(rtdev);
+    return ret;
+
+  err:
+    kfree_rtskb(rtskb);
+    rtdev_dereference(rtdev);
+    return ret;
+}
+
+
+
+int rtcfg_send_stage_1(struct rtcfg_connection *conn)
+{
+    struct rtnet_device          *rtdev;
+    struct rtskb                 *rtskb;
+    unsigned int                 rtskb_size;
+    struct rtcfg_frm_stage_1_cfg *stage_1_frm;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_1_cfg) + conn->stage1_size +
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	(((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	2*RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	0;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_1_cfg));
+
+    stage_1_frm->head.id      = RTCFG_ID_STAGE_1_CFG;
+    stage_1_frm->head.version = 0;
+    stage_1_frm->addr_type    = conn->addr_type & RTCFG_ADDR_MASK;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+    if (stage_1_frm->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, 2*RTCFG_ADDRSIZE_IP);
+
+	memcpy(stage_1_frm->client_addr, &(conn->addr.ip_addr), 4);
+
+	stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	    (((u8 *)stage_1_frm) + RTCFG_ADDRSIZE_IP);
+
+	memcpy(stage_1_frm->server_addr, &(rtdev->local_ip), 4);
+
+	stage_1_frm = (struct rtcfg_frm_stage_1_cfg *)
+	    (((u8 *)stage_1_frm) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    stage_1_frm->burstrate = device[conn->ifindex].burstrate;
+    stage_1_frm->cfg_len   = htons(conn->stage1_size);
+
+    memcpy(rtskb_put(rtskb, conn->stage1_size), conn->stage1_data,
+	   conn->stage1_size);
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_stage_2(struct rtcfg_connection *conn, int send_data)
+{
+    struct rtnet_device          *rtdev;
+    struct rtcfg_device          *rtcfg_dev = &device[conn->ifindex];
+    struct rtskb                 *rtskb;
+    unsigned int                 rtskb_size;
+    struct rtcfg_frm_stage_2_cfg *stage_2_frm;
+    size_t                       total_size;
+    size_t                       frag_size;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    if (send_data) {
+	total_size = conn->stage2_file->size;
+	frag_size  = MIN(rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO) -
+			 sizeof(struct rtcfg_frm_stage_2_cfg), total_size);
+    } else {
+	total_size = 0;
+	frag_size  = 0;
+    }
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_2_cfg) + frag_size;
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_2_frm = (struct rtcfg_frm_stage_2_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg));
+
+    stage_2_frm->head.id          = RTCFG_ID_STAGE_2_CFG;
+    stage_2_frm->head.version     = 0;
+    stage_2_frm->flags            = rtcfg_dev->flags;
+    stage_2_frm->stations         = htonl(rtcfg_dev->other_stations);
+    stage_2_frm->heartbeat_period = htons(rtcfg_dev->spec.srv.heartbeat);
+    stage_2_frm->cfg_len          = htonl(total_size);
+
+    if (send_data)
+	memcpy(rtskb_put(rtskb, frag_size), conn->stage2_file->buffer,
+	       frag_size);
+    conn->cfg_offs = frag_size;
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_stage_2_frag(struct rtcfg_connection *conn)
+{
+    struct rtnet_device               *rtdev;
+    struct rtskb                      *rtskb;
+    unsigned int                      rtskb_size;
+    struct rtcfg_frm_stage_2_cfg_frag *stage_2_frm;
+    size_t                            frag_size;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    frag_size = MIN(rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO) -
+		    sizeof(struct rtcfg_frm_stage_2_cfg_frag),
+		    conn->stage2_file->size - conn->cfg_offs);
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_stage_2_cfg_frag) + frag_size;
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    stage_2_frm = (struct rtcfg_frm_stage_2_cfg_frag *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_stage_2_cfg_frag));
+
+    stage_2_frm->head.id      = RTCFG_ID_STAGE_2_CFG_FRAG;
+    stage_2_frm->head.version = 0;
+    stage_2_frm->frag_offs    = htonl(conn->cfg_offs);
+
+    memcpy(rtskb_put(rtskb, frag_size),
+	   conn->stage2_file->buffer + conn->cfg_offs, frag_size);
+    conn->cfg_offs += frag_size;
+
+    return rtcfg_send_frame(rtskb, rtdev, conn->mac_addr);
+}
+
+
+
+int rtcfg_send_announce_new(int ifindex)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct rtnet_device       *rtdev;
+    struct rtskb              *rtskb;
+    unsigned int              rtskb_size;
+    struct rtcfg_frm_announce *announce_new;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_announce) +
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	(((rtcfg_dev->spec.clt.addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	0;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    announce_new = (struct rtcfg_frm_announce *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_announce));
+
+    announce_new->head.id      = RTCFG_ID_ANNOUNCE_NEW;
+    announce_new->head.version = 0;
+    announce_new->addr_type    = rtcfg_dev->spec.clt.addr_type;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+    if (announce_new->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(announce_new->addr, &(rtdev->local_ip), 4);
+
+	announce_new = (struct rtcfg_frm_announce *)
+	    (((u8 *)announce_new) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    announce_new->flags     = rtcfg_dev->flags;
+    announce_new->burstrate = rtcfg_dev->burstrate;
+
+    return rtcfg_send_frame(rtskb, rtdev, rtdev->broadcast);
+}
+
+
+
+int rtcfg_send_announce_reply(int ifindex, u8 *dest_mac_addr)
+{
+    struct rtcfg_device       *rtcfg_dev = &device[ifindex];
+    struct rtnet_device       *rtdev;
+    struct rtskb              *rtskb;
+    unsigned int              rtskb_size;
+    struct rtcfg_frm_announce *announce_rpl;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_announce) +
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	((rtcfg_dev->spec.clt.addr_type == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	0;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    announce_rpl = (struct rtcfg_frm_announce *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_announce));
+
+    announce_rpl->head.id      = RTCFG_ID_ANNOUNCE_REPLY;
+    announce_rpl->head.version = 0;
+    announce_rpl->addr_type    = rtcfg_dev->spec.clt.addr_type;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+    if (announce_rpl->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(announce_rpl->addr, &(rtdev->local_ip), 4);
+
+	announce_rpl = (struct rtcfg_frm_announce *)
+	    (((u8 *)announce_rpl) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    announce_rpl->flags     = rtcfg_dev->flags & _RTCFG_FLAG_READY;
+    announce_rpl->burstrate = 0; /* padding field */
+
+    return rtcfg_send_frame(rtskb, rtdev, dest_mac_addr);
+}
+
+
+
+int rtcfg_send_ack(int ifindex)
+{
+    struct rtnet_device      *rtdev;
+    struct rtskb             *rtskb;
+    unsigned int             rtskb_size;
+    struct rtcfg_frm_ack_cfg *ack_frm;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_ack_cfg);
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    ack_frm = (struct rtcfg_frm_ack_cfg *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_ack_cfg));
+
+    ack_frm->head.id      = RTCFG_ID_ACK_CFG;
+    ack_frm->head.version = 0;
+    ack_frm->ack_len      = htonl(device[ifindex].spec.clt.cfg_offs);
+
+    return rtcfg_send_frame(rtskb, rtdev,
+			    device[ifindex].spec.clt.srv_mac_addr);
+}
+
+
+
+int rtcfg_send_simple_frame(int ifindex, int frame_id, u8 *dest_addr)
+{
+    struct rtnet_device     *rtdev;
+    struct rtskb            *rtskb;
+    unsigned int            rtskb_size;
+    struct rtcfg_frm_simple *simple_frm;
+
+
+    rtdev = rtdev_get_by_index(ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len + sizeof(struct rtcfg_frm_simple);
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    simple_frm = (struct rtcfg_frm_simple *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_simple));
+
+    simple_frm->head.id      = frame_id;
+    simple_frm->head.version = 0;
+
+    return rtcfg_send_frame(rtskb, rtdev,
+			    (dest_addr) ? dest_addr : rtdev->broadcast);
+}
+
+
+
+int rtcfg_send_dead_station(struct rtcfg_connection *conn)
+{
+    struct rtnet_device           *rtdev;
+    struct rtskb                  *rtskb;
+    unsigned int                  rtskb_size;
+    struct rtcfg_frm_dead_station *dead_station_frm;
+
+
+    rtdev = rtdev_get_by_index(conn->ifindex);
+    if (rtdev == NULL)
+	return -ENODEV;
+
+    rtskb_size = rtdev->hard_header_len +
+	sizeof(struct rtcfg_frm_dead_station) +
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	(((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP) ?
+	RTCFG_ADDRSIZE_IP : 0);
+#else /* !CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+	0;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    rtskb = alloc_rtskb(rtskb_size, &rtcfg_pool);
+    if (rtskb == NULL) {
+	rtdev_dereference(rtdev);
+	return -ENOBUFS;
+    }
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len);
+
+    dead_station_frm = (struct rtcfg_frm_dead_station *)
+	rtskb_put(rtskb, sizeof(struct rtcfg_frm_dead_station));
+
+    dead_station_frm->head.id      = RTCFG_ID_DEAD_STATION;
+    dead_station_frm->head.version = 0;
+    dead_station_frm->addr_type    = conn->addr_type & RTCFG_ADDR_MASK;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+    if (dead_station_frm->addr_type == RTCFG_ADDR_IP) {
+	rtskb_put(rtskb, RTCFG_ADDRSIZE_IP);
+
+	memcpy(dead_station_frm->logical_addr, &(conn->addr.ip_addr), 4);
+
+	dead_station_frm = (struct rtcfg_frm_dead_station *)
+	    (((u8 *)dead_station_frm) + RTCFG_ADDRSIZE_IP);
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+    /* Ethernet-specific! */
+    memcpy(dead_station_frm->physical_addr, conn->mac_addr, ETH_ALEN);
+    memset(&dead_station_frm->physical_addr[ETH_ALEN], 0,
+	sizeof(dead_station_frm->physical_addr) - ETH_ALEN);
+
+    return rtcfg_send_frame(rtskb, rtdev, rtdev->broadcast);
+}
+
+
+
+static struct rtpacket_type rtcfg_packet_type = {
+    .type =     __constant_htons(ETH_RTCFG),
+    .handler =  rtcfg_rx_handler
+};
+
+
+
+int __init rtcfg_init_frames(void)
+{
+    int ret;
+
+
+    if (rtskb_module_pool_init(&rtcfg_pool, num_rtskbs) < num_rtskbs)
+	return -ENOMEM;
+
+    rtskb_queue_init(&rx_queue);
+    rtdm_event_init(&rx_event, 0);
+
+    ret = rtdm_task_init(&rx_task, "rtcfg-rx", rtcfg_rx_task, 0,
+			 RTDM_TASK_LOWEST_PRIORITY, 0);
+    if (ret < 0) {
+	rtdm_event_destroy(&rx_event);
+	goto error1;
+    }
+
+    ret = rtdev_add_pack(&rtcfg_packet_type);
+    if (ret < 0)
+	goto error2;
+
+    return 0;
+
+  error2:
+    rtdm_event_destroy(&rx_event);
+    rtdm_task_destroy(&rx_task);
+
+  error1:
+    rtskb_pool_release(&rtcfg_pool);
+
+    return ret;
+}
+
+
+
+void rtcfg_cleanup_frames(void)
+{
+    struct rtskb *rtskb;
+
+
+    rtdev_remove_pack(&rtcfg_packet_type);
+
+    rtdm_event_destroy(&rx_event);
+    rtdm_task_destroy(&rx_task);
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	kfree_rtskb(rtskb);
+    }
+
+    rtskb_pool_release(&rtcfg_pool);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_ioctl.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_ioctl.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_ioctl.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,455 @@
+/***
+ *
+ *  rtcfg/rtcfg_ioctl.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/file.h>
+#include <linux/vmalloc.h>
+
+#include <rtcfg_chrdev.h>
+#include <rtnet_rtpc.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_proc.h>
+
+
+int rtcfg_event_handler(struct rt_proc_call *call)
+{
+    struct rtcfg_cmd *cmd_event;
+
+
+    cmd_event = rtpc_get_priv(call, struct rtcfg_cmd);
+    return rtcfg_do_main_event(cmd_event->internal.data.ifindex,
+                               cmd_event->internal.data.event_id, call);
+}
+
+
+
+void keep_cmd_add(struct rt_proc_call *call, void *priv_data)
+{
+    /* do nothing on error (<0), or if file already present (=0) */
+    if (rtpc_get_result(call) <= 0)
+        return;
+
+    /* Don't cleanup any buffers, we are going to recycle them! */
+    rtpc_set_cleanup_handler(call, NULL);
+}
+
+
+
+void cleanup_cmd_add(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    buf = cmd->args.add.conn_buf;
+    if (buf != NULL)
+        kfree(buf);
+
+    buf = cmd->args.add.stage1_data;
+    if (buf != NULL)
+        kfree(buf);
+
+    if (cmd->args.add.stage2_file != NULL) {
+        buf = cmd->args.add.stage2_file->buffer;
+        if (buf != NULL)
+            vfree(buf);
+        kfree(cmd->args.add.stage2_file);
+    }
+}
+
+
+
+void cleanup_cmd_del(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    if (cmd->args.del.conn_buf != NULL) {
+        buf = cmd->args.del.conn_buf->stage1_data;
+        if (buf != NULL)
+            kfree(buf);
+        kfree(cmd->args.del.conn_buf);
+    }
+
+    if (cmd->args.del.stage2_file != NULL) {
+        buf = cmd->args.del.stage2_file->buffer;
+        if (buf != NULL)
+            vfree(buf);
+        kfree(cmd->args.del.stage2_file);
+    }
+}
+
+
+
+void copy_stage_1_data(struct rt_proc_call *call, void *priv_data)
+{
+    struct rtcfg_cmd *cmd;
+    int              result = rtpc_get_result(call);
+
+
+    if (result <= 0)
+        return;
+
+    cmd = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    if (cmd->args.client.buffer_size < (size_t)result)
+        rtpc_set_result(call, -ENOSPC);
+    else if (copy_to_user(cmd->args.client.buffer,
+                          cmd->args.client.rtskb->data, result) != 0)
+        rtpc_set_result(call, -EFAULT);
+}
+
+
+
+void cleanup_cmd_client(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *station_buf;
+    struct rtskb     *rtskb;
+
+
+    station_buf = cmd->args.client.station_buf;
+    if (station_buf != NULL)
+        kfree(station_buf);
+
+    rtskb = cmd->args.client.rtskb;
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+void copy_stage_2_data(struct rt_proc_call *call, void *priv_data)
+{
+    struct rtcfg_cmd *cmd;
+    int              result = rtpc_get_result(call);
+    struct rtskb     *rtskb;
+
+
+    if (result <= 0)
+        return;
+
+    cmd = rtpc_get_priv(call, struct rtcfg_cmd);
+
+    if (cmd->args.announce.buffer_size < (size_t)result)
+        rtpc_set_result(call, -ENOSPC);
+    else {
+        rtskb = cmd->args.announce.rtskb;
+        do {
+            if (copy_to_user(cmd->args.announce.buffer,
+                             rtskb->data, rtskb->len) != 0) {
+                rtpc_set_result(call, -EFAULT);
+                break;
+            }
+            cmd->args.announce.buffer += rtskb->len;
+            rtskb = rtskb->next;
+        } while (rtskb != NULL);
+    }
+}
+
+
+
+void cleanup_cmd_announce(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    struct rtskb     *rtskb;
+
+
+    rtskb = cmd->args.announce.rtskb;
+    if (rtskb != NULL)
+        kfree_rtskb(rtskb);
+}
+
+
+
+void cleanup_cmd_detach(void *priv_data)
+{
+    struct rtcfg_cmd *cmd = (struct rtcfg_cmd *)priv_data;
+    void             *buf;
+
+
+    /* unlock proc and update directory structure */
+    rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+
+    if (cmd->args.detach.conn_buf) {
+        buf = cmd->args.detach.conn_buf->stage1_data;
+        if (buf != NULL)
+            kfree(buf);
+        kfree(cmd->args.detach.conn_buf);
+    }
+
+    if (cmd->args.detach.stage2_file != NULL) {
+        buf = cmd->args.detach.stage2_file->buffer;
+        if (buf)
+            vfree(buf);
+        kfree(cmd->args.detach.stage2_file);
+    }
+
+    if (cmd->args.detach.station_addr_list)
+        kfree(cmd->args.detach.station_addr_list);
+
+    if (cmd->args.detach.stage2_chain)
+        kfree_rtskb(cmd->args.detach.stage2_chain);
+}
+
+
+
+int rtcfg_ioctl_add(struct rtnet_device *rtdev, struct rtcfg_cmd *cmd)
+{
+    struct rtcfg_connection *conn_buf;
+    struct rtcfg_file       *file = NULL;
+    void                    *data_buf;
+    size_t                  size;
+    int                     ret;
+
+
+    conn_buf = kmalloc(sizeof(struct rtcfg_connection), GFP_KERNEL);
+    if (conn_buf == NULL)
+        return -ENOMEM;
+    cmd->args.add.conn_buf = conn_buf;
+
+    data_buf = NULL;
+    size = cmd->args.add.stage1_size;
+    if (size > 0) {
+        /* check stage 1 data size */
+        if (sizeof(struct rtcfg_frm_stage_1_cfg) + 2*RTCFG_ADDRSIZE_IP + size >
+                rtdev->get_mtu(rtdev, RTCFG_SKB_PRIO)) {
+            ret = -ESTAGE1SIZE;
+            goto err;
+        }
+
+        data_buf = kmalloc(size, GFP_KERNEL);
+        if (data_buf == NULL) {
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        ret = copy_from_user(data_buf, cmd->args.add.stage1_data, size);
+        if (ret != 0) {
+            ret = -EFAULT;
+            goto err;
+        }
+    }
+    cmd->args.add.stage1_data = data_buf;
+
+    if (cmd->args.add.stage2_filename != NULL) {
+        size = strnlen_user(cmd->args.add.stage2_filename, PATH_MAX);
+
+        file = kmalloc(sizeof(struct rtcfg_file) + size, GFP_KERNEL);
+        if (file == NULL) {
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        file->name   = (char *)file + sizeof(struct rtcfg_file);
+        file->buffer = NULL;
+
+        ret = copy_from_user((void *)file + sizeof(struct rtcfg_file),
+                             (const void *)cmd->args.add.stage2_filename,
+                             size);
+        if (ret != 0) {
+            ret = -EFAULT;
+            goto err;
+        }
+    }
+    cmd->args.add.stage2_file = file;
+
+    /* lock proc structure for modification */
+    rtcfg_lockwr_proc(cmd->internal.data.ifindex);
+
+    ret = rtpc_dispatch_call(rtcfg_event_handler, 0, cmd,
+                             sizeof(*cmd), keep_cmd_add,
+                             cleanup_cmd_add);
+
+    /* load file if missing */
+    if (ret > 0) {
+        struct file  *filp;
+        mm_segment_t oldfs;
+
+
+        filp = filp_open(file->name, O_RDONLY, 0);
+        if (IS_ERR(filp)) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            ret = PTR_ERR(filp);
+            goto err;
+        }
+
+        file->size = filp->f_path.dentry->d_inode->i_size;
+
+        /* allocate buffer even for empty files */
+        file->buffer = vmalloc((file->size)? file->size : 1);
+        if (file->buffer == NULL) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            fput(filp);
+            ret = -ENOMEM;
+            goto err;
+        }
+
+        oldfs = get_fs();
+        set_fs(KERNEL_DS);
+        filp->f_pos = 0;
+
+        ret = filp->f_op->read(filp, file->buffer, file->size,
+                               &filp->f_pos);
+
+        set_fs(oldfs);
+        fput(filp);
+
+        if (ret != (int)file->size) {
+            rtcfg_unlockwr_proc(cmd->internal.data.ifindex);
+            ret = -EIO;
+            goto err;
+        }
+
+        /* dispatch again, this time with new file attached */
+        ret = rtpc_dispatch_call(rtcfg_event_handler, 0, cmd,
+                                 sizeof(*cmd), NULL, cleanup_cmd_add);
+    }
+
+    return ret;
+
+  err:
+    kfree(conn_buf);
+    if (data_buf != NULL)
+        kfree(data_buf);
+    if (file != NULL) {
+        if (file->buffer != NULL)
+            vfree(file->buffer);
+        kfree(file);
+    }
+    return ret;
+}
+
+
+
+int rtcfg_ioctl(struct rtnet_device *rtdev, unsigned int request, unsigned long arg)
+{
+    struct rtcfg_cmd        cmd;
+    struct rtcfg_station    *station_buf;
+    int                     ret;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+        return -EFAULT;
+
+    cmd.internal.data.ifindex  = rtdev->ifindex;
+    cmd.internal.data.event_id = _IOC_NR(request);
+
+    switch (request) {
+        case RTCFG_IOC_SERVER:
+            ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_ADD:
+            ret = rtcfg_ioctl_add(rtdev, &cmd);
+            break;
+
+        case RTCFG_IOC_DEL:
+            cmd.args.del.conn_buf    = NULL;
+            cmd.args.del.stage2_file = NULL;
+
+            /* lock proc structure for modification
+               (unlock in cleanup_cmd_del) */
+            rtcfg_lockwr_proc(cmd.internal.data.ifindex);
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                     sizeof(cmd), NULL, cleanup_cmd_del);
+            break;
+
+        case RTCFG_IOC_WAIT:
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.wait.timeout, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_CLIENT:
+            station_buf = kmalloc(sizeof(struct rtcfg_station) *
+                                  cmd.args.client.max_stations, GFP_KERNEL);
+            if (station_buf == NULL)
+                return -ENOMEM;
+            cmd.args.client.station_buf = station_buf;
+            cmd.args.client.rtskb       = NULL;
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.client.timeout, &cmd,
+                                     sizeof(cmd), copy_stage_1_data,
+                                     cleanup_cmd_client);
+            break;
+
+        case RTCFG_IOC_ANNOUNCE:
+            cmd.args.announce.rtskb = NULL;
+
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.announce.timeout, &cmd,
+                                     sizeof(cmd), copy_stage_2_data,
+                                     cleanup_cmd_announce);
+            break;
+
+        case RTCFG_IOC_READY:
+            ret = rtpc_dispatch_call(rtcfg_event_handler,
+                                     cmd.args.ready.timeout, &cmd,
+                                     sizeof(cmd), NULL, NULL);
+            break;
+
+        case RTCFG_IOC_DETACH:
+            do {
+                cmd.args.detach.conn_buf          = NULL;
+                cmd.args.detach.stage2_file       = NULL;
+                cmd.args.detach.station_addr_list = NULL;
+                cmd.args.detach.stage2_chain      = NULL;
+
+                /* lock proc structure for modification
+                   (unlock in cleanup_cmd_detach) */
+                rtcfg_lockwr_proc(cmd.internal.data.ifindex);
+
+                ret = rtpc_dispatch_call(rtcfg_event_handler, 0, &cmd,
+                                         sizeof(cmd), NULL,
+                                         cleanup_cmd_detach);
+            } while (ret == -EAGAIN);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+struct rtnet_ioctls rtcfg_ioctls = {
+    .service_name = "RTcfg",
+    .ioctl_type =   RTNET_IOC_TYPE_RTCFG,
+    .handler =      rtcfg_ioctl
+};
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_module.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_module.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_module.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_module.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,89 @@
+/***
+ *
+ *  rtcfg/rtcfg_module.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003, 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_ioctl.h>
+#include <rtcfg/rtcfg_proc.h>
+
+
+MODULE_LICENSE("GPL");
+
+int __init rtcfg_init(void)
+{
+    int ret;
+
+
+    printk("RTcfg: init real-time configuration distribution protocol\n");
+
+    ret = rtcfg_init_ioctls();
+    if (ret != 0)
+	goto error1;
+
+    rtcfg_init_state_machines();
+
+    ret = rtcfg_init_frames();
+    if (ret != 0)
+	goto error2;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtcfg_init_proc();
+    if (ret != 0) {
+	rtcfg_cleanup_frames();
+	goto error2;
+    }
+#endif
+
+    return 0;
+
+  error2:
+    rtcfg_cleanup_state_machines();
+    rtcfg_cleanup_ioctls();
+
+  error1:
+    return ret;
+}
+
+
+
+void rtcfg_cleanup(void)
+{
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtcfg_cleanup_proc();
+#endif
+    rtcfg_cleanup_frames();
+    rtcfg_cleanup_state_machines();
+    rtcfg_cleanup_ioctls();
+
+    printk("RTcfg: unloaded\n");
+}
+
+
+
+module_init(rtcfg_init);
+module_exit(rtcfg_cleanup);
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_proc.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_proc.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_proc.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_proc.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,350 @@
+/***
+ *
+ *	rtcfg/rtcfg_proc.c
+ *
+ *	Real-Time Configuration Distribution Protocol
+ *
+ *	Copyright (C) 2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License as published by
+ *	the Free Software Foundation; either version 2 of the License, or
+ *	(at your option) any later version.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	You should have received a copy of the GNU General Public License
+ *	along with this program; if not, write to the Free Software
+ *	Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_port.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+DEFINE_MUTEX(nrt_proc_lock);
+static struct xnvfile_directory rtcfg_proc_root;
+
+static int rtnet_rtcfg_proc_lock_get(struct xnvfile *vfile)
+{
+	return mutex_lock_interruptible(&nrt_proc_lock);
+}
+
+static void rtnet_rtcfg_proc_lock_put(struct xnvfile *vfile)
+{
+	return mutex_unlock(&nrt_proc_lock);
+}
+
+static struct xnvfile_lock_ops rtnet_rtcfg_proc_lock_ops = {
+	.get = rtnet_rtcfg_proc_lock_get,
+	.put = rtnet_rtcfg_proc_lock_put,
+};
+
+int rtnet_rtcfg_dev_state_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct rtcfg_device *rtcfg_dev = xnvfile_priv(it->vfile);
+	const char *state_name[] = {
+		"OFF", "SERVER_RUNNING", "CLIENT_0", "CLIENT_1",
+		"CLIENT_ANNOUNCED", "CLIENT_ALL_KNOWN",
+		"CLIENT_ALL_FRAMES", "CLIENT_2", "CLIENT_READY"
+	};
+
+	xnvfile_printf(it, "state:\t\t\t%d (%s)\n"
+				"flags:\t\t\t%08lX\n"
+				"other stations:\t\t%d\n"
+				"stations found:\t\t%d\n"
+				"stations ready:\t\t%d\n",
+				rtcfg_dev->state, state_name[rtcfg_dev->state],
+				rtcfg_dev->flags, rtcfg_dev->other_stations,
+				rtcfg_dev->stations_found,
+				rtcfg_dev->stations_ready);
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+		xnvfile_printf(it, "configured clients:\t%d\n"
+					"burstrate:\t\t%d\n"
+					"heartbeat period:\t%d ms\n",
+					rtcfg_dev->spec.srv.clients_configured,
+					rtcfg_dev->burstrate, rtcfg_dev->spec.srv.heartbeat);
+	} else if (rtcfg_dev->state != RTCFG_MAIN_OFF) {
+		xnvfile_printf(it, "address type:\t\t%d\n"
+					"server address:\t\t%02X:%02X:%02X:%02X:%02X:%02X\n"
+					"stage 2 config:\t\t%d/%d\n",
+					rtcfg_dev->spec.clt.addr_type,
+					rtcfg_dev->spec.clt.srv_mac_addr[0],
+					rtcfg_dev->spec.clt.srv_mac_addr[1],
+					rtcfg_dev->spec.clt.srv_mac_addr[2],
+					rtcfg_dev->spec.clt.srv_mac_addr[3],
+					rtcfg_dev->spec.clt.srv_mac_addr[4],
+					rtcfg_dev->spec.clt.srv_mac_addr[5],
+					rtcfg_dev->spec.clt.cfg_offs,
+					rtcfg_dev->spec.clt.cfg_len);
+	}
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_state_vfile_ops = {
+	.show = rtnet_rtcfg_dev_state_show,
+};
+
+int rtnet_rtcfg_dev_stations_show(struct xnvfile_regular_iterator *it, void *d)
+{
+	struct rtcfg_device *rtcfg_dev = xnvfile_priv(it->vfile);
+	struct rtcfg_connection *conn;
+	struct rtcfg_station *station;
+	int i;
+
+	if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+		list_for_each_entry(conn, &rtcfg_dev->spec.srv.conn_list, entry) {
+			if ((conn->state != RTCFG_CONN_SEARCHING) &&
+				(conn->state != RTCFG_CONN_DEAD))
+				xnvfile_printf(it, "%02X:%02X:%02X:%02X:%02X:%02X\t%02X\n",
+							conn->mac_addr[0], conn->mac_addr[1],
+							conn->mac_addr[2], conn->mac_addr[3],
+							conn->mac_addr[4], conn->mac_addr[5],
+							conn->flags);
+		}
+	} else if (rtcfg_dev->spec.clt.station_addr_list) {
+		for (i = 0; i < rtcfg_dev->stations_found; i++) {
+			station = &rtcfg_dev->spec.clt.station_addr_list[i];
+
+			xnvfile_printf(it, "%02X:%02X:%02X:%02X:%02X:%02X\t%02X\n",
+						station->mac_addr[0], station->mac_addr[1],
+						station->mac_addr[2], station->mac_addr[3],
+						station->mac_addr[4], station->mac_addr[5],
+						station->flags);
+		}
+	}
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_stations_vfile_ops = {
+	.show = rtnet_rtcfg_dev_stations_show,
+};
+
+int
+rtnet_rtcfg_dev_conn_state_show(struct xnvfile_regular_iterator *it, void *d)
+{
+	struct rtcfg_connection *conn = xnvfile_priv(it->vfile);
+	char *state_name[] =
+		{ "SEARCHING", "STAGE_1", "STAGE_2", "READY", "DEAD" };
+
+	xnvfile_printf(it, "state:\t\t\t%d (%s)\n"
+				"flags:\t\t\t%02X\n"
+				"stage 1 size:\t\t%zd\n"
+				"stage 2 filename:\t%s\n"
+				"stage 2 size:\t\t%zd\n"
+				"stage 2 offset:\t\t%d\n"
+				"burstrate:\t\t%d\n"
+				"mac address:\t\t%02X:%02X:%02X:%02X:%02X:%02X\n",
+				conn->state, state_name[conn->state], conn->flags,
+				conn->stage1_size,
+				(conn->stage2_file)? conn->stage2_file->name: "-",
+				(conn->stage2_file)? conn->stage2_file->size: 0,
+				conn->cfg_offs, conn->burstrate,
+				conn->mac_addr[0], conn->mac_addr[1],
+				conn->mac_addr[2], conn->mac_addr[3],
+				conn->mac_addr[4], conn->mac_addr[5]);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+	if ((conn->addr_type & RTCFG_ADDR_MASK) == RTCFG_ADDR_IP)
+		xnvfile_printf(it, "ip:\t\t\t%u.%u.%u.%u\n",
+					NIPQUAD(conn->addr.ip_addr));
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtcfg_dev_conn_state_vfile_ops = {
+	.show = rtnet_rtcfg_dev_conn_state_show,
+};
+
+void rtcfg_update_conn_proc_entries(int ifindex)
+{
+	struct rtcfg_device		*dev = &device[ifindex];
+	struct rtcfg_connection *conn;
+	char					name_buf[64];
+
+	if (dev->state != RTCFG_MAIN_SERVER_RUNNING)
+		return;
+
+	list_for_each_entry(conn, &dev->spec.srv.conn_list, entry) {
+		switch (conn->addr_type & RTCFG_ADDR_MASK) {
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_RTIPV4)
+		case RTCFG_ADDR_IP:
+			snprintf(name_buf, 64, "CLIENT_%u.%u.%u.%u",
+					NIPQUAD(conn->addr.ip_addr));
+			break;
+#endif /* CONFIG_XENO_DRIVERS_NET_RTIPV4 */
+
+		default: /* RTCFG_ADDR_MAC */
+			snprintf(name_buf, 64,
+					"CLIENT_%02X%02X%02X%02X%02X%02X",
+					conn->mac_addr[0], conn->mac_addr[1],
+					conn->mac_addr[2], conn->mac_addr[3],
+					conn->mac_addr[4], conn->mac_addr[5]);
+			break;
+		}
+		memset(&conn->proc_entry, '\0', sizeof(conn->proc_entry));
+		conn->proc_entry.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+		conn->proc_entry.ops = &rtnet_rtcfg_dev_conn_state_vfile_ops;
+		xnvfile_priv(&conn->proc_entry) = conn;
+
+		xnvfile_init_regular(name_buf, &conn->proc_entry, &dev->proc_entry);
+	}
+}
+
+
+
+void rtcfg_remove_conn_proc_entries(int ifindex)
+{
+	struct rtcfg_device		*dev = &device[ifindex];
+	struct rtcfg_connection *conn;
+
+
+	if (dev->state != RTCFG_MAIN_SERVER_RUNNING)
+		return;
+
+	list_for_each_entry(conn, &dev->spec.srv.conn_list, entry)
+		xnvfile_destroy_regular(&conn->proc_entry);
+}
+
+
+
+void rtcfg_new_rtdev(struct rtnet_device *rtdev)
+{
+	struct rtcfg_device *dev = &device[rtdev->ifindex];
+	int err;
+
+
+	mutex_lock(&nrt_proc_lock);
+
+	memset(&dev->proc_entry, '\0', sizeof(dev->proc_entry));
+	err = xnvfile_init_dir(rtdev->name, &dev->proc_entry, &rtcfg_proc_root);
+	if (err < 0)
+		goto error1;
+
+	memset(&dev->proc_state_vfile, '\0', sizeof(dev->proc_state_vfile));
+	dev->proc_state_vfile.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+	dev->proc_state_vfile.ops = &rtnet_rtcfg_dev_state_vfile_ops;
+	xnvfile_priv(&dev->proc_state_vfile) = dev;
+
+	err = xnvfile_init_regular("state",
+							&dev->proc_state_vfile, &dev->proc_entry);
+	if (err < 0)
+		goto error2;
+
+	memset(&dev->proc_stations_vfile, '\0', sizeof(dev->proc_stations_vfile));
+	dev->proc_stations_vfile.entry.lockops = &rtnet_rtcfg_proc_lock_ops;
+	dev->proc_stations_vfile.ops = &rtnet_rtcfg_dev_stations_vfile_ops;
+	xnvfile_priv(&dev->proc_stations_vfile) = dev;
+
+	err = xnvfile_init_regular("stations_list",
+							&dev->proc_stations_vfile, &dev->proc_entry);
+	if (err < 0)
+		goto error3;
+
+	mutex_unlock(&nrt_proc_lock);
+
+	return;
+
+  error3:
+	xnvfile_destroy_regular(&dev->proc_state_vfile);
+  error2:
+	xnvfile_destroy_dir(&dev->proc_entry);
+  error1:
+	dev->proc_entry.entry.pde = NULL;
+	mutex_unlock(&nrt_proc_lock);
+}
+
+
+
+void rtcfg_remove_rtdev(struct rtnet_device *rtdev)
+{
+	struct rtcfg_device *dev = &device[rtdev->ifindex];
+
+
+	// To-Do: issue down command
+
+	mutex_lock(&nrt_proc_lock);
+
+	if (dev->proc_entry.entry.pde) {
+		rtcfg_remove_conn_proc_entries(rtdev->ifindex);
+
+		xnvfile_destroy_regular(&dev->proc_stations_vfile);
+		xnvfile_destroy_regular(&dev->proc_state_vfile);
+		xnvfile_destroy_dir(&dev->proc_entry);
+		dev->proc_entry.entry.pde = NULL;
+	}
+
+	mutex_unlock(&nrt_proc_lock);
+}
+
+
+
+static struct rtdev_event_hook rtdev_hook = {
+	.register_device =	rtcfg_new_rtdev,
+	.unregister_device =rtcfg_remove_rtdev,
+	.ifup =				NULL,
+	.ifdown =			NULL
+};
+
+
+
+int rtcfg_init_proc(void)
+{
+	struct rtnet_device *rtdev;
+	int					i, err;
+
+	err = xnvfile_init_dir("rtcfg", &rtcfg_proc_root, &rtnet_proc_root);
+	if (err < 0)
+		goto err1;
+
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev) {
+			rtcfg_new_rtdev(rtdev);
+			rtdev_dereference(rtdev);
+		}
+	}
+
+	rtdev_add_event_hook(&rtdev_hook);
+	return 0;
+
+  err1:
+	printk("RTcfg: unable to initialise /proc entries\n");
+	return err;
+}
+
+
+
+void rtcfg_cleanup_proc(void)
+{
+	struct rtnet_device *rtdev;
+	int					i;
+
+
+	rtdev_del_event_hook(&rtdev_hook);
+
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev) {
+			rtcfg_remove_rtdev(rtdev);
+			rtdev_dereference(rtdev);
+		}
+	}
+
+	xnvfile_destroy_dir(&rtcfg_proc_root);
+}
+
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Nur linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_timer.c linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_timer.c
--- linux-5.4.5/net/rtnet/stack/rtcfg/rtcfg_timer.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtcfg/rtcfg_timer.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,104 @@
+/***
+ *
+ *  rtcfg/rtcfg_timer.c
+ *
+ *  Real-Time Configuration Distribution Protocol
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtcfg/rtcfg.h>
+#include <rtcfg/rtcfg_conn_event.h>
+#include <rtcfg/rtcfg_event.h>
+#include <rtcfg/rtcfg_frame.h>
+#include <rtcfg/rtcfg_timer.h>
+
+void rtcfg_timer(rtdm_timer_t *t)
+{
+    struct rtcfg_device *rtcfg_dev =
+	container_of(t, struct rtcfg_device, timer);
+
+    set_bit(FLAG_TIMER_PENDING, &rtcfg_dev->flags);
+    rtcfg_thread_signal();
+}
+
+void rtcfg_timer_run_one(int ifindex)
+{
+    struct rtcfg_device     *rtcfg_dev = &device[ifindex];
+    struct list_head        *entry;
+    struct rtcfg_connection *conn;
+    int                     last_stage_1 = -1;
+    int                     burst_credit;
+    int                     index;
+    int                     ret, shutdown;
+
+    shutdown = test_and_clear_bit(FLAG_TIMER_SHUTDOWN, &rtcfg_dev->flags);
+
+    if (!test_and_clear_bit(FLAG_TIMER_PENDING, &rtcfg_dev->flags)
+	|| shutdown)
+	return;
+
+    rtdm_mutex_lock(&rtcfg_dev->dev_mutex);
+
+    if (rtcfg_dev->state == RTCFG_MAIN_SERVER_RUNNING) {
+	index = 0;
+	burst_credit = rtcfg_dev->burstrate;
+
+	list_for_each(entry, &rtcfg_dev->spec.srv.conn_list) {
+	    conn = list_entry(entry, struct rtcfg_connection, entry);
+
+	    if ((conn->state == RTCFG_CONN_SEARCHING) ||
+		(conn->state == RTCFG_CONN_DEAD)){
+		if ((burst_credit > 0) && (index > last_stage_1)) {
+		    if ((ret = rtcfg_send_stage_1(conn)) < 0) {
+			RTCFG_DEBUG(2, "RTcfg: error %d while sending "
+                                        "stage 1 frame\n", ret);
+		    }
+		    burst_credit--;
+		    last_stage_1 = index;
+		}
+	    } else {
+		/* skip connection in history */
+		if (last_stage_1 == (index-1))
+		    last_stage_1 = index;
+
+		rtcfg_do_conn_event(conn, RTCFG_TIMER, NULL);
+	    }
+	    index++;
+	}
+
+	/* handle pointer overrun of the last stage 1 transmission */
+	if (last_stage_1 == (index-1))
+	    last_stage_1 = -1;
+    } else if (rtcfg_dev->state == RTCFG_MAIN_CLIENT_READY)
+	rtcfg_send_heartbeat(ifindex);
+
+    rtdm_mutex_unlock(&rtcfg_dev->dev_mutex);
+}
+
+void rtcfg_timer_run(void)
+{
+    int ifindex;
+
+    for (ifindex = 0; ifindex < MAX_RT_DEVICES; ifindex++)
+	rtcfg_timer_run_one(ifindex);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtdev.c linux-5.4.5-new/net/rtnet/stack/rtdev.c
--- linux-5.4.5/net/rtnet/stack/rtdev.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtdev.c	2020-06-15 16:12:31.523695391 +0300
@@ -0,0 +1,823 @@
+/***
+ *
+ *  stack/rtdev.c - NIC device driver layer
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/spinlock.h>
+#include <linux/if.h>
+#include <linux/if_arp.h> /* ARPHRD_ETHER */
+#include <linux/netdevice.h>
+#include <linux/moduleparam.h>
+
+#include <rtnet_internal.h>
+#include <rtskb.h>
+#include <ethernet/eth.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtnet_port.h>
+
+
+static unsigned int device_rtskbs = DEFAULT_DEVICE_RTSKBS;
+module_param(device_rtskbs, uint, 0444);
+MODULE_PARM_DESC(device_rtskbs, "Number of additional global realtime socket "
+		 "buffers per network adapter");
+
+struct rtnet_device         *rtnet_devices[MAX_RT_DEVICES];
+static struct rtnet_device  *loopback_device;
+static raw_spinlock_t rtnet_devices_rt_lock;
+static int initialized_rtnet_devices_rt_lock = 0;
+static LIST_HEAD(rtskb_mapped_list);
+static LIST_HEAD(rtskb_mapwait_list);
+
+LIST_HEAD(event_hook_list);
+DEFINE_MUTEX(rtnet_devices_nrt_lock);
+
+static int rtdev_locked_xmit(struct rtskb *skb, struct rtnet_device *rtdev);
+
+int rtdev_reference(struct rtnet_device *rtdev)
+{
+    smp_mb__before_atomic();
+    if (rtdev->rt_owner && atomic_add_unless(&rtdev->refcount, 1, 0) == 0) {
+	if (!try_module_get(rtdev->rt_owner))
+	    return 0;
+	if (atomic_inc_return(&rtdev->refcount) != 1)
+	    module_put(rtdev->rt_owner);
+    }
+    return 1;
+}
+EXPORT_SYMBOL_GPL(rtdev_reference);
+
+struct rtskb *rtnetdev_alloc_rtskb(struct rtnet_device *rtdev, unsigned int size)
+{
+    struct rtskb *rtskb = alloc_rtskb(size, &rtdev->dev_pool);
+    if (rtskb)
+	rtskb->rtdev = rtdev;
+    return rtskb;
+}
+EXPORT_SYMBOL_GPL(rtnetdev_alloc_rtskb);
+
+/***
+ *  __rtdev_get_by_name - find a rtnet_device by its name
+ *  @name: name to find
+ *  @note: caller must hold rtnet_devices_nrt_lock
+ */
+static struct rtnet_device *__rtdev_get_by_name(const char *name)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if ((rtdev != NULL) && (strncmp(rtdev->name, name, IFNAMSIZ) == 0))
+	    return rtdev;
+    }
+    return NULL;
+}
+
+
+/***
+ *  rtdev_get_by_name - find and lock a rtnet_device by its name
+ *  @name: name to find
+ */
+struct rtnet_device *rtdev_get_by_name(const char *name)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_name(name);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_get_by_index - find and lock a rtnet_device by its ifindex
+ *  @ifindex: index of device
+ */
+struct rtnet_device *rtdev_get_by_index(int ifindex)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    if ((ifindex <= 0) || (ifindex > MAX_RT_DEVICES))
+	return NULL;
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_index(ifindex);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  __rtdev_get_by_hwaddr - find a rtnetdevice by its mac-address
+ *  @type:          Type of the net_device (may be ARPHRD_ETHER)
+ *  @hw_addr:       MAC-Address
+ */
+static inline struct rtnet_device *__rtdev_get_by_hwaddr(unsigned short type, char *hw_addr)
+{
+    int                 i;
+    struct rtnet_device *rtdev;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if ((rtdev != NULL) && (rtdev->type == type) &&
+	    (!memcmp(rtdev->dev_addr, hw_addr, rtdev->addr_len))) {
+	    return rtdev;
+	}
+    }
+    return NULL;
+}
+
+
+
+/***
+ *  rtdev_get_by_hwaddr - find and lock a rtnetdevice by its mac-address
+ *  @type:          Type of the net_device (may be ARPHRD_ETHER)
+ *  @hw_addr:       MAC-Address
+ */
+struct rtnet_device *rtdev_get_by_hwaddr(unsigned short type, char *hw_addr)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = __rtdev_get_by_hwaddr(type, hw_addr);
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_get_by_hwaddr - find and lock the loopback device if available
+ */
+struct rtnet_device *rtdev_get_loopback(void)
+{
+    struct rtnet_device *rtdev;
+    unsigned long      context;
+
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    rtdev = loopback_device;
+    if (rtdev != NULL && !rtdev_reference(rtdev))
+	    rtdev = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_alloc_name - allocate a name for the rtnet_device
+ *  @rtdev:         the rtnet_device
+ *  @name_mask:     a name mask (e.g. "rteth%d" for ethernet)
+ *
+ *  This function have to be called from the driver probe function.
+ */
+void rtdev_alloc_name(struct rtnet_device *rtdev, const char *mask)
+{
+    char                buf[IFNAMSIZ];
+    int                 i;
+    struct rtnet_device *tmp;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	snprintf(buf, IFNAMSIZ, mask, i);
+	if ((tmp = rtdev_get_by_name(buf)) == NULL) {
+	    strncpy(rtdev->name, buf, IFNAMSIZ);
+	    break;
+	}
+	else
+	    rtdev_dereference(tmp);
+    }
+}
+
+static int rtdev_pool_trylock(void *cookie)
+{
+    return rtdev_reference(cookie);
+}
+
+static void rtdev_pool_unlock(void *cookie)
+{
+    rtdev_dereference(cookie);
+}
+
+static const struct rtskb_pool_lock_ops rtdev_ops = {
+    .trylock = rtdev_pool_trylock,
+    .unlock = rtdev_pool_unlock,
+};
+
+/***
+ *  rtdev_alloc
+ *  @int sizeof_priv:
+ *
+ *  allocate memory for a new rt-network-adapter
+ */
+struct rtnet_device *rtdev_alloc(unsigned sizeof_priv, unsigned dev_pool_size)
+{
+    struct rtnet_device *rtdev;
+    unsigned            alloc_size;
+    int                 ret;
+
+
+    /* ensure 32-byte alignment of the private area */
+    alloc_size = sizeof (*rtdev) + sizeof_priv + 31;
+
+    rtdev = (struct rtnet_device *)kmalloc(alloc_size, GFP_KERNEL);
+    if (rtdev == NULL) {
+	printk(KERN_ERR "RTnet: cannot allocate rtnet device\n");
+	return NULL;
+    }
+
+    memset(rtdev, 0, alloc_size);
+
+    ret = rtskb_pool_init(&rtdev->dev_pool, dev_pool_size, &rtdev_ops, rtdev);
+    if (ret < dev_pool_size) {
+	printk(KERN_ERR "RTnet: cannot allocate rtnet device pool\n");
+	rtskb_pool_release(&rtdev->dev_pool);
+	kfree(rtdev);
+	return NULL;
+    }
+
+    rt_mutex_init(&rtdev->xmit_mutex);
+    raw_spin_lock_init(&rtdev->rtdev_lock);
+    mutex_init(&rtdev->nrt_lock);
+
+    atomic_set(&rtdev->refcount, 0);
+
+    /* scale global rtskb pool */
+    rtdev->add_rtskbs = rtskb_pool_extend(&global_pool, device_rtskbs);
+
+    if (sizeof_priv)
+	rtdev->priv = (void *)(((long)(rtdev + 1) + 31) & ~31);
+
+    if(!initialized_rtnet_devices_rt_lock) {
+    	raw_spin_lock_init(&rtnet_devices_rt_lock);
+	initialized_rtnet_devices_rt_lock = 1;
+    }
+
+    return rtdev;
+}
+
+
+
+/***
+ *  rtdev_free
+ */
+void rtdev_free (struct rtnet_device *rtdev)
+{
+    if (rtdev != NULL) {
+	rtskb_pool_release(&rtdev->dev_pool);
+	rtskb_pool_shrink(&global_pool, rtdev->add_rtskbs);
+	rtdev->stack_event = NULL;
+	rt_mutex_destroy(&rtdev->xmit_mutex);
+	kfree(rtdev);
+    }
+}
+
+
+
+/**
+ * rtalloc_etherdev - Allocates and sets up an ethernet device
+ * @sizeof_priv: size of additional driver-private structure to
+ *               be allocated for this ethernet device
+ * @dev_pool_size: size of the rx pool
+ * @module: module creating the deivce
+ *
+ * Fill in the fields of the device structure with ethernet-generic
+ * values. Basically does everything except registering the device.
+ *
+ * A 32-byte alignment is enforced for the private data area.
+ */
+struct rtnet_device *__rt_alloc_etherdev(unsigned sizeof_priv,
+					unsigned dev_pool_size,
+					struct module *module)
+{
+    struct rtnet_device *rtdev;
+
+    rtdev = rtdev_alloc(sizeof_priv, dev_pool_size);
+    if (!rtdev)
+	return NULL;
+
+    rtdev->hard_header     = rt_eth_header;
+    rtdev->type            = ARPHRD_ETHER;
+    rtdev->hard_header_len = ETH_HLEN;
+    rtdev->mtu             = 1500; /* eth_mtu */
+    rtdev->addr_len        = ETH_ALEN;
+    rtdev->flags           = IFF_BROADCAST; /* TODO: IFF_MULTICAST; */
+    rtdev->get_mtu         = rt_hard_mtu;
+    rtdev->rt_owner	   = module;
+
+    memset(rtdev->broadcast, 0xFF, ETH_ALEN);
+    strcpy(rtdev->name, "rteth%d");
+
+    return rtdev;
+}
+
+
+
+static inline int __rtdev_new_index(void)
+{
+    int i;
+
+
+    for (i = 0; i < MAX_RT_DEVICES; i++)
+	if (rtnet_devices[i] == NULL)
+	     return i+1;
+
+    return -ENOMEM;
+}
+
+
+
+static int rtskb_map(struct rtnet_device *rtdev, struct rtskb *skb)
+{
+    dma_addr_t addr;
+
+    addr = rtdev->map_rtskb(rtdev, skb);
+
+    if (WARN_ON(addr == RTSKB_UNMAPPED))
+	return -ENOMEM;
+
+    if (skb->buf_dma_addr != RTSKB_UNMAPPED &&
+	addr != skb->buf_dma_addr) {
+	printk("RTnet: device %s maps skb differently than others. "
+	       "Different IOMMU domain?\nThis is not supported.\n",
+	       rtdev->name);
+	return -EACCES;
+    }
+
+    skb->buf_dma_addr = addr;
+
+    return 0;
+}
+
+
+
+int rtdev_map_rtskb(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev;
+    int err = 0;
+    int i;
+
+    skb->buf_dma_addr = RTSKB_UNMAPPED;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    for (i = 0; i < MAX_RT_DEVICES; i++) {
+	rtdev = rtnet_devices[i];
+	if (rtdev && rtdev->map_rtskb) {
+	    err = rtskb_map(rtdev, skb);
+	    if (err)
+		break;
+	}
+    }
+
+    if (!err) {
+        if (skb->buf_dma_addr != RTSKB_UNMAPPED)
+	    list_add(&skb->entry, &rtskb_mapped_list);
+        else
+	    list_add(&skb->entry, &rtskb_mapwait_list);
+    }
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    return err;
+}
+
+
+
+static int rtdev_map_all_rtskbs(struct rtnet_device *rtdev)
+{
+    struct rtskb *skb, *n;
+    int err = 0;
+
+    if (!rtdev->map_rtskb)
+	return 0;
+
+    list_for_each_entry(skb, &rtskb_mapped_list, entry) {
+	err = rtskb_map(rtdev, skb);
+	if (err)
+	   break;
+    }
+
+    list_for_each_entry_safe(skb, n, &rtskb_mapwait_list, entry) {
+	err = rtskb_map(rtdev, skb);
+	if (err)
+	   break;
+	list_del(&skb->entry);
+	list_add(&skb->entry, &rtskb_mapped_list);
+    }
+
+    return err;
+}
+
+
+
+void rtdev_unmap_rtskb(struct rtskb *skb)
+{
+    struct rtnet_device *rtdev;
+    int i;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    list_del(&skb->entry);
+
+    if (skb->buf_dma_addr != RTSKB_UNMAPPED) {
+	for (i = 0; i < MAX_RT_DEVICES; i++) {
+	    rtdev = rtnet_devices[i];
+	    if (rtdev && rtdev->unmap_rtskb) {
+		rtdev->unmap_rtskb(rtdev, skb);
+	    }
+	}
+    }
+
+    skb->buf_dma_addr = RTSKB_UNMAPPED;
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+static void rtdev_unmap_all_rtskbs(struct rtnet_device *rtdev)
+{
+    struct rtskb *skb;
+
+    if (!rtdev->unmap_rtskb)
+	return;
+
+    list_for_each_entry(skb, &rtskb_mapped_list, entry) {
+	rtdev->unmap_rtskb(rtdev, skb);
+    }
+}
+
+
+
+/***
+ * rt_register_rtnetdev: register a new rtnet_device (linux-like)
+ * @rtdev:               the device
+ */
+int rt_register_rtnetdev(struct rtnet_device *rtdev)
+{
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    unsigned long          context;
+    int                     ifindex;
+    int                     err;
+
+
+    /* requires at least driver layer version 2.0 */
+    if (rtdev->vers < RTDEV_VERS_2_0)
+	return -EINVAL;
+
+    if (rtdev->features & NETIF_F_LLTX)
+	rtdev->start_xmit = rtdev->hard_start_xmit;
+    else
+	rtdev->start_xmit = rtdev_locked_xmit;
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+
+    ifindex = __rtdev_new_index();
+    if (ifindex < 0) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return ifindex;
+    }
+    rtdev->ifindex = ifindex;
+
+    if (strchr(rtdev->name,'%') != NULL)
+	rtdev_alloc_name(rtdev, rtdev->name);
+
+    if (__rtdev_get_by_name(rtdev->name) != NULL) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return -EEXIST;
+    }
+
+    err = rtdev_map_all_rtskbs(rtdev);
+    if (err) {
+	mutex_unlock(&rtnet_devices_nrt_lock);
+	return err;
+    }
+
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    if (rtdev->flags & IFF_LOOPBACK) {
+	/* allow only one loopback device */
+	if (loopback_device) {
+	    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+	    mutex_unlock(&rtnet_devices_nrt_lock);
+	    return -EEXIST;
+	}
+	loopback_device = rtdev;
+    }
+    rtnet_devices[rtdev->ifindex-1] = rtdev;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    list_for_each(entry, &event_hook_list) {
+	hook = list_entry(entry, struct rtdev_event_hook, entry);
+	if (hook->register_device)
+	    hook->register_device(rtdev);
+    }
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    /* Default state at registration is that the device is present. */
+    set_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+
+    printk("RTnet: registered %s\n", rtdev->name);
+
+    return 0;
+}
+
+
+
+/***
+ * rt_unregister_rtnetdev: unregister a rtnet_device
+ * @rtdev:                 the device
+ */
+int rt_unregister_rtnetdev(struct rtnet_device *rtdev)
+{
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    unsigned long          context;
+
+
+    RTNET_ASSERT(rtdev->ifindex != 0,
+	printk("RTnet: device %s/%p was not registered\n", rtdev->name, rtdev);
+	return -ENODEV;);
+
+    mutex_lock(&rtnet_devices_nrt_lock);
+    raw_spin_lock_irqsave(&rtnet_devices_rt_lock, context);
+
+    RTNET_ASSERT(atomic_read(&rtdev->refcount) == 0, BUG());
+    rtnet_devices[rtdev->ifindex-1] = NULL;
+    if (rtdev->flags & IFF_LOOPBACK)
+	loopback_device = NULL;
+
+    raw_spin_unlock_irqrestore(&rtnet_devices_rt_lock, context);
+
+    list_for_each(entry, &event_hook_list) {
+	hook = list_entry(entry, struct rtdev_event_hook, entry);
+	if (hook->unregister_device)
+	    hook->unregister_device(rtdev);
+    }
+
+    rtdev_unmap_all_rtskbs(rtdev);
+
+    mutex_unlock(&rtnet_devices_nrt_lock);
+
+    clear_bit(__RTNET_LINK_STATE_PRESENT, &rtdev->link_state);
+
+    RTNET_ASSERT(atomic_read(&rtdev->refcount) == 0,
+	   printk("RTnet: rtdev reference counter < 0!\n"););
+
+    printk("RTnet: unregistered %s\n", rtdev->name);
+
+    return 0;
+}
+
+
+
+void rtdev_add_event_hook(struct rtdev_event_hook *hook)
+{
+    mutex_lock(&rtnet_devices_nrt_lock);
+    list_add(&hook->entry, &event_hook_list);
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+void rtdev_del_event_hook(struct rtdev_event_hook *hook)
+{
+    mutex_lock(&rtnet_devices_nrt_lock);
+    list_del(&hook->entry);
+    mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+
+
+/***
+ *  rtdev_open
+ *
+ *  Prepare an interface for use.
+ */
+int rtdev_open(struct rtnet_device *rtdev)
+{
+    int ret = 0;
+
+
+    if (rtdev->flags & IFF_UP)              /* Is it already up?                */
+	return 0;
+
+    if (!rtdev_reference(rtdev))
+	return -EIDRM;
+
+    if (rtdev->open)                        /* Call device private open method  */
+	ret = rtdev->open(rtdev);
+
+    if ( !ret )  {
+	rtdev->flags |= IFF_UP;
+	set_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+    } else
+	rtdev_dereference(rtdev);
+
+    return ret;
+}
+
+
+
+/***
+ *  rtdev_close
+ */
+int rtdev_close(struct rtnet_device *rtdev)
+{
+    int ret = 0;
+
+
+    if ( !(rtdev->flags & IFF_UP) )
+	return 0;
+
+    if (rtdev->stop)
+	ret = rtdev->stop(rtdev);
+
+    rtdev->flags &= ~(IFF_UP|IFF_RUNNING);
+    clear_bit(__RTNET_LINK_STATE_START, &rtdev->link_state);
+
+    if (ret == 0)
+	rtdev_dereference(rtdev);
+
+    return ret;
+}
+
+
+
+static int rtdev_locked_xmit(struct rtskb *skb, struct rtnet_device *rtdev)
+{
+    int ret;
+
+
+    rt_mutex_lock(&rtdev->xmit_mutex);
+    ret = rtdev->hard_start_xmit(skb, rtdev);
+    rt_mutex_unlock(&rtdev->xmit_mutex);
+
+    return ret;
+}
+
+
+
+/***
+ *  rtdev_xmit - send real-time packet
+ */
+int rtdev_xmit(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev;
+    int                 err;
+
+
+    RTNET_ASSERT(rtskb != NULL, return -EINVAL;);
+
+    rtdev = rtskb->rtdev;
+
+    if (!rtnetif_carrier_ok(rtdev)) {
+	err = -EAGAIN;
+	kfree_rtskb(rtskb);
+	return err;
+    }
+
+    if (rtskb_acquire(rtskb, &rtdev->dev_pool) != 0) {
+	err = -ENOBUFS;
+	kfree_rtskb(rtskb);
+	return err;
+    }
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    err = rtdev->start_xmit(rtskb, rtdev);
+    if (err) {
+	/* on error we must free the rtskb here */
+	kfree_rtskb(rtskb);
+
+	printk("hard_start_xmit returned %d\n", err);
+    }
+
+    return err;
+}
+
+
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+/***
+ *      rtdev_xmit_proxy - send rtproxy packet
+ */
+int rtdev_xmit_proxy(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev;
+    int                 err;
+
+
+    RTNET_ASSERT(rtskb != NULL, return -EINVAL;);
+
+    rtdev = rtskb->rtdev;
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    /* TODO: make these lines race-condition-safe */
+    if (rtdev->mac_disc) {
+	RTNET_ASSERT(rtdev->mac_disc->nrt_packet_tx != NULL, return -EINVAL;);
+
+	err = rtdev->mac_disc->nrt_packet_tx(rtskb);
+    } else {
+	err = rtdev->start_xmit(rtskb, rtdev);
+	if (err) {
+	    /* on error we must free the rtskb here */
+	    kfree_rtskb(rtskb);
+
+	    printk("hard_start_xmit returned %d\n", err);
+	}
+    }
+
+    return err;
+}
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_PROXY */
+
+
+
+unsigned int rt_hard_mtu(struct rtnet_device *rtdev, unsigned int priority)
+{
+    return rtdev->mtu;
+}
+
+
+EXPORT_SYMBOL_GPL(__rt_alloc_etherdev);
+EXPORT_SYMBOL_GPL(rtdev_free);
+
+EXPORT_SYMBOL_GPL(rtdev_alloc_name);
+
+EXPORT_SYMBOL_GPL(rt_register_rtnetdev);
+EXPORT_SYMBOL_GPL(rt_unregister_rtnetdev);
+
+EXPORT_SYMBOL_GPL(rtdev_add_event_hook);
+EXPORT_SYMBOL_GPL(rtdev_del_event_hook);
+
+EXPORT_SYMBOL_GPL(rtdev_get_by_name);
+EXPORT_SYMBOL_GPL(rtdev_get_by_index);
+EXPORT_SYMBOL_GPL(rtdev_get_by_hwaddr);
+EXPORT_SYMBOL_GPL(rtdev_get_loopback);
+
+EXPORT_SYMBOL_GPL(rtdev_xmit);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+EXPORT_SYMBOL_GPL(rtdev_xmit_proxy);
+#endif
+
+EXPORT_SYMBOL_GPL(rt_hard_mtu);
diff -Nur linux-5.4.5/net/rtnet/stack/rtdev_mgr.c linux-5.4.5-new/net/rtnet/stack/rtdev_mgr.c
--- linux-5.4.5/net/rtnet/stack/rtdev_mgr.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtdev_mgr.c	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,132 @@
+/***
+ *
+ *  stack/rtdev_mgr.c - device error manager
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/netdevice.h>
+
+#include <rtdev.h>
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+
+/***
+ *  rtnetif_err_rx: will be called from the  driver
+ *
+ *
+ *  @rtdev - the network-device
+ */
+void rtnetif_err_rx(struct rtnet_device *rtdev)
+{
+}
+
+/***
+ *  rtnetif_err_tx: will be called from the  driver
+ *
+ *
+ *  @rtdev - the network-device
+ */
+void rtnetif_err_tx(struct rtnet_device *rtdev)
+{
+}
+
+/***
+ *  do_rtdev_task
+ */
+/*static void do_rtdev_task(int mgr_id)
+{
+    struct rtnet_msg msg;
+    struct rtnet_mgr *mgr = (struct rtnet_mgr *)mgr_id;
+
+    while (1) {
+        rt_mbx_receive(&(mgr->mbx), &msg, sizeof(struct rtnet_msg));
+        if (msg.rtdev) {
+            rt_printk("RTnet: error on rtdev %s\n", msg.rtdev->name);
+        }
+    }
+}*/
+
+/***
+ *  rt_rtdev_connect
+ */
+void rt_rtdev_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr)
+{
+/*    rtdev->rtdev_mbx=&(mgr->mbx);*/
+}
+
+/***
+ *  rt_rtdev_disconnect
+ */
+void rt_rtdev_disconnect (struct rtnet_device *rtdev)
+{
+/*    rtdev->rtdev_mbx=NULL;*/
+}
+
+/***
+ *  rt_rtdev_mgr_start
+ */
+int rt_rtdev_mgr_start (struct rtnet_mgr *mgr)
+{
+    return /*(rt_task_resume(&(mgr->task)))*/ 0;
+}
+
+/***
+ *  rt_rtdev_mgr_stop
+ */
+int rt_rtdev_mgr_stop (struct rtnet_mgr *mgr)
+{
+    return /*(rt_task_suspend(&(mgr->task)))*/ 0;
+}
+
+/***
+ *  rt_rtdev_mgr_init
+ */
+int rt_rtdev_mgr_init (struct rtnet_mgr *mgr)
+{
+    int ret = 0;
+
+/*    if ( (ret=rt_mbx_init (&(mgr->mbx), sizeof(struct rtnet_msg))) )
+        return ret;
+    if ( (ret=rt_task_init(&(mgr->task), &do_rtdev_task, (int)mgr, 4096, RTNET_RTDEV_PRIORITY, 0, 0)) )
+        return ret;
+    if ( (ret=rt_task_resume(&(mgr->task))) )
+        return ret;*/
+
+    return (ret);
+}
+
+/***
+ *  rt_rtdev_mgr_delete
+ */
+void rt_rtdev_mgr_delete (struct rtnet_mgr *mgr)
+{
+/*    rt_task_delete(&(mgr->task));
+    rt_mbx_delete(&(mgr->mbx));*/
+}
+
+
+EXPORT_SYMBOL_GPL(rtnetif_err_rx);
+EXPORT_SYMBOL_GPL(rtnetif_err_tx);
+
+EXPORT_SYMBOL_GPL(rt_rtdev_connect);
+EXPORT_SYMBOL_GPL(rt_rtdev_disconnect);
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/Kconfig linux-5.4.5-new/net/rtnet/stack/rtmac/Kconfig
--- linux-5.4.5/net/rtnet/stack/rtmac/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/Kconfig	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,16 @@
+menuconfig XENO_DRIVERS_NET_RTMAC
+    depends on XENO_DRIVERS_NET
+    tristate "RTmac Layer"
+    default y
+    ---help---
+    The Real-Time Media Access Control layer allows to extend the RTnet
+    stack with software-based access control mechanisms (also called
+    disciplines) for nondeterministic transport media. Disciplines can be
+    attached and detached per real-time device. RTmac also provides a
+    framework for tunnelling non-time-critical packets through real-time
+    networks by installing virtual NICs (VNIC) in the Linux domain.
+
+    See Documentation/README.rtmac for further information.
+
+source "net/rtnet/stack/rtmac/tdma/Kconfig"
+source "net/rtnet/stack/rtmac/nomac/Kconfig"
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/Makefile linux-5.4.5-new/net/rtnet/stack/rtmac/Makefile
--- linux-5.4.5/net/rtnet/stack/rtmac/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/Makefile	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,15 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_NOMAC) += nomac/
+
+obj-$(CONFIG_XENO_DRIVERS_NET_TDMA) += tdma/
+
+obj-$(CONFIG_XENO_DRIVERS_NET_RTMAC) += rtmac.o
+
+rtmac-y := \
+	rtmac_disc.o \
+	rtmac_module.o \
+	rtmac_proc.o \
+	rtmac_proto.o \
+	rtmac_syms.o \
+	rtmac_vnic.o
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/Kconfig linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/Kconfig
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/Kconfig	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,9 @@
+config XENO_DRIVERS_NET_NOMAC
+    tristate "NoMAC discipline for RTmac"
+    depends on XENO_DRIVERS_NET_RTMAC
+    default n
+    ---help---
+    This no-operation RTmac discipline is intended to act as a template
+    for new implementations. However, it can be compiled and used (see
+    nomaccfg management tool), but don't expect any improved determinism
+    of your network. ;)
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/Makefile linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/Makefile
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/Makefile	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,9 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_NOMAC) += nomac.o
+
+nomac-y := \
+	nomac_dev.o \
+	nomac_ioctl.o \
+	nomac_module.o \
+	nomac_proto.o
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_dev.c linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_dev.c
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_dev.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_dev.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,87 @@
+/***
+ *
+ *  rtmac/nomac/nomac_dev.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtmac.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static int nomac_dev_openclose(void)
+{
+    return 0;
+}
+
+
+
+static int nomac_dev_ioctl(struct rtdm_fd *fd, unsigned int request, void *arg)
+{
+    struct nomac_priv   *nomac;
+
+
+    nomac = container_of(rtdm_fd_to_context(fd)->device,
+			 struct nomac_priv, api_device);
+
+    switch (request) {
+	case RTMAC_RTIOC_TIMEOFFSET:
+
+	case RTMAC_RTIOC_WAITONCYCLE:
+
+	default:
+	    return -ENOTTY;
+    }
+}
+
+static struct rtdm_driver nomac_driver = {
+    .profile_info = RTDM_PROFILE_INFO(nomac,
+				    RTDM_CLASS_RTMAC,
+				    RTDM_SUBCLASS_UNMANAGED,
+				    RTNET_RTDM_VER),
+    .device_flags = RTDM_NAMED_DEVICE,
+    .device_count = 1,
+    .context_size = 0,
+    .ops = {
+	.open =         (typeof(nomac_driver.ops.open))nomac_dev_openclose,
+	.ioctl_rt =     nomac_dev_ioctl,
+	.ioctl_nrt =    nomac_dev_ioctl,
+	.close =        (typeof(nomac_driver.ops.close))nomac_dev_openclose,
+    }
+};
+
+int nomac_dev_init(struct rtnet_device *rtdev, struct nomac_priv *nomac)
+{
+    char    *pos;
+
+    strcpy(nomac->device_name, "NOMAC");
+    for (pos = rtdev->name + strlen(rtdev->name) - 1;
+	(pos >= rtdev->name) && ((*pos) >= '0') && (*pos <= '9'); pos--);
+    strncat(nomac->device_name+5, pos+1, IFNAMSIZ-5);
+
+    nomac->api_driver           = nomac_driver;
+    nomac->api_device.driver    = &nomac->api_driver;
+    nomac->api_device.label     = nomac->device_name;
+
+    return rtdm_dev_register(&nomac->api_device);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_ioctl.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,107 @@
+/***
+ *
+ *  rtmac/nomac/nomac_ioctl.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/uaccess.h>
+
+#include <nomac_chrdev.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static int nomac_ioctl_attach(struct rtnet_device *rtdev)
+{
+    struct nomac_priv   *nomac;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &nomac_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    nomac = (struct nomac_priv *)rtdev->mac_priv->disc_priv;
+    if (nomac->magic != NOMAC_MAGIC)
+        return -ENOTTY;
+
+    /* ... */
+
+    return 0;
+}
+
+
+
+static int nomac_ioctl_detach(struct rtnet_device *rtdev)
+{
+    struct nomac_priv   *nomac;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    nomac = (struct nomac_priv *)rtdev->mac_priv->disc_priv;
+    if (nomac->magic != NOMAC_MAGIC)
+        return -ENOTTY;
+
+    ret = rtmac_disc_detach(rtdev);
+
+    /* ... */
+
+    return ret;
+}
+
+
+
+int nomac_ioctl(struct rtnet_device *rtdev, unsigned int request,
+                unsigned long arg)
+{
+    struct nomac_config cfg;
+    int                 ret;
+
+
+    ret = copy_from_user(&cfg, (void *)arg, sizeof(cfg));
+    if (ret != 0)
+        return -EFAULT;
+
+    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+        return -ERESTARTSYS;
+
+    switch (request) {
+        case NOMAC_IOC_ATTACH:
+            ret = nomac_ioctl_attach(rtdev);
+            break;
+
+        case NOMAC_IOC_DETACH:
+            ret = nomac_ioctl_detach(rtdev);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    mutex_unlock(&rtdev->nrt_lock);
+
+    return ret;
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_module.c linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_module.c
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_module.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_module.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,182 @@
+/***
+ *
+ *  rtmac/nomac/nomac_module.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002       Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca,
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+
+#include <rtdm/driver.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/nomac/nomac.h>
+#include <rtmac/nomac/nomac_dev.h>
+#include <rtmac/nomac/nomac_ioctl.h>
+#include <rtmac/nomac/nomac_proto.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+LIST_HEAD(nomac_devices);
+DEFINE_RT_MUTEX(nomac_nrt_lock);
+
+
+int nomac_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    struct nomac_priv *entry;
+
+    mutex_lock(&nomac_nrt_lock);
+
+    xnvfile_printf(it, "Interface       API Device      State\n");
+
+    list_for_each_entry(entry, &nomac_devices, list_entry)
+	xnvfile_printf(it, "%-15s %-15s Attached\n", entry->rtdev->name,
+		    entry->api_device.name);
+
+    mutex_unlock(&nomac_nrt_lock);
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int nomac_attach(struct rtnet_device *rtdev, void *priv)
+{
+    struct nomac_priv   *nomac = (struct nomac_priv *)priv;
+    int                 ret;
+
+
+    nomac->magic = NOMAC_MAGIC;
+    nomac->rtdev = rtdev;
+
+    /* ... */
+
+    ret = nomac_dev_init(rtdev, nomac);
+    if (ret < 0)
+	return ret;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    mutex_lock(&nomac_nrt_lock);
+    list_add(&nomac->list_entry, &nomac_devices);
+    mutex_unlock(&nomac_nrt_lock);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    return 0;
+}
+
+
+
+int nomac_detach(struct rtnet_device *rtdev, void *priv)
+{
+    struct nomac_priv   *nomac = (struct nomac_priv *)priv;
+
+
+    nomac_dev_release(nomac);
+
+    /* ... */
+#ifdef CONFIG_XENO_OPT_VFILE
+    mutex_lock(&nomac_nrt_lock);
+    list_del(&nomac->list_entry);
+    mutex_unlock(&nomac_nrt_lock);
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct rtmac_proc_entry nomac_proc_entries[] = {
+    { name: "nomac", handler: nomac_proc_read },
+};
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+struct rtmac_disc nomac_disc = {
+    name:           "NoMAC",
+    priv_size:      sizeof(struct nomac_priv),
+    disc_type:      __constant_htons(RTMAC_TYPE_NOMAC),
+
+    packet_rx:      nomac_packet_rx,
+    rt_packet_tx:   nomac_rt_packet_tx,
+    nrt_packet_tx:  nomac_nrt_packet_tx,
+
+    get_mtu:        NULL,
+
+    vnic_xmit:      RTMAC_DEFAULT_VNIC,
+
+    attach:         nomac_attach,
+    detach:         nomac_detach,
+
+    ioctls:         {
+	service_name:   "RTmac/NoMAC",
+	ioctl_type:     RTNET_IOC_TYPE_RTMAC_NOMAC,
+	handler:        nomac_ioctl
+    },
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    proc_entries:   nomac_proc_entries,
+    nr_proc_entries: ARRAY_SIZE(nomac_proc_entries),
+#endif /* CONFIG_XENO_OPT_VFILE */
+};
+
+
+
+int __init nomac_init(void)
+{
+    int ret;
+
+
+    printk("RTmac/NoMAC: init void media access control mechanism\n");
+
+    ret = nomac_proto_init();
+    if (ret < 0)
+	return ret;
+
+    ret = rtmac_disc_register(&nomac_disc);
+    if (ret < 0) {
+	nomac_proto_cleanup();
+	return ret;
+    }
+
+    return 0;
+}
+
+
+
+void nomac_release(void)
+{
+    rtmac_disc_deregister(&nomac_disc);
+    nomac_proto_cleanup();
+
+    printk("RTmac/NoMAC: unloaded\n");
+}
+
+
+
+module_init(nomac_init);
+module_exit(nomac_release);
+
+MODULE_AUTHOR("Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_proto.c linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_proto.c
--- linux-5.4.5/net/rtnet/stack/rtmac/nomac/nomac_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/nomac/nomac_proto.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,143 @@
+/***
+ *
+ *  rtmac/nomac/nomac_proto.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+
+#include <rtdev.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/nomac/nomac.h>
+
+
+static struct rtskb_queue   nrt_rtskb_queue;
+static rtdm_task_t          wrapper_task;
+static rtdm_event_t         wakeup_sem;
+
+
+int nomac_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    /* unused here, just to demonstrate access to the discipline state
+    struct nomac_priv   *nomac =
+        (struct nomac_priv *)rtdev->mac_priv->disc_priv; */
+    int                 ret;
+
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    /* no MAC: we simply transmit the packet under xmit_lock */
+    rtdm_mutex_lock(&rtdev->xmit_mutex);
+    ret = rtmac_xmit(rtskb);
+    rtdm_mutex_unlock(&rtdev->xmit_mutex);
+
+    return ret;
+}
+
+
+
+int nomac_nrt_packet_tx(struct rtskb *rtskb)
+{
+    struct rtnet_device *rtdev = rtskb->rtdev;
+    /* unused here, just to demonstrate access to the discipline state
+    struct nomac_priv   *nomac =
+        (struct nomac_priv *)rtdev->mac_priv->disc_priv; */
+    int                 ret;
+
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    /* note: this routine may be called both in rt and non-rt context
+     *       => detect and wrap the context if necessary */
+    if (!rtdm_in_rt_context()) {
+        rtskb_queue_tail(&nrt_rtskb_queue, rtskb);
+        rtdm_event_signal(&wakeup_sem);
+        return 0;
+    } else {
+        /* no MAC: we simply transmit the packet under xmit_lock */
+        rtdm_mutex_lock(&rtdev->xmit_mutex);
+        ret = rtmac_xmit(rtskb);
+        rtdm_mutex_unlock(&rtdev->xmit_mutex);
+
+        return ret;
+    }
+}
+
+
+
+void nrt_xmit_task(void *arg)
+{
+    struct rtskb        *rtskb;
+    struct rtnet_device *rtdev;
+
+
+    while (!rtdm_task_should_stop()) {
+	if (rtdm_event_wait(&wakeup_sem) < 0)
+	    break;
+
+        while ((rtskb = rtskb_dequeue(&nrt_rtskb_queue))) {
+            rtdev = rtskb->rtdev;
+
+            /* no MAC: we simply transmit the packet under xmit_lock */
+            rtdm_mutex_lock(&rtdev->xmit_mutex);
+            rtmac_xmit(rtskb);
+            rtdm_mutex_unlock(&rtdev->xmit_mutex);
+        }
+    }
+}
+
+
+
+int nomac_packet_rx(struct rtskb *rtskb)
+{
+    /* actually, NoMAC doesn't expect any control packet */
+    kfree_rtskb(rtskb);
+
+    return 0;
+}
+
+
+
+int __init nomac_proto_init(void)
+{
+    int ret;
+
+
+    rtskb_queue_init(&nrt_rtskb_queue);
+    rtdm_event_init(&wakeup_sem, 0);
+
+    ret = rtdm_task_init(&wrapper_task, "rtnet-nomac", nrt_xmit_task, 0,
+                         RTDM_TASK_LOWEST_PRIORITY, 0);
+    if (ret < 0) {
+        rtdm_event_destroy(&wakeup_sem);
+        return ret;
+    }
+
+    return 0;
+}
+
+
+
+void nomac_proto_cleanup(void)
+{
+    rtdm_event_destroy(&wakeup_sem);
+    rtdm_task_destroy(&wrapper_task);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_disc.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_disc.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_disc.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_disc.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,290 @@
+/***
+ *
+ *  rtmac_disc.c
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003, 2004 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/netdevice.h>
+#include <linux/mutex.h>
+
+#include <rtnet_internal.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proc.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+
+static DEFINE_RT_MUTEX(disc_list_lock);
+static LIST_HEAD(disc_list);
+
+
+
+/***
+ *  rtmac_disc_attach
+ *
+ *  @rtdev       attaches a discipline to a device
+ *  @disc        discipline to attach
+ *
+ *  0            success
+ *  -EBUSY       other discipline active
+ *  -ENOMEM      could not allocate memory
+ *
+ *  Note: must be called with rtdev->nrt_lock acquired
+ */
+int rtmac_disc_attach(struct rtnet_device *rtdev, struct rtmac_disc *disc)
+{
+    int                 ret;
+    struct rtmac_priv   *priv;
+
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->attach != NULL, return -EINVAL;);
+
+    if (rtdev->mac_disc) {
+	printk("RTmac: another discipline for rtdev '%s' active.\n", rtdev->name);
+	return -EBUSY;
+    }
+
+    if (rtdev->flags & IFF_LOOPBACK)
+	return -EINVAL;
+
+    if (!try_module_get(disc->owner))
+	return -EIDRM;
+
+    if (!rtdev_reference(rtdev)) {
+	ret = -EIDRM;
+	goto err_module_put;
+    }
+
+    /* alloc memory */
+    priv = kmalloc(sizeof(struct rtmac_priv) + disc->priv_size, GFP_KERNEL);
+    if (!priv) {
+	printk("RTmac: kmalloc returned NULL for rtmac!\n");
+	return -ENOMEM;
+    }
+    priv->orig_start_xmit = rtdev->start_xmit;
+
+    /* call attach function of discipline */
+    ret = disc->attach(rtdev, priv->disc_priv);
+    if (ret < 0)
+	goto err_kfree_priv;
+
+    /* now attach RTmac to device */
+    rtdev->mac_disc = disc;
+    rtdev->mac_priv = priv;
+    rtdev->start_xmit = disc->rt_packet_tx;
+    if (disc->get_mtu)
+	rtdev->get_mtu = disc->get_mtu;
+    rtdev->mac_detach = rtmac_disc_detach;
+
+    /* create the VNIC */
+    ret = rtmac_vnic_add(rtdev, disc->vnic_xmit);
+    if (ret < 0) {
+	printk("RTmac: Warning, VNIC creation failed for rtdev %s.\n", rtdev->name);
+	goto err_disc_detach;
+    }
+
+    return 0;
+
+  err_disc_detach:
+    disc->detach(rtdev, priv->disc_priv);
+  err_kfree_priv:
+    kfree(priv);
+    rtdev_dereference(rtdev);
+  err_module_put:
+    module_put(disc->owner);
+    return ret;
+}
+
+
+
+/***
+ *  rtmac_disc_detach
+ *
+ *  @rtdev       detaches a discipline from a device
+ *
+ *  0            success
+ *  -1           discipline has no detach function
+ *  -EINVAL      called with rtdev=NULL
+ *  -ENODEV      no discipline active on dev
+ *
+ *  Note: must be called with rtdev->nrt_lock acquired
+ */
+int rtmac_disc_detach(struct rtnet_device *rtdev)
+{
+    int                 ret;
+    struct rtmac_disc   *disc;
+    struct rtmac_priv   *priv;
+
+
+    RTNET_ASSERT(rtdev != NULL, return -EINVAL;);
+
+    disc = rtdev->mac_disc;
+    if (!disc)
+	return -ENODEV;
+
+    RTNET_ASSERT(disc->detach != NULL, return -EINVAL;);
+
+    priv = rtdev->mac_priv;
+    RTNET_ASSERT(priv != NULL, return -EINVAL;);
+
+    ret = rtmac_vnic_unregister(rtdev);
+    if (ret < 0)
+	return ret;
+
+    /* call release function of discipline */
+    ret = disc->detach(rtdev, priv->disc_priv);
+    if (ret < 0)
+	return ret;
+
+    rtmac_vnic_cleanup(rtdev);
+
+    /* restore start_xmit and get_mtu */
+    rtdev->start_xmit = priv->orig_start_xmit;
+    rtdev->get_mtu    = rt_hard_mtu;
+
+    /* remove pointers from rtdev */
+    rtdev->mac_disc   = NULL;
+    rtdev->mac_priv   = NULL;
+    rtdev->mac_detach = NULL;
+
+    rtdev_dereference(rtdev);
+
+    kfree(priv);
+
+    module_put(disc->owner);
+
+    return 0;
+}
+
+
+
+static struct rtmac_disc *rtmac_get_disc_by_name(const char *name)
+{
+    struct list_head    *disc;
+
+
+    mutex_lock(&disc_list_lock);
+
+    list_for_each(disc, &disc_list) {
+	if (strcmp(((struct rtmac_disc *)disc)->name, name) == 0) {
+	    mutex_unlock(&disc_list_lock);
+	    return (struct rtmac_disc *)disc;
+	}
+    }
+
+    mutex_unlock(&disc_list_lock);
+
+    return NULL;
+}
+
+
+
+int __rtmac_disc_register(struct rtmac_disc *disc, struct module *module)
+{
+    int ret;
+
+
+    RTNET_ASSERT(disc != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->name != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->rt_packet_tx != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->nrt_packet_tx != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->attach != NULL, return -EINVAL;);
+    RTNET_ASSERT(disc->detach != NULL, return -EINVAL;);
+
+    disc->owner = module;
+
+    if (rtmac_get_disc_by_name(disc->name) != NULL)
+    {
+	printk("RTmac: discipline '%s' already registered!\n", disc->name);
+	return -EBUSY;
+    }
+
+    ret = rtnet_register_ioctls(&disc->ioctls);
+    if (ret < 0)
+	return ret;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtmac_disc_proc_register(disc);
+    if (ret < 0) {
+	rtnet_unregister_ioctls(&disc->ioctls);
+	return ret;
+    }
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+    mutex_lock(&disc_list_lock);
+
+    list_add(&disc->list, &disc_list);
+
+    mutex_unlock(&disc_list_lock);
+
+    return 0;
+}
+
+
+
+void rtmac_disc_deregister(struct rtmac_disc *disc)
+{
+    RTNET_ASSERT(disc != NULL, return;);
+
+    mutex_lock(&disc_list_lock);
+
+    list_del(&disc->list);
+
+    mutex_unlock(&disc_list_lock);
+
+    rtnet_unregister_ioctls(&disc->ioctls);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_disc_proc_unregister(disc);
+#endif /* CONFIG_XENO_OPT_VFILE */
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_disciplines_show(struct xnvfile_regular_iterator *it, void *d)
+{
+    struct rtmac_disc    *disc;
+    int err;
+
+    err = mutex_lock_interruptible(&disc_list_lock);
+    if (err < 0)
+	return err;
+
+    xnvfile_printf(it, "Name\t\tID\n");
+
+    list_for_each_entry(disc, &disc_list, list)
+	xnvfile_printf(it, "%-15s %04X\n",disc->name, ntohs(disc->disc_type));
+
+    mutex_unlock(&disc_list_lock);
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_module.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_module.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_module.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_module.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,86 @@
+/* rtmac_module.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+#include <rtdm/driver.h>
+
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+int __init rtmac_init(void)
+{
+    int ret = 0;
+
+
+    printk("RTmac: init realtime media access control\n");
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    ret = rtmac_proc_register();
+    if (ret < 0)
+        return ret;
+#endif
+
+    ret = rtmac_vnic_module_init();
+    if (ret < 0)
+        goto error1;
+
+    ret = rtmac_proto_init();
+    if (ret < 0)
+        goto error2;
+
+    return 0;
+
+error2:
+    rtmac_vnic_module_cleanup();
+
+error1:
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_proc_release();
+#endif
+    return ret;
+}
+
+
+
+void rtmac_release(void)
+{
+    rtmac_proto_release();
+    rtmac_vnic_module_cleanup();
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtmac_proc_release();
+#endif
+
+    printk("RTmac: unloaded\n");
+}
+
+
+
+module_init(rtmac_init);
+module_exit(rtmac_release);
+
+MODULE_AUTHOR("Marc Kleine-Budde, Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_proc.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_proc.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_proc.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_proc.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,137 @@
+/***
+ *
+ *  rtmac_proc.c
+ *
+ *  rtmac - real-time networking medium access control subsystem
+ *  Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2004 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+
+#include <rtnet_internal.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/rtmac_proc.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct xnvfile_directory rtmac_proc_root;
+
+static struct xnvfile_regular_ops rtnet_rtmac_disciplines_vfile_ops = {
+    .show = rtnet_rtmac_disciplines_show,
+};
+
+static struct xnvfile_regular rtnet_rtmac_disciplines_vfile = {
+    .ops = &rtnet_rtmac_disciplines_vfile_ops,
+};
+
+static struct xnvfile_regular_ops rtnet_rtmac_vnics_vfile_ops = {
+    .show = rtnet_rtmac_vnics_show,
+};
+
+static struct xnvfile_regular rtnet_rtmac_vnics_vfile = {
+    .ops = &rtnet_rtmac_vnics_vfile_ops,
+};
+
+static int
+rtnet_rtmac_disc_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    struct rtmac_proc_entry *entry;
+    entry = container_of(it->vfile, struct rtmac_proc_entry, vfile);
+    return entry->handler(it, data);
+}
+
+static struct xnvfile_regular_ops rtnet_rtmac_disc_vfile_ops = {
+    .show = rtnet_rtmac_disc_show,
+};
+
+int rtmac_disc_proc_register(struct rtmac_disc *disc)
+{
+    int                     i, err;
+    struct rtmac_proc_entry *entry;
+
+
+    for (i = 0; i < disc->nr_proc_entries; i++) {
+	entry = &disc->proc_entries[i];
+
+	entry->vfile.ops = &rtnet_rtmac_disc_vfile_ops;
+	err = xnvfile_init_regular(entry->name, &entry->vfile, &rtmac_proc_root);
+	if (err < 0) {
+	    while (--i >= 0)
+		xnvfile_destroy_regular(&disc->proc_entries[i].vfile);
+	    return err;
+	}
+    }
+
+    return 0;
+}
+
+
+
+void rtmac_disc_proc_unregister(struct rtmac_disc *disc)
+{
+    int i;
+
+    for (i = 0; i < disc->nr_proc_entries; i++)
+	xnvfile_destroy_regular(&disc->proc_entries[i].vfile);
+}
+
+
+
+int rtmac_proc_register(void)
+{
+    int err;
+
+    err = xnvfile_init_dir("rtmac", &rtmac_proc_root, &rtnet_proc_root);
+    if (err < 0)
+	goto err1;
+
+    err = xnvfile_init_regular("disciplines", &rtnet_rtmac_disciplines_vfile,
+			    &rtmac_proc_root);
+    if (err < 0)
+	goto err2;
+
+    err = xnvfile_init_regular("vnics", &rtnet_rtmac_vnics_vfile,
+			    &rtmac_proc_root);
+    if (err < 0)
+	goto err3;
+
+    return 0;
+
+  err3:
+    xnvfile_destroy_regular(&rtnet_rtmac_disciplines_vfile);
+
+  err2:
+    xnvfile_destroy_dir(&rtmac_proc_root);
+
+  err1:
+    /*ERRMSG*/printk("RTmac: unable to initialize /proc entries\n");
+    return err;
+}
+
+
+
+void rtmac_proc_release(void)
+{
+    xnvfile_destroy_regular(&rtnet_rtmac_vnics_vfile);
+    xnvfile_destroy_regular(&rtnet_rtmac_disciplines_vfile);
+    xnvfile_destroy_dir(&rtmac_proc_root);
+}
+
+#endif /* CONFIG_XENO_OPT_VFILE */
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_proto.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_proto.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_proto.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,77 @@
+/***
+ *
+ *  rtmac/rtmac_proto.c
+ *
+ *  rtmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+
+#include <rtdm/driver.h>
+#include <stack_mgr.h>
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+
+int rtmac_proto_rx(struct rtskb *skb, struct rtpacket_type *pt)
+{
+    struct rtmac_disc *disc = skb->rtdev->mac_disc;
+    struct rtmac_hdr  *hdr;
+
+
+    if (disc == NULL) {
+        goto error;
+    }
+
+    hdr = (struct rtmac_hdr *)skb->data;
+    rtskb_pull(skb, sizeof(struct rtmac_hdr));
+
+    if (hdr->ver != RTMAC_VERSION) {
+        rtdm_printk("RTmac: received unsupported RTmac protocol version on "
+                    "device %s.  Got 0x%x but expected 0x%x\n",
+                    skb->rtdev->name, hdr->ver, RTMAC_VERSION);
+        goto error;
+    }
+
+    if (hdr->flags & RTMAC_FLAG_TUNNEL)
+        rtmac_vnic_rx(skb, hdr->type);
+    else if (disc->disc_type == hdr->type)
+        disc->packet_rx(skb);
+    return 0;
+
+  error:
+    kfree_rtskb(skb);
+    return 0;
+}
+
+
+
+struct rtpacket_type rtmac_packet_type = {
+    .type =     __constant_htons(ETH_RTMAC),
+    .handler =  rtmac_proto_rx
+};
+
+
+
+void rtmac_proto_release(void)
+{
+    rtdev_remove_pack(&rtmac_packet_type);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_syms.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_syms.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_syms.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_syms.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,37 @@
+/* rtmac_syms.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002 Marc Kleine-Budde <kleine-budde@gmx.de>
+ *               2003 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+EXPORT_SYMBOL_GPL(__rtmac_disc_register);
+EXPORT_SYMBOL_GPL(rtmac_disc_deregister);
+
+EXPORT_SYMBOL_GPL(rtmac_disc_attach);
+EXPORT_SYMBOL_GPL(rtmac_disc_detach);
+
+EXPORT_SYMBOL_GPL(rtmac_vnic_set_max_mtu);
+
+EXPORT_SYMBOL_GPL(rtmac_vnic_xmit);
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/rtmac_vnic.c linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_vnic.c
--- linux-5.4.5/net/rtnet/stack/rtmac/rtmac_vnic.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/rtmac_vnic.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,360 @@
+/* rtmac_vnic.c
+ *
+ * rtmac - real-time networking media access control subsystem
+ * Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *               2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#include <linux/moduleparam.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/rtnetlink.h>
+
+#include <rtnet_internal.h>
+#include <rtdev.h>
+#include <rtnet_port.h> /* for netdev_priv() */
+#include <rtmac/rtmac_disc.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/rtmac_vnic.h>
+
+
+static unsigned int vnic_rtskbs = DEFAULT_VNIC_RTSKBS;
+module_param(vnic_rtskbs, uint, 0444);
+MODULE_PARM_DESC(vnic_rtskbs, "Number of realtime socket buffers per virtual NIC");
+
+static rtdm_nrtsig_t        vnic_signal;
+static struct rtskb_queue   rx_queue;
+
+
+
+int rtmac_vnic_rx(struct rtskb *rtskb, u16 type)
+{
+    struct rtmac_priv *mac_priv = rtskb->rtdev->mac_priv;
+    struct rtskb_pool *pool = &mac_priv->vnic_skb_pool;
+
+
+    if (rtskb_acquire(rtskb, pool) != 0) {
+	mac_priv->vnic_stats.rx_dropped++;
+	kfree_rtskb(rtskb);
+	return -1;
+    }
+
+    rtskb->protocol = type;
+
+    rtskb_queue_tail(&rx_queue, rtskb);
+    rtdm_nrtsig_pend(&vnic_signal);
+
+    return 0;
+}
+
+
+
+static void rtmac_vnic_signal_handler(rtdm_nrtsig_t *nrtsig, void *arg)
+{
+    struct rtskb            *rtskb;
+    struct sk_buff          *skb;
+    unsigned                hdrlen;
+    struct net_device_stats *stats;
+    struct rtnet_device     *rtdev;
+
+
+    while (1)
+    {
+	rtskb = rtskb_dequeue(&rx_queue);
+	if (!rtskb)
+	    break;
+
+	rtdev  = rtskb->rtdev;
+	hdrlen = rtdev->hard_header_len;
+
+	skb = dev_alloc_skb(hdrlen + rtskb->len + 2);
+	if (skb) {
+	    /* the rtskb stamp is useless (different clock), get new one */
+	    __net_timestamp(skb);
+
+	    skb_reserve(skb, 2); /* Align IP on 16 byte boundaries */
+
+	    /* copy Ethernet header */
+	    memcpy(skb_put(skb, hdrlen),
+		   rtskb->data - hdrlen - sizeof(struct rtmac_hdr), hdrlen);
+
+	    /* patch the protocol field in the original Ethernet header */
+	    ((struct ethhdr*)skb->data)->h_proto = rtskb->protocol;
+
+	    /* copy data */
+	    memcpy(skb_put(skb, rtskb->len), rtskb->data, rtskb->len);
+
+	    skb->dev      = rtskb->rtdev->mac_priv->vnic;
+	    skb->protocol = eth_type_trans(skb, skb->dev);
+
+	    stats = &rtskb->rtdev->mac_priv->vnic_stats;
+
+	    kfree_rtskb(rtskb);
+
+	    stats->rx_packets++;
+	    stats->rx_bytes += skb->len;
+
+	    netif_rx(skb);
+	}
+	else {
+	    printk("RTmac: VNIC fails to allocate linux skb\n");
+	    kfree_rtskb(rtskb);
+	}
+    }
+}
+
+
+
+static int rtmac_vnic_copy_mac(struct net_device *dev)
+{
+    memcpy(dev->dev_addr,
+	   (*(struct rtnet_device **)netdev_priv(dev))->dev_addr,
+	   MAX_ADDR_LEN);
+
+    return 0;
+}
+
+
+
+int rtmac_vnic_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+    struct rtnet_device     *rtdev = *(struct rtnet_device **)netdev_priv(dev);
+    struct net_device_stats *stats = &rtdev->mac_priv->vnic_stats;
+    struct rtskb_pool       *pool = &rtdev->mac_priv->vnic_skb_pool;
+    struct ethhdr           *ethernet = (struct ethhdr*)skb->data;
+    struct rtskb            *rtskb;
+    int                     res;
+    int                     data_len;
+
+
+    rtskb =
+	alloc_rtskb((skb->len + sizeof(struct rtmac_hdr) + 15) & ~15, pool);
+    if (!rtskb)
+	return NETDEV_TX_BUSY;
+
+    rtskb_reserve(rtskb, rtdev->hard_header_len + sizeof(struct rtmac_hdr));
+
+    data_len = skb->len - dev->hard_header_len;
+    memcpy(rtskb_put(rtskb, data_len), skb->data + dev->hard_header_len,
+	   data_len);
+
+    res = rtmac_add_header(rtdev, ethernet->h_dest, rtskb,
+			   ntohs(ethernet->h_proto), RTMAC_FLAG_TUNNEL);
+    if (res < 0) {
+	stats->tx_dropped++;
+	kfree_rtskb(rtskb);
+	goto done;
+    }
+
+    RTNET_ASSERT(rtdev->mac_disc->nrt_packet_tx != NULL, kfree_rtskb(rtskb);
+		 goto done;);
+
+    res = rtdev->mac_disc->nrt_packet_tx(rtskb);
+    if (res < 0) {
+	stats->tx_dropped++;
+	kfree_rtskb(rtskb);
+    } else {
+	stats->tx_packets++;
+	stats->tx_bytes += skb->len;
+    }
+
+done:
+    dev_kfree_skb(skb);
+    return NETDEV_TX_OK;
+}
+
+
+
+static struct net_device_stats *rtmac_vnic_get_stats(struct net_device *dev)
+{
+    return &(*(struct rtnet_device **)netdev_priv(dev))->mac_priv->vnic_stats;
+}
+
+
+
+static int rtmac_vnic_change_mtu(struct net_device *dev, int new_mtu)
+{
+    if ((new_mtu < 68) ||
+	((unsigned)new_mtu > 1500 - sizeof(struct rtmac_hdr)))
+	return -EINVAL;
+    dev->mtu = new_mtu;
+    return 0;
+}
+
+
+
+void rtmac_vnic_set_max_mtu(struct rtnet_device *rtdev, unsigned int max_mtu)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+    struct net_device   *vnic = mac_priv->vnic;
+    unsigned int        prev_mtu  = mac_priv->vnic_max_mtu;
+
+
+    mac_priv->vnic_max_mtu = max_mtu - sizeof(struct rtmac_hdr);
+
+    /* set vnic mtu in case max_mtu is smaller than the current mtu or
+       the current mtu was set to previous max_mtu */
+    rtnl_lock();
+    if ((vnic->mtu > mac_priv->vnic_max_mtu) || (prev_mtu == mac_priv->vnic_max_mtu)) {
+	dev_set_mtu(vnic, mac_priv->vnic_max_mtu);
+    }
+    rtnl_unlock();
+}
+
+
+static struct net_device_ops vnic_netdev_ops = {
+    .ndo_open       = rtmac_vnic_copy_mac,
+    .ndo_get_stats  = rtmac_vnic_get_stats,
+    .ndo_change_mtu = rtmac_vnic_change_mtu,
+};
+
+static void rtmac_vnic_setup(struct net_device *dev)
+{
+    ether_setup(dev);
+
+    dev->netdev_ops      = &vnic_netdev_ops;
+    dev->flags           &= ~IFF_MULTICAST;
+}
+
+int rtmac_vnic_add(struct rtnet_device *rtdev, vnic_xmit_handler vnic_xmit)
+{
+    int                 res;
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+    struct net_device   *vnic;
+    char                buf[IFNAMSIZ];
+
+
+    /* does the discipline request vnic support? */
+    if (!vnic_xmit)
+	return 0;
+
+    mac_priv->vnic = NULL;
+    mac_priv->vnic_max_mtu = rtdev->mtu - sizeof(struct rtmac_hdr);
+    memset(&mac_priv->vnic_stats, 0, sizeof(mac_priv->vnic_stats));
+
+    /* create the rtskb pool */
+    if (rtskb_pool_init(&mac_priv->vnic_skb_pool,
+			    vnic_rtskbs, NULL, NULL) < vnic_rtskbs) {
+	res = -ENOMEM;
+	goto error;
+    }
+
+    snprintf(buf, sizeof(buf), "vnic%d", rtdev->ifindex-1);
+
+    vnic = alloc_netdev(sizeof(struct rtnet_device *), buf,
+		    NET_NAME_UNKNOWN, rtmac_vnic_setup);
+    if (!vnic) {
+	res = -ENOMEM;
+	goto error;
+    }
+
+    vnic_netdev_ops.ndo_start_xmit = vnic_xmit;
+    vnic->mtu = mac_priv->vnic_max_mtu;
+    *(struct rtnet_device **)netdev_priv(vnic) = rtdev;
+    rtmac_vnic_copy_mac(vnic);
+
+    res = register_netdev(vnic);
+    if (res < 0)
+	goto error;
+
+    mac_priv->vnic = vnic;
+
+    return 0;
+
+ error:
+    rtskb_pool_release(&mac_priv->vnic_skb_pool);
+    return res;
+}
+
+
+
+int rtmac_vnic_unregister(struct rtnet_device *rtdev)
+{
+    struct rtmac_priv   *mac_priv = rtdev->mac_priv;
+
+    if (mac_priv->vnic) {
+	rtskb_pool_release(&mac_priv->vnic_skb_pool);
+	unregister_netdev(mac_priv->vnic);
+	free_netdev(mac_priv->vnic);
+	mac_priv->vnic = NULL;
+    }
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int rtnet_rtmac_vnics_show(struct xnvfile_regular_iterator *it, void *d)
+{
+    struct rtnet_device *rtdev;
+    int                 i;
+    int                 err;
+
+    xnvfile_printf(it, "RT-NIC name\tVNIC name\n");
+
+    for (i = 1; i <= MAX_RT_DEVICES; i++) {
+	rtdev = rtdev_get_by_index(i);
+	if (rtdev == NULL)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    return err;
+	}
+
+	if (rtdev->mac_priv != NULL) {
+	    struct rtmac_priv *rtmac;
+
+	    rtmac = (struct rtmac_priv *)rtdev->mac_priv;
+	    xnvfile_printf(it, "%-15s %s\n", rtdev->name, rtmac->vnic->name);
+	}
+
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return 0;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int __init rtmac_vnic_module_init(void)
+{
+    rtskb_queue_init(&rx_queue);
+
+    rtdm_nrtsig_init(&vnic_signal, rtmac_vnic_signal_handler, NULL);
+
+    return 0;
+}
+
+
+
+void rtmac_vnic_module_cleanup(void)
+{
+    struct rtskb *rtskb;
+
+
+    rtdm_nrtsig_destroy(&vnic_signal);
+
+    while ((rtskb = rtskb_dequeue(&rx_queue)) != NULL) {
+	kfree_rtskb(rtskb);
+    }
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/Kconfig linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/Kconfig
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/Kconfig	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/Kconfig	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,21 @@
+config XENO_DRIVERS_NET_TDMA
+    tristate "TDMA discipline for RTmac"
+    depends on XENO_DRIVERS_NET_RTMAC
+    default y
+    ---help---
+    The Time Division Multiple Access discipline is the default RTmac
+    protocol for Ethernet networks. It consists of a master synchronising
+    the access of the slaves to the media by periodically issuing frames.
+    Backup masters can be set up to take over if the primary master fails.
+    TDMA also provides a global clock across all participants. The tdmacfg
+    tool can be used to configure a real-time NIC to use TDMA.
+
+    See Documenatation/README.rtmac for further details.
+
+config XENO_DRIVERS_NET_TDMA_MASTER
+    bool "TDMA master support"
+    depends on XENO_DRIVERS_NET_TDMA
+    default y
+    ---help---
+    Enables TDMA master and backup master support for the node. This can
+    be switched of to reduce the memory footprint of pure slave nodes.
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/Makefile linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/Makefile
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/Makefile	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/Makefile	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,10 @@
+ccflags-y += -Inet/rtnet/stack/include
+
+obj-$(CONFIG_XENO_DRIVERS_NET_TDMA) += tdma.o
+
+tdma-y := \
+	tdma_dev.o \
+	tdma_ioctl.o \
+	tdma_module.o \
+	tdma_proto.o \
+	tdma_worker.o
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_dev.c linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_dev.c
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_dev.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_dev.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,195 @@
+/***
+ *
+ *  rtmac/tdma/tdma_dev.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>
+ *                2003-2006 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/list.h>
+
+#include <rtdev.h>
+#include <rtmac.h>
+#include <rtmac/tdma/tdma.h>
+
+
+struct tdma_dev_ctx {
+    rtdm_task_t *cycle_waiter;
+};
+
+
+static int tdma_dev_open(struct rtdm_fd *fd, int oflags)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+
+    ctx->cycle_waiter = NULL;
+
+    return 0;
+}
+
+
+static void tdma_dev_close(struct rtdm_fd *fd)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+    rtdm_lockctx_t lock_ctx;
+
+
+    cobalt_atomic_enter(lock_ctx);
+    if (ctx->cycle_waiter)
+	rtdm_task_unblock(ctx->cycle_waiter);
+    cobalt_atomic_leave(lock_ctx);
+}
+
+
+static int wait_on_sync(struct tdma_dev_ctx *tdma_ctx,
+			rtdm_event_t *sync_event)
+{
+    rtdm_lockctx_t lock_ctx;
+    int ret;
+
+
+    cobalt_atomic_enter(lock_ctx);
+    /* keep it simple: only one waiter per device instance allowed */
+    if (!tdma_ctx->cycle_waiter) {
+	tdma_ctx->cycle_waiter = rtdm_task_current();
+	ret = rtdm_event_wait(sync_event);
+	tdma_ctx->cycle_waiter = NULL;
+    } else
+	ret = -EBUSY;
+    cobalt_atomic_leave(lock_ctx);
+
+    return ret;
+}
+
+
+static int tdma_dev_ioctl(struct rtdm_fd *fd, unsigned int request, void *arg)
+{
+    struct tdma_dev_ctx *ctx = rtdm_fd_to_private(fd);
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      lock_ctx;
+    int                 ret;
+
+
+    tdma = container_of(rtdm_fd_to_context(fd)->device,
+			struct tdma_priv, api_device);
+
+    switch (request) {
+    case RTMAC_RTIOC_TIMEOFFSET: {
+	nanosecs_rel_t offset;
+
+	rtdm_lock_get_irqsave(&tdma->lock, lock_ctx);
+	offset = tdma->clock_offset;
+	rtdm_lock_put_irqrestore(&tdma->lock, lock_ctx);
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (!rtdm_rw_user_ok(fd, arg, sizeof(__s64)) ||
+		rtdm_copy_to_user(fd, arg, &offset, sizeof(__s64)))
+		return -EFAULT;
+	} else
+	    *(__s64 *)arg = offset;
+
+	return 0;
+    }
+    case RTMAC_RTIOC_WAITONCYCLE:
+	if (!rtdm_in_rt_context())
+	    return -ENOSYS;
+
+	if ((long)arg !=TDMA_WAIT_ON_SYNC)
+	    return -EINVAL;
+
+	return wait_on_sync(ctx, &tdma->sync_event);
+
+    case RTMAC_RTIOC_WAITONCYCLE_EX: {
+	struct rtmac_waitinfo   *waitinfo = (struct rtmac_waitinfo *)arg;
+	struct rtmac_waitinfo   waitinfo_buf;
+
+#define WAITINFO_HEAD_SIZE                                              \
+	((char *)&waitinfo_buf.cycle_no - (char *)&waitinfo_buf)
+
+	if (!rtdm_in_rt_context())
+	    return -ENOSYS;
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (!rtdm_rw_user_ok(fd, waitinfo,
+				    sizeof(struct rtmac_waitinfo)) ||
+		rtdm_copy_from_user(fd, &waitinfo_buf, arg,
+				    WAITINFO_HEAD_SIZE))
+		return -EFAULT;
+
+	    waitinfo = &waitinfo_buf;
+	}
+
+	if ((waitinfo->type != TDMA_WAIT_ON_SYNC) ||
+	    (waitinfo->size < sizeof(struct rtmac_waitinfo)))
+	    return -EINVAL;
+
+	ret = wait_on_sync(ctx, &tdma->sync_event);
+	if (ret)
+	    return ret;
+
+	rtdm_lock_get_irqsave(&tdma->lock, lock_ctx);
+	waitinfo->cycle_no     = tdma->current_cycle;
+	waitinfo->cycle_start  = tdma->current_cycle_start;
+	waitinfo->clock_offset = tdma->clock_offset;
+	rtdm_lock_put_irqrestore(&tdma->lock, lock_ctx);
+
+	if (rtdm_fd_is_user(fd)) {
+	    if (rtdm_copy_to_user(fd, arg, &waitinfo_buf,
+				    sizeof(struct rtmac_waitinfo)))
+		return -EFAULT;
+	}
+
+	return 0;
+    }
+    default:
+	return -ENOTTY;
+    }
+}
+
+static struct rtdm_driver tdma_driver = {
+    .profile_info = RTDM_PROFILE_INFO(tdma,
+				    RTDM_CLASS_RTMAC,
+				    RTDM_SUBCLASS_TDMA,
+				    RTNET_RTDM_VER),
+    .device_flags = RTDM_NAMED_DEVICE,
+    .device_count = 1,
+    .context_size = sizeof(struct tdma_dev_ctx),
+    .ops = {
+	.open =         tdma_dev_open,
+	.ioctl_rt =     tdma_dev_ioctl,
+	.ioctl_nrt =    tdma_dev_ioctl,
+	.close =        tdma_dev_close,
+    }
+};
+
+int tdma_dev_init(struct rtnet_device *rtdev, struct tdma_priv *tdma)
+{
+    char    *pos;
+
+    strcpy(tdma->device_name, "TDMA");
+    for (pos = rtdev->name + strlen(rtdev->name) - 1;
+	(pos >= rtdev->name) && ((*pos) >= '0') && (*pos <= '9'); pos--);
+    strncat(tdma->device_name+4, pos+1, IFNAMSIZ-4);
+
+    tdma->api_driver            = tdma_driver;
+    tdma->api_device.driver     = &tdma->api_driver;
+    tdma->api_device.label      = tdma->device_name;
+
+    return rtdm_dev_register(&tdma->api_device);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_ioctl.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,683 @@
+/***
+ *
+ *  rtmac/tdma/tdma_ioctl.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+#include <asm/div64.h>
+
+#include <tdma_chrdev.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/tdma/tdma.h>
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+static int tdma_ioctl_master(struct rtnet_device *rtdev,
+                             struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    u64                 cycle_ms;
+    unsigned int        table_size;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &tdma_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC) {
+        /* note: we don't clean up an unknown discipline */
+        return -ENOTTY;
+    }
+
+    if (test_bit(TDMA_FLAG_ATTACHED, &tdma->flags)) {
+        /* already attached */
+        return -EBUSY;
+    }
+
+    set_bit(TDMA_FLAG_MASTER, &tdma->flags);
+
+    tdma->cal_rounds = cfg->args.master.cal_rounds;
+
+    /* search at least 3 cycle periods for other masters */
+    cycle_ms = cfg->args.master.cycle_period;
+    do_div(cycle_ms, 1000000);
+    if (cycle_ms == 0)
+        cycle_ms = 1;
+    msleep(3*cycle_ms);
+
+    if (rtskb_module_pool_init(&tdma->cal_rtskb_pool,
+                                cfg->args.master.max_cal_requests) !=
+        cfg->args.master.max_cal_requests) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+
+    table_size = sizeof(struct tdma_slot *) *
+        ((cfg->args.master.max_slot_id >= 1) ?
+            cfg->args.master.max_slot_id + 1 : 2);
+
+    tdma->slot_table = (struct tdma_slot **)kmalloc(table_size, GFP_KERNEL);
+    if (!tdma->slot_table) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+    tdma->max_slot_id = cfg->args.master.max_slot_id;
+    memset(tdma->slot_table, 0, table_size);
+
+    tdma->cycle_period = cfg->args.master.cycle_period;
+    tdma->sync_job.ref_count = 0;
+    INIT_LIST_HEAD(&tdma->sync_job.entry);
+
+    if (cfg->args.master.backup_sync_offset == 0)
+        tdma->sync_job.id = XMIT_SYNC;
+    else {
+        set_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags);
+        tdma->sync_job.id     = BACKUP_SYNC;
+        tdma->backup_sync_inc =
+                cfg->args.master.backup_sync_offset + tdma->cycle_period;
+    }
+
+    /* did we detect another active master? */
+    if (test_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags)) {
+        /* become a slave, we need to calibrate first */
+        tdma->sync_job.id = WAIT_ON_SYNC;
+    } else {
+        if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+            printk("TDMA: warning, no primary master detected!\n");
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+        tdma->current_cycle_start = rtdm_clock_read();
+    }
+
+    tdma->first_job = tdma->current_job = &tdma->sync_job;
+
+    rtdm_event_signal(&tdma->worker_wakeup);
+
+    set_bit(TDMA_FLAG_ATTACHED, &tdma->flags);
+
+    return 0;
+
+  err_out:
+    rtmac_disc_detach(rtdev);
+    return ret;
+}
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+
+
+
+static int tdma_ioctl_slave(struct rtnet_device *rtdev,
+                            struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    unsigned int        table_size;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL) {
+        ret = rtmac_disc_attach(rtdev, &tdma_disc);
+        if (ret < 0)
+            return ret;
+    }
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC) {
+        /* note: we don't clean up an unknown discipline */
+        return -ENOTTY;
+    }
+
+    if (test_bit(TDMA_FLAG_ATTACHED, &tdma->flags)) {
+        /* already attached */
+        return -EBUSY;
+    }
+
+    tdma->cal_rounds = cfg->args.slave.cal_rounds;
+    if (tdma->cal_rounds == 0)
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+
+    table_size = sizeof(struct tdma_slot *) *
+        ((cfg->args.slave.max_slot_id >= 1) ?
+            cfg->args.slave.max_slot_id + 1 : 2);
+
+    tdma->slot_table = (struct tdma_slot **)kmalloc(table_size, GFP_KERNEL);
+    if (!tdma->slot_table) {
+        ret = -ENOMEM;
+        goto err_out;
+    }
+    tdma->max_slot_id = cfg->args.slave.max_slot_id;
+    memset(tdma->slot_table, 0, table_size);
+
+    tdma->sync_job.id        = WAIT_ON_SYNC;
+    tdma->sync_job.ref_count = 0;
+    INIT_LIST_HEAD(&tdma->sync_job.entry);
+
+    tdma->first_job = tdma->current_job = &tdma->sync_job;
+
+    rtdm_event_signal(&tdma->worker_wakeup);
+
+    set_bit(TDMA_FLAG_ATTACHED, &tdma->flags);
+
+    return 0;
+
+  err_out:
+    rtmac_disc_detach(rtdev);
+    return ret;
+}
+
+
+
+static int tdma_ioctl_cal_result_size(struct rtnet_device *rtdev,
+                                      struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    if (!test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags))
+        return tdma->cal_rounds;
+    else
+        return 0;
+}
+
+
+
+int start_calibration(struct rt_proc_call *call)
+{
+    struct tdma_request_cal *req_cal;
+    struct tdma_priv        *tdma;
+    rtdm_lockctx_t          context;
+
+
+    req_cal = rtpc_get_priv(call, struct tdma_request_cal);
+    tdma    = req_cal->tdma;
+
+    /* there are no slots yet, simply add this job after first_job */
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+    tdma->calibration_call = call;
+    tdma->job_list_revision++;
+    list_add(&req_cal->head.entry, &tdma->first_job->entry);
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return -CALL_PENDING;
+}
+
+
+
+void copyback_calibration(struct rt_proc_call *call, void *priv_data)
+{
+    struct tdma_request_cal *req_cal;
+    struct tdma_priv        *tdma;
+    int                     i;
+    u64                     value;
+    u64                     average = 0;
+    u64                     min = 0x7FFFFFFFFFFFFFFFLL;
+    u64                     max = 0;
+
+
+    req_cal = rtpc_get_priv(call, struct tdma_request_cal);
+    tdma    = req_cal->tdma;
+
+    for (i = 0; i < tdma->cal_rounds; i++) {
+        value = req_cal->result_buffer[i];
+        average += value;
+        if (value < min)
+            min = value;
+        if (value > max)
+            max = value;
+        if ((req_cal->cal_results) &&
+            (copy_to_user(&req_cal->cal_results[i], &value,
+                          sizeof(value)) != 0))
+            rtpc_set_result(call, -EFAULT);
+    }
+    do_div(average, tdma->cal_rounds);
+    tdma->master_packet_delay_ns = average;
+
+    average += 500;
+    do_div(average, 1000);
+    min += 500;
+    do_div(min, 1000);
+    max += 500;
+    do_div(max, 1000);
+    printk("TDMA: calibrated master-to-slave packet delay: "
+           "%ld us (min/max: %ld/%ld us)\n",
+           (unsigned long)average, (unsigned long)min,
+           (unsigned long)max);
+}
+
+
+
+void cleanup_calibration(void *priv_data)
+{
+    struct tdma_request_cal *req_cal;
+
+
+    req_cal = (struct tdma_request_cal *)priv_data;
+    kfree(req_cal->result_buffer);
+}
+
+
+
+static int tdma_ioctl_set_slot(struct rtnet_device *rtdev,
+                               struct tdma_config *cfg)
+{
+    struct tdma_priv        *tdma;
+    int                     id;
+    int                     jnt_id;
+    struct tdma_slot        *slot, *old_slot;
+    struct tdma_job         *job, *prev_job;
+    struct tdma_request_cal req_cal;
+    struct rtskb            *rtskb;
+    unsigned int            job_list_revision;
+    rtdm_lockctx_t          context;
+    int                     ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    id = cfg->args.set_slot.id;
+    if (id > tdma->max_slot_id)
+        return -EINVAL;
+
+    if (cfg->args.set_slot.size == 0)
+        cfg->args.set_slot.size = rtdev->mtu;
+    else if (cfg->args.set_slot.size > rtdev->mtu)
+        return -EINVAL;
+
+    jnt_id = cfg->args.set_slot.joint_slot;
+    if ((jnt_id >= 0) &&
+        ((jnt_id >= tdma->max_slot_id) ||
+         (tdma->slot_table[jnt_id] == 0) ||
+         (tdma->slot_table[jnt_id]->mtu != cfg->args.set_slot.size)))
+        return -EINVAL;
+
+    slot = (struct tdma_slot *)kmalloc(sizeof(struct tdma_slot), GFP_KERNEL);
+    if (!slot)
+        return -ENOMEM;
+
+    if (!test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags)) {
+        req_cal.head.id        = XMIT_REQ_CAL;
+        req_cal.head.ref_count = 0;
+        req_cal.tdma           = tdma;
+        req_cal.offset         = cfg->args.set_slot.offset;
+        req_cal.period         = cfg->args.set_slot.period;
+        req_cal.phasing        = cfg->args.set_slot.phasing;
+        req_cal.cal_rounds     = tdma->cal_rounds;
+        req_cal.cal_results    = cfg->args.set_slot.cal_results;
+
+        req_cal.result_buffer =
+            kmalloc(req_cal.cal_rounds * sizeof(u64), GFP_KERNEL);
+        if (!req_cal.result_buffer) {
+            kfree(slot);
+            return -ENOMEM;
+        }
+
+        ret = rtpc_dispatch_call(start_calibration, 0, &req_cal,
+                                 sizeof(req_cal), copyback_calibration,
+                                 cleanup_calibration);
+        if (ret < 0) {
+            /* kick out any pending calibration job before returning */
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            job = list_entry(tdma->first_job->entry.next, struct tdma_job,
+                             entry);
+            if (job != tdma->first_job) {
+                __list_del(job->entry.prev, job->entry.next);
+
+                while (job->ref_count > 0) {
+                    rtdm_lock_put_irqrestore(&tdma->lock, context);
+                    msleep(100);
+                    rtdm_lock_get_irqsave(&tdma->lock, context);
+                }
+            }
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            kfree(slot);
+            return ret;
+        }
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+        if (test_bit(TDMA_FLAG_MASTER, &tdma->flags)) {
+            u32 cycle_no = (volatile u32)tdma->current_cycle;
+            u64 cycle_ms;
+
+
+            /* switch back to [backup] master mode */
+            if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+                tdma->sync_job.id = BACKUP_SYNC;
+            else
+                tdma->sync_job.id = XMIT_SYNC;
+
+            /* wait 2 cycle periods for the mode switch */
+            cycle_ms = tdma->cycle_period;
+            do_div(cycle_ms, 1000000);
+            if (cycle_ms == 0)
+                cycle_ms = 1;
+            msleep(2*cycle_ms);
+
+            /* catch the very unlikely case that the current master died
+               while we just switched the mode */
+            if (cycle_no == (volatile u32)tdma->current_cycle) {
+                kfree(slot);
+                return -ETIME;
+            }
+        }
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+
+        set_bit(TDMA_FLAG_CALIBRATED, &tdma->flags);
+    }
+
+    slot->head.id        = id;
+    slot->head.ref_count = 0;
+    slot->period         = cfg->args.set_slot.period;
+    slot->phasing        = cfg->args.set_slot.phasing;
+    slot->mtu            = cfg->args.set_slot.size;
+    slot->size           = cfg->args.set_slot.size + rtdev->hard_header_len;
+    slot->offset         = cfg->args.set_slot.offset;
+    slot->queue          = &slot->local_queue;
+    rtskb_prio_queue_init(&slot->local_queue);
+
+    if (jnt_id >= 0)    /* all other validation tests performed above */
+        slot->queue = tdma->slot_table[jnt_id]->queue;
+
+    old_slot = tdma->slot_table[id];
+    if ((id == DEFAULT_NRT_SLOT) &&
+        (old_slot == tdma->slot_table[DEFAULT_SLOT]))
+        old_slot = NULL;
+
+  restart:
+    job_list_revision = tdma->job_list_revision;
+
+    if (!old_slot) {
+        job = tdma->first_job;
+        while (1) {
+            prev_job = job;
+            job = list_entry(job->entry.next, struct tdma_job, entry);
+            if (((job->id >= 0) &&
+                 ((slot->offset < SLOT_JOB(job)->offset) ||
+                  ((slot->offset == SLOT_JOB(job)->offset) &&
+                   (slot->head.id <= SLOT_JOB(job)->head.id)))) ||
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+                ((job->id == XMIT_RPL_CAL) &&
+                  (slot->offset < REPLY_CAL_JOB(job)->reply_offset)) ||
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+                (job == tdma->first_job))
+                break;
+        }
+
+    } else
+        prev_job = list_entry(old_slot->head.entry.prev,
+                              struct tdma_job, entry);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    if (job_list_revision != tdma->job_list_revision) {
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+        msleep(100);
+        goto restart;
+    }
+
+    if (old_slot)
+        __list_del(old_slot->head.entry.prev, old_slot->head.entry.next);
+
+    list_add(&slot->head.entry, &prev_job->entry);
+    tdma->slot_table[id] = slot;
+    if ((id == DEFAULT_SLOT) &&
+        (tdma->slot_table[DEFAULT_NRT_SLOT] == old_slot))
+        tdma->slot_table[DEFAULT_NRT_SLOT] = slot;
+
+    if (old_slot) {
+        while (old_slot->head.ref_count > 0) {
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+            msleep(100);
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+        }
+
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+        /* search for other slots linked to the old one */
+        for (jnt_id = 0; jnt_id < tdma->max_slot_id; jnt_id++)
+            if ((tdma->slot_table[jnt_id] != 0) &&
+                (tdma->slot_table[jnt_id]->queue == &old_slot->local_queue)) {
+                /* found a joint slot, move or detach it now */
+                rtdm_lock_get_irqsave(&tdma->lock, context);
+
+                while (tdma->slot_table[jnt_id]->head.ref_count > 0) {
+                    rtdm_lock_put_irqrestore(&tdma->lock, context);
+                    msleep(100);
+                    rtdm_lock_get_irqsave(&tdma->lock, context);
+                }
+
+                /* If the new slot size is larger, detach the other slot,
+                 * update it otherwise. */
+                if (slot->mtu > tdma->slot_table[jnt_id]->mtu)
+                    tdma->slot_table[jnt_id]->queue =
+                        &tdma->slot_table[jnt_id]->local_queue;
+                else {
+                    tdma->slot_table[jnt_id]->mtu   = slot->mtu;
+                    tdma->slot_table[jnt_id]->queue = slot->queue;
+                }
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+            }
+    } else
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    rtmac_vnic_set_max_mtu(rtdev, cfg->args.set_slot.size);
+
+    if (old_slot) {
+        /* avoid that the formerly joint queue gets purged */
+        old_slot->queue = &old_slot->local_queue;
+
+        /* Without any reference to the old job and no joint slots we can
+         * safely purge its queue without lock protection.
+         * NOTE: Reconfiguring a slot during runtime may lead to packet
+         *       drops! */
+        while ((rtskb = __rtskb_prio_dequeue(old_slot->queue)))
+            kfree_rtskb(rtskb);
+
+        kfree(old_slot);
+    }
+
+    return 0;
+}
+
+
+
+int tdma_cleanup_slot(struct tdma_priv *tdma, struct tdma_slot *slot)
+{
+    struct rtskb        *rtskb;
+    unsigned int        id, jnt_id;
+    rtdm_lockctx_t      context;
+
+
+    if (!slot)
+        return -EINVAL;
+
+    id = slot->head.id;
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    __list_del(slot->head.entry.prev, slot->head.entry.next);
+
+    if (id == DEFAULT_NRT_SLOT)
+        tdma->slot_table[DEFAULT_NRT_SLOT] = tdma->slot_table[DEFAULT_SLOT];
+    else {
+        if ((id == DEFAULT_SLOT) &&
+            (tdma->slot_table[DEFAULT_NRT_SLOT] == slot))
+            tdma->slot_table[DEFAULT_NRT_SLOT] = NULL;
+        tdma->slot_table[id] = NULL;
+    }
+
+    while (slot->head.ref_count > 0) {
+        rtdm_lock_put_irqrestore(&tdma->lock, context);
+        msleep(100);
+        rtdm_lock_get_irqsave(&tdma->lock, context);
+    }
+
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    /* search for other slots linked to this one */
+    for (jnt_id = 0; jnt_id < tdma->max_slot_id; jnt_id++)
+        if ((tdma->slot_table[jnt_id] != 0) &&
+            (tdma->slot_table[jnt_id]->queue == &slot->local_queue)) {
+            /* found a joint slot, detach it now under lock protection */
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            while (tdma->slot_table[jnt_id]->head.ref_count > 0) {
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+                msleep(100);
+                rtdm_lock_get_irqsave(&tdma->lock, context);
+            }
+            tdma->slot_table[jnt_id]->queue =
+                &tdma->slot_table[jnt_id]->local_queue;
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+        }
+
+    /* avoid that the formerly joint queue gets purged */
+    slot->queue = &slot->local_queue;
+
+    /* No need to protect the queue access here -
+     * no one is referring to this job anymore
+     * (ref_count == 0, all joint slots detached). */
+    while ((rtskb = __rtskb_prio_dequeue(slot->queue)))
+        kfree_rtskb(rtskb);
+
+    kfree(slot);
+
+    return 0;
+}
+
+
+
+static int tdma_ioctl_remove_slot(struct rtnet_device *rtdev,
+                                  struct tdma_config *cfg)
+{
+    struct tdma_priv    *tdma;
+    int                 id;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    id = cfg->args.remove_slot.id;
+    if (id > tdma->max_slot_id)
+        return -EINVAL;
+
+    if ((id == DEFAULT_NRT_SLOT) &&
+        (tdma->slot_table[DEFAULT_NRT_SLOT] == tdma->slot_table[DEFAULT_SLOT]))
+        return -EINVAL;
+
+    return tdma_cleanup_slot(tdma, tdma->slot_table[id]);
+}
+
+
+
+static int tdma_ioctl_detach(struct rtnet_device *rtdev)
+{
+    struct tdma_priv    *tdma;
+    int                 ret;
+
+
+    if (rtdev->mac_priv == NULL)
+        return -ENOTTY;
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+    if (tdma->magic != TDMA_MAGIC)
+        return -ENOTTY;
+
+    ret = rtmac_disc_detach(rtdev);
+
+    return ret;
+}
+
+
+
+int tdma_ioctl(struct rtnet_device *rtdev, unsigned int request,
+               unsigned long arg)
+{
+    struct tdma_config  cfg;
+    int                 ret;
+
+
+    ret = copy_from_user(&cfg, (void *)arg, sizeof(cfg));
+    if (ret != 0)
+        return -EFAULT;
+
+    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+        return -ERESTARTSYS;
+
+    switch (request) {
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+        case TDMA_IOC_MASTER:
+            ret = tdma_ioctl_master(rtdev, &cfg);
+            break;
+#endif
+        case TDMA_IOC_SLAVE:
+            ret = tdma_ioctl_slave(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_CAL_RESULT_SIZE:
+            ret = tdma_ioctl_cal_result_size(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_SET_SLOT:
+            ret = tdma_ioctl_set_slot(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_REMOVE_SLOT:
+            ret = tdma_ioctl_remove_slot(rtdev, &cfg);
+            break;
+
+        case TDMA_IOC_DETACH:
+            ret = tdma_ioctl_detach(rtdev);
+            break;
+
+        default:
+            ret = -ENOTTY;
+    }
+
+    mutex_unlock(&rtdev->nrt_lock);
+
+    return ret;
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_module.c linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_module.c
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_module.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_module.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,327 @@
+/***
+ *
+ *  rtmac/tdma/tdma_module.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <asm/div64.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+#include <rtdm/driver.h>
+#include <rtmac/rtmac_vnic.h>
+#include <rtmac/tdma/tdma.h>
+#include <rtmac/tdma/tdma_dev.h>
+#include <rtmac/tdma/tdma_ioctl.h>
+#include <rtmac/tdma/tdma_proto.h>
+#include <rtmac/tdma/tdma_worker.h>
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+int tdma_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    int                 d, err = 0;
+    struct rtnet_device *rtdev;
+    struct tdma_priv    *tdma;
+    const char          *state;
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+    u64                 cycle;
+#endif
+
+    xnvfile_printf(it, "Interface       API Device      Operation Mode  "
+	    "Cycle   State\n");
+
+    for (d = 1; d <= MAX_RT_DEVICES; d++) {
+	rtdev = rtdev_get_by_index(d);
+	if (!rtdev)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    break;
+	}
+
+	if (!rtdev->mac_priv)
+	    goto unlock_dev;
+	tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+	xnvfile_printf(it, "%-15s %-15s ",
+		    rtdev->name, tdma->api_device.name);
+
+	if (test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags)) {
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+	    if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags) &&
+		!test_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags))
+		state = "stand-by";
+	    else
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+		state = "active";
+	} else
+	    state = "init";
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+	if (test_bit(TDMA_FLAG_MASTER, &tdma->flags)) {
+	    cycle = tdma->cycle_period + 500;
+	    do_div(cycle, 1000);
+	    if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags))
+		xnvfile_printf(it, "Backup Master   %-7ld %s\n",
+			    (unsigned long)cycle, state);
+	    else
+		xnvfile_printf(it, "Master          %-7ld %s\n",
+			    (unsigned long)cycle, state);
+	} else
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+	      xnvfile_printf(it, "Slave           -       %s\n", state);
+
+unlock_dev:
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return err;
+}
+
+
+
+int tdma_slots_proc_read(struct xnvfile_regular_iterator *it, void *data)
+{
+    int                 d, i, err = 0;
+    struct rtnet_device *rtdev;
+    struct tdma_priv    *tdma;
+    struct tdma_slot    *slot;
+    int                 jnt_id;
+    u64                 slot_offset;
+
+
+    xnvfile_printf(it, "Interface       "
+		"Slots (id[->joint]:offset:phasing/period:size)\n");
+
+    for (d = 1; d <= MAX_RT_DEVICES; d++) {
+	rtdev = rtdev_get_by_index(d);
+	if (!rtdev)
+	    continue;
+
+	err = mutex_lock_interruptible(&rtdev->nrt_lock);
+	if (err < 0) {
+	    rtdev_dereference(rtdev);
+	    break;
+	}
+
+	if (!rtdev->mac_priv)
+	    goto unlock_dev;
+	tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+	xnvfile_printf(it, "%-15s ", rtdev->name);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+	if (test_bit(TDMA_FLAG_BACKUP_MASTER, &tdma->flags)) {
+	    slot_offset = tdma->backup_sync_inc - tdma->cycle_period + 500;
+	    do_div(slot_offset, 1000);
+	    xnvfile_printf(it, "bak:%ld  ", (unsigned long)slot_offset);
+	}
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+
+	if (tdma->slot_table)
+	    for (i = 0; i <= tdma->max_slot_id; i++) {
+		slot = tdma->slot_table[i];
+		if (!slot ||
+		    ((i == DEFAULT_NRT_SLOT) &&
+		     (tdma->slot_table[DEFAULT_SLOT] == slot)))
+		    continue;
+
+		if (slot->queue == &slot->local_queue) {
+		    xnvfile_printf(it, "%d", i);
+		} else
+		    for (jnt_id = 0; jnt_id <= tdma->max_slot_id; jnt_id++)
+			if (&tdma->slot_table[jnt_id]->local_queue ==
+			    slot->queue) {
+			    xnvfile_printf(it, "%d->%d", i, jnt_id);
+			    break;
+			}
+
+		slot_offset = slot->offset + 500;
+		do_div(slot_offset, 1000);
+		xnvfile_printf(it, ":%ld:%d/%d:%d  ",
+			    (unsigned long)slot_offset, slot->phasing + 1,
+			    slot->period, slot->mtu);
+	    }
+
+	xnvfile_printf(it, "\n");
+
+unlock_dev:
+	mutex_unlock(&rtdev->nrt_lock);
+	rtdev_dereference(rtdev);
+    }
+
+    return err;
+}
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+
+
+int tdma_attach(struct rtnet_device *rtdev, void *priv)
+{
+    struct tdma_priv   *tdma = (struct tdma_priv *)priv;
+    int                 ret;
+
+
+    memset(tdma, 0, sizeof(struct tdma_priv));
+
+    tdma->magic        = TDMA_MAGIC;
+    tdma->rtdev        = rtdev;
+
+    rtdm_lock_init(&tdma->lock);
+
+    rtdm_event_init(&tdma->worker_wakeup, 0);
+    rtdm_event_init(&tdma->xmit_event, 0);
+    rtdm_event_init(&tdma->sync_event, 0);
+
+    ret = tdma_dev_init(rtdev, tdma);
+    if (ret < 0)
+	goto err_out1;
+
+    ret = rtdm_task_init(&tdma->worker_task, "rtnet-tdma", tdma_worker, tdma,
+			 DEF_WORKER_PRIO, 0);
+    if (ret != 0)
+	goto err_out2;
+
+    return 0;
+
+
+  err_out2:
+    tdma_dev_release(tdma);
+
+  err_out1:
+    rtdm_event_destroy(&tdma->sync_event);
+    rtdm_event_destroy(&tdma->xmit_event);
+    rtdm_event_destroy(&tdma->worker_wakeup);
+
+    return ret;
+}
+
+
+
+int tdma_detach(struct rtnet_device *rtdev, void *priv)
+{
+    struct tdma_priv    *tdma = (struct tdma_priv *)priv;
+    struct tdma_job     *job, *tmp;
+
+
+    rtdm_event_destroy(&tdma->sync_event);
+    rtdm_event_destroy(&tdma->xmit_event);
+    rtdm_event_destroy(&tdma->worker_wakeup);
+
+    tdma_dev_release(tdma);
+
+    rtdm_task_destroy(&tdma->worker_task);
+
+    list_for_each_entry_safe(job, tmp, &tdma->first_job->entry, entry) {
+	if (job->id >= 0)
+	    tdma_cleanup_slot(tdma, SLOT_JOB(job));
+	else if (job->id == XMIT_RPL_CAL) {
+	    __list_del(job->entry.prev, job->entry.next);
+	    kfree_rtskb(REPLY_CAL_JOB(job)->reply_rtskb);
+	}
+    }
+
+    if (tdma->slot_table)
+	kfree(tdma->slot_table);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+    if (test_bit(TDMA_FLAG_MASTER, &tdma->flags))
+	rtskb_pool_release(&tdma->cal_rtskb_pool);
+#endif
+
+    return 0;
+}
+
+
+
+#ifdef CONFIG_XENO_OPT_VFILE
+struct rtmac_proc_entry tdma_proc_entries[] = {
+    { name: "tdma", handler: tdma_proc_read },
+    { name: "tdma_slots", handler: tdma_slots_proc_read },
+};
+#endif /* CONFIG_XENO_OPT_VFILE */
+
+struct rtmac_disc tdma_disc = {
+    name:           "TDMA",
+    priv_size:      sizeof(struct tdma_priv),
+    disc_type:      __constant_htons(RTMAC_TYPE_TDMA),
+
+    packet_rx:      tdma_packet_rx,
+    rt_packet_tx:   tdma_rt_packet_tx,
+    nrt_packet_tx:  tdma_nrt_packet_tx,
+
+    get_mtu:        tdma_get_mtu,
+
+    vnic_xmit:      RTMAC_DEFAULT_VNIC,
+
+    attach:         tdma_attach,
+    detach:         tdma_detach,
+
+    ioctls:         {
+	service_name:   "RTmac/TDMA",
+	ioctl_type:     RTNET_IOC_TYPE_RTMAC_TDMA,
+	handler:        tdma_ioctl
+    },
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    proc_entries:   tdma_proc_entries,
+    nr_proc_entries: ARRAY_SIZE(tdma_proc_entries),
+#endif /* CONFIG_XENO_OPT_VFILE */
+};
+
+
+
+int __init tdma_init(void)
+{
+    int ret;
+
+
+    printk("RTmac/TDMA: init time division multiple access control "
+	   "mechanism\n");
+
+    ret = rtmac_disc_register(&tdma_disc);
+    if (ret < 0)
+	return ret;
+
+    return 0;
+}
+
+
+
+void tdma_release(void)
+{
+    rtmac_disc_deregister(&tdma_disc);
+
+    printk("RTmac/TDMA: unloaded\n");
+}
+
+
+
+module_init(tdma_init);
+module_exit(tdma_release);
+
+MODULE_AUTHOR("Jan Kiszka");
+MODULE_LICENSE("GPL");
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_proto.c linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_proto.c
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_proto.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_proto.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,412 @@
+/***
+ *
+ *  rtmac/tdma/tdma_proto.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/init.h>
+#include "asm/div64.h"
+
+#include <rtdev.h>
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/tdma/tdma_proto.h>
+
+
+void tdma_xmit_sync_frame(struct tdma_priv *tdma)
+{
+    struct rtnet_device     *rtdev = tdma->rtdev;
+    struct rtskb            *rtskb;
+    struct tdma_frm_sync    *sync;
+
+
+    rtskb = alloc_rtskb(rtdev->hard_header_len + sizeof(struct rtmac_hdr) +
+                        sizeof(struct tdma_frm_sync) + 15, &global_pool);
+    if (!rtskb)
+        goto err_out;
+
+    rtskb_reserve(rtskb,
+        (rtdev->hard_header_len + sizeof(struct rtmac_hdr) + 15) & ~15);
+
+    sync = (struct tdma_frm_sync *)rtskb_put(rtskb,
+                                             sizeof(struct tdma_frm_sync));
+
+    if (rtmac_add_header(rtdev, rtdev->broadcast,
+                         rtskb, RTMAC_TYPE_TDMA, 0) < 0) {
+        kfree_rtskb(rtskb);
+        goto err_out;
+    }
+
+    sync->head.version = __constant_htons(TDMA_FRM_VERSION);
+    sync->head.id      = __constant_htons(TDMA_FRM_SYNC);
+
+    sync->cycle_no         = htonl(tdma->current_cycle);
+    sync->xmit_stamp       = tdma->clock_offset;
+    sync->sched_xmit_stamp =
+            cpu_to_be64(tdma->clock_offset + tdma->current_cycle_start);
+
+    rtskb->xmit_stamp = &sync->xmit_stamp;
+
+    rtmac_xmit(rtskb);
+
+    /* signal local waiters */
+    rtdm_event_pulse(&tdma->sync_event);
+
+    return;
+
+  err_out:
+    /*ERROR*/rtdm_printk("TDMA: Failed to transmit sync frame!\n");
+    return;
+}
+
+
+
+int tdma_xmit_request_cal_frame(struct tdma_priv *tdma, u32 reply_cycle,
+                                u64 reply_slot_offset)
+{
+    struct rtnet_device     *rtdev = tdma->rtdev;
+    struct rtskb            *rtskb;
+    struct tdma_frm_req_cal *req_cal;
+    int                     ret;
+
+
+    rtskb = alloc_rtskb(rtdev->hard_header_len + sizeof(struct rtmac_hdr) +
+                        sizeof(struct tdma_frm_req_cal) + 15, &global_pool);
+    ret = -ENOMEM;
+    if (!rtskb)
+        goto err_out;
+
+    rtskb_reserve(rtskb,
+        (rtdev->hard_header_len + sizeof(struct rtmac_hdr) + 15) & ~15);
+
+    req_cal = (struct tdma_frm_req_cal *)
+        rtskb_put(rtskb, sizeof(struct tdma_frm_req_cal));
+
+    if ((ret = rtmac_add_header(rtdev, tdma->master_hw_addr,
+                                rtskb, RTMAC_TYPE_TDMA, 0)) < 0) {
+        kfree_rtskb(rtskb);
+        goto err_out;
+    }
+
+    req_cal->head.version = __constant_htons(TDMA_FRM_VERSION);
+    req_cal->head.id      = __constant_htons(TDMA_FRM_REQ_CAL);
+
+    req_cal->xmit_stamp        = 0;
+    req_cal->reply_cycle       = htonl(reply_cycle);
+    req_cal->reply_slot_offset = cpu_to_be64(reply_slot_offset);
+
+    rtskb->xmit_stamp = &req_cal->xmit_stamp;
+
+    ret = rtmac_xmit(rtskb);
+    if (ret < 0)
+        goto err_out;
+
+    return 0;
+
+  err_out:
+    /*ERROR*/rtdm_printk("TDMA: Failed to transmit request calibration "
+                         "frame!\n");
+    return ret;
+}
+
+
+
+int tdma_rt_packet_tx(struct rtskb *rtskb, struct rtnet_device *rtdev)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    int                 ret = 0;
+
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[(rtskb->priority & RTSKB_CHANNEL_MASK) >>
+                            RTSKB_CHANNEL_SHIFT];
+
+    if (unlikely(!slot)) {
+        ret = -EAGAIN;
+        goto err_out;
+    }
+
+    if (unlikely(rtskb->len > slot->size)) {
+        ret = -EMSGSIZE;
+        goto err_out;
+    }
+
+    __rtskb_prio_queue_tail(slot->queue, rtskb);
+
+  err_out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return ret;
+}
+
+
+
+int tdma_nrt_packet_tx(struct rtskb *rtskb)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    int                 ret = 0;
+
+
+    tdma = (struct tdma_priv *)rtskb->rtdev->mac_priv->disc_priv;
+
+    rtcap_mark_rtmac_enqueue(rtskb);
+
+    rtskb->priority = RTSKB_PRIO_VALUE(QUEUE_MIN_PRIO, DEFAULT_NRT_SLOT);
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[DEFAULT_NRT_SLOT];
+
+    if (unlikely(!slot)) {
+        ret = -EAGAIN;
+        goto err_out;
+    }
+
+    if (unlikely(rtskb->len > slot->size)) {
+        ret = -EMSGSIZE;
+        goto err_out;
+    }
+
+    __rtskb_prio_queue_tail(slot->queue, rtskb);
+
+  err_out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return ret;
+}
+
+
+
+int tdma_packet_rx(struct rtskb *rtskb)
+{
+    struct tdma_priv        *tdma;
+    struct tdma_frm_head    *head;
+    u64                     delay;
+    u64                     cycle_start;
+    nanosecs_rel_t          clock_offset;
+    struct rt_proc_call     *call;
+    struct tdma_request_cal *req_cal_job;
+    rtdm_lockctx_t          context;
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+    struct rtskb            *reply_rtskb;
+    struct rtnet_device     *rtdev;
+    struct tdma_frm_rpl_cal *rpl_cal_frm;
+    struct tdma_reply_cal   *rpl_cal_job;
+    struct tdma_job         *job;
+#endif
+
+
+    tdma = (struct tdma_priv *)rtskb->rtdev->mac_priv->disc_priv;
+
+    head = (struct tdma_frm_head *)rtskb->data;
+
+    if (head->version != __constant_htons(TDMA_FRM_VERSION))
+        goto kfree_out;
+
+    switch (head->id) {
+        case __constant_htons(TDMA_FRM_SYNC):
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_sync));
+
+            /* see "Time Arithmetics" in the TDMA specification */
+            clock_offset = be64_to_cpu(SYNC_FRM(head)->xmit_stamp) +
+                    tdma->master_packet_delay_ns;
+            clock_offset -= rtskb->time_stamp;
+
+            cycle_start = be64_to_cpu(SYNC_FRM(head)->sched_xmit_stamp) -
+                    clock_offset;
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+            tdma->current_cycle       = ntohl(SYNC_FRM(head)->cycle_no);
+            tdma->current_cycle_start = cycle_start;
+            tdma->clock_offset        = clock_offset;
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            /* note: Ethernet-specific! */
+            memcpy(tdma->master_hw_addr, rtskb->mac.ethernet->h_source,
+                   ETH_ALEN);
+
+            set_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags);
+
+            rtdm_event_pulse(&tdma->sync_event);
+            break;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+        case __constant_htons(TDMA_FRM_REQ_CAL):
+            RTNET_ASSERT(test_bit(TDMA_FLAG_MASTER, &tdma->flags) &&
+                         test_bit(TDMA_FLAG_CALIBRATED, &tdma->flags),
+                         break;);
+
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_req_cal));
+
+            rtdev = rtskb->rtdev;
+
+            reply_rtskb = alloc_rtskb(rtdev->hard_header_len +
+                                      sizeof(struct rtmac_hdr) +
+                                      sizeof(struct tdma_frm_rpl_cal) + 15,
+                                      &tdma->cal_rtskb_pool);
+            if (unlikely(!reply_rtskb)) {
+                /*ERROR*/rtdm_printk("TDMA: Too many calibration requests "
+                                     "pending!\n");
+                break;
+            }
+
+            rtskb_reserve(reply_rtskb, (rtdev->hard_header_len +
+                          sizeof(struct rtmac_hdr) + 15) & ~15);
+
+            rpl_cal_frm = (struct tdma_frm_rpl_cal *)
+                rtskb_put(reply_rtskb, sizeof(struct tdma_frm_rpl_cal));
+
+            /* note: Ethernet-specific! */
+            if (unlikely(rtmac_add_header(rtdev, rtskb->mac.ethernet->h_source,
+                                          reply_rtskb, RTMAC_TYPE_TDMA,
+                                          0) < 0)) {
+                kfree_rtskb(reply_rtskb);
+                break;
+            }
+
+            rpl_cal_frm->head.version = __constant_htons(TDMA_FRM_VERSION);
+            rpl_cal_frm->head.id      = __constant_htons(TDMA_FRM_RPL_CAL);
+
+            rpl_cal_frm->request_xmit_stamp = REQ_CAL_FRM(head)->xmit_stamp;
+            rpl_cal_frm->reception_stamp    = cpu_to_be64(rtskb->time_stamp);
+            rpl_cal_frm->xmit_stamp         = 0;
+
+            reply_rtskb->xmit_stamp = &rpl_cal_frm->xmit_stamp;
+
+            /* use reply_rtskb memory behind the frame as job buffer */
+            rpl_cal_job = (struct tdma_reply_cal *)reply_rtskb->tail;
+            RTNET_ASSERT(reply_rtskb->tail +
+                sizeof(struct tdma_reply_cal) <= reply_rtskb->buf_end,
+                rtskb_over_panic(reply_rtskb, sizeof(struct tdma_reply_cal),
+                                 current_text_addr()););
+
+            rpl_cal_job->head.id        = XMIT_RPL_CAL;
+            rpl_cal_job->head.ref_count = 0;
+            rpl_cal_job->reply_cycle    =
+                    ntohl(REQ_CAL_FRM(head)->reply_cycle);
+            rpl_cal_job->reply_rtskb    = reply_rtskb;
+            rpl_cal_job->reply_offset   =
+                    be64_to_cpu(REQ_CAL_FRM(head)->reply_slot_offset);
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            job = tdma->current_job;
+            while (1) {
+                job = list_entry(job->entry.prev, struct tdma_job, entry);
+                if ((job == tdma->first_job) ||
+                    ((job->id >= 0) &&
+                     (SLOT_JOB(job)->offset < rpl_cal_job->reply_offset)) ||
+                    ((job->id == XMIT_RPL_CAL) &&
+                     (REPLY_CAL_JOB(job)->reply_offset <
+                            rpl_cal_job->reply_offset)))
+                    break;
+            }
+            list_add(&rpl_cal_job->head.entry, &job->entry);
+            tdma->job_list_revision++;
+
+            rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            break;
+#endif
+
+        case __constant_htons(TDMA_FRM_RPL_CAL):
+            rtskb_pull(rtskb, sizeof(struct tdma_frm_rpl_cal));
+
+            /* see "Time Arithmetics" in the TDMA specification */
+            delay = (rtskb->time_stamp -
+                     be64_to_cpu(RPL_CAL_FRM(head)->request_xmit_stamp)) -
+                    (be64_to_cpu(RPL_CAL_FRM(head)->xmit_stamp) -
+                     be64_to_cpu(RPL_CAL_FRM(head)->reception_stamp));
+            delay = (delay + 1) >> 1;
+
+            rtdm_lock_get_irqsave(&tdma->lock, context);
+
+            call = tdma->calibration_call;
+            if (call == NULL) {
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+                break;
+            }
+            req_cal_job = rtpc_get_priv(call, struct tdma_request_cal);
+
+            req_cal_job->result_buffer[--req_cal_job->cal_rounds] = delay;
+
+            if (req_cal_job->cal_rounds > 0) {
+                tdma->job_list_revision++;
+                list_add(&req_cal_job->head.entry, &tdma->first_job->entry);
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+            } else {
+                tdma->calibration_call = NULL;
+
+                rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+                rtpc_complete_call(call, 0);
+            }
+
+            break;
+
+        default:
+            /*ERROR*/rtdm_printk("TDMA: Unknown frame %d!\n", ntohs(head->id));
+    }
+
+  kfree_out:
+    kfree_rtskb(rtskb);
+    return 0;
+}
+
+
+
+unsigned int tdma_get_mtu(struct rtnet_device *rtdev, unsigned int priority)
+{
+    struct tdma_priv    *tdma;
+    rtdm_lockctx_t      context;
+    struct tdma_slot    *slot;
+    unsigned int        mtu;
+
+
+    tdma = (struct tdma_priv *)rtdev->mac_priv->disc_priv;
+
+    rtdm_lock_get_irqsave(&tdma->lock, context);
+
+    slot = tdma->slot_table[(priority & RTSKB_CHANNEL_MASK) >>
+                            RTSKB_CHANNEL_SHIFT];
+
+    if (unlikely(!slot)) {
+        mtu = rtdev->mtu;
+        goto out;
+    }
+
+    mtu = slot->mtu;
+
+  out:
+    rtdm_lock_put_irqrestore(&tdma->lock, context);
+
+    return mtu;
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_worker.c linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_worker.c
--- linux-5.4.5/net/rtnet/stack/rtmac/tdma/tdma_worker.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtmac/tdma/tdma_worker.c	2020-06-15 16:12:31.519695406 +0300
@@ -0,0 +1,229 @@
+/***
+ *
+ *  rtmac/tdma/tdma_worker.c
+ *
+ *  RTmac - real-time networking media access control subsystem
+ *  Copyright (C) 2002      Marc Kleine-Budde <kleine-budde@gmx.de>,
+ *                2003-2005 Jan Kiszka <Jan.Kiszka@web.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <rtmac/rtmac_proto.h>
+#include <rtmac/tdma/tdma_proto.h>
+
+
+static void do_slot_job(struct tdma_priv *tdma, struct tdma_slot *job,
+                        rtdm_lockctx_t lockctx)
+{
+    struct rtskb *rtskb;
+
+    if ((job->period != 1) &&
+        (tdma->current_cycle % job->period != job->phasing))
+        return;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for slot begin, then send one pending packet */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + SLOT_JOB(job)->offset,
+                        RTDM_TIMERMODE_REALTIME);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+    rtskb = __rtskb_prio_dequeue(SLOT_JOB(job)->queue);
+    if (!rtskb)
+        return;
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    rtmac_xmit(rtskb);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static void do_xmit_sync_job(struct tdma_priv *tdma, rtdm_lockctx_t lockctx)
+{
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for beginning of next cycle, then send sync */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + tdma->cycle_period,
+                        RTDM_TIMERMODE_REALTIME);
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+    tdma->current_cycle++;
+    tdma->current_cycle_start += tdma->cycle_period;
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    tdma_xmit_sync_frame(tdma);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static void do_backup_sync_job(struct tdma_priv *tdma, rtdm_lockctx_t lockctx)
+{
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    /* wait for backup slot */
+    rtdm_task_sleep_abs(tdma->current_cycle_start + tdma->backup_sync_inc,
+                        RTDM_TIMERMODE_REALTIME);
+
+    /* take over sync transmission if all earlier masters failed */
+    if (!test_and_clear_bit(TDMA_FLAG_RECEIVED_SYNC, &tdma->flags)) {
+        rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+        tdma->current_cycle++;
+        tdma->current_cycle_start += tdma->cycle_period;
+        rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+        tdma_xmit_sync_frame(tdma);
+
+        set_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags);
+    } else
+        clear_bit(TDMA_FLAG_BACKUP_ACTIVE, &tdma->flags);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+}
+
+static struct tdma_job *do_request_cal_job(struct tdma_priv *tdma,
+                                           struct tdma_request_cal *job,
+                                           rtdm_lockctx_t lockctx)
+{
+    struct rt_proc_call *call;
+    struct tdma_job     *prev_job;
+    int                 err;
+
+    if ((job->period != 1) &&
+        (tdma->current_cycle % job->period != job->phasing))
+        return &job->head;
+
+    /* remove job until we get a reply */
+    __list_del(job->head.entry.prev, job->head.entry.next);
+    job->head.ref_count--;
+    prev_job = tdma->current_job =
+        list_entry(job->head.entry.prev, struct tdma_job, entry);
+    prev_job->ref_count++;
+    tdma->job_list_revision++;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    rtdm_task_sleep_abs(tdma->current_cycle_start + job->offset,
+                        RTDM_TIMERMODE_REALTIME);
+    err = tdma_xmit_request_cal_frame(tdma,
+            tdma->current_cycle + job->period, job->offset);
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    /* terminate call on error */
+    if (err < 0) {
+        call = tdma->calibration_call;
+        tdma->calibration_call = NULL;
+
+        if (call) {
+            rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+            rtpc_complete_call(call, err);
+            rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+        }
+    }
+
+    return prev_job;
+}
+
+static struct tdma_job *do_reply_cal_job(struct tdma_priv *tdma,
+                                         struct tdma_reply_cal *job,
+                                         rtdm_lockctx_t lockctx)
+{
+    struct tdma_job *prev_job;
+
+    if (job->reply_cycle > tdma->current_cycle)
+        return &job->head;
+
+    /* remove the job */
+    __list_del(job->head.entry.prev, job->head.entry.next);
+    job->head.ref_count--;
+    prev_job = tdma->current_job =
+        list_entry(job->head.entry.prev, struct tdma_job, entry);
+    prev_job->ref_count++;
+    tdma->job_list_revision++;
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+
+    if (job->reply_cycle == tdma->current_cycle) {
+        /* send reply in the assigned slot */
+        rtdm_task_sleep_abs(tdma->current_cycle_start + job->reply_offset,
+                            RTDM_TIMERMODE_REALTIME);
+        rtmac_xmit(job->reply_rtskb);
+    } else {
+        /* cleanup if cycle already passed */
+        kfree_rtskb(job->reply_rtskb);
+    }
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    return prev_job;
+}
+
+void tdma_worker(void *arg)
+{
+    struct tdma_priv    *tdma = arg;
+    struct tdma_job     *job;
+    rtdm_lockctx_t      lockctx;
+    int ret;
+
+    ret = rtdm_event_wait(&tdma->worker_wakeup);
+    if (ret)
+	    return;
+
+    rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+
+    job = tdma->first_job;
+
+    while (!rtdm_task_should_stop()) {
+        job->ref_count++;
+        switch (job->id) {
+            case WAIT_ON_SYNC:
+                rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+                ret = rtdm_event_wait(&tdma->sync_event);
+		if (ret)
+			return;
+                rtdm_lock_get_irqsave(&tdma->lock, lockctx);
+                break;
+
+            case XMIT_REQ_CAL:
+                job = do_request_cal_job(tdma, REQUEST_CAL_JOB(job), lockctx);
+                break;
+
+#ifdef CONFIG_XENO_DRIVERS_NET_TDMA_MASTER
+            case XMIT_SYNC:
+                do_xmit_sync_job(tdma, lockctx);
+                break;
+
+            case BACKUP_SYNC:
+                do_backup_sync_job(tdma, lockctx);
+                break;
+
+            case XMIT_RPL_CAL:
+                job = do_reply_cal_job(tdma, REPLY_CAL_JOB(job), lockctx);
+                break;
+#endif /* CONFIG_XENO_DRIVERS_NET_TDMA_MASTER */
+
+            default:
+                do_slot_job(tdma, SLOT_JOB(job), lockctx);
+                break;
+        }
+        job->ref_count--;
+
+        job = tdma->current_job =
+            list_entry(job->entry.next, struct tdma_job, entry);
+    }
+
+    rtdm_lock_put_irqrestore(&tdma->lock, lockctx);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtnet_chrdev.c linux-5.4.5-new/net/rtnet/stack/rtnet_chrdev.c
--- linux-5.4.5/net/rtnet/stack/rtnet_chrdev.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtnet_chrdev.c	2020-06-15 16:12:31.511695434 +0300
@@ -0,0 +1,341 @@
+/***
+ *
+ *  stack/rtnet_chrdev.c - implements char device for management interface
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@fet.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/if_arp.h>
+#include <linux/kmod.h>
+#include <linux/miscdevice.h>
+#include <linux/netdevice.h>
+#include <linux/spinlock.h>
+
+#include <rtnet_chrdev.h>
+#include <rtnet_internal.h>
+#include <ipv4/route.h>
+
+
+static DEFINE_SPINLOCK(ioctl_handler_lock);
+static LIST_HEAD(ioctl_handlers);
+
+static long rtnet_ioctl(struct file *file,
+			unsigned int request, unsigned long arg)
+{
+    struct rtnet_ioctl_head head;
+    struct rtnet_device     *rtdev = NULL;
+    struct rtnet_ioctls     *ioctls;
+    struct list_head        *entry;
+    int                     ret;
+
+
+    if (!capable(CAP_SYS_ADMIN))
+	return -EPERM;
+
+    ret = copy_from_user(&head, (void *)arg, sizeof(head));
+    if (ret != 0)
+	return -EFAULT;
+
+    spin_lock(&ioctl_handler_lock);
+
+    list_for_each(entry, &ioctl_handlers) {
+	ioctls = list_entry(entry, struct rtnet_ioctls, entry);
+
+	if (ioctls->ioctl_type == _IOC_TYPE(request)) {
+	    atomic_inc(&ioctls->ref_count);
+
+	    spin_unlock(&ioctl_handler_lock);
+
+	    if ((_IOC_NR(request) & RTNET_IOC_NODEV_PARAM) == 0) {
+		rtdev = rtdev_get_by_name(head.if_name);
+		if (!rtdev) {
+		    atomic_dec(&ioctls->ref_count);
+		    return -ENODEV;
+		}
+	    }
+
+	    ret = ioctls->handler(rtdev, request, arg);
+
+	    if (rtdev)
+		rtdev_dereference(rtdev);
+	    atomic_dec(&ioctls->ref_count);
+
+	    return ret;
+	}
+    }
+
+    spin_unlock(&ioctl_handler_lock);
+
+    return -ENOTTY;
+}
+
+
+
+static int rtnet_core_ioctl(struct rtnet_device *rtdev, unsigned int request,
+			    unsigned long arg)
+{
+    struct rtnet_core_cmd   cmd;
+    struct list_head        *entry;
+    struct rtdev_event_hook *hook;
+    int                     ret;
+    unsigned long          context;
+
+
+    ret = copy_from_user(&cmd, (void *)arg, sizeof(cmd));
+    if (ret != 0)
+	return -EFAULT;
+
+    switch (request) {
+	case IOC_RT_IFUP:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    /* We cannot change the promisc flag or the hardware address if
+	       the device is already up. */
+	    if ((rtdev->flags & IFF_UP) &&
+		(((cmd.args.up.set_dev_flags | cmd.args.up.clear_dev_flags) &
+		  IFF_PROMISC) ||
+		 (cmd.args.up.dev_addr_type != ARPHRD_VOID))) {
+		ret = -EBUSY;
+		goto up_out;
+	    }
+
+	    rtdev->flags |= cmd.args.up.set_dev_flags;
+	    rtdev->flags &= ~cmd.args.up.clear_dev_flags;
+
+	    if (cmd.args.up.dev_addr_type != ARPHRD_VOID) {
+		if (cmd.args.up.dev_addr_type != rtdev->type) {
+		    ret = -EINVAL;
+		    goto up_out;
+		}
+		memcpy(rtdev->dev_addr, cmd.args.up.dev_addr, MAX_ADDR_LEN);
+	    }
+
+	    set_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	    ret = rtdev_open(rtdev);    /* also == 0 if rtdev is already up */
+
+	    if (ret == 0) {
+		mutex_lock(&rtnet_devices_nrt_lock);
+
+		list_for_each(entry, &event_hook_list) {
+		    hook = list_entry(entry, struct rtdev_event_hook, entry);
+		    if (hook->ifup)
+			hook->ifup(rtdev, &cmd);
+		}
+
+		mutex_unlock(&rtnet_devices_nrt_lock);
+	    } else
+		clear_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	  up_out:
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_IFDOWN:
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock))
+		return -ERESTARTSYS;
+
+	    /* spin lock required for sync with routing code */
+	    raw_spin_lock_irqsave(&rtdev->rtdev_lock, context);
+
+	    if (test_bit(PRIV_FLAG_ADDING_ROUTE, &rtdev->priv_flags)) {
+		raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+		mutex_unlock(&rtdev->nrt_lock);
+		return -EBUSY;
+	    }
+	    clear_bit(PRIV_FLAG_UP, &rtdev->priv_flags);
+
+	    raw_spin_unlock_irqrestore(&rtdev->rtdev_lock, context);
+
+	    ret = 0;
+	    if (rtdev->mac_detach != NULL)
+		ret = rtdev->mac_detach(rtdev);
+
+	    if (ret == 0) {
+		mutex_lock(&rtnet_devices_nrt_lock);
+
+		list_for_each(entry, &event_hook_list) {
+		    hook = list_entry(entry, struct rtdev_event_hook, entry);
+		    if (hook->ifdown)
+			hook->ifdown(rtdev);
+		}
+
+		mutex_unlock(&rtnet_devices_nrt_lock);
+
+		ret = rtdev_close(rtdev);
+	    }
+
+	    mutex_unlock(&rtdev->nrt_lock);
+	    break;
+
+	case IOC_RT_IFINFO:
+	    if (cmd.args.info.ifindex > 0)
+		rtdev = rtdev_get_by_index(cmd.args.info.ifindex);
+	    else
+		rtdev = rtdev_get_by_name(cmd.head.if_name);
+	    if (rtdev == NULL)
+		return -ENODEV;
+
+	    if (mutex_lock_interruptible(&rtdev->nrt_lock)) {
+		rtdev_dereference(rtdev);
+		return -ERESTARTSYS;
+	    }
+
+	    memcpy(cmd.head.if_name, rtdev->name, IFNAMSIZ);
+	    cmd.args.info.ifindex      = rtdev->ifindex;
+	    cmd.args.info.type         = rtdev->type;
+	    cmd.args.info.ip_addr      = rtdev->local_ip;
+	    cmd.args.info.broadcast_ip = rtdev->broadcast_ip;
+	    cmd.args.info.mtu          = rtdev->mtu;
+	    cmd.args.info.flags        = rtdev->flags;
+            if ((cmd.args.info.flags & IFF_UP)
+		    && (rtdev->link_state
+			    & (RTNET_LINK_STATE_PRESENT
+				    | RTNET_LINK_STATE_NOCARRIER))
+                    == RTNET_LINK_STATE_PRESENT)
+                    cmd.args.info.flags |= IFF_RUNNING;
+
+	    memcpy(cmd.args.info.dev_addr, rtdev->dev_addr, MAX_ADDR_LEN);
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    rtdev_dereference(rtdev);
+
+	    if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+		return -EFAULT;
+	    break;
+
+	default:
+	    ret = -ENOTTY;
+    }
+
+    return ret;
+}
+
+
+
+int rtnet_register_ioctls(struct rtnet_ioctls *ioctls)
+{
+    struct list_head    *entry;
+    struct rtnet_ioctls *registered_ioctls;
+
+
+    RTNET_ASSERT(ioctls->handler != NULL, return -EINVAL;);
+
+    spin_lock(&ioctl_handler_lock);
+
+    list_for_each(entry, &ioctl_handlers) {
+	registered_ioctls = list_entry(entry, struct rtnet_ioctls, entry);
+	if (registered_ioctls->ioctl_type == ioctls->ioctl_type) {
+	    spin_unlock(&ioctl_handler_lock);
+	    return -EEXIST;
+	}
+    }
+
+    list_add_tail(&ioctls->entry, &ioctl_handlers);
+    atomic_set(&ioctls->ref_count, 0);
+
+    spin_unlock(&ioctl_handler_lock);
+
+    return 0;
+}
+
+
+
+void rtnet_unregister_ioctls(struct rtnet_ioctls *ioctls)
+{
+    spin_lock(&ioctl_handler_lock);
+
+    while (atomic_read(&ioctls->ref_count) != 0) {
+	spin_unlock(&ioctl_handler_lock);
+
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	schedule_timeout(1*HZ); /* wait a second */
+
+	spin_lock(&ioctl_handler_lock);
+    }
+
+    list_del(&ioctls->entry);
+
+    spin_unlock(&ioctl_handler_lock);
+}
+
+
+
+static struct file_operations rtnet_fops = {
+    .owner = THIS_MODULE,
+    .unlocked_ioctl = rtnet_ioctl,
+};
+
+static struct miscdevice rtnet_chr_misc_dev = {
+    .minor= RTNET_MINOR,
+    .name = "rtnet",
+    .fops = &rtnet_fops,
+};
+
+static struct rtnet_ioctls core_ioctls = {
+    .service_name = "RTnet Core",
+    .ioctl_type =   RTNET_IOC_TYPE_CORE,
+    .handler =      rtnet_core_ioctl
+};
+
+
+
+/**
+ * rtnet_chrdev_init -
+ *
+ */
+int __init rtnet_chrdev_init(void)
+{
+    int err;
+
+    err = misc_register(&rtnet_chr_misc_dev);
+    if (err) {
+	printk("RTnet: unable to register rtnet management device/class "
+	       "(error %d)\n", err);
+	return err;
+    }
+
+    rtnet_register_ioctls(&core_ioctls);
+    return 0;
+}
+
+
+
+/**
+ * rtnet_chrdev_release -
+ *
+ */
+void rtnet_chrdev_release(void)
+{
+    misc_deregister(&rtnet_chr_misc_dev);
+}
+
+
+EXPORT_SYMBOL_GPL(rtnet_register_ioctls);
+EXPORT_SYMBOL_GPL(rtnet_unregister_ioctls);
diff -Nur linux-5.4.5/net/rtnet/stack/rtnet_module.c linux-5.4.5-new/net/rtnet/stack/rtnet_module.c
--- linux-5.4.5/net/rtnet/stack/rtnet_module.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtnet_module.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,402 @@
+/***
+ *
+ *  stack/rtnet_module.c - module framework, proc file system
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include <rtdev_mgr.h>
+#include <rtnet_chrdev.h>
+#include <rtnet_internal.h>
+#include <rtnet_socket.h>
+#include <rtnet_rtpc.h>
+#include <stack_mgr.h>
+#include <rtwlan.h>
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("RTnet stack core");
+
+
+struct rtnet_mgr STACK_manager;
+struct rtnet_mgr RTDEV_manager;
+
+EXPORT_SYMBOL_GPL(STACK_manager);
+EXPORT_SYMBOL_GPL(RTDEV_manager);
+
+const char rtnet_rtdm_provider_name[] =
+    "(C) 1999-2008 RTnet Development Team, http://www.rtnet.org";
+
+EXPORT_SYMBOL_GPL(rtnet_rtdm_provider_name);
+
+#ifdef CONFIG_XENO_OPT_VFILE
+/***
+ *      proc filesystem section
+ */
+struct xnvfile_directory rtnet_proc_root;
+EXPORT_SYMBOL_GPL(rtnet_proc_root);
+
+
+static int rtnet_devices_nrt_lock_get(struct xnvfile *vfile)
+{
+	return mutex_lock_interruptible(&rtnet_devices_nrt_lock);
+}
+
+static void rtnet_devices_nrt_lock_put(struct xnvfile *vfile)
+{
+	mutex_unlock(&rtnet_devices_nrt_lock);
+}
+
+static struct xnvfile_lock_ops rtnet_devices_nrt_lock_ops = {
+	.get = rtnet_devices_nrt_lock_get,
+	.put = rtnet_devices_nrt_lock_put,
+};
+
+static void *rtnet_devices_begin(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos == 0)
+		return VFILE_SEQ_START;
+
+	return (void *)2UL;
+}
+
+static void *rtnet_devices_next(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos >= MAX_RT_DEVICES)
+		return NULL;
+
+	return (void *)2UL;
+}
+
+static int rtnet_devices_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct rtnet_device *rtdev;
+
+	if (data == NULL) {
+	    xnvfile_printf(it, "Index\tName\t\tFlags\n");
+		return 0;
+	}
+
+	rtdev = __rtdev_get_by_index(it->pos);
+	if (rtdev == NULL)
+		return VFILE_SEQ_SKIP;
+
+	xnvfile_printf(it, "%d\t%-15s %s%s%s%s\n",
+				rtdev->ifindex, rtdev->name,
+				(rtdev->flags & IFF_UP) ? "UP" : "DOWN",
+				(rtdev->flags & IFF_BROADCAST) ? " BROADCAST" : "",
+				(rtdev->flags & IFF_LOOPBACK) ? " LOOPBACK" : "",
+				(rtdev->flags & IFF_PROMISC) ? " PROMISC" : "");
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_devices_vfile_ops = {
+	.begin = rtnet_devices_begin,
+	.next = rtnet_devices_next,
+	.show = rtnet_devices_show,
+};
+
+static struct xnvfile_regular rtnet_devices_vfile = {
+	.entry = { .lockops = &rtnet_devices_nrt_lock_ops, },
+	.ops = &rtnet_devices_vfile_ops,
+};
+
+static int rtnet_rtskb_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    unsigned int rtskb_len;
+
+    rtskb_len = ALIGN_RTSKB_STRUCT_LEN + SKB_DATA_ALIGN(RTSKB_SIZE);
+
+    xnvfile_printf(it, "Statistics\t\tCurrent\tMaximum\n"
+		     "rtskb pools\t\t%d\t%d\n"
+		     "rtskbs\t\t\t%d\t%d\n"
+		     "rtskb memory need\t%d\t%d\n",
+		     rtskb_pools, rtskb_pools_max,
+		     rtskb_amount, rtskb_amount_max,
+		     rtskb_amount * rtskb_len, rtskb_amount_max * rtskb_len);
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_rtskb_vfile_ops = {
+	.show = rtnet_rtskb_show,
+};
+
+static struct xnvfile_regular rtnet_rtskb_vfile = {
+	.ops = &rtnet_rtskb_vfile_ops,
+};
+
+static int rtnet_version_show(struct xnvfile_regular_iterator *it, void *data)
+{
+    const char verstr[] =
+	    "RTnet for Xenomai v" XENO_VERSION_STRING "\n"
+		"RTcap:      "
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		"rtnetproxy: "
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_PROXY)
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		"bug checks: "
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+	    "yes\n"
+#else
+	    "no\n"
+#endif
+		;
+
+	xnvfile_printf(it, "%s", verstr);
+
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_version_vfile_ops = {
+	.show = rtnet_version_show,
+};
+
+static struct xnvfile_regular rtnet_version_vfile = {
+	.ops = &rtnet_version_vfile_ops,
+};
+
+static void *rtnet_stats_begin(struct xnvfile_regular_iterator *it)
+{
+	return (void *)1UL;
+}
+
+static void *rtnet_stats_next(struct xnvfile_regular_iterator *it)
+{
+	if (it->pos >= MAX_RT_DEVICES)
+		return NULL;
+
+	return (void *)1UL;
+}
+
+static int rtnet_stats_show(struct xnvfile_regular_iterator *it, void *data)
+{
+	struct net_device_stats *stats;
+	struct rtnet_device *rtdev;
+
+	if (it->pos == 0) {
+		xnvfile_printf(it, "Inter-|   Receive                            "
+					"                    |  Transmit\n");
+		xnvfile_printf(it, " face |bytes    packets errs drop fifo frame "
+					"compressed multicast|bytes    packets errs "
+					"drop fifo colls carrier compressed\n");
+		return 0;
+	}
+
+	rtdev = __rtdev_get_by_index(it->pos);
+	if (rtdev == NULL)
+		return VFILE_SEQ_SKIP;
+
+	if (rtdev->get_stats == NULL) {
+		xnvfile_printf(it, "%6s: No statistics available.\n", rtdev->name);
+		return 0;
+	}
+
+	stats = rtdev->get_stats(rtdev);
+	xnvfile_printf(it,
+				"%6s:%8lu %7lu %4lu %4lu %4lu %5lu %10lu %9lu "
+				"%8lu %7lu %4lu %4lu %4lu %5lu %7lu %10lu\n",
+				rtdev->name, stats->rx_bytes, stats->rx_packets,
+				stats->rx_errors,
+				stats->rx_dropped + stats->rx_missed_errors,
+				stats->rx_fifo_errors,
+				stats->rx_length_errors + stats->rx_over_errors +
+				stats->rx_crc_errors + stats->rx_frame_errors,
+				stats->rx_compressed, stats->multicast,
+				stats->tx_bytes, stats->tx_packets,
+				stats->tx_errors, stats->tx_dropped,
+				stats->tx_fifo_errors, stats->collisions,
+				stats->tx_carrier_errors +
+				stats->tx_aborted_errors +
+				stats->tx_window_errors +
+				stats->tx_heartbeat_errors,
+				stats->tx_compressed);
+	return 0;
+}
+
+static struct xnvfile_regular_ops rtnet_stats_vfile_ops = {
+	.begin = rtnet_stats_begin,
+	.next = rtnet_stats_next,
+	.show = rtnet_stats_show,
+};
+
+static struct xnvfile_regular rtnet_stats_vfile = {
+	.entry = { .lockops = &rtnet_devices_nrt_lock_ops, },
+	.ops = &rtnet_stats_vfile_ops,
+};
+
+static int rtnet_proc_register(void)
+{
+	int err;
+
+	err = xnvfile_init_dir("rtnet", &rtnet_proc_root, NULL);
+	if (err < 0)
+		goto error1;
+
+	err = xnvfile_init_regular("devices", &rtnet_devices_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error2;
+
+	err = xnvfile_init_regular("rtskb", &rtnet_rtskb_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error3;
+
+	err = xnvfile_init_regular("version", &rtnet_version_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error4;
+
+	err = xnvfile_init_regular("stats", &rtnet_stats_vfile, &rtnet_proc_root);
+	if (err < 0)
+		goto error5;
+
+    return 0;
+
+  error5:
+	xnvfile_destroy_regular(&rtnet_version_vfile);
+
+  error4:
+	xnvfile_destroy_regular(&rtnet_rtskb_vfile);
+
+  error3:
+	xnvfile_destroy_regular(&rtnet_devices_vfile);
+
+  error2:
+	xnvfile_destroy_dir(&rtnet_proc_root);
+
+  error1:
+    printk("RTnet: unable to initialize /proc entries\n");
+    return err;
+}
+
+
+
+static void rtnet_proc_unregister(void)
+{
+	xnvfile_destroy_regular(&rtnet_stats_vfile);
+	xnvfile_destroy_regular(&rtnet_version_vfile);
+	xnvfile_destroy_regular(&rtnet_rtskb_vfile);
+	xnvfile_destroy_regular(&rtnet_devices_vfile);
+	xnvfile_destroy_dir(&rtnet_proc_root);
+}
+#endif  /* CONFIG_XENO_OPT_VFILE */
+
+
+
+/**
+ *  rtnet_init()
+ */
+int __init rtnet_init(void)
+{
+    int err = 0;
+
+
+    printk("\n*** RTnet for PREEMPT_RT ***\n\n");
+    printk("RTnet: initialising real-time networking\n");
+
+    if ((err = rtskb_pools_init()) != 0)
+	goto err_out1;
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    if ((err = rtnet_proc_register()) != 0)
+	goto err_out2;
+#endif
+
+    /* initialize the Stack-Manager */
+    if ((err = rt_stack_mgr_init(&STACK_manager)) != 0)
+	goto err_out3;
+
+    /* initialize the RTDEV-Manager */
+    if ((err = rt_rtdev_mgr_init(&RTDEV_manager)) != 0)
+	goto err_out4;
+
+    rtnet_chrdev_init();
+
+    if ((err = rtwlan_init()) != 0)
+	goto err_out5;
+
+    if ((err = rtpc_init()) != 0)
+	goto err_out6;
+
+    return 0;
+
+
+err_out6:
+    rtwlan_exit();
+
+err_out5:
+    rtnet_chrdev_release();
+    rt_rtdev_mgr_delete(&RTDEV_manager);
+
+err_out4:
+    rt_stack_mgr_delete(&STACK_manager);
+
+err_out3:
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtnet_proc_unregister();
+
+err_out2:
+#endif
+    rtskb_pools_release();
+
+err_out1:
+    return err;
+}
+
+
+/**
+ *  rtnet_release()
+ */
+void __exit rtnet_release(void)
+{
+    rtpc_cleanup();
+
+    rtwlan_exit();
+
+    rtnet_chrdev_release();
+
+    rt_stack_mgr_delete(&STACK_manager);
+    rt_rtdev_mgr_delete(&RTDEV_manager);
+
+    rtskb_pools_release();
+
+#ifdef CONFIG_XENO_OPT_VFILE
+    rtnet_proc_unregister();
+#endif
+
+    printk("RTnet: unloaded\n");
+}
+
+
+module_init(rtnet_init);
+module_exit(rtnet_release);
diff -Nur linux-5.4.5/net/rtnet/stack/rtnet_rtdm.c linux-5.4.5-new/net/rtnet/stack/rtnet_rtdm.c
--- linux-5.4.5/net/rtnet/stack/rtnet_rtdm.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtnet_rtdm.c	2020-06-15 16:12:31.583695180 +0300
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * RTNet RTDM functions
+ *
+ * Copyright (C) 2020, Laurentiu-Cristian Duca
+ *		laurentiu [dot] duca [at] gmail [dot] com
+ */
+
+#include <linux/sched.h>
+#include <linux/swait.h>
+#include <rtnet_rtdm.h>
+
+void __rtdm_event_init(rtdm_event_t *event)
+{
+	event->condition = 0;
+	init_swait_queue_head(&event->head_swait);
+}
+
+void rtdm_event_signal(rtdm_event_t *event)
+{
+	/* use swait.h model to signal condition true */
+	event->condition = 1;
+	smp_mb();
+ 	if (swait_active(&event->head_swait))
+		swake_up_one(&event->head_swait);
+}
+EXPORT_SYMBOL_GPL(rtdm_event_signal);
+
+int rtdm_event_wait(rtdm_event_t *event)
+{
+	DECLARE_SWAITQUEUE(swait);
+
+	/* wait for condition using swait.h model */
+	for (;;) {
+		prepare_to_swait_exclusive(&event->head_swait, &swait, TASK_INTERRUPTIBLE);
+ 		/* smp_mb() from set_current_state() */
+ 		if (event->condition)
+ 			break;
+ 		schedule();
+ 	}
+	finish_swait(&event->head_swait, &swait);
+	event->condition = 0;
+	smp_mb();
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rtdm_event_wait);
+
+int rtdm_get_iovec(int fd, struct iovec **iovp,
+                   const struct user_msghdr *msg,
+                   struct iovec *iov_fast, int msg_in_userspace)
+{
+        size_t len = sizeof(struct iovec) * msg->msg_iovlen;
+        struct iovec *iov = iov_fast;
+
+        /*
+         * If the I/O vector doesn't fit in the fast memory, allocate
+         * a chunk from the system heap which is large enough to hold
+         * it.
+         */
+        if (msg->msg_iovlen > RTDM_IOV_FASTMAX) {
+                iov = kzalloc(len, GFP_ATOMIC);
+                if (iov == NULL)
+                        return -ENOMEM;
+        }
+
+        *iovp = iov;
+
+        if (!msg_in_userspace) {
+                memcpy(iov, msg->msg_iov, len);
+                return 0;
+        }
+
+	return copy_from_user(iov, msg->msg_iov, len);
+}
+EXPORT_SYMBOL_GPL(rtdm_get_iovec);
+
+ssize_t rtdm_get_iov_flatlen(struct iovec *iov, int iovlen)
+{
+        ssize_t len;
+        int nvec;
+
+        /* Return the flattened vector length. */
+        for (len = 0, nvec = 0; nvec < iovlen; nvec++) {
+                ssize_t l = iov[nvec].iov_len;
+                if (l < 0 || len + l < len) /* SuS wants this. */
+                        return -EINVAL;
+                len += l;
+        }
+
+        return len;
+}
+EXPORT_SYMBOL_GPL(rtdm_get_iov_flatlen);
+
diff -Nur linux-5.4.5/net/rtnet/stack/rtnet_rtpc.c linux-5.4.5-new/net/rtnet/stack/rtnet_rtpc.c
--- linux-5.4.5/net/rtnet/stack/rtnet_rtpc.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtnet_rtpc.c	2020-06-15 16:12:31.567695236 +0300
@@ -0,0 +1,323 @@
+/***
+ *
+ *  stack/rtnet_rtpc.c
+ *
+ *  RTnet - real-time networking subsystem
+ *
+ *  Copyright (C) 2003-2005 Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched/task.h>
+#include <uapi/linux/sched/types.h>
+
+#include <rtnet_rtpc.h>
+
+
+static raw_spinlock_t pending_calls_lock;
+static raw_spinlock_t processed_calls_lock;
+static rtdm_event_t     dispatch_event;
+static rtdm_event_t     rtpc_nrt_signal_event;
+static struct task_struct *dispatch_task;
+static struct task_struct *rtpc_nrt_signal_task;
+static u8 stop_dispatch_task = 0;
+static u8 stop_rtpc_nrt_signal_task = 0;
+
+LIST_HEAD(pending_calls);
+LIST_HEAD(processed_calls);
+
+
+#ifndef __wait_event_interruptible_timeout
+#define __wait_event_interruptible_timeout(wq, condition, ret)              \
+do {                                                                        \
+    wait_queue_t __wait;                                                    \
+    init_waitqueue_entry(&__wait, current);                                 \
+									    \
+    add_wait_queue(&wq, &__wait);                                           \
+    for (;;) {                                                              \
+	set_current_state(TASK_INTERRUPTIBLE);                              \
+	if (condition)                                                      \
+	    break;                                                          \
+	if (!signal_pending(current)) {                                     \
+	    ret = schedule_timeout(ret);                                    \
+	    if (!ret)                                                       \
+		break;                                                      \
+	    continue;                                                       \
+	}                                                                   \
+	ret = -ERESTARTSYS;                                                 \
+	break;                                                              \
+    }                                                                       \
+    current->state = TASK_RUNNING;                                          \
+    remove_wait_queue(&wq, &__wait);                                        \
+} while (0)
+#endif
+
+#ifndef wait_event_interruptible_timeout
+#define wait_event_interruptible_timeout(wq, condition, timeout)            \
+({                                                                          \
+    long __ret = timeout;                                                   \
+    if (!(condition))                                                       \
+	__wait_event_interruptible_timeout(wq, condition, __ret);           \
+    __ret;                                                                  \
+})
+#endif
+
+
+
+int rtnet_rtpc_dispatch_call(rtpc_proc proc, unsigned int timeout,
+			     void* priv_data, size_t priv_data_size,
+			     rtpc_copy_back_proc copy_back_handler,
+			     rtpc_cleanup_proc cleanup_handler)
+{
+    struct rt_proc_call *call;
+    unsigned long      context;
+    int                 ret;
+
+
+    call = kmalloc(sizeof(struct rt_proc_call) + priv_data_size, GFP_KERNEL);
+    if (call == NULL)
+	return -ENOMEM;
+
+    memcpy(call->priv_data, priv_data, priv_data_size);
+
+    call->processed       = 0;
+    call->proc            = proc;
+    call->result          = 0;
+    call->cleanup_handler = cleanup_handler;
+    atomic_set(&call->ref_count, 2);    /* dispatcher + rt-procedure */
+    init_waitqueue_head(&call->call_wq);
+
+    raw_spin_lock_irqsave(&pending_calls_lock, context);
+    list_add_tail(&call->list_entry, &pending_calls);
+    raw_spin_unlock_irqrestore(&pending_calls_lock, context);
+
+    rtdm_event_signal(&dispatch_event);
+
+    if (timeout > 0) {
+	ret = wait_event_interruptible_timeout(call->call_wq,
+	    call->processed, (timeout * HZ) / 1000);
+	if (ret == 0)
+	    ret = -ETIME;
+    } else
+	ret = wait_event_interruptible(call->call_wq, call->processed);
+
+    if (ret >= 0) {
+	if (copy_back_handler != NULL)
+	    copy_back_handler(call, priv_data);
+	ret = call->result;
+    }
+
+    if (atomic_dec_and_test(&call->ref_count)) {
+	if (call->cleanup_handler != NULL)
+	    call->cleanup_handler(&call->priv_data);
+	kfree(call);
+    }
+
+    return ret;
+}
+
+
+
+static inline struct rt_proc_call *rtpc_dequeue_pending_call(void)
+{
+    unsigned long      context;
+    struct rt_proc_call *call = NULL;
+
+
+    raw_spin_lock_irqsave(&pending_calls_lock, context);
+    if (!list_empty(&pending_calls)) {
+	call = (struct rt_proc_call *)pending_calls.next;
+	list_del(&call->list_entry);
+    }
+    raw_spin_unlock_irqrestore(&pending_calls_lock, context);
+
+    return call;
+}
+
+
+
+static inline void rtpc_queue_processed_call(struct rt_proc_call *call)
+{
+    unsigned long  context;
+
+
+    raw_spin_lock_irqsave(&processed_calls_lock, context);
+    list_add_tail(&call->list_entry, &processed_calls);
+    raw_spin_unlock_irqrestore(&processed_calls_lock, context);
+
+    rtdm_event_signal(&rtpc_nrt_signal_event);
+}
+
+
+
+static inline struct rt_proc_call *rtpc_dequeue_processed_call(void)
+{
+    unsigned long      context;
+    struct rt_proc_call *call = NULL;
+
+
+    raw_spin_lock_irqsave(&processed_calls_lock, context);
+    if (!list_empty(&processed_calls)) {
+	call = (struct rt_proc_call *)processed_calls.next;
+	list_del(&call->list_entry);
+    }
+    raw_spin_unlock_irqrestore(&processed_calls_lock, context);
+
+    return call;
+}
+
+
+
+static int rtpc_dispatch_handler(void *arg)
+{
+    struct rt_proc_call *call;
+    int                 ret;
+
+
+    while (!stop_dispatch_task) {
+	if (rtdm_event_wait(&dispatch_event) < 0)
+	    break;
+
+	while ((call = rtpc_dequeue_pending_call())) {
+	    ret = call->proc(call);
+	    if (ret != -CALL_PENDING)
+		rtpc_complete_call(call, ret);
+	}
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+
+static int rtpc_signal_handler(void *arg)
+{
+    struct rt_proc_call *call;
+
+    while(!stop_rtpc_nrt_signal_task) {
+	if (rtdm_event_wait(&rtpc_nrt_signal_event) < 0)
+        	break;
+    	while ((call = rtpc_dequeue_processed_call()) != NULL) {
+		call->processed = 1;
+		wake_up(&call->call_wq);
+
+		if (atomic_dec_and_test(&call->ref_count)) {
+	    	if (call->cleanup_handler != NULL)
+			call->cleanup_handler(&call->priv_data);
+	    	kfree(call);
+		}
+      	}
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+
+void rtnet_rtpc_complete_call(struct rt_proc_call *call, int result)
+{
+    call->result = result;
+    rtpc_queue_processed_call(call);
+}
+
+
+
+void rtnet_rtpc_complete_call_nrt(struct rt_proc_call *call, int result)
+{
+    RTNET_ASSERT(!rtdm_in_rt_context(),
+		 rtnet_rtpc_complete_call(call, result); return;);
+
+    call->processed = 1;
+    wake_up(&call->call_wq);
+
+    if (atomic_dec_and_test(&call->ref_count)) {
+	if (call->cleanup_handler != NULL)
+	    call->cleanup_handler(&call->priv_data);
+	kfree(call);
+    }
+}
+
+
+
+int __init rtpc_init(void)
+{
+    int ret = 0;
+    struct sched_param dispatch_task_param = { .sched_priority = (RTDM_TASK_LOWEST_PRIORITY+1) };
+
+    raw_spin_lock_init(&pending_calls_lock);
+    raw_spin_lock_init(&processed_calls_lock);
+
+    rtdm_event_init(&dispatch_event, 0);
+    rtdm_event_init(&rtpc_nrt_signal_event, 0);
+
+    stop_dispatch_task = 0;
+    dispatch_task = kthread_create(rtpc_dispatch_handler, NULL, "rtnet-rtpc");
+    if (!dispatch_task)
+	    goto dispatch_task_failed; 
+    sched_setscheduler(dispatch_task, SCHED_FIFO, &dispatch_task_param);
+    wake_up_process(dispatch_task);
+
+    stop_rtpc_nrt_signal_task = 0;
+    rtpc_nrt_signal_task = kthread_run(rtpc_signal_handler, NULL, "rtnet-rtpc_nrt_signal");
+    if(rtpc_nrt_signal_task)
+	    return 0;
+
+    stop_dispatch_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(dispatch_task);
+    /* release the task structure */
+    put_task_struct(dispatch_task);
+dispatch_task_failed:
+    rtdm_event_destroy(&dispatch_event);
+    rtdm_event_destroy(&rtpc_nrt_signal_event);
+
+    return ret;
+}
+
+
+
+void rtpc_cleanup(void)
+{
+    rtdm_event_destroy(&dispatch_event);
+    rtdm_event_destroy(&rtpc_nrt_signal_event);
+
+    stop_dispatch_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(dispatch_task);
+    /* release the task structure */
+    put_task_struct(dispatch_task);
+
+    stop_rtpc_nrt_signal_task = 1;
+    kthread_stop(rtpc_nrt_signal_task);
+    put_task_struct(rtpc_nrt_signal_task);
+}
+
+
+EXPORT_SYMBOL_GPL(rtnet_rtpc_dispatch_call);
+EXPORT_SYMBOL_GPL(rtnet_rtpc_complete_call);
+EXPORT_SYMBOL_GPL(rtnet_rtpc_complete_call_nrt);
diff -Nur linux-5.4.5/net/rtnet/stack/rtskb.c linux-5.4.5-new/net/rtnet/stack/rtskb.c
--- linux-5.4.5/net/rtnet/stack/rtskb.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtskb.c	2020-06-15 16:12:31.527695378 +0300
@@ -0,0 +1,569 @@
+/***
+ *
+ *  stack/rtskb.c - rtskb implementation for rtnet
+ *
+ *  Copyright (C) 2002      Ulrich Marx <marx@fet.uni-hannover.de>,
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <net/checksum.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtskb.h>
+#include <rtnet_port.h>
+
+static unsigned int global_rtskbs    = DEFAULT_GLOBAL_RTSKBS;
+module_param(global_rtskbs, uint, 0444);
+MODULE_PARM_DESC(global_rtskbs, "Number of realtime socket buffers in global pool");
+
+
+/* Linux slab pool for rtskbs */
+static struct kmem_cache *rtskb_slab_pool;
+
+/* pool of rtskbs for global use */
+struct rtskb_pool global_pool;
+EXPORT_SYMBOL_GPL(global_pool);
+
+/* pool statistics */
+unsigned int rtskb_pools=0;
+unsigned int rtskb_pools_max=0;
+unsigned int rtskb_amount=0;
+unsigned int rtskb_amount_max=0;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+/* RTcap interface */
+rtdm_lock_t rtcap_lock;
+EXPORT_SYMBOL_GPL(rtcap_lock);
+
+void (*rtcap_handler)(struct rtskb *skb) = NULL;
+EXPORT_SYMBOL_GPL(rtcap_handler);
+#endif
+
+
+/***
+ *  rtskb_copy_and_csum_bits
+ */
+unsigned int rtskb_copy_and_csum_bits(const struct rtskb *skb, int offset,
+				      u8 *to, int len, unsigned int csum)
+{
+    int copy;
+
+    /* Copy header. */
+    if ((copy = skb->len-offset) > 0) {
+	if (copy > len)
+	    copy = len;
+	csum = csum_partial_copy_nocheck(skb->data+offset, to, copy, csum);
+	if ((len -= copy) == 0)
+	    return csum;
+	offset += copy;
+	to += copy;
+    }
+
+    RTNET_ASSERT(len == 0, );
+    return csum;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_copy_and_csum_bits);
+
+
+/***
+ *  rtskb_copy_and_csum_dev
+ */
+void rtskb_copy_and_csum_dev(const struct rtskb *skb, u8 *to)
+{
+    unsigned int csum;
+    unsigned int csstart;
+
+    if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	csstart = skb->h.raw - skb->data;
+
+	if (csstart > skb->len)
+	    BUG();
+    } else
+	csstart = skb->len;
+
+    memcpy(to, skb->data, csstart);
+
+    csum = 0;
+    if (csstart != skb->len)
+	csum = rtskb_copy_and_csum_bits(skb, csstart, to+csstart, skb->len-csstart, 0);
+
+    if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	unsigned int csstuff = csstart + skb->csum;
+
+	*((unsigned short *)(to + csstuff)) = csum_fold(csum);
+    }
+}
+
+EXPORT_SYMBOL_GPL(rtskb_copy_and_csum_dev);
+
+
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+/**
+ *  skb_over_panic - private function
+ *  @skb: buffer
+ *  @sz: size
+ *  @here: address
+ *
+ *  Out of line support code for rtskb_put(). Not user callable.
+ */
+void rtskb_over_panic(struct rtskb *skb, int sz, void *here)
+{
+    rtdm_printk("RTnet: rtskb_put :over: %p:%d put:%d dev:%s\n", here,
+		skb->len, sz, (skb->rtdev) ? skb->rtdev->name : "<NULL>");
+}
+
+EXPORT_SYMBOL_GPL(rtskb_over_panic);
+
+
+/**
+ *  skb_under_panic - private function
+ *  @skb: buffer
+ *  @sz: size
+ *  @here: address
+ *
+ *  Out of line support code for rtskb_push(). Not user callable.
+ */
+void rtskb_under_panic(struct rtskb *skb, int sz, void *here)
+{
+    rtdm_printk("RTnet: rtskb_push :under: %p:%d put:%d dev:%s\n", here,
+		skb->len, sz, (skb->rtdev) ? skb->rtdev->name : "<NULL>");
+}
+
+EXPORT_SYMBOL_GPL(rtskb_under_panic);
+#endif /* CONFIG_XENO_DRIVERS_NET_CHECKED */
+
+static struct rtskb *__rtskb_pool_dequeue(struct rtskb_pool *pool)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    struct rtskb *skb;
+
+    if (!pool->lock_ops->trylock(pool->lock_cookie))
+	    return NULL;
+    skb = __rtskb_dequeue(queue);
+    if (skb == NULL)
+	    pool->lock_ops->unlock(pool->lock_cookie);
+
+    return skb;
+}
+
+struct rtskb *rtskb_pool_dequeue(struct rtskb_pool *pool)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    unsigned long context;
+    struct rtskb *skb;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    skb = __rtskb_pool_dequeue(pool);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+
+    return skb;
+}
+EXPORT_SYMBOL_GPL(rtskb_pool_dequeue);
+
+static void __rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb)
+{
+    struct rtskb_queue *queue = &pool->queue;
+
+    __rtskb_queue_tail(queue,skb);
+    pool->lock_ops->unlock(pool->lock_cookie);
+}
+
+void rtskb_pool_queue_tail(struct rtskb_pool *pool, struct rtskb *skb)
+{
+    struct rtskb_queue *queue = &pool->queue;
+    unsigned long context;
+
+    raw_spin_lock_irqsave(&queue->lock, context);
+    __rtskb_pool_queue_tail(pool, skb);
+    raw_spin_unlock_irqrestore(&queue->lock, context);
+}
+EXPORT_SYMBOL_GPL(rtskb_pool_queue_tail);
+
+/***
+ *  alloc_rtskb - allocate an rtskb from a pool
+ *  @size: required buffer size (to check against maximum boundary)
+ *  @pool: pool to take the rtskb from
+ */
+struct rtskb *alloc_rtskb(unsigned int size, struct rtskb_pool *pool)
+{
+    struct rtskb *skb;
+
+    RTNET_ASSERT(size <= SKB_DATA_ALIGN(RTSKB_SIZE), return NULL;);
+
+    skb = rtskb_pool_dequeue(pool);
+    if (!skb)
+	return NULL;
+
+    /* Load the data pointers. */
+    skb->data = skb->buf_start;
+    skb->tail = skb->buf_start;
+    skb->end  = skb->buf_start + size;
+
+    /* Set up other states */
+    skb->chain_end = skb;
+    skb->len = 0;
+    skb->pkt_type = PACKET_HOST;
+    skb->xmit_stamp = NULL;
+    skb->ip_summed = CHECKSUM_NONE;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+    skb->cap_flags = 0;
+#endif
+
+    return skb;
+}
+
+EXPORT_SYMBOL_GPL(alloc_rtskb);
+
+
+/***
+ *  kfree_rtskb
+ *  @skb    rtskb
+ */
+void kfree_rtskb(struct rtskb *skb)
+{
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+    unsigned long  context;
+    struct rtskb    *comp_skb;
+    struct rtskb    *next_skb;
+    struct rtskb    *chain_end;
+#endif
+
+
+    RTNET_ASSERT(skb != NULL, return;);
+    RTNET_ASSERT(skb->pool != NULL, return;);
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+    next_skb  = skb;
+    chain_end = skb->chain_end;
+
+    do {
+	skb      = next_skb;
+	next_skb = skb->next;
+
+	raw_spin_lock_irqsave(&rtcap_lock, context);
+
+	if (skb->cap_flags & RTSKB_CAP_SHARED) {
+	    skb->cap_flags &= ~RTSKB_CAP_SHARED;
+
+	    comp_skb  = skb->cap_comp_skb;
+	    skb->pool = xchg(&comp_skb->pool, skb->pool);
+
+	    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+
+	    rtskb_pool_queue_tail(comp_skb->pool, comp_skb);
+	}
+	else {
+	    raw_spin_unlock_irqrestore(&rtcap_lock, context);
+
+	    skb->chain_end = skb;
+	    rtskb_pool_queue_tail(skb->pool, skb);
+	}
+
+    } while (chain_end != skb);
+
+#else  /* CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP */
+
+    rtskb_pool_queue_tail(skb->pool, skb);
+
+
+#endif /* CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP */
+}
+
+EXPORT_SYMBOL_GPL(kfree_rtskb);
+
+
+static int rtskb_nop_pool_trylock(void *cookie)
+{
+    return 1;
+}
+
+static void rtskb_nop_pool_unlock(void *cookie)
+{
+}
+
+static const struct rtskb_pool_lock_ops rtskb_nop_pool_lock_ops = {
+    .trylock = rtskb_nop_pool_trylock,
+    .unlock = rtskb_nop_pool_unlock,
+};
+
+
+/***
+ *  rtskb_pool_init
+ *  @pool: pool to be initialized
+ *  @initial_size: number of rtskbs to allocate
+ *  return: number of actually allocated rtskbs
+ */
+unsigned int rtskb_pool_init(struct rtskb_pool *pool,
+			    unsigned int initial_size,
+			    const struct rtskb_pool_lock_ops *lock_ops,
+			    void *lock_cookie)
+{
+    unsigned int i;
+
+    rtskb_queue_init(&pool->queue);
+
+    i = rtskb_pool_extend(pool, initial_size);
+
+    rtskb_pools++;
+    if (rtskb_pools > rtskb_pools_max)
+	rtskb_pools_max = rtskb_pools;
+
+    pool->lock_ops = lock_ops ?: &rtskb_nop_pool_lock_ops;
+    pool->lock_cookie = lock_cookie;
+
+    return i;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_pool_init);
+
+static int rtskb_module_pool_trylock(void *cookie)
+{
+    int err = 1;
+    if (cookie)
+	err = try_module_get(cookie);
+    return err;
+}
+
+static void rtskb_module_pool_unlock(void *cookie)
+{
+    if (cookie)
+	module_put(cookie);
+}
+
+static const struct rtskb_pool_lock_ops rtskb_module_lock_ops = {
+    .trylock = rtskb_module_pool_trylock,
+    .unlock = rtskb_module_pool_unlock,
+};
+
+unsigned int __rtskb_module_pool_init(struct rtskb_pool *pool,
+				    unsigned int initial_size,
+				    struct module *module)
+{
+    return rtskb_pool_init(pool, initial_size, &rtskb_module_lock_ops, module);
+}
+EXPORT_SYMBOL_GPL(__rtskb_module_pool_init);
+
+
+/***
+ *  __rtskb_pool_release
+ *  @pool: pool to release
+ */
+void rtskb_pool_release(struct rtskb_pool *pool)
+{
+    struct rtskb *skb;
+
+    while ((skb = rtskb_dequeue(&pool->queue)) != NULL) {
+	rtdev_unmap_rtskb(skb);
+	kmem_cache_free(rtskb_slab_pool, skb);
+	rtskb_amount--;
+    }
+
+    rtskb_pools--;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_pool_release);
+
+
+unsigned int rtskb_pool_extend(struct rtskb_pool *pool,
+			       unsigned int add_rtskbs)
+{
+    unsigned int i;
+    struct rtskb *skb;
+
+
+    RTNET_ASSERT(pool != NULL, return -EINVAL;);
+
+    for (i = 0; i < add_rtskbs; i++) {
+	/* get rtskb from slab pool */
+	if (!(skb = kmem_cache_alloc(rtskb_slab_pool, GFP_KERNEL))) {
+	    printk(KERN_ERR "RTnet: rtskb allocation from slab pool failed\n");
+	    break;
+	}
+
+	/* fill the header with zero */
+	memset(skb, 0, sizeof(struct rtskb));
+
+	skb->chain_end = skb;
+	skb->pool = pool;
+	skb->buf_start = ((unsigned char *)skb) + ALIGN_RTSKB_STRUCT_LEN;
+#ifdef CONFIG_XENO_DRIVERS_NET_CHECKED
+	skb->buf_end = skb->buf_start + SKB_DATA_ALIGN(RTSKB_SIZE) - 1;
+#endif
+
+	if (rtdev_map_rtskb(skb) < 0) {
+	    kmem_cache_free(rtskb_slab_pool, skb);
+	    break;
+	}
+
+	rtskb_queue_tail(&pool->queue, skb);
+
+	rtskb_amount++;
+	if (rtskb_amount > rtskb_amount_max)
+	    rtskb_amount_max = rtskb_amount;
+    }
+
+    return i;
+}
+
+
+unsigned int rtskb_pool_shrink(struct rtskb_pool *pool,
+			       unsigned int rem_rtskbs)
+{
+    unsigned int    i;
+    struct rtskb    *skb;
+
+
+    for (i = 0; i < rem_rtskbs; i++) {
+	if ((skb = rtskb_dequeue(&pool->queue)) == NULL)
+	    break;
+
+	rtdev_unmap_rtskb(skb);
+	kmem_cache_free(rtskb_slab_pool, skb);
+	rtskb_amount--;
+    }
+
+    return i;
+}
+
+
+/* Note: acquires only the first skb of a chain! */
+int rtskb_acquire(struct rtskb *rtskb, struct rtskb_pool *comp_pool)
+{
+    struct rtskb *comp_rtskb;
+    struct rtskb_pool *release_pool;
+    unsigned long context;
+
+
+    raw_spin_lock_irqsave(&comp_pool->queue.lock, context);
+
+    comp_rtskb = __rtskb_pool_dequeue(comp_pool);
+    if (!comp_rtskb) {
+	raw_spin_unlock_irqrestore(&comp_pool->queue.lock, context);
+	return -ENOMEM;
+    }
+
+    raw_spin_unlock(&comp_pool->queue.lock);
+
+    comp_rtskb->chain_end = comp_rtskb;
+    comp_rtskb->pool = release_pool = rtskb->pool;
+
+    raw_spin_lock(&release_pool->queue.lock);
+
+    __rtskb_pool_queue_tail(release_pool, comp_rtskb);
+
+    raw_spin_unlock_irqrestore(&release_pool->queue.lock, context);
+
+    rtskb->pool = comp_pool;
+
+    return 0;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_acquire);
+
+
+/* clone rtskb to another, allocating the new rtskb from pool */
+struct rtskb* rtskb_clone(struct rtskb *rtskb, struct rtskb_pool *pool)
+{
+    struct rtskb    *clone_rtskb;
+    unsigned int    total_len;
+
+    clone_rtskb = alloc_rtskb(rtskb->end - rtskb->buf_start, pool);
+    if (clone_rtskb == NULL)
+	return NULL;
+
+    /* Note: We don't clone
+	- rtskb.sk
+	- rtskb.xmit_stamp
+       until real use cases show up. */
+
+    clone_rtskb->priority   = rtskb->priority;
+    clone_rtskb->rtdev      = rtskb->rtdev;
+    clone_rtskb->time_stamp = rtskb->time_stamp;
+
+    clone_rtskb->mac.raw    = clone_rtskb->buf_start;
+    clone_rtskb->nh.raw     = clone_rtskb->buf_start;
+    clone_rtskb->h.raw      = clone_rtskb->buf_start;
+
+    clone_rtskb->data       += rtskb->data - rtskb->buf_start;
+    clone_rtskb->tail       += rtskb->tail - rtskb->buf_start;
+    clone_rtskb->mac.raw    += rtskb->mac.raw - rtskb->buf_start;
+    clone_rtskb->nh.raw     += rtskb->nh.raw - rtskb->buf_start;
+    clone_rtskb->h.raw      += rtskb->h.raw - rtskb->buf_start;
+
+    clone_rtskb->protocol   = rtskb->protocol;
+    clone_rtskb->pkt_type   = rtskb->pkt_type;
+
+    clone_rtskb->ip_summed  = rtskb->ip_summed;
+    clone_rtskb->csum       = rtskb->csum;
+
+    total_len = rtskb->len + rtskb->data - rtskb->mac.raw;
+    memcpy(clone_rtskb->mac.raw, rtskb->mac.raw, total_len);
+    clone_rtskb->len = rtskb->len;
+
+    return clone_rtskb;
+}
+
+EXPORT_SYMBOL_GPL(rtskb_clone);
+
+
+int rtskb_pools_init(void)
+{
+    rtskb_slab_pool = kmem_cache_create("rtskb_slab_pool",
+	ALIGN_RTSKB_STRUCT_LEN + SKB_DATA_ALIGN(RTSKB_SIZE),
+	0, SLAB_HWCACHE_ALIGN, NULL);
+    if (rtskb_slab_pool == NULL)
+	return -ENOMEM;
+
+    /* reset the statistics (cache is accounted separately) */
+    rtskb_pools      = 0;
+    rtskb_pools_max  = 0;
+    rtskb_amount     = 0;
+    rtskb_amount_max = 0;
+
+    /* create the global rtskb pool */
+    if (rtskb_module_pool_init(&global_pool, global_rtskbs) < global_rtskbs)
+	goto err_out;
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_ADDON_RTCAP)
+    rtdm_lock_init(&rtcap_lock);
+#endif
+
+    return 0;
+
+err_out:
+    rtskb_pool_release(&global_pool);
+    kmem_cache_destroy(rtskb_slab_pool);
+
+    return -ENOMEM;
+}
+
+
+void rtskb_pools_release(void)
+{
+    rtskb_pool_release(&global_pool);
+    kmem_cache_destroy(rtskb_slab_pool);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/rtwlan.c linux-5.4.5-new/net/rtnet/stack/rtwlan.c
--- linux-5.4.5/net/rtnet/stack/rtwlan.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/rtwlan.c	2020-06-15 16:12:31.499695476 +0300
@@ -0,0 +1,222 @@
+/* rtwlan.c
+ *
+ * rtwlan protocol stack
+ * Copyright (c) 2006, Daniel Gregorek <dxg@gmx.de>
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/module.h>
+
+#include <rtnet_port.h>
+
+#include <rtwlan.h>
+
+int rtwlan_rx(struct rtskb * rtskb, struct rtnet_device * rtnet_dev)
+{
+    struct ieee80211_hdr  * hdr = (struct ieee80211_hdr *)rtskb->data;
+    u16 fc = le16_to_cpu(hdr->frame_ctl);
+
+    /* strip rtwlan header */
+    rtskb_pull(rtskb, ieee80211_get_hdrlen(fc));
+    rtskb->protocol = rt_eth_type_trans (rtskb, rtnet_dev);
+
+    /* forward rtskb to rtnet */
+    rtnetif_rx(rtskb);
+
+    return 0;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_rx);
+
+
+int rtwlan_tx(struct rtskb *rtskb, struct rtnet_device *rtnet_dev)
+{
+    struct rtwlan_device * rtwlan_dev = rtnetdev_priv(rtnet_dev);
+    struct ieee80211_hdr_3addr header = {	/* Ensure zero initialized */
+	.duration_id = 0,
+	.seq_ctl = 0
+    };
+    int ret;
+    u8 dest[ETH_ALEN], src[ETH_ALEN];
+
+    /* Get source and destination addresses */
+
+    memcpy(src, rtskb->data + ETH_ALEN, ETH_ALEN);
+
+    if(rtwlan_dev->mode == RTWLAN_TXMODE_MCAST) {
+	memcpy(dest, rtnet_dev->dev_addr, ETH_ALEN);
+	dest[0] |= 0x01;
+    } else {
+	memcpy(dest, rtskb->data, ETH_ALEN);
+    }
+
+    /*
+     * Generate ieee80211 compatible header
+     */
+    memcpy(header.addr3, src, ETH_ALEN);	/* BSSID */
+    memcpy(header.addr2, src, ETH_ALEN);	/* SA */
+    memcpy(header.addr1, dest, ETH_ALEN);	/* DA */
+
+    /* Write frame control field */
+    header.frame_ctl = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);
+
+    memcpy(rtskb_push(rtskb, IEEE80211_3ADDR_LEN), &header, IEEE80211_3ADDR_LEN);
+
+    ret = (*rtwlan_dev->hard_start_xmit)(rtskb,rtnet_dev);
+
+    return ret;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_tx);
+
+
+/**
+ * rtalloc_wlandev - Allocates and sets up a wlan device
+ * @sizeof_priv: size of additional driver-private structure to
+ *               be allocated for this wlan device
+ *
+ * Fill in the fields of the device structure with wlan-generic
+ * values. Basically does everything except registering the device.
+ *
+ * A 32-byte alignment is enforced for the private data area.
+ */
+
+struct rtnet_device *rtwlan_alloc_dev(unsigned sizeof_priv, unsigned dev_pool_size)
+{
+    struct rtnet_device *rtnet_dev;
+
+    RTWLAN_DEBUG("Start.\n");
+
+    rtnet_dev = rt_alloc_etherdev(sizeof(struct rtwlan_device) + sizeof_priv,
+			    dev_pool_size);
+    if (!rtnet_dev)
+	return NULL;
+
+    rtnet_dev->hard_start_xmit = rtwlan_tx;
+
+    rtdev_alloc_name(rtnet_dev, "rtwlan%d");
+
+    return rtnet_dev;
+}
+
+EXPORT_SYMBOL_GPL(rtwlan_alloc_dev);
+
+
+int rtwlan_ioctl(struct rtnet_device * rtdev,
+		 unsigned int request,
+		 unsigned long arg)
+{
+    struct rtwlan_cmd cmd;
+    struct ifreq ifr;
+    int ret=0;
+
+    if (copy_from_user(&cmd, (void *)arg, sizeof(cmd)) != 0)
+	return -EFAULT;
+
+    /*
+     * FIXME: proper do_ioctl() should expect a __user pointer
+     * arg. This only works with the existing WLAN support because the
+     * only driver currently providing this feature is broken, not
+     * doing the copy_to/from_user dance.
+     */
+    memset(&ifr, 0, sizeof(ifr));
+    ifr.ifr_data = &cmd;
+   
+    switch(request) {
+    case IOC_RTWLAN_IFINFO:
+	if (cmd.args.info.ifindex > 0)
+	    rtdev = rtdev_get_by_index(cmd.args.info.ifindex);
+	else
+	    rtdev = rtdev_get_by_name(cmd.head.if_name);
+	if (rtdev == NULL)
+	    return -ENODEV;
+
+	if (mutex_lock_interruptible(&rtdev->nrt_lock)) {
+	    rtdev_dereference(rtdev);
+	    return -ERESTARTSYS;
+	}
+
+	if (rtdev->do_ioctl)
+	    ret = rtdev->do_ioctl(rtdev, &ifr, request);
+	else
+	    ret = -ENORTWLANDEV;
+
+	memcpy(cmd.head.if_name, rtdev->name, IFNAMSIZ);
+	cmd.args.info.ifindex      = rtdev->ifindex;
+	cmd.args.info.flags        = rtdev->flags;
+
+	mutex_unlock(&rtdev->nrt_lock);
+
+	rtdev_dereference(rtdev);
+
+	break;
+
+    case IOC_RTWLAN_TXMODE:
+    case IOC_RTWLAN_BITRATE:
+    case IOC_RTWLAN_CHANNEL:
+    case IOC_RTWLAN_RETRY:
+    case IOC_RTWLAN_TXPOWER:
+    case IOC_RTWLAN_AUTORESP:
+    case IOC_RTWLAN_DROPBCAST:
+    case IOC_RTWLAN_DROPMCAST:
+    case IOC_RTWLAN_REGREAD:
+    case IOC_RTWLAN_REGWRITE:
+    case IOC_RTWLAN_BBPWRITE:
+    case IOC_RTWLAN_BBPREAD:
+    case IOC_RTWLAN_BBPSENS:
+            if (mutex_lock_interruptible(&rtdev->nrt_lock))
+	        return -ERESTARTSYS;
+
+	    if (rtdev->do_ioctl)
+	        ret = rtdev->do_ioctl(rtdev, &ifr, request);
+	    else
+		ret = -ENORTWLANDEV;
+
+	    mutex_unlock(&rtdev->nrt_lock);
+
+	    break;
+
+    default:
+	ret = -ENOTTY;
+    }
+
+    if (copy_to_user((void *)arg, &cmd, sizeof(cmd)) != 0)
+	return -EFAULT;
+
+    return ret;
+}
+
+
+struct rtnet_ioctls rtnet_wlan_ioctls = {
+    service_name: "rtwlan ioctl",
+    ioctl_type: RTNET_IOC_TYPE_RTWLAN,
+    handler: rtwlan_ioctl
+};
+
+int __init rtwlan_init(void)
+{
+    if (rtnet_register_ioctls(&rtnet_wlan_ioctls))
+	rtdm_printk(KERN_ERR "Failed to register rtnet_wlan_ioctl!\n");
+
+    return 0;
+}
+
+
+void rtwlan_exit(void)
+{
+    rtnet_unregister_ioctls(&rtnet_wlan_ioctls);
+}
diff -Nur linux-5.4.5/net/rtnet/stack/socket.c linux-5.4.5-new/net/rtnet/stack/socket.c
--- linux-5.4.5/net/rtnet/stack/socket.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/socket.c	2020-06-15 16:12:31.503695461 +0300
@@ -0,0 +1,493 @@
+/***
+ *
+ *  stack/socket.c - sockets implementation for rtnet
+ *
+ *  Copyright (C) 1999       Lineo, Inc
+ *                1999, 2002 David A. Schleef <ds@schleef.org>
+ *                2002       Ulrich Marx <marx@kammer.uni-hannover.de>
+ *                2003-2005  Jan Kiszka <jan.kiszka@web.de>
+ *
+ *  RTnet port to PREEMPT_RT,
+ *  Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ *  laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/spinlock.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/err.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/semaphore.h>
+#include <asm/bitops.h>
+
+#include <rtdm_net.h>
+#include <rtnet_internal.h>
+#include <rtnet_iovec.h>
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
+
+#define SKB_POOL_CLOSED     0
+
+static raw_spinlock_t fdtree_lock;
+static unsigned char fdtree_lock_initialized;
+
+static unsigned int socket_rtskbs = DEFAULT_SOCKET_RTSKBS;
+module_param(socket_rtskbs, uint, 0444);
+MODULE_PARM_DESC(socket_rtskbs, "Default number of realtime socket buffers in socket pools");
+
+
+/************************************************************************
+ *  internal socket functions                                           *
+ ************************************************************************/
+
+int __rt_bare_socket_init_icmp(struct rtsocket *sock, unsigned short protocol,
+                        unsigned int priority, unsigned int pool_size,
+                        struct module *module)
+{
+    int err;
+
+    err = try_module_get(module);
+    if (!err)
+        return -EAFNOSUPPORT;
+
+    err = rtskb_pool_init(&sock->skb_pool, pool_size, NULL, sock);
+    if (err < 0) {
+        module_put(module);
+        return err;
+    }
+
+    sock->protocol = protocol;
+    sock->priority = priority;
+    sock->owner = module;
+
+    return err;
+}
+EXPORT_SYMBOL_GPL(__rt_bare_socket_init_icmp);
+
+int __rt_bare_socket_init(struct rtsocket *sock, unsigned short protocol,
+			unsigned int priority, unsigned int pool_size,
+			struct module *module)
+{
+    int err;
+
+    err = try_module_get(module);
+    if (!err)
+	return -EAFNOSUPPORT;
+
+    err = rtskb_pool_init(&sock->skb_pool, pool_size, NULL, sock);
+    if (err < 0) {
+	module_put(module);
+	return err;
+    }
+
+    sock->protocol = protocol;
+    sock->priority = priority;
+    sock->owner = module;
+
+    return err;
+}
+EXPORT_SYMBOL_GPL(__rt_bare_socket_init);
+
+/***
+ *  rt_socket_init - initialises a new socket structure
+ */
+int __rt_socket_init(struct rtsocket *sock, unsigned short protocol,
+		struct module *module)
+{
+    unsigned int    pool_size;
+
+    sock->fd_refs = 1;
+    if(!fdtree_lock_initialized) {
+	raw_spin_lock_init(&fdtree_lock);
+	fdtree_lock_initialized = 1;
+    }
+
+    sock->flags = 0;
+    sock->callback_func = NULL;
+
+    rtskb_queue_init(&sock->incoming);
+
+    sock->timeout = 0;
+
+    raw_spin_lock_init(&sock->param_lock);
+    sema_init(&sock->pending_sem, 0);
+
+    pool_size = __rt_bare_socket_init(sock, protocol,
+				    RTSKB_PRIO_VALUE(SOCK_DEF_PRIO,
+						    RTSKB_DEF_RT_CHANNEL),
+				    socket_rtskbs, module);
+    sock->pool_size = pool_size;
+    mutex_init(&sock->pool_nrt_lock);
+
+    if (pool_size < socket_rtskbs) {
+	/* fix statistics */
+	if (pool_size == 0)
+	    rtskb_pools--;
+
+	rt_socket_cleanup(sock);
+	return -ENOMEM;
+    }
+
+    return 0;
+}
+EXPORT_SYMBOL_GPL(__rt_socket_init);
+
+
+/***
+ *  rt_socket_cleanup - releases resources allocated for the socket
+ */
+void rt_socket_cleanup(struct rtsocket *sock)
+{
+#if 0
+    rtdm_sem_destroy(&sock->pending_sem);
+#endif
+
+    mutex_lock(&sock->pool_nrt_lock);
+
+    set_bit(SKB_POOL_CLOSED, &sock->flags);
+
+    if (sock->pool_size > 0)
+	rtskb_pool_release(&sock->skb_pool);
+
+    mutex_unlock(&sock->pool_nrt_lock);
+
+    module_put(sock->owner);
+}
+EXPORT_SYMBOL_GPL(rt_socket_cleanup);
+
+
+
+/***
+ *  rt_socket_common_ioctl
+ */
+int rt_socket_common_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    int                     ret = 0;
+    const unsigned int *val;
+    unsigned int _val;
+    const nanosecs_rel_t *timeout;
+    nanosecs_rel_t _timeout;
+#if 0
+    struct rtnet_callback   *callback;
+    unsigned long          context;
+#endif
+
+    switch (request) {
+	case RTNET_RTIOC_XMITPARAMS:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+		sock->priority = *val;
+		break;
+
+	case RTNET_RTIOC_TIMEOUT:
+		timeout = rtnet_get_arg(sock, &_timeout, arg, sizeof(_timeout), 1);
+		if (IS_ERR(timeout))
+			return PTR_ERR(timeout);
+		sock->timeout = *timeout;
+		break;
+
+	case RTNET_RTIOC_CALLBACK:
+#if 0
+	    if (rtdm_fd_is_user(fd))
+		return -EACCES;
+#endif
+	    return -EACCES;
+#if 0
+	    raw_spin_lock_irqsave(&sock->param_lock, context);
+
+	    callback = arg;
+	    sock->callback_func = callback->func;
+	    sock->callback_arg  = callback->arg;
+
+	    raw_spin_unlock_irqrestore(&sock->param_lock, context);
+	    break;
+#endif
+
+	case RTNET_RTIOC_EXTPOOL:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+
+		if (rtdm_in_rt_context())
+			return -ENOSYS;
+
+		mutex_lock(&sock->pool_nrt_lock);
+
+		if (test_bit(SKB_POOL_CLOSED, &sock->flags)) {
+			mutex_unlock(&sock->pool_nrt_lock);
+			return -EBADF;
+		}
+		ret = rtskb_pool_extend(&sock->skb_pool, *val);
+		sock->pool_size += ret;
+
+		mutex_unlock(&sock->pool_nrt_lock);
+
+		if (ret == 0 && *val > 0)
+			ret = -ENOMEM;
+
+		break;
+
+	case RTNET_RTIOC_SHRPOOL:
+		val = rtnet_get_arg(sock, &_val, arg, sizeof(_val), 1);
+		if (IS_ERR(val))
+			return PTR_ERR(val);
+
+		if (rtdm_in_rt_context())
+			return -ENOSYS;
+
+		mutex_lock(&sock->pool_nrt_lock);
+
+		ret = rtskb_pool_shrink(&sock->skb_pool, *val);
+		sock->pool_size -= ret;
+
+		mutex_unlock(&sock->pool_nrt_lock);
+
+		if (ret == 0 && *val > 0)
+			ret = -EBUSY;
+
+		break;
+
+	default:
+	    ret = -EOPNOTSUPP;
+	    break;
+    }
+
+    return ret;
+}
+EXPORT_SYMBOL_GPL(rt_socket_common_ioctl);
+
+
+
+/***
+ *  rt_socket_if_ioctl
+ */
+int rt_socket_if_ioctl(struct rtsocket *sock, int request, void __user *arg)
+{
+    struct rtnet_device *rtdev;
+    struct ifreq _ifr, *ifr, *u_ifr;
+    struct sockaddr_in  _sin;
+    struct ifconf _ifc, *ifc, *u_ifc;
+    int ret = 0, size = 0, i;
+    short flags;
+
+
+    if (request == SIOCGIFCONF) {
+	u_ifc = arg;
+	ifc = rtnet_get_arg(sock, &_ifc, u_ifc, sizeof(_ifc), 1);
+	if (IS_ERR(ifc))
+		return PTR_ERR(ifc);
+
+	for (u_ifr = ifc->ifc_req, i = 1; i <= MAX_RT_DEVICES; i++, u_ifr++) {
+		rtdev = rtdev_get_by_index(i);
+		if (rtdev == NULL)
+			continue;
+
+		if ((rtdev->flags & IFF_UP) == 0) {
+			rtdev_dereference(rtdev);
+			continue;
+		}
+
+		size += sizeof(struct ifreq);
+		if (size > ifc->ifc_len) {
+			rtdev_dereference(rtdev);
+			size = ifc->ifc_len;
+			break;
+		}
+
+		ret = rtnet_put_arg(sock, u_ifr->ifr_name, rtdev->name, IFNAMSIZ, 1);
+		if (ret == 0) {
+			memset(&_sin, 0, sizeof(_sin));
+			_sin.sin_family      = AF_INET;
+			_sin.sin_addr.s_addr = rtdev->local_ip;
+			ret = rtnet_put_arg(sock, &u_ifr->ifr_addr, &_sin, sizeof(_sin), 1);
+		}
+		
+		rtdev_dereference(rtdev);
+		if (ret)
+			return ret;
+	}
+
+	return rtnet_put_arg(sock, &u_ifc->ifc_len, &size, sizeof(size), 1);
+    }
+
+    u_ifr = arg;
+    ifr = rtnet_get_arg(sock, &_ifr, u_ifr, sizeof(_ifr), 1);
+    if (IS_ERR(ifr))
+	    return PTR_ERR(ifr);
+
+    if (request == SIOCGIFNAME) {
+        rtdev = rtdev_get_by_index(ifr->ifr_ifindex);
+        if (rtdev == NULL)
+            return -ENODEV;
+	ret = rtnet_put_arg(sock, u_ifr->ifr_name, rtdev->name, IFNAMSIZ, 1);
+	goto out;
+    }
+
+    rtdev = rtdev_get_by_name(ifr->ifr_name);
+    if (rtdev == NULL)
+	    return -ENODEV;
+
+    switch (request) {
+	case SIOCGIFINDEX:
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_ifindex, &rtdev->ifindex,
+				    sizeof(u_ifr->ifr_ifindex), 1);
+		break;
+
+	case SIOCGIFFLAGS:
+		flags = rtdev->flags;
+		if ((ifr->ifr_flags & IFF_UP)
+		    && (rtdev->link_state
+			& (RTNET_LINK_STATE_PRESENT
+			   | RTNET_LINK_STATE_NOCARRIER))
+                    == RTNET_LINK_STATE_PRESENT)
+			flags |= IFF_RUNNING;
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_flags, &flags,
+				    sizeof(u_ifr->ifr_flags), 1);
+		break;
+
+	case SIOCGIFHWADDR:
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_hwaddr.sa_data,
+				    rtdev->dev_addr, rtdev->addr_len, 1);
+		if (!ret)
+			ret = rtnet_put_arg(sock, &u_ifr->ifr_hwaddr.sa_family,
+					    &rtdev->type, sizeof(u_ifr->ifr_hwaddr.sa_family), 1);
+		break;
+
+	case SIOCGIFADDR:
+		memset(&_sin, 0, sizeof(_sin));
+		_sin.sin_family      = AF_INET;
+		_sin.sin_addr.s_addr = rtdev->local_ip;
+		ret = rtnet_put_arg(sock, &u_ifr->ifr_addr, &_sin, sizeof(_sin), 1);
+		break;
+
+	case SIOCETHTOOL:
+		if (rtdev->do_ioctl != NULL) {
+			if (rtdm_in_rt_context())
+				return -ENOSYS;
+			ret = rtdev->do_ioctl(rtdev, ifr, request);
+		} else
+			ret = -EOPNOTSUPP;
+		break;
+
+	case SIOCDEVPRIVATE ... SIOCDEVPRIVATE + 15:
+		if (rtdev->do_ioctl != NULL)
+			ret = rtdev->do_ioctl(rtdev, ifr, request);
+		else
+			ret = -EOPNOTSUPP;
+		break;
+
+	default:
+		ret = -EOPNOTSUPP;
+		break;
+    }
+
+  out:
+    rtdev_dereference(rtdev);
+    return ret;
+}
+EXPORT_SYMBOL_GPL(rt_socket_if_ioctl);
+
+#if 0
+/* no select on the socket for the moment */
+int rt_socket_select_bind(struct rtdm_fd *fd,
+			  rtdm_selector_t *selector,
+			  enum rtdm_selecttype type,
+			  unsigned fd_index)
+{
+    struct rtsocket *sock = rtdm_fd_to_private(fd);
+
+    switch (type) {
+	case XNSELECT_READ:
+	    return rtdm_sem_select(&sock->pending_sem, selector,
+				XNSELECT_READ, fd_index);
+	default:
+	    return -EBADF;
+    }
+
+    return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(rt_socket_select_bind);
+#endif
+
+void *rtnet_get_arg(struct rtsocket *sock, void *tmp, const void *src, size_t len, int msg_in_userspace)
+{
+	int ret;
+	
+	if (!msg_in_userspace)
+		return (void *)src;
+
+	ret = copy_from_user(tmp, src, len);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return tmp;
+}
+EXPORT_SYMBOL_GPL(rtnet_get_arg);
+
+int rtnet_put_arg(struct rtsocket *sock, void *dst, const void *src, size_t len, int msg_in_userspace)
+{
+	if (!msg_in_userspace) {
+		if (dst != src)
+			memcpy(dst, src, len);
+		return 0;
+	}
+
+	return copy_to_user(dst, src, len);
+}
+EXPORT_SYMBOL_GPL(rtnet_put_arg);
+
+int rtdm_fd_lock(struct rtsocket *sock)
+{
+	unsigned long context;
+
+	/* xenomai-3/kernel/cobalt/rtdm/fd.c */
+
+        raw_spin_lock_irqsave(&fdtree_lock, context);
+        if (sock->fd_refs == 0) {
+                raw_spin_unlock_irqrestore(&fdtree_lock, context);
+                return -EIDRM;
+        }
+        ++sock->fd_refs;
+        raw_spin_unlock_irqrestore(&fdtree_lock, context);
+
+        return 0;
+
+}
+EXPORT_SYMBOL_GPL(rtdm_fd_lock);
+
+int rtdm_fd_unlock(struct rtsocket *sock)
+{
+        unsigned long context;
+
+	/* xenomai-3/kernel/cobalt/rtdm/fd.c */
+        raw_spin_lock_irqsave(&fdtree_lock, context);
+
+	if(sock->fd_refs <= 0)
+		printk(KERN_WARNING "%s sock->fd=%d sock->fd_refs=%d\n", __func__, sock->fd, sock->fd_refs);
+	
+	--sock->fd_refs;
+	
+	raw_spin_unlock_irqrestore(&fdtree_lock, context);
+	
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rtdm_fd_unlock);
diff -Nur linux-5.4.5/net/rtnet/stack/stack_mgr.c linux-5.4.5-new/net/rtnet/stack/stack_mgr.c
--- linux-5.4.5/net/rtnet/stack/stack_mgr.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/net/rtnet/stack/stack_mgr.c	2020-06-15 16:12:31.515695420 +0300
@@ -0,0 +1,291 @@
+/***
+ *
+ *  stack/stack_mgr.c - Stack-Manager
+ *
+ *  Copyright (C) 2002 Ulrich Marx <marx@kammer.uni-hannover.de>
+ *  Copyright (C) 2003-2006 Jan Kiszka <jan.kiszka@web.de>
+ *  Copyright (C) 2006 Jorge Almeida <j-almeida@criticalsoftware.com>
+ *
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#include <linux/moduleparam.h>
+#include <uapi/linux/sched/types.h>
+
+#include <rtdev.h>
+#include <rtnet_internal.h>
+#include <rtskb_fifo.h>
+#include <stack_mgr.h>
+
+static unsigned int stack_mgr_prio = RTNET_DEF_STACK_PRIORITY;
+static u8 stop_rt_stack_mgr_task = 0;
+module_param(stack_mgr_prio, uint, 0444);
+MODULE_PARM_DESC(stack_mgr_prio, "Priority of the stack manager task");
+
+
+#if (CONFIG_XENO_DRIVERS_NET_RX_FIFO_SIZE & (CONFIG_XENO_DRIVERS_NET_RX_FIFO_SIZE-1)) != 0
+#error CONFIG_XENO_DRIVERS_NET_RX_FIFO_SIZE must be power of 2!
+#endif
+static DECLARE_RTSKB_FIFO(rx, CONFIG_XENO_DRIVERS_NET_RX_FIFO_SIZE);
+
+struct list_head    rt_packets[RTPACKET_HASH_TBL_SIZE];
+#ifdef CONFIG_XENO_DRIVERS_NET_ETH_P_ALL
+struct list_head    rt_packets_all;
+#endif /* CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+raw_spinlock_t rt_packets_lock;
+
+/***
+ *  rtdev_add_pack:         add protocol (Layer 3)
+ *  @pt:                    the new protocol
+ */
+int __rtdev_add_pack(struct rtpacket_type *pt, struct module *module)
+{
+    int                     ret = 0;
+    unsigned long          context;
+
+    INIT_LIST_HEAD(&pt->list_entry);
+    pt->refcount = 0;
+    if (pt->trylock == NULL)
+	pt->trylock = rtdev_lock_pack;
+    if (pt->unlock == NULL)
+	pt->unlock = rtdev_unlock_pack;
+    pt->owner = module;
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+
+    if (pt->type == htons(ETH_P_ALL))
+#ifdef CONFIG_XENO_DRIVERS_NET_ETH_P_ALL
+	list_add_tail(&pt->list_entry, &rt_packets_all);
+#else /* !CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+	ret = -EINVAL;
+#endif /* CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+    else
+	list_add_tail(&pt->list_entry,
+		      &rt_packets[ntohs(pt->type) & RTPACKET_HASH_KEY_MASK]);
+
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+    return ret;
+}
+
+EXPORT_SYMBOL_GPL(__rtdev_add_pack);
+
+
+/***
+ *  rtdev_remove_pack:  remove protocol (Layer 3)
+ *  @pt:                protocol
+ */
+void rtdev_remove_pack(struct rtpacket_type *pt)
+{
+    unsigned long  context;
+
+
+    RTNET_ASSERT(pt != NULL, return;);
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+    list_del(&pt->list_entry);
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+}
+
+EXPORT_SYMBOL_GPL(rtdev_remove_pack);
+
+
+/***
+ *  rtnetif_rx: will be called from the driver interrupt handler
+ *  (IRQs disabled!) and send a message to rtdev-owned stack-manager
+ *
+ *  @skb - the packet
+ */
+void rtnetif_rx(struct rtskb *skb)
+{
+    RTNET_ASSERT(skb != NULL, return;);
+    RTNET_ASSERT(skb->rtdev != NULL, return;);
+
+    if (unlikely(rtskb_fifo_insert_inirq(&rx.fifo, skb) < 0)) {
+	printk("RTnet: dropping packet in %s()\n", __FUNCTION__);
+	kfree_rtskb(skb);
+    }
+}
+
+EXPORT_SYMBOL_GPL(rtnetif_rx);
+
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK)
+#define __DELIVER_PREFIX
+#else /* !CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK */
+#define __DELIVER_PREFIX static inline
+#endif /* CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK */
+
+__DELIVER_PREFIX void rt_stack_deliver(struct rtskb *rtskb)
+{
+    unsigned short          hash;
+    struct rtpacket_type    *pt_entry;
+    unsigned long          context;
+    struct rtnet_device     *rtdev = rtskb->rtdev;
+    int                     err;
+    int                     eth_p_all_hit = 0;
+
+
+    rtcap_report_incoming(rtskb);
+
+    rtskb->nh.raw = rtskb->data;
+
+    raw_spin_lock_irqsave(&rt_packets_lock, context);
+
+#ifdef CONFIG_XENO_DRIVERS_NET_ETH_P_ALL
+    eth_p_all_hit = 0;
+    list_for_each_entry(pt_entry, &rt_packets_all, list_entry) {
+	if (!pt_entry->trylock(pt_entry))
+	    continue;
+	raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+	pt_entry->handler(rtskb, pt_entry);
+
+	raw_spin_lock_irqsave(&rt_packets_lock, context);
+	pt_entry->unlock(pt_entry);
+	eth_p_all_hit = 1;
+    }
+#endif /* CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+
+    hash = ntohs(rtskb->protocol) & RTPACKET_HASH_KEY_MASK;
+
+    list_for_each_entry(pt_entry, &rt_packets[hash], list_entry)
+	if (pt_entry->type == rtskb->protocol) {
+	    if (!pt_entry->trylock(pt_entry))
+		continue;
+	    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+	    err = pt_entry->handler(rtskb, pt_entry);
+
+	    raw_spin_lock_irqsave(&rt_packets_lock, context);
+	    pt_entry->unlock(pt_entry);
+
+	    if (likely(!err)) {
+		raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+		return;
+	    }
+	}
+
+    raw_spin_unlock_irqrestore(&rt_packets_lock, context);
+
+    /* Don't warn if ETH_P_ALL listener were present or when running in
+       promiscuous mode (RTcap). */
+    if (unlikely(!eth_p_all_hit && !(rtdev->flags & IFF_PROMISC)))
+	printk("RTnet: no one cared for packet with layer 3 "
+		    "protocol type 0x%04x\n", ntohs(rtskb->protocol));
+
+    kfree_rtskb(rtskb);
+}
+
+#if IS_ENABLED(CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK)
+EXPORT_SYMBOL_GPL(rt_stack_deliver);
+#endif /* CONFIG_XENO_DRIVERS_NET_DRV_LOOPBACK */
+
+
+static int rt_stack_mgr_task(void *arg)
+{
+    rtdm_event_t            *mgr_event = &((struct rtnet_mgr *)arg)->event;
+    struct rtskb            *rtskb;
+
+    while (!stop_rt_stack_mgr_task) {
+	if (rtdm_event_wait(mgr_event) < 0)
+	    break;
+
+	/* we are the only reader => no locking required */
+	while ((rtskb = __rtskb_fifo_remove(&rx.fifo)))
+	    rt_stack_deliver(rtskb);
+    }
+
+    do_exit(0);
+    return 0;
+}
+
+
+/***
+ *  rt_stack_connect
+ */
+void rt_stack_connect (struct rtnet_device *rtdev, struct rtnet_mgr *mgr)
+{
+    rtdev->stack_event = &mgr->event;
+}
+
+EXPORT_SYMBOL_GPL(rt_stack_connect);
+
+
+/***
+ *  rt_stack_disconnect
+ */
+void rt_stack_disconnect (struct rtnet_device *rtdev)
+{
+    rtdev->stack_event = NULL;
+}
+
+EXPORT_SYMBOL_GPL(rt_stack_disconnect);
+
+
+/***
+ *  rt_stack_mgr_init
+ */
+int rt_stack_mgr_init (struct rtnet_mgr *mgr)
+{
+    int i;
+    struct sched_param mgr_task_param;
+    mgr_task_param.sched_priority = stack_mgr_prio;
+
+    raw_spin_lock_init(&rt_packets_lock);
+    
+    rtskb_fifo_init(&rx.fifo, CONFIG_XENO_DRIVERS_NET_RX_FIFO_SIZE);
+
+    for (i = 0; i < RTPACKET_HASH_TBL_SIZE; i++)
+	INIT_LIST_HEAD(&rt_packets[i]);
+#ifdef CONFIG_XENO_DRIVERS_NET_ETH_P_ALL
+    INIT_LIST_HEAD(&rt_packets_all);
+#endif /* CONFIG_XENO_DRIVERS_NET_ETH_P_ALL */
+
+    rtdm_event_init(&mgr->event, 0);
+
+    stop_rt_stack_mgr_task = 0;
+    mgr->task = kthread_create(rt_stack_mgr_task, mgr, "rtnet-stack");
+    if (!mgr->task)
+            goto mgr_task_failed; 
+    sched_setscheduler(mgr->task, SCHED_FIFO, &mgr_task_param);
+    wake_up_process(mgr->task);
+    
+    return 0;
+
+mgr_task_failed:
+    rtdm_event_destroy(&mgr->event);
+    return -ENOMEM;
+}
+
+
+/***
+ *  rt_stack_mgr_delete
+ */
+void rt_stack_mgr_delete (struct rtnet_mgr *mgr)
+{
+    rtdm_event_destroy(&mgr->event);
+
+    stop_rt_stack_mgr_task = 1;
+    /* wait for the thread termination */
+    kthread_stop(mgr->task);
+    /* release the task structure */
+    put_task_struct(mgr->task);
+}
diff -Nur linux-5.4.5/net/sched/sch_api.c linux-5.4.5-new/net/sched/sch_api.c
--- linux-5.4.5/net/sched/sch_api.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/sched/sch_api.c	2020-06-15 16:12:31.123696801 +0300
@@ -1248,7 +1248,7 @@
 		rcu_assign_pointer(sch->stab, stab);
 	}
 	if (tca[TCA_RATE]) {
-		seqcount_t *running;
+		net_seqlock_t *running;
 
 		err = -EOPNOTSUPP;
 		if (sch->flags & TCQ_F_MQROOT) {
diff -Nur linux-5.4.5/net/sched/sch_generic.c linux-5.4.5-new/net/sched/sch_generic.c
--- linux-5.4.5/net/sched/sch_generic.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/sched/sch_generic.c	2020-06-15 16:12:31.123696801 +0300
@@ -557,7 +557,11 @@
 	.ops		=	&noop_qdisc_ops,
 	.q.lock		=	__SPIN_LOCK_UNLOCKED(noop_qdisc.q.lock),
 	.dev_queue	=	&noop_netdev_queue,
+#ifdef CONFIG_PREEMPT_RT
+	.running	=	__SEQLOCK_UNLOCKED(noop_qdisc.running),
+#else
 	.running	=	SEQCNT_ZERO(noop_qdisc.running),
+#endif
 	.busylock	=	__SPIN_LOCK_UNLOCKED(noop_qdisc.busylock),
 	.gso_skb = {
 		.next = (struct sk_buff *)&noop_qdisc.gso_skb,
@@ -853,7 +857,11 @@
 	spin_lock_init(&sch->busylock);
 	/* seqlock has the same scope of busylock, for NOLOCK qdisc */
 	spin_lock_init(&sch->seqlock);
+#ifdef CONFIG_PREEMPT_RT
+	seqlock_init(&sch->running);
+#else
 	seqcount_init(&sch->running);
+#endif
 
 	sch->ops = ops;
 	sch->flags = ops->static_flags;
@@ -867,7 +875,12 @@
 	if (sch != &noop_qdisc) {
 		lockdep_set_class(&sch->busylock, &dev->qdisc_tx_busylock_key);
 		lockdep_set_class(&sch->seqlock, &dev->qdisc_tx_busylock_key);
+#ifdef CONFIG_PREEMPT_RT
+		lockdep_set_class(&sch->running.seqcount, &dev->qdisc_running_key);
+		lockdep_set_class(&sch->running.lock, &dev->qdisc_running_key);
+#else
 		lockdep_set_class(&sch->running, &dev->qdisc_running_key);
+#endif
 	}
 
 	return sch;
@@ -1215,7 +1228,7 @@
 	/* Wait for outstanding qdisc_run calls. */
 	list_for_each_entry(dev, head, close_list) {
 		while (some_qdisc_is_busy(dev))
-			yield();
+			msleep(1);
 		/* The new qdisc is assigned at this point so we can safely
 		 * unwind stale skb lists and qdisc statistics
 		 */
diff -Nur linux-5.4.5/net/socket.c linux-5.4.5-new/net/socket.c
--- linux-5.4.5/net/socket.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/socket.c	2020-06-15 14:49:08.695974363 +0300
@@ -1,5 +1,9 @@
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*
+ * RTnet port to PREEMPT_RT,
+ * Copyright (C) 2020 Laurentiu-Crstian Duca, 
+ * laurentiu [dot] duca [at] gmail [dot] com
+ *
  * NET		An implementation of the SOCKET network access protocol.
  *
  * Version:	@(#)socket.c	1.1.93	18/02/95
@@ -104,6 +108,10 @@
 #include <net/busy_poll.h>
 #include <linux/errqueue.h>
 
+/* rtnet */
+#include <rtnet_socket.h>
+#include <ipv4/protocol.h>
+
 #ifdef CONFIG_NET_RX_BUSY_POLL
 unsigned int sysctl_net_busy_read __read_mostly;
 unsigned int sysctl_net_busy_poll __read_mostly;
@@ -114,9 +122,11 @@
 static int sock_mmap(struct file *file, struct vm_area_struct *vma);
 
 static int sock_close(struct inode *inode, struct file *file);
+static int sock_close_rtnet(struct inode *inode, struct file *filp);
 static __poll_t sock_poll(struct file *file,
 			      struct poll_table_struct *wait);
 static long sock_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static long sock_ioctl_rtnet(struct file *file, unsigned cmd, unsigned long arg);
 #ifdef CONFIG_COMPAT
 static long compat_sock_ioctl(struct file *file,
 			      unsigned int cmd, unsigned long arg);
@@ -151,6 +161,21 @@
 	.splice_read =	sock_splice_read,
 };
 
+static const struct file_operations socket_file_ops_rtnet = {
+	.owner =	THIS_MODULE,
+	.llseek =	no_llseek,
+	.read_iter =	NULL,
+	.write_iter =	NULL,
+	.poll =		NULL,
+	.unlocked_ioctl = sock_ioctl_rtnet,
+	.mmap =		NULL,
+	.release =	sock_close_rtnet,
+	.fasync =	NULL,
+	.sendpage =	NULL,
+	.splice_write = NULL,
+	.splice_read =	NULL,
+};
+
 /*
  *	The protocol list. Each protocol is registered in here.
  */
@@ -408,6 +433,29 @@
 }
 EXPORT_SYMBOL(sock_alloc_file);
 
+struct file *sock_alloc_file_rtnet(struct rtsocket *rtsock,
+								   struct socket *sock, int flags, const char *dname)
+{
+	struct file *file;
+
+	if (!dname)
+		dname = sock->sk ? sock->sk->sk_prot_creator->name : "";
+
+	file = alloc_file_pseudo(SOCK_INODE(sock), sock_mnt, dname,
+				O_RDWR | (flags & O_NONBLOCK),
+				&socket_file_ops_rtnet); /* rtnet */
+	if (IS_ERR(file)) {
+		sock_release(sock);
+		return file;
+	}
+
+	sock->file = file;
+	rtsock->file = file;
+	file->private_data = rtsock;
+	return file;
+}
+EXPORT_SYMBOL(sock_alloc_file_rtnet);
+
 static int sock_map_fd(struct socket *sock, int flags)
 {
 	struct file *newfile;
@@ -427,6 +475,25 @@
 	return PTR_ERR(newfile);
 }
 
+static int sock_map_fd_rtnet(struct rtsocket *rtsock, struct socket *sock, int flags)
+{
+	struct file *newfile;
+	int fd = get_unused_fd_flags(flags);
+	if (unlikely(fd < 0)) {
+		sock_release(sock);
+		return fd;
+	}
+
+	newfile = sock_alloc_file_rtnet(rtsock, sock, flags, NULL);
+	if (!IS_ERR(newfile)) {
+		fd_install(fd, newfile);
+		return fd;
+	}
+
+	put_unused_fd(fd);
+	return PTR_ERR(newfile);
+}
+
 /**
  *	sock_from_file - Return the &socket bounded to @file.
  *	@file: file
@@ -445,6 +512,16 @@
 }
 EXPORT_SYMBOL(sock_from_file);
 
+struct rtsocket *sock_from_file_rtnet(struct file *file, int *err)
+{
+	if (file->f_op == &socket_file_ops_rtnet)
+		return file->private_data;	/* set in sock_map_fd */
+
+	*err = -ENOTSOCK;
+	return NULL;
+}
+EXPORT_SYMBOL(sock_from_file_rtnet);
+
 /**
  *	sockfd_lookup - Go from a file number to its socket slot
  *	@fd: file handle
@@ -493,6 +570,23 @@
 	return NULL;
 }
 
+static struct rtsocket *sockfd_lookup_light_rtnet(int fd, int *err, int *fput_needed)
+{
+	struct fd f = fdget(fd);
+	struct rtsocket *rtsock;
+
+	*err = -EBADF;
+	if (f.file) {
+		rtsock = sock_from_file_rtnet(f.file, err);
+		if (likely(rtsock)) {
+			*fput_needed = f.flags;
+			return rtsock;
+		}
+		fdput(f);
+	}
+	return NULL;
+}
+
 static ssize_t sockfs_listxattr(struct dentry *dentry, char *buffer,
 				size_t size)
 {
@@ -955,7 +1049,7 @@
 			     .msg_iocb = iocb};
 	ssize_t res;
 
-	if (file->f_flags & O_NONBLOCK)
+	if (file->f_flags & O_NONBLOCK || (iocb->ki_flags & IOCB_NOWAIT))
 		msg.msg_flags = MSG_DONTWAIT;
 
 	if (iocb->ki_pos != 0)
@@ -980,7 +1074,7 @@
 	if (iocb->ki_pos != 0)
 		return -ESPIPE;
 
-	if (file->f_flags & O_NONBLOCK)
+	if (file->f_flags & O_NONBLOCK || (iocb->ki_flags & IOCB_NOWAIT))
 		msg.msg_flags = MSG_DONTWAIT;
 
 	if (sock->type == SOCK_SEQPACKET)
@@ -1192,6 +1286,15 @@
 	return err;
 }
 
+static long sock_ioctl_rtnet(struct file *file, unsigned cmd, unsigned long arg)
+{
+	struct rtsocket *rtsock;
+	void __user *argp = (void __user *)arg;
+
+	rtsock = file->private_data;
+	return rt_udp_ioctl(rtsock, cmd, argp);
+}
+
 /**
  *	sock_create_lite - creates a socket
  *	@family: protocol family (AF_INET, ...)
@@ -1269,6 +1372,29 @@
 	return 0;
 }
 
+static int sock_close_rtnet(struct inode *inode, struct file *filp)
+{
+	struct rtsocket *rtsock;
+	int err;
+	
+	if(!filp) {
+		return -EBADF;		
+	}
+	
+	rtsock = sock_from_file_rtnet(filp, &err);
+	if(!rtsock) {
+		return err;
+	}
+	
+	/* for the moment we support only udp */
+	rt_udp_close(rtsock);
+	kfree(rtsock);
+
+	/* free the linux socket */
+	__sock_release(SOCKET_I(inode), inode);
+	return 0;
+}
+
 /*
  *	Update the socket async list
  *
@@ -1520,6 +1646,56 @@
 	return __sys_socket(family, type, protocol);
 }
 
+int __sys_socket_rtnet(int family, int type, int protocol)
+{
+	int retval;
+	struct socket *sock;
+	int flags;
+	int fd;
+	struct rtsocket *rtsock;
+		
+	/* Check the SOCK_* constants for consistency.  */
+	BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);
+	BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK);
+	BUILD_BUG_ON(SOCK_CLOEXEC & SOCK_TYPE_MASK);
+	BUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);
+
+	flags = type & ~SOCK_TYPE_MASK;
+	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
+		return -EINVAL;
+	type &= SOCK_TYPE_MASK;
+
+	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
+		flags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;
+
+	retval = sock_create(family, type, protocol, &sock);
+	if (retval < 0)
+		return retval;
+
+	rtsock = kzalloc(sizeof(struct rtsocket), GFP_KERNEL);
+	if(!rtsock)
+		return -ENOMEM;
+
+	fd = sock_map_fd_rtnet(rtsock, sock, flags & (O_CLOEXEC | O_NONBLOCK));
+	if(fd < 0)
+		return fd;
+	rtsock->fd = fd;
+
+	retval = rt_inet_socket(rtsock, type, protocol);
+	if(retval < 0) {
+		fput(rtsock->file);
+		kfree(rtsock);
+		return retval;
+	}
+	
+	return fd;
+}
+
+SYSCALL_DEFINE3(socket_rtnet, int, family, int, type, int, protocol)
+{
+	return __sys_socket_rtnet(family, type, protocol);
+}
+
 /*
  *	Create a pair of connected sockets.
  */
@@ -1653,11 +1829,30 @@
 	return err;
 }
 
+int __sys_bind_rtnet(int fd, struct sockaddr __user *umyaddr, int addrlen)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (rtsock) {
+		err = rt_udp_bind(rtsock, umyaddr, addrlen);
+		fput_light(rtsock->file, fput_needed);
+	}
+	
+	return err;
+}
+
 SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
 {
 	return __sys_bind(fd, umyaddr, addrlen);
 }
 
+SYSCALL_DEFINE3(bind_rtnet, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
+{
+	return __sys_bind_rtnet(fd, umyaddr, addrlen);
+}
+
 /*
  *	Perform a listen. Basically, we allow the protocol to do anything
  *	necessary for a listen, and if that works, we mark the socket as
@@ -1957,6 +2152,51 @@
 	return err;
 }
 
+int __sys_sendto_rtnet(int fd, void __user *buff, size_t len, unsigned int flags,
+		 struct sockaddr __user *addr,  int addr_len)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+	struct user_msghdr msg;
+	struct iovec iov;
+
+	/* dirty way because iov_base is not in userspace */
+	iov.iov_base = kmalloc(len, GFP_ATOMIC);
+	if((err = copy_from_user(iov.iov_base, buff, len)))
+		goto out;
+	iov.iov_len = len;
+	msg.msg_iov = &iov;
+	msg.msg_iovlen = 1;
+	
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	msg.msg_name = NULL;
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	msg.msg_namelen = 0;
+	if (addr) {
+		msg.msg_name = kmalloc(addr_len, GFP_ATOMIC);
+		if(!msg.msg_name)
+			return -ENOMEM;
+		if((err = copy_from_user(msg.msg_name, addr, addr_len)))
+			goto out;
+		msg.msg_namelen = addr_len;
+	}
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	msg.msg_flags = flags;
+	err = rt_udp_sendmsg(rtsock, &msg, msg.msg_flags, 0);
+
+	fput_light(rtsock->file, fput_needed);
+
+	kfree(msg.msg_name);
+out:
+	kfree(iov.iov_base);
+	return err;
+}
+
 SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
 		unsigned int, flags, struct sockaddr __user *, addr,
 		int, addr_len)
@@ -1964,6 +2204,13 @@
 	return __sys_sendto(fd, buff, len, flags, addr, addr_len);
 }
 
+SYSCALL_DEFINE6(sendto_rtnet, int, fd, void __user *, buff, size_t, len,
+		unsigned int, flags, struct sockaddr __user *, addr,
+		int, addr_len)
+{
+	return __sys_sendto_rtnet(fd, buff, len, flags, addr, addr_len);
+}
+
 /*
  *	Send a datagram down a socket.
  */
@@ -2020,6 +2267,60 @@
 	return err;
 }
 
+int __sys_recvfrom_rtnet(int fd, void __user *ubuf, size_t size, unsigned int flags,
+		   struct sockaddr __user *addr, int __user *addr_len)
+{
+	struct rtsocket *rtsock;
+	struct iovec iov;
+	struct user_msghdr msg;
+	struct sockaddr_storage address;
+	int err, err2;
+	int fput_needed;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	iov.iov_base = kmalloc(size, GFP_ATOMIC);
+	if(!iov.iov_base)
+		return -ENOMEM;
+	iov.iov_len = size;
+	msg.msg_iov = &iov;
+	msg.msg_iovlen = 1;
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	/* Save some cycles and don't copy the address if not needed */
+	msg.msg_name = addr ? (struct sockaddr *)&address : NULL;
+	/* We assume all kernel code knows the size of sockaddr_storage */
+	msg.msg_namelen = sizeof(struct sockaddr);
+	msg.msg_flags = 0;
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	err = rt_udp_recvmsg(rtsock, &msg, flags, 0);
+	if(err < 0)
+		goto out;
+	else if (addr != NULL) {
+		err2 = move_addr_to_user(&address,
+					 msg.msg_namelen, addr, addr_len);
+		if (err2 < 0) {
+			err = err2;
+			goto out;
+		}
+	}
+
+	if(size > err)
+		size = err;
+	err = copy_to_user(ubuf, msg.msg_iov->iov_base, size);
+	if(!err)
+		err = size;
+
+	fput_light(rtsock->file, fput_needed);
+
+out:
+	kfree(iov.iov_base);
+	return err;
+}
+
 SYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,
 		unsigned int, flags, struct sockaddr __user *, addr,
 		int __user *, addr_len)
@@ -2027,6 +2328,13 @@
 	return __sys_recvfrom(fd, ubuf, size, flags, addr, addr_len);
 }
 
+SYSCALL_DEFINE6(recvfrom_rtnet, int, fd, void __user *, ubuf, size_t, size,
+		unsigned int, flags, struct sockaddr __user *, addr,
+		int __user *, addr_len)
+{
+	return __sys_recvfrom_rtnet(fd, ubuf, size, flags, addr, addr_len);
+}
+
 /*
  *	Receive a datagram from a socket.
  */
@@ -2401,11 +2709,36 @@
 	return err;
 }
 
+long __sys_sendmsg_rtnet(int fd, struct user_msghdr __user *msg, unsigned int flags,
+		   bool forbid_cmsg_compat)
+{
+	int fput_needed, err;
+	struct rtsocket *rtsock;
+
+	if (forbid_cmsg_compat && (flags & MSG_CMSG_COMPAT))
+		return -EINVAL;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	err = rt_udp_sendmsg(rtsock, msg, flags, 1);
+
+	fput_light(rtsock->file, fput_needed);
+out:
+	return err;
+}
+
 SYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)
 {
 	return __sys_sendmsg(fd, msg, flags, true);
 }
 
+SYSCALL_DEFINE3(sendmsg_rtnet, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)
+{
+	return __sys_sendmsg_rtnet(fd, msg, flags, true);
+}
+
 /*
  *	Linux sendmmsg interface
  */
@@ -2621,12 +2954,37 @@
 	return err;
 }
 
+long __sys_recvmsg_rtnet(int fd, struct user_msghdr __user *msg, unsigned int flags,
+		   bool forbid_cmsg_compat)
+{
+	struct rtsocket *rtsock;
+	int err, fput_needed;
+
+	rtsock = sockfd_lookup_light_rtnet(fd, &err, &fput_needed);
+	if (!rtsock)
+		goto out;
+
+	if (rtsock->file->f_flags & O_NONBLOCK)
+		flags |= MSG_DONTWAIT;
+	err = rt_udp_recvmsg(rtsock, msg, flags, 1);
+
+	fput_light(rtsock->file, fput_needed);
+out:
+	return err;
+}
+
 SYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,
 		unsigned int, flags)
 {
 	return __sys_recvmsg(fd, msg, flags, true);
 }
 
+SYSCALL_DEFINE3(recvmsg_rtnet, int, fd, struct user_msghdr __user *, msg,
+		unsigned int, flags)
+{
+	return __sys_recvmsg_rtnet(fd, msg, flags, true);
+}
+
 /*
  *     Linux recvmmsg interface
  */
@@ -3532,6 +3890,7 @@
 	case SIOCSARP:
 	case SIOCGARP:
 	case SIOCDARP:
+	case SIOCOUTQNSD:
 	case SIOCATMARK:
 		return sock_do_ioctl(net, sock, cmd, arg);
 	}
diff -Nur linux-5.4.5/net/sunrpc/svc_xprt.c linux-5.4.5-new/net/sunrpc/svc_xprt.c
--- linux-5.4.5/net/sunrpc/svc_xprt.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/net/sunrpc/svc_xprt.c	2020-06-15 16:12:31.215696477 +0300
@@ -411,7 +411,7 @@
 	if (test_and_set_bit(XPT_BUSY, &xprt->xpt_flags))
 		return;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = svc_pool_for_cpu(xprt->xpt_server, cpu);
 
 	atomic_long_inc(&pool->sp_stats.packets);
@@ -435,7 +435,7 @@
 	rqstp = NULL;
 out_unlock:
 	rcu_read_unlock();
-	put_cpu();
+	put_cpu_light();
 	trace_svc_xprt_do_enqueue(xprt, rqstp);
 }
 EXPORT_SYMBOL_GPL(svc_xprt_do_enqueue);
diff -Nur linux-5.4.5/security/apparmor/include/path.h linux-5.4.5-new/security/apparmor/include/path.h
--- linux-5.4.5/security/apparmor/include/path.h	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/security/apparmor/include/path.h	2020-06-15 16:12:31.787694461 +0300
@@ -36,8 +36,10 @@
 
 #include <linux/percpu.h>
 #include <linux/preempt.h>
+#include <linux/locallock.h>
 
 DECLARE_PER_CPU(struct aa_buffers, aa_buffers);
+DECLARE_LOCAL_IRQ_LOCK(aa_buffers_lock);
 
 #define ASSIGN(FN, A, X, N) ((X) = FN(A, N))
 #define EVAL1(FN, A, X) ASSIGN(FN, A, X, 0) /*X = FN(0)*/
@@ -47,7 +49,17 @@
 
 #define for_each_cpu_buffer(I) for ((I) = 0; (I) < MAX_PATH_BUFFERS; (I)++)
 
-#ifdef CONFIG_DEBUG_PREEMPT
+#ifdef CONFIG_PREEMPT_RT
+static inline void AA_BUG_PREEMPT_ENABLED(const char *s)
+{
+	struct local_irq_lock *lv;
+
+	lv = this_cpu_ptr(&aa_buffers_lock);
+	WARN_ONCE(lv->owner != current,
+		  "__get_buffer without aa_buffers_lock\n");
+}
+
+#elif defined(CONFIG_DEBUG_PREEMPT)
 #define AA_BUG_PREEMPT_ENABLED(X) AA_BUG(preempt_count() <= 0, X)
 #else
 #define AA_BUG_PREEMPT_ENABLED(X) /* nop */
@@ -63,14 +75,15 @@
 
 #define get_buffers(X...)						\
 do {									\
-	struct aa_buffers *__cpu_var = get_cpu_ptr(&aa_buffers);	\
+	struct aa_buffers *__cpu_var;					\
+	__cpu_var = get_locked_ptr(aa_buffers_lock, &aa_buffers);	\
 	__get_buffers(__cpu_var, X);					\
 } while (0)
 
 #define put_buffers(X, Y...)		\
 do {					\
 	__put_buffers(X, Y);		\
-	put_cpu_ptr(&aa_buffers);	\
+	put_locked_ptr(aa_buffers_lock, &aa_buffers);	\
 } while (0)
 
 #endif /* __AA_PATH_H */
diff -Nur linux-5.4.5/security/apparmor/lsm.c linux-5.4.5-new/security/apparmor/lsm.c
--- linux-5.4.5/security/apparmor/lsm.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/security/apparmor/lsm.c	2020-06-15 16:12:31.791694447 +0300
@@ -44,7 +44,7 @@
 int apparmor_initialized;
 
 DEFINE_PER_CPU(struct aa_buffers, aa_buffers);
-
+DEFINE_LOCAL_IRQ_LOCK(aa_buffers_lock);
 
 /*
  * LSM hook functions
diff -Nur linux-5.4.5/tools/objtool/arch/x86/lib/inat-tables.c linux-5.4.5-new/tools/objtool/arch/x86/lib/inat-tables.c
--- linux-5.4.5/tools/objtool/arch/x86/lib/inat-tables.c	1970-01-01 02:00:00.000000000 +0200
+++ linux-5.4.5-new/tools/objtool/arch/x86/lib/inat-tables.c	2020-06-15 16:12:32.919690470 +0300
@@ -0,0 +1,1393 @@
+/* x86 opcode map generated from x86-opcode-map.txt */
+/* Do not change this code. */
+
+/* Table: one byte opcode */
+const insn_attr_t inat_primary_table[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM,
+	[0x01] = INAT_MODRM,
+	[0x02] = INAT_MODRM,
+	[0x03] = INAT_MODRM,
+	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x05] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x08] = INAT_MODRM,
+	[0x09] = INAT_MODRM,
+	[0x0a] = INAT_MODRM,
+	[0x0b] = INAT_MODRM,
+	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x0d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x0f] = INAT_MAKE_ESCAPE(1),
+	[0x10] = INAT_MODRM,
+	[0x11] = INAT_MODRM,
+	[0x12] = INAT_MODRM,
+	[0x13] = INAT_MODRM,
+	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x15] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x18] = INAT_MODRM,
+	[0x19] = INAT_MODRM,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x1c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x1d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x20] = INAT_MODRM,
+	[0x21] = INAT_MODRM,
+	[0x22] = INAT_MODRM,
+	[0x23] = INAT_MODRM,
+	[0x24] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x25] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x26] = INAT_MAKE_PREFIX(INAT_PFX_ES),
+	[0x28] = INAT_MODRM,
+	[0x29] = INAT_MODRM,
+	[0x2a] = INAT_MODRM,
+	[0x2b] = INAT_MODRM,
+	[0x2c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x2d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x2e] = INAT_MAKE_PREFIX(INAT_PFX_CS),
+	[0x30] = INAT_MODRM,
+	[0x31] = INAT_MODRM,
+	[0x32] = INAT_MODRM,
+	[0x33] = INAT_MODRM,
+	[0x34] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x35] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x36] = INAT_MAKE_PREFIX(INAT_PFX_SS),
+	[0x38] = INAT_MODRM,
+	[0x39] = INAT_MODRM,
+	[0x3a] = INAT_MODRM,
+	[0x3b] = INAT_MODRM,
+	[0x3c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x3d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x3e] = INAT_MAKE_PREFIX(INAT_PFX_DS),
+	[0x40] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x41] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x42] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x43] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x44] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x45] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x46] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x47] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x48] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x49] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4a] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4b] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4c] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4d] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4e] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4f] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x50] = INAT_FORCE64,
+	[0x51] = INAT_FORCE64,
+	[0x52] = INAT_FORCE64,
+	[0x53] = INAT_FORCE64,
+	[0x54] = INAT_FORCE64,
+	[0x55] = INAT_FORCE64,
+	[0x56] = INAT_FORCE64,
+	[0x57] = INAT_FORCE64,
+	[0x58] = INAT_FORCE64,
+	[0x59] = INAT_FORCE64,
+	[0x5a] = INAT_FORCE64,
+	[0x5b] = INAT_FORCE64,
+	[0x5c] = INAT_FORCE64,
+	[0x5d] = INAT_FORCE64,
+	[0x5e] = INAT_FORCE64,
+	[0x5f] = INAT_FORCE64,
+	[0x62] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_EVEX),
+	[0x63] = INAT_MODRM | INAT_MODRM,
+	[0x64] = INAT_MAKE_PREFIX(INAT_PFX_FS),
+	[0x65] = INAT_MAKE_PREFIX(INAT_PFX_GS),
+	[0x66] = INAT_MAKE_PREFIX(INAT_PFX_OPNDSZ),
+	[0x67] = INAT_MAKE_PREFIX(INAT_PFX_ADDRSZ),
+	[0x68] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x69] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x6a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0x6b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x71] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x72] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x73] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x74] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x75] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x76] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x77] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x78] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x79] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7a] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7b] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7d] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7e] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7f] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x80] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x82] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x83] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x84] = INAT_MODRM,
+	[0x85] = INAT_MODRM,
+	[0x86] = INAT_MODRM,
+	[0x87] = INAT_MODRM,
+	[0x88] = INAT_MODRM,
+	[0x89] = INAT_MODRM,
+	[0x8a] = INAT_MODRM,
+	[0x8b] = INAT_MODRM,
+	[0x8c] = INAT_MODRM,
+	[0x8d] = INAT_MODRM,
+	[0x8e] = INAT_MODRM,
+	[0x8f] = INAT_MAKE_GROUP(2) | INAT_MODRM | INAT_FORCE64,
+	[0x9a] = INAT_MAKE_IMM(INAT_IMM_PTR),
+	[0x9c] = INAT_FORCE64,
+	[0x9d] = INAT_FORCE64,
+	[0xa0] = INAT_MOFFSET,
+	[0xa1] = INAT_MOFFSET,
+	[0xa2] = INAT_MOFFSET,
+	[0xa3] = INAT_MOFFSET,
+	[0xa8] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xa9] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0xb0] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb1] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb2] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb3] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb6] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb8] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xb9] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xba] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbb] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbc] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbd] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbe] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbf] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xc0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xc1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_FORCE64,
+	[0xc4] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX3),
+	[0xc5] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX2),
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(4),
+	[0xc7] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(5),
+	[0xc8] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_SCNDIMM,
+	[0xc9] = INAT_FORCE64,
+	[0xca] = INAT_MAKE_IMM(INAT_IMM_WORD),
+	[0xcd] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd0] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd1] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd2] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd3] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd8] = INAT_MODRM,
+	[0xd9] = INAT_MODRM,
+	[0xda] = INAT_MODRM,
+	[0xdb] = INAT_MODRM,
+	[0xdc] = INAT_MODRM,
+	[0xdd] = INAT_MODRM,
+	[0xde] = INAT_MODRM,
+	[0xdf] = INAT_MODRM,
+	[0xe0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe6] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe8] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0xe9] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0xea] = INAT_MAKE_IMM(INAT_IMM_PTR),
+	[0xeb] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xf0] = INAT_MAKE_PREFIX(INAT_PFX_LOCK),
+	[0xf2] = INAT_MAKE_PREFIX(INAT_PFX_REPNE) | INAT_MAKE_PREFIX(INAT_PFX_REPNE),
+	[0xf3] = INAT_MAKE_PREFIX(INAT_PFX_REPE) | INAT_MAKE_PREFIX(INAT_PFX_REPE),
+	[0xf6] = INAT_MODRM | INAT_MAKE_GROUP(6),
+	[0xf7] = INAT_MODRM | INAT_MAKE_GROUP(7),
+	[0xfe] = INAT_MAKE_GROUP(8),
+	[0xff] = INAT_MAKE_GROUP(9),
+};
+
+/* Table: 2-byte opcode (0x0f) */
+const insn_attr_t inat_escape_table_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MAKE_GROUP(10),
+	[0x01] = INAT_MAKE_GROUP(11),
+	[0x02] = INAT_MODRM,
+	[0x03] = INAT_MODRM,
+	[0x0d] = INAT_MAKE_GROUP(12),
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x17] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x18] = INAT_MAKE_GROUP(13),
+	[0x1a] = INAT_MODRM | INAT_VARIANT,
+	[0x1b] = INAT_MODRM | INAT_VARIANT,
+	[0x1f] = INAT_MODRM,
+	[0x20] = INAT_MODRM,
+	[0x21] = INAT_MODRM,
+	[0x22] = INAT_MODRM,
+	[0x23] = INAT_MODRM,
+	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2a] = INAT_MODRM | INAT_VARIANT,
+	[0x2b] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2c] = INAT_MODRM | INAT_VARIANT,
+	[0x2d] = INAT_MODRM | INAT_VARIANT,
+	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x38] = INAT_MAKE_ESCAPE(2),
+	[0x3a] = INAT_MAKE_ESCAPE(3),
+	[0x40] = INAT_MODRM,
+	[0x41] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x42] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x43] = INAT_MODRM,
+	[0x44] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x45] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x46] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x47] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x48] = INAT_MODRM,
+	[0x49] = INAT_MODRM,
+	[0x4a] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x4b] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x4c] = INAT_MODRM,
+	[0x4d] = INAT_MODRM,
+	[0x4e] = INAT_MODRM,
+	[0x4f] = INAT_MODRM,
+	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x52] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x53] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x54] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x55] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x56] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x57] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5c] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5d] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x60] = INAT_MODRM | INAT_VARIANT,
+	[0x61] = INAT_MODRM | INAT_VARIANT,
+	[0x62] = INAT_MODRM | INAT_VARIANT,
+	[0x63] = INAT_MODRM | INAT_VARIANT,
+	[0x64] = INAT_MODRM | INAT_VARIANT,
+	[0x65] = INAT_MODRM | INAT_VARIANT,
+	[0x66] = INAT_MODRM | INAT_VARIANT,
+	[0x67] = INAT_MODRM | INAT_VARIANT,
+	[0x68] = INAT_MODRM | INAT_VARIANT,
+	[0x69] = INAT_MODRM | INAT_VARIANT,
+	[0x6a] = INAT_MODRM | INAT_VARIANT,
+	[0x6b] = INAT_MODRM | INAT_VARIANT,
+	[0x6c] = INAT_VARIANT,
+	[0x6d] = INAT_VARIANT,
+	[0x6e] = INAT_MODRM | INAT_VARIANT,
+	[0x6f] = INAT_MODRM | INAT_VARIANT,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x71] = INAT_MAKE_GROUP(14),
+	[0x72] = INAT_MAKE_GROUP(15),
+	[0x73] = INAT_MAKE_GROUP(16),
+	[0x74] = INAT_MODRM | INAT_VARIANT,
+	[0x75] = INAT_MODRM | INAT_VARIANT,
+	[0x76] = INAT_MODRM | INAT_VARIANT,
+	[0x77] = INAT_VEXOK | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x79] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x7a] = INAT_VARIANT,
+	[0x7b] = INAT_VARIANT,
+	[0x7c] = INAT_VARIANT,
+	[0x7d] = INAT_VARIANT,
+	[0x7e] = INAT_MODRM | INAT_VARIANT,
+	[0x7f] = INAT_MODRM | INAT_VARIANT,
+	[0x80] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x82] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x83] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x84] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x85] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x86] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x87] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x88] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x89] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8a] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8b] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8c] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8d] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8e] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8f] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x90] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x91] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x92] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x93] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x94] = INAT_MODRM,
+	[0x95] = INAT_MODRM,
+	[0x96] = INAT_MODRM,
+	[0x97] = INAT_MODRM,
+	[0x98] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x99] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x9a] = INAT_MODRM,
+	[0x9b] = INAT_MODRM,
+	[0x9c] = INAT_MODRM,
+	[0x9d] = INAT_MODRM,
+	[0x9e] = INAT_MODRM,
+	[0x9f] = INAT_MODRM,
+	[0xa0] = INAT_FORCE64,
+	[0xa1] = INAT_FORCE64,
+	[0xa3] = INAT_MODRM,
+	[0xa4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xa5] = INAT_MODRM,
+	[0xa6] = INAT_MAKE_GROUP(17),
+	[0xa7] = INAT_MAKE_GROUP(18),
+	[0xa8] = INAT_FORCE64,
+	[0xa9] = INAT_FORCE64,
+	[0xab] = INAT_MODRM,
+	[0xac] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xad] = INAT_MODRM,
+	[0xae] = INAT_MAKE_GROUP(19),
+	[0xaf] = INAT_MODRM,
+	[0xb0] = INAT_MODRM,
+	[0xb1] = INAT_MODRM,
+	[0xb2] = INAT_MODRM,
+	[0xb3] = INAT_MODRM,
+	[0xb4] = INAT_MODRM,
+	[0xb5] = INAT_MODRM,
+	[0xb6] = INAT_MODRM,
+	[0xb7] = INAT_MODRM,
+	[0xb8] = INAT_VARIANT,
+	[0xb9] = INAT_MAKE_GROUP(20),
+	[0xba] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(21),
+	[0xbb] = INAT_MODRM,
+	[0xbc] = INAT_MODRM | INAT_VARIANT,
+	[0xbd] = INAT_MODRM | INAT_VARIANT,
+	[0xbe] = INAT_MODRM,
+	[0xbf] = INAT_MODRM,
+	[0xc0] = INAT_MODRM,
+	[0xc1] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0xc3] = INAT_MODRM,
+	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0xc7] = INAT_MAKE_GROUP(22),
+	[0xd0] = INAT_VARIANT,
+	[0xd1] = INAT_MODRM | INAT_VARIANT,
+	[0xd2] = INAT_MODRM | INAT_VARIANT,
+	[0xd3] = INAT_MODRM | INAT_VARIANT,
+	[0xd4] = INAT_MODRM | INAT_VARIANT,
+	[0xd5] = INAT_MODRM | INAT_VARIANT,
+	[0xd6] = INAT_VARIANT,
+	[0xd7] = INAT_MODRM | INAT_VARIANT,
+	[0xd8] = INAT_MODRM | INAT_VARIANT,
+	[0xd9] = INAT_MODRM | INAT_VARIANT,
+	[0xda] = INAT_MODRM | INAT_VARIANT,
+	[0xdb] = INAT_MODRM | INAT_VARIANT,
+	[0xdc] = INAT_MODRM | INAT_VARIANT,
+	[0xdd] = INAT_MODRM | INAT_VARIANT,
+	[0xde] = INAT_MODRM | INAT_VARIANT,
+	[0xdf] = INAT_MODRM | INAT_VARIANT,
+	[0xe0] = INAT_MODRM | INAT_VARIANT,
+	[0xe1] = INAT_MODRM | INAT_VARIANT,
+	[0xe2] = INAT_MODRM | INAT_VARIANT,
+	[0xe3] = INAT_MODRM | INAT_VARIANT,
+	[0xe4] = INAT_MODRM | INAT_VARIANT,
+	[0xe5] = INAT_MODRM | INAT_VARIANT,
+	[0xe6] = INAT_VARIANT,
+	[0xe7] = INAT_MODRM | INAT_VARIANT,
+	[0xe8] = INAT_MODRM | INAT_VARIANT,
+	[0xe9] = INAT_MODRM | INAT_VARIANT,
+	[0xea] = INAT_MODRM | INAT_VARIANT,
+	[0xeb] = INAT_MODRM | INAT_VARIANT,
+	[0xec] = INAT_MODRM | INAT_VARIANT,
+	[0xed] = INAT_MODRM | INAT_VARIANT,
+	[0xee] = INAT_MODRM | INAT_VARIANT,
+	[0xef] = INAT_MODRM | INAT_VARIANT,
+	[0xf0] = INAT_VARIANT,
+	[0xf1] = INAT_MODRM | INAT_VARIANT,
+	[0xf2] = INAT_MODRM | INAT_VARIANT,
+	[0xf3] = INAT_MODRM | INAT_VARIANT,
+	[0xf4] = INAT_MODRM | INAT_VARIANT,
+	[0xf5] = INAT_MODRM | INAT_VARIANT,
+	[0xf6] = INAT_MODRM | INAT_VARIANT,
+	[0xf7] = INAT_MODRM | INAT_VARIANT,
+	[0xf8] = INAT_MODRM | INAT_VARIANT,
+	[0xf9] = INAT_MODRM | INAT_VARIANT,
+	[0xfa] = INAT_MODRM | INAT_VARIANT,
+	[0xfb] = INAT_MODRM | INAT_VARIANT,
+	[0xfc] = INAT_MODRM | INAT_VARIANT,
+	[0xfd] = INAT_MODRM | INAT_VARIANT,
+	[0xfe] = INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_1_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x13] = INAT_MODRM | INAT_VEXOK,
+	[0x14] = INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x28] = INAT_MODRM | INAT_VEXOK,
+	[0x29] = INAT_MODRM | INAT_VEXOK,
+	[0x2a] = INAT_MODRM,
+	[0x2b] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM,
+	[0x2d] = INAT_MODRM,
+	[0x2e] = INAT_MODRM | INAT_VEXOK,
+	[0x2f] = INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MODRM | INAT_VEXOK,
+	[0x44] = INAT_MODRM | INAT_VEXOK,
+	[0x45] = INAT_MODRM | INAT_VEXOK,
+	[0x46] = INAT_MODRM | INAT_VEXOK,
+	[0x47] = INAT_MODRM | INAT_VEXOK,
+	[0x4a] = INAT_MODRM | INAT_VEXOK,
+	[0x4b] = INAT_MODRM | INAT_VEXOK,
+	[0x50] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x54] = INAT_MODRM | INAT_VEXOK,
+	[0x55] = INAT_MODRM | INAT_VEXOK,
+	[0x56] = INAT_MODRM | INAT_VEXOK,
+	[0x57] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x60] = INAT_MODRM | INAT_VEXOK,
+	[0x61] = INAT_MODRM | INAT_VEXOK,
+	[0x62] = INAT_MODRM | INAT_VEXOK,
+	[0x63] = INAT_MODRM | INAT_VEXOK,
+	[0x64] = INAT_MODRM | INAT_VEXOK,
+	[0x65] = INAT_MODRM | INAT_VEXOK,
+	[0x66] = INAT_MODRM | INAT_VEXOK,
+	[0x67] = INAT_MODRM | INAT_VEXOK,
+	[0x68] = INAT_MODRM | INAT_VEXOK,
+	[0x69] = INAT_MODRM | INAT_VEXOK,
+	[0x6a] = INAT_MODRM | INAT_VEXOK,
+	[0x6b] = INAT_MODRM | INAT_VEXOK,
+	[0x6c] = INAT_MODRM | INAT_VEXOK,
+	[0x6d] = INAT_MODRM | INAT_VEXOK,
+	[0x6e] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x74] = INAT_MODRM | INAT_VEXOK,
+	[0x75] = INAT_MODRM | INAT_VEXOK,
+	[0x76] = INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK,
+	[0x7d] = INAT_MODRM | INAT_VEXOK,
+	[0x7e] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x90] = INAT_MODRM | INAT_VEXOK,
+	[0x91] = INAT_MODRM | INAT_VEXOK,
+	[0x92] = INAT_MODRM | INAT_VEXOK,
+	[0x93] = INAT_MODRM | INAT_VEXOK,
+	[0x98] = INAT_MODRM | INAT_VEXOK,
+	[0x99] = INAT_MODRM | INAT_VEXOK,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd0] = INAT_MODRM | INAT_VEXOK,
+	[0xd1] = INAT_MODRM | INAT_VEXOK,
+	[0xd2] = INAT_MODRM | INAT_VEXOK,
+	[0xd3] = INAT_MODRM | INAT_VEXOK,
+	[0xd4] = INAT_MODRM | INAT_VEXOK,
+	[0xd5] = INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM | INAT_VEXOK,
+	[0xd7] = INAT_MODRM | INAT_VEXOK,
+	[0xd8] = INAT_MODRM | INAT_VEXOK,
+	[0xd9] = INAT_MODRM | INAT_VEXOK,
+	[0xda] = INAT_MODRM | INAT_VEXOK,
+	[0xdb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xdc] = INAT_MODRM | INAT_VEXOK,
+	[0xdd] = INAT_MODRM | INAT_VEXOK,
+	[0xde] = INAT_MODRM | INAT_VEXOK,
+	[0xdf] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xe0] = INAT_MODRM | INAT_VEXOK,
+	[0xe1] = INAT_MODRM | INAT_VEXOK,
+	[0xe2] = INAT_MODRM | INAT_VEXOK,
+	[0xe3] = INAT_MODRM | INAT_VEXOK,
+	[0xe4] = INAT_MODRM | INAT_VEXOK,
+	[0xe5] = INAT_MODRM | INAT_VEXOK,
+	[0xe6] = INAT_MODRM | INAT_VEXOK,
+	[0xe7] = INAT_MODRM | INAT_VEXOK,
+	[0xe8] = INAT_MODRM | INAT_VEXOK,
+	[0xe9] = INAT_MODRM | INAT_VEXOK,
+	[0xea] = INAT_MODRM | INAT_VEXOK,
+	[0xeb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xec] = INAT_MODRM | INAT_VEXOK,
+	[0xed] = INAT_MODRM | INAT_VEXOK,
+	[0xee] = INAT_MODRM | INAT_VEXOK,
+	[0xef] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xf1] = INAT_MODRM | INAT_VEXOK,
+	[0xf2] = INAT_MODRM | INAT_VEXOK,
+	[0xf3] = INAT_MODRM | INAT_VEXOK,
+	[0xf4] = INAT_MODRM | INAT_VEXOK,
+	[0xf5] = INAT_MODRM | INAT_VEXOK,
+	[0xf6] = INAT_MODRM | INAT_VEXOK,
+	[0xf7] = INAT_MODRM | INAT_VEXOK,
+	[0xf8] = INAT_MODRM | INAT_VEXOK,
+	[0xf9] = INAT_MODRM | INAT_VEXOK,
+	[0xfa] = INAT_MODRM | INAT_VEXOK,
+	[0xfb] = INAT_MODRM | INAT_VEXOK,
+	[0xfc] = INAT_MODRM | INAT_VEXOK,
+	[0xfd] = INAT_MODRM | INAT_VEXOK,
+	[0xfe] = INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_1_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x52] = INAT_MODRM | INAT_VEXOK,
+	[0x53] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7e] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xb8] = INAT_MODRM,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM,
+	[0xe6] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_1_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK,
+	[0x7d] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x92] = INAT_MODRM | INAT_VEXOK,
+	[0x93] = INAT_MODRM | INAT_VEXOK,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd0] = INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM,
+	[0xe6] = INAT_MODRM | INAT_VEXOK,
+	[0xf0] = INAT_MODRM | INAT_VEXOK,
+};
+
+/* Table: 3-byte opcode 1 (0x0f 0x38) */
+const insn_attr_t inat_escape_table_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM | INAT_VARIANT,
+	[0x01] = INAT_MODRM | INAT_VARIANT,
+	[0x02] = INAT_MODRM | INAT_VARIANT,
+	[0x03] = INAT_MODRM | INAT_VARIANT,
+	[0x04] = INAT_MODRM | INAT_VARIANT,
+	[0x05] = INAT_MODRM | INAT_VARIANT,
+	[0x06] = INAT_MODRM | INAT_VARIANT,
+	[0x07] = INAT_MODRM | INAT_VARIANT,
+	[0x08] = INAT_MODRM | INAT_VARIANT,
+	[0x09] = INAT_MODRM | INAT_VARIANT,
+	[0x0a] = INAT_MODRM | INAT_VARIANT,
+	[0x0b] = INAT_MODRM | INAT_VARIANT,
+	[0x0c] = INAT_VARIANT,
+	[0x0d] = INAT_VARIANT,
+	[0x0e] = INAT_VARIANT,
+	[0x0f] = INAT_VARIANT,
+	[0x10] = INAT_VARIANT,
+	[0x11] = INAT_VARIANT,
+	[0x12] = INAT_VARIANT,
+	[0x13] = INAT_VARIANT,
+	[0x14] = INAT_VARIANT,
+	[0x15] = INAT_VARIANT,
+	[0x16] = INAT_VARIANT,
+	[0x17] = INAT_VARIANT,
+	[0x18] = INAT_VARIANT,
+	[0x19] = INAT_VARIANT,
+	[0x1a] = INAT_VARIANT,
+	[0x1b] = INAT_VARIANT,
+	[0x1c] = INAT_MODRM | INAT_VARIANT,
+	[0x1d] = INAT_MODRM | INAT_VARIANT,
+	[0x1e] = INAT_MODRM | INAT_VARIANT,
+	[0x1f] = INAT_VARIANT,
+	[0x20] = INAT_VARIANT,
+	[0x21] = INAT_VARIANT,
+	[0x22] = INAT_VARIANT,
+	[0x23] = INAT_VARIANT,
+	[0x24] = INAT_VARIANT,
+	[0x25] = INAT_VARIANT,
+	[0x26] = INAT_VARIANT,
+	[0x27] = INAT_VARIANT,
+	[0x28] = INAT_VARIANT,
+	[0x29] = INAT_VARIANT,
+	[0x2a] = INAT_VARIANT,
+	[0x2b] = INAT_VARIANT,
+	[0x2c] = INAT_VARIANT,
+	[0x2d] = INAT_VARIANT,
+	[0x2e] = INAT_VARIANT,
+	[0x2f] = INAT_VARIANT,
+	[0x30] = INAT_VARIANT,
+	[0x31] = INAT_VARIANT,
+	[0x32] = INAT_VARIANT,
+	[0x33] = INAT_VARIANT,
+	[0x34] = INAT_VARIANT,
+	[0x35] = INAT_VARIANT,
+	[0x36] = INAT_VARIANT,
+	[0x37] = INAT_VARIANT,
+	[0x38] = INAT_VARIANT,
+	[0x39] = INAT_VARIANT,
+	[0x3a] = INAT_VARIANT,
+	[0x3b] = INAT_VARIANT,
+	[0x3c] = INAT_VARIANT,
+	[0x3d] = INAT_VARIANT,
+	[0x3e] = INAT_VARIANT,
+	[0x3f] = INAT_VARIANT,
+	[0x40] = INAT_VARIANT,
+	[0x41] = INAT_VARIANT,
+	[0x42] = INAT_VARIANT,
+	[0x43] = INAT_VARIANT,
+	[0x44] = INAT_VARIANT,
+	[0x45] = INAT_VARIANT,
+	[0x46] = INAT_VARIANT,
+	[0x47] = INAT_VARIANT,
+	[0x4c] = INAT_VARIANT,
+	[0x4d] = INAT_VARIANT,
+	[0x4e] = INAT_VARIANT,
+	[0x4f] = INAT_VARIANT,
+	[0x58] = INAT_VARIANT,
+	[0x59] = INAT_VARIANT,
+	[0x5a] = INAT_VARIANT,
+	[0x5b] = INAT_VARIANT,
+	[0x64] = INAT_VARIANT,
+	[0x65] = INAT_VARIANT,
+	[0x66] = INAT_VARIANT,
+	[0x75] = INAT_VARIANT,
+	[0x76] = INAT_VARIANT,
+	[0x77] = INAT_VARIANT,
+	[0x78] = INAT_VARIANT,
+	[0x79] = INAT_VARIANT,
+	[0x7a] = INAT_VARIANT,
+	[0x7b] = INAT_VARIANT,
+	[0x7c] = INAT_VARIANT,
+	[0x7d] = INAT_VARIANT,
+	[0x7e] = INAT_VARIANT,
+	[0x7f] = INAT_VARIANT,
+	[0x80] = INAT_VARIANT,
+	[0x81] = INAT_VARIANT,
+	[0x82] = INAT_VARIANT,
+	[0x83] = INAT_VARIANT,
+	[0x88] = INAT_VARIANT,
+	[0x89] = INAT_VARIANT,
+	[0x8a] = INAT_VARIANT,
+	[0x8b] = INAT_VARIANT,
+	[0x8c] = INAT_VARIANT,
+	[0x8d] = INAT_VARIANT,
+	[0x8e] = INAT_VARIANT,
+	[0x90] = INAT_VARIANT,
+	[0x91] = INAT_VARIANT,
+	[0x92] = INAT_VARIANT,
+	[0x93] = INAT_VARIANT,
+	[0x96] = INAT_VARIANT,
+	[0x97] = INAT_VARIANT,
+	[0x98] = INAT_VARIANT,
+	[0x99] = INAT_VARIANT,
+	[0x9a] = INAT_VARIANT,
+	[0x9b] = INAT_VARIANT,
+	[0x9c] = INAT_VARIANT,
+	[0x9d] = INAT_VARIANT,
+	[0x9e] = INAT_VARIANT,
+	[0x9f] = INAT_VARIANT,
+	[0xa0] = INAT_VARIANT,
+	[0xa1] = INAT_VARIANT,
+	[0xa2] = INAT_VARIANT,
+	[0xa3] = INAT_VARIANT,
+	[0xa6] = INAT_VARIANT,
+	[0xa7] = INAT_VARIANT,
+	[0xa8] = INAT_VARIANT,
+	[0xa9] = INAT_VARIANT,
+	[0xaa] = INAT_VARIANT,
+	[0xab] = INAT_VARIANT,
+	[0xac] = INAT_VARIANT,
+	[0xad] = INAT_VARIANT,
+	[0xae] = INAT_VARIANT,
+	[0xaf] = INAT_VARIANT,
+	[0xb4] = INAT_VARIANT,
+	[0xb5] = INAT_VARIANT,
+	[0xb6] = INAT_VARIANT,
+	[0xb7] = INAT_VARIANT,
+	[0xb8] = INAT_VARIANT,
+	[0xb9] = INAT_VARIANT,
+	[0xba] = INAT_VARIANT,
+	[0xbb] = INAT_VARIANT,
+	[0xbc] = INAT_VARIANT,
+	[0xbd] = INAT_VARIANT,
+	[0xbe] = INAT_VARIANT,
+	[0xbf] = INAT_VARIANT,
+	[0xc4] = INAT_VARIANT,
+	[0xc6] = INAT_MAKE_GROUP(23),
+	[0xc7] = INAT_MAKE_GROUP(24),
+	[0xc8] = INAT_MODRM | INAT_VARIANT,
+	[0xc9] = INAT_MODRM,
+	[0xca] = INAT_MODRM | INAT_VARIANT,
+	[0xcb] = INAT_MODRM | INAT_VARIANT,
+	[0xcc] = INAT_MODRM | INAT_VARIANT,
+	[0xcd] = INAT_MODRM | INAT_VARIANT,
+	[0xdb] = INAT_VARIANT,
+	[0xdc] = INAT_VARIANT,
+	[0xdd] = INAT_VARIANT,
+	[0xde] = INAT_VARIANT,
+	[0xdf] = INAT_VARIANT,
+	[0xf0] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0xf1] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0xf2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf3] = INAT_MAKE_GROUP(25),
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
+	[0xf6] = INAT_VARIANT,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_2_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM | INAT_VEXOK,
+	[0x01] = INAT_MODRM | INAT_VEXOK,
+	[0x02] = INAT_MODRM | INAT_VEXOK,
+	[0x03] = INAT_MODRM | INAT_VEXOK,
+	[0x04] = INAT_MODRM | INAT_VEXOK,
+	[0x05] = INAT_MODRM | INAT_VEXOK,
+	[0x06] = INAT_MODRM | INAT_VEXOK,
+	[0x07] = INAT_MODRM | INAT_VEXOK,
+	[0x08] = INAT_MODRM | INAT_VEXOK,
+	[0x09] = INAT_MODRM | INAT_VEXOK,
+	[0x0a] = INAT_MODRM | INAT_VEXOK,
+	[0x0b] = INAT_MODRM | INAT_VEXOK,
+	[0x0c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x10] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x14] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MODRM | INAT_VEXOK,
+	[0x18] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x19] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x1b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1c] = INAT_MODRM | INAT_VEXOK,
+	[0x1d] = INAT_MODRM | INAT_VEXOK,
+	[0x1e] = INAT_MODRM | INAT_VEXOK,
+	[0x1f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MODRM | INAT_VEXOK,
+	[0x21] = INAT_MODRM | INAT_VEXOK,
+	[0x22] = INAT_MODRM | INAT_VEXOK,
+	[0x23] = INAT_MODRM | INAT_VEXOK,
+	[0x24] = INAT_MODRM | INAT_VEXOK,
+	[0x25] = INAT_MODRM | INAT_VEXOK,
+	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x28] = INAT_MODRM | INAT_VEXOK,
+	[0x29] = INAT_MODRM | INAT_VEXOK,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2b] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x30] = INAT_MODRM | INAT_VEXOK,
+	[0x31] = INAT_MODRM | INAT_VEXOK,
+	[0x32] = INAT_MODRM | INAT_VEXOK,
+	[0x33] = INAT_MODRM | INAT_VEXOK,
+	[0x34] = INAT_MODRM | INAT_VEXOK,
+	[0x35] = INAT_MODRM | INAT_VEXOK,
+	[0x36] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x37] = INAT_MODRM | INAT_VEXOK,
+	[0x38] = INAT_MODRM | INAT_VEXOK,
+	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3a] = INAT_MODRM | INAT_VEXOK,
+	[0x3b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3c] = INAT_MODRM | INAT_VEXOK,
+	[0x3d] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3e] = INAT_MODRM | INAT_VEXOK,
+	[0x3f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x64] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x65] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x75] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x76] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x77] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x80] = INAT_MODRM,
+	[0x81] = INAT_MODRM,
+	[0x82] = INAT_MODRM,
+	[0x83] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x88] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x89] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x8d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x90] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x91] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x92] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x93] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x96] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x97] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x98] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x99] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9b] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa0] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa3] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xaa] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xab] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xac] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xae] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xb5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xb6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xba] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbb] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbc] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbd] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbe] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xc4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xc8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xca] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcb] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcc] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcd] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xdb] = INAT_MODRM | INAT_VEXOK,
+	[0xdc] = INAT_MODRM | INAT_VEXOK,
+	[0xdd] = INAT_MODRM | INAT_VEXOK,
+	[0xde] = INAT_MODRM | INAT_VEXOK,
+	[0xdf] = INAT_MODRM | INAT_VEXOK,
+	[0xf0] = INAT_MODRM,
+	[0xf1] = INAT_MODRM,
+	[0xf6] = INAT_MODRM,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+const insn_attr_t inat_escape_table_2_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x21] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x22] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x23] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x24] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x25] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x30] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x31] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x32] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x33] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x34] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x35] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x38] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf6] = INAT_MODRM,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+const insn_attr_t inat_escape_table_2_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0xf0] = INAT_MODRM | INAT_MODRM,
+	[0xf1] = INAT_MODRM | INAT_MODRM,
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* Table: 3-byte opcode 2 (0x0f 0x3a) */
+const insn_attr_t inat_escape_table_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_VARIANT,
+	[0x01] = INAT_VARIANT,
+	[0x02] = INAT_VARIANT,
+	[0x03] = INAT_VARIANT,
+	[0x04] = INAT_VARIANT,
+	[0x05] = INAT_VARIANT,
+	[0x06] = INAT_VARIANT,
+	[0x08] = INAT_VARIANT,
+	[0x09] = INAT_VARIANT,
+	[0x0a] = INAT_VARIANT,
+	[0x0b] = INAT_VARIANT,
+	[0x0c] = INAT_VARIANT,
+	[0x0d] = INAT_VARIANT,
+	[0x0e] = INAT_VARIANT,
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x14] = INAT_VARIANT,
+	[0x15] = INAT_VARIANT,
+	[0x16] = INAT_VARIANT,
+	[0x17] = INAT_VARIANT,
+	[0x18] = INAT_VARIANT,
+	[0x19] = INAT_VARIANT,
+	[0x1a] = INAT_VARIANT,
+	[0x1b] = INAT_VARIANT,
+	[0x1d] = INAT_VARIANT,
+	[0x1e] = INAT_VARIANT,
+	[0x1f] = INAT_VARIANT,
+	[0x20] = INAT_VARIANT,
+	[0x21] = INAT_VARIANT,
+	[0x22] = INAT_VARIANT,
+	[0x23] = INAT_VARIANT,
+	[0x25] = INAT_VARIANT,
+	[0x26] = INAT_VARIANT,
+	[0x27] = INAT_VARIANT,
+	[0x30] = INAT_VARIANT,
+	[0x31] = INAT_VARIANT,
+	[0x32] = INAT_VARIANT,
+	[0x33] = INAT_VARIANT,
+	[0x38] = INAT_VARIANT,
+	[0x39] = INAT_VARIANT,
+	[0x3a] = INAT_VARIANT,
+	[0x3b] = INAT_VARIANT,
+	[0x3e] = INAT_VARIANT,
+	[0x3f] = INAT_VARIANT,
+	[0x40] = INAT_VARIANT,
+	[0x41] = INAT_VARIANT,
+	[0x42] = INAT_VARIANT,
+	[0x43] = INAT_VARIANT,
+	[0x44] = INAT_VARIANT,
+	[0x46] = INAT_VARIANT,
+	[0x4a] = INAT_VARIANT,
+	[0x4b] = INAT_VARIANT,
+	[0x4c] = INAT_VARIANT,
+	[0x50] = INAT_VARIANT,
+	[0x51] = INAT_VARIANT,
+	[0x54] = INAT_VARIANT,
+	[0x55] = INAT_VARIANT,
+	[0x56] = INAT_VARIANT,
+	[0x57] = INAT_VARIANT,
+	[0x60] = INAT_VARIANT,
+	[0x61] = INAT_VARIANT,
+	[0x62] = INAT_VARIANT,
+	[0x63] = INAT_VARIANT,
+	[0x66] = INAT_VARIANT,
+	[0x67] = INAT_VARIANT,
+	[0xcc] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xdf] = INAT_VARIANT,
+	[0xf0] = INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_3_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x01] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x02] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x03] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x05] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x06] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x08] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x09] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x18] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x19] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x1e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x21] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x22] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x23] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x25] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x26] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x30] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x31] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x32] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x33] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x38] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x39] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x3a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x40] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x43] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x44] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x46] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x50] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x51] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x54] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x55] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x56] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x57] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x60] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x61] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x62] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x63] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x66] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x67] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xdf] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_3_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0xf0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* GrpTable: Grp1 */
+
+/* GrpTable: Grp1A */
+
+/* GrpTable: Grp2 */
+
+/* GrpTable: Grp3_1 */
+const insn_attr_t inat_group_table_6[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp3_2 */
+const insn_attr_t inat_group_table_7[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp4 */
+const insn_attr_t inat_group_table_8[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+};
+
+/* GrpTable: Grp5 */
+const insn_attr_t inat_group_table_9[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM | INAT_FORCE64,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM | INAT_FORCE64,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM | INAT_FORCE64,
+};
+
+/* GrpTable: Grp6 */
+const insn_attr_t inat_group_table_10[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+};
+
+/* GrpTable: Grp7 */
+const insn_attr_t inat_group_table_11[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp8 */
+
+/* GrpTable: Grp9 */
+const insn_attr_t inat_group_table_22[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM,
+	[0x6] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0x7] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_22_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x6] = INAT_MODRM,
+};
+const insn_attr_t inat_group_table_22_2[INAT_GROUP_TABLE_SIZE] = {
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp10 */
+
+/* GrpTable: Grp11A */
+const insn_attr_t inat_group_table_4[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+};
+
+/* GrpTable: Grp11B */
+const insn_attr_t inat_group_table_5[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+};
+
+/* GrpTable: Grp12 */
+const insn_attr_t inat_group_table_14[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_14_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp13 */
+const insn_attr_t inat_group_table_15[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_VARIANT,
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_15_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp14 */
+const insn_attr_t inat_group_table_16[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x3] = INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x7] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_16_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp15 */
+const insn_attr_t inat_group_table_19[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_VARIANT,
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x4] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_19_2[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+};
+
+/* GrpTable: Grp16 */
+const insn_attr_t inat_group_table_13[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+};
+
+/* GrpTable: Grp17 */
+const insn_attr_t inat_group_table_25[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* GrpTable: Grp18 */
+const insn_attr_t inat_group_table_23[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_VARIANT,
+	[0x5] = INAT_VARIANT,
+	[0x6] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_23_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+};
+
+/* GrpTable: Grp19 */
+const insn_attr_t inat_group_table_24[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_VARIANT,
+	[0x5] = INAT_VARIANT,
+	[0x6] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_24_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+};
+
+/* GrpTable: GrpP */
+
+/* GrpTable: GrpPDLK */
+
+/* GrpTable: GrpRNG */
+
+/* Escape opcode map array */
+const insn_attr_t * const inat_escape_tables[INAT_ESC_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[1][0] = inat_escape_table_1,
+	[1][1] = inat_escape_table_1_1,
+	[1][2] = inat_escape_table_1_2,
+	[1][3] = inat_escape_table_1_3,
+	[2][0] = inat_escape_table_2,
+	[2][1] = inat_escape_table_2_1,
+	[2][2] = inat_escape_table_2_2,
+	[2][3] = inat_escape_table_2_3,
+	[3][0] = inat_escape_table_3,
+	[3][1] = inat_escape_table_3_1,
+	[3][3] = inat_escape_table_3_3,
+};
+
+/* Group opcode map array */
+const insn_attr_t * const inat_group_tables[INAT_GRP_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[4][0] = inat_group_table_4,
+	[5][0] = inat_group_table_5,
+	[6][0] = inat_group_table_6,
+	[7][0] = inat_group_table_7,
+	[8][0] = inat_group_table_8,
+	[9][0] = inat_group_table_9,
+	[10][0] = inat_group_table_10,
+	[11][0] = inat_group_table_11,
+	[13][0] = inat_group_table_13,
+	[14][0] = inat_group_table_14,
+	[14][1] = inat_group_table_14_1,
+	[15][0] = inat_group_table_15,
+	[15][1] = inat_group_table_15_1,
+	[16][0] = inat_group_table_16,
+	[16][1] = inat_group_table_16_1,
+	[19][0] = inat_group_table_19,
+	[19][2] = inat_group_table_19_2,
+	[22][0] = inat_group_table_22,
+	[22][1] = inat_group_table_22_1,
+	[22][2] = inat_group_table_22_2,
+	[23][0] = inat_group_table_23,
+	[23][1] = inat_group_table_23_1,
+	[24][0] = inat_group_table_24,
+	[24][1] = inat_group_table_24_1,
+	[25][0] = inat_group_table_25,
+};
+
+/* AVX opcode map array */
+const insn_attr_t * const inat_avx_tables[X86_VEX_M_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[1][0] = inat_escape_table_1,
+	[1][1] = inat_escape_table_1_1,
+	[1][2] = inat_escape_table_1_2,
+	[1][3] = inat_escape_table_1_3,
+	[2][0] = inat_escape_table_2,
+	[2][1] = inat_escape_table_2_1,
+	[2][2] = inat_escape_table_2_2,
+	[2][3] = inat_escape_table_2_3,
+	[3][0] = inat_escape_table_3,
+	[3][1] = inat_escape_table_3_1,
+	[3][3] = inat_escape_table_3_3,
+};
Binary files linux-5.4.5/tools/objtool/fixdep and linux-5.4.5-new/tools/objtool/fixdep differ
Binary files linux-5.4.5/tools/objtool/objtool and linux-5.4.5-new/tools/objtool/objtool differ
diff -Nur linux-5.4.5/virt/kvm/arm/arch_timer.c linux-5.4.5-new/virt/kvm/arm/arch_timer.c
--- linux-5.4.5/virt/kvm/arm/arch_timer.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/virt/kvm/arm/arch_timer.c	2020-06-15 16:12:33.767687481 +0300
@@ -80,7 +80,7 @@
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
-		      HRTIMER_MODE_ABS);
+		      HRTIMER_MODE_ABS_HARD);
 }
 
 static void soft_timer_cancel(struct hrtimer *hrt)
@@ -697,11 +697,11 @@
 	update_vtimer_cntvoff(vcpu, kvm_phys_timer_read());
 	ptimer->cntvoff = 0;
 
-	hrtimer_init(&timer->bg_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	hrtimer_init(&timer->bg_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
 	timer->bg_timer.function = kvm_bg_timer_expire;
 
-	hrtimer_init(&vtimer->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
-	hrtimer_init(&ptimer->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	hrtimer_init(&vtimer->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
+	hrtimer_init(&ptimer->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
 	vtimer->hrtimer.function = kvm_hrtimer_expire;
 	ptimer->hrtimer.function = kvm_hrtimer_expire;
 
diff -Nur linux-5.4.5/virt/kvm/arm/arm.c linux-5.4.5-new/virt/kvm/arm/arm.c
--- linux-5.4.5/virt/kvm/arm/arm.c	2019-12-18 17:09:17.000000000 +0200
+++ linux-5.4.5-new/virt/kvm/arm/arm.c	2020-06-15 16:12:33.767687481 +0300
@@ -700,7 +700,7 @@
 		 * involves poking the GIC, which must be done in a
 		 * non-preemptible context.
 		 */
-		preempt_disable();
+		migrate_disable();
 
 		kvm_pmu_flush_hwstate(vcpu);
 
@@ -749,7 +749,7 @@
 				kvm_timer_sync_hwstate(vcpu);
 			kvm_vgic_sync_hwstate(vcpu);
 			local_irq_enable();
-			preempt_enable();
+			migrate_enable();
 			continue;
 		}
 
@@ -827,7 +827,7 @@
 		/* Exit types that need handling before we can be preempted */
 		handle_exit_early(vcpu, run, ret);
 
-		preempt_enable();
+		migrate_enable();
 
 		ret = handle_exit(vcpu, run, ret);
 	}
